{"paragraph": "Organizations need to innovate in response to changing customer demands and opportunities offered by technology and changing marketplaces, structures and dynamics. Joshi, Chi, Datta, and Han examine the relationship between IT and firm innovation focusing on knowledge capabilities that are enhanced through the use of IT, and demonstrate that IT plays a significant role in enhancing firm innovation. The combination of Big Data and Business Analytics represents one of the latest opportunities for organizations to change their practices by the use of IT. It is argued that organizations need to act swiftly to benefit from Big Data and Business Analytics by using them to create innovation and competitive advantage. The concept of Business Analytics is not new, but has recently re-emerged as an important area of study owing to its developing capabilities to handle Big Data. New IT processing technologies such as Hadoop and cloud services enable Business Analytics to deal with Big Data to provide descriptive, predictive and prescriptive analysis. Business Analytics thus clearly has commonalities with Operational Research. Ranyard, Fildes, and Hu refer to Business Analytics as apparently extending the scope of Operational Research practice, but the precise relationship between Business Analytics and Operational Research remains a matter of debate. Although Business Analytics is increasingly being used in organizations, there is a lack of theory linking analytics to innovation, and hence also a lack of practical guidance for managers. In particular, models of the innovation process do not usually include any explicit form of data acquisition, analysis or use. For example, Choi, Narasimhan, and Kim include only generating rates of product and process knowledge, the process of generation being unspecified, and Pan and Li similarly use only learning rate parameters. An exception is the work of Vidgen, Shaw, and Grant. One of the research questions they considered was how do organizations extract or create value from data. Their analysis - a Delphi study and three case studies - led to 21 recommendations, though there was no attempt to structure these into a causal model. Despite strong claims that Business Analytics can enhance innovation through product or service differentiation using Big Data, there remains a need for theory and empirical evidence to link Business Analytics and innovation. Many businesses are still struggling to figure out how, where and when to use Business Analytics to achieve a worthwhile return. Until the mechanisms underlying Business Analytics and its contribution to improved business performance are better understood, realizing desired outcomes, such as innovation, remains uncertain. It is notable that the research agenda for Operational Research in the analytics age set out by Mortenson et al. concentrates on research into Business Analytics itself rather than on links between Business Analytics and outcomes. Therefore, it is imperative to investigate and confirm if, how and to what extent Business Analytics contributes to innovation. This paper seeks to fill this research gap by proposing and validating a new model to explain the relationships between Business Analytics and product or service innovation. In so doing, it is important not to regard Business Analytics as just a technical development, but also one related to organizational culture. Like any technique, Business Analytics will always yield findings of some sort, but only if organizations choose to act on those findings can any innovation occur. Achieving competitive advantage as a result would be clear evidence that organizations have acted on the Business Analytics findings. An appropriate cultural focus when examining Business Analytics is the concept of data-driven culture. The term data-driven culture has been in use for many years, but with the emergence of Big Data, it has attracted much more attention from practitioners and researchers because they argue that to maximize the potential Business Analytics business value, a relevant organizational culture must be in place. Most Operational Research writers on Business Analytics acknowledge the importance of organizational culture, but few consider the acquisition of the data being analyzed, Hindle and Vidgen and Pape being notable exceptions. Yet the acquisition of data needs to be a purposeful activity – part of environmental scanning, which is a basic process of any organization to acquire and use data from the external environment to assist management in problem definition and decision making. As Big Data technologies enable organizations to acquire a vast array of data about their environments, the role of environmental scanning Big Data must be considered when studying Business Analytics' impact on innovation. To link analytics, data and culture, absorptive capacity theory thus appears highly relevant, because this theory relates to an organization's ability to recognize the value of new, external information, assimilate it and apply it to commercial ends. This is a crucial element of the path from Business Analytics to innovation. Yet as far as we are aware, ours is the first study to use it to help understand how Business Analytics affects innovation, and how managers might change their organizations to reap the benefits from Business Analytics. Therefore, this research aims to examine specifically the relationships between Business Analytics, data-driven culture, environmental scanning, new product or service innovation, and competitive advantage. To achieve this research aim, this study employs a deductive approach. A number of hypotheses are proposed from an information processing and use perspective, drawing on absorptive capacity theory. These hypotheses are integrated into a research model to explain how Business Analytics, working through environmental scanning and data-driven culture, contributes to new product or service innovation, and subsequently competitive advantage. To test the research model, a survey questionnaire is designed to collect quantitative data from UK commercial organizations. Survey data collected from 218 UK companies are used to test the research model. The remainder of this paper is structured as follows. Section 2 provides a literature review on the key concepts and theoretical considerations. Section 3 discusses the development of the research model. Section 4 explains the research method including research constructs, the associated measurements, and data collection process. Section 5 presents the data analysis and results. It is followed by discussion in section 6 and conclusion in Section 7.", "section": "Introduction", "doi": "10.1016/j.ejor.2018.06.021", "references": [92988379, 1419724723, 1542083448, 1555809017, 1721421031, 1920432947, 1969359483, 2013261784, 2013593749, 2034957137, 2038195563, 2041579435, 2070564502, 2081084322, 2096460314, 2107023540, 2121991049, 2122141202, 2126071066, 2132747867, 2136252976, 2141975087, 2164921310, 2264142365, 2281782812, 2284916459, 2304517173, 2498362352, 2590255024, 2626664234, 2769967834, 2803453471]}
{"paragraph": "Pricing Bermudan options in high dimensions requires Monte Carlo methods, and two simulation-based prices have been developed: lower and dual upper bounds. Specifically, Longstaff and Schwartz use a standard least-squares Monte Carlo approach to compute lower bounds. Likewise, upper bounds are also based on least squares and simulation. Although both bounds are widely used, upper bounds are hardly optimized, which is important because simulation is time consuming, demanding a smart approach. In this paper, we optimize recursive upper bounds and provide two new results. Lower and upper bounds generated by simulation depend on an exercise policy, whereby the upper bound is derived from a martingale based on this policy. First, we show a recursive upper bound is independent of the next-stage exercise decision and hence cannot be optimized. Therefore, we optimize the recursive lower bound, following a local least-squares Monte Carlo approach, and use its optimal recursive policy to evaluate the upper bound as well. We find these two bounds, which have a similar cost to the reciprocal bounds based on a policy estimated by the standard least-squares Monte Carlo method, are very tight. Second, we study separately an upper bound generated from a martingale based on continuation-value functions, a bound that is less time intensive yet more upper biased, and show how to reduce its bias as well. In our first approach, we consider a given family of exercise policies or stopping times. A local method maximizes a recursive lower bound with regard to this family at each exercise stage. An open question is which exercise strategy minimizes the upper bound. We show the exercise strategy that maximizes a recursive lower bound also minimizes not the recursive upper bound itself, but rather the gap between them. We provide a recursive expression for the gap, and show a recursive upper bound is independent of the next-stage exercise policy. Therefore, minimizing the gap is equivalent to maximizing the Bermudan price recursively. In the second approach, we consider a family of continuation-value functions. We show a recursive upper bound is independent of the next-stage continuation-value function as well. By factorizing the two martingales that are based on either stopping times or continuation values, the latter martingale includes a third error term, which ensures the process is actually a martingale yet implies more biased upper bounds. The other two terms of the martingale are those of the standard factorization of the American option into an early-exercise premium and the European counterpart. This third term, however, depends only on the option continuation value in the waiting or continuation region. The latter constraint is critical because Bermudan options are highly nonlinear near the exercise boundary but less so in the waiting region, and fitting a continuation-value function only in this region is easier. This new upper bound, based on a continuation value estimated only in the waiting region, is as accurate as an upper bound based on an exercise policy estimated by the least-squares Monte Carlo method, but in a fraction of the time. The new bound is especially accurate for at- or in-the-money options, which depend mostly on sample paths that cross the exercise region and that do not contribute to the martingale’s third error term. This dual waiting-region constraint is the reciprocal constraint of using in-the-money paths, and hence, the exercise region, to estimate the continuation value in the least-squares Monte Carlo or primal method. In the numerical exercise, we price up-and-out Bermudan max-options. The up-and-out barrier makes this option very sensitive to suboptimal exercise, providing a good test. From the local least-squares Monte Carlo method, we derive the optimal recursive exercise policy and compute the two bounds associated with this policy: the lower bound improves upon the reciprocal bounds based on the standard least-squares Monte Carlo and pathwise optimization by more than 100–200 cents; the upper bound yields a one-digit gap. This small gap implies the recursive policy and the associated martingale are near optimal and the two bounds are close to the true price. The local policy is so good that reducing the number of subsimulation paths by 20 decreases the time effort by a factor of 10, yet the upper bound increases only by a few cents. Notably, the upper bound based on the local least-squares Monte Carlo policy only changes marginally with the number of subsimulations and is robust to all refinements, implying the upper bound is tighter and closer to the true price than the lower bound. With other methods that yield a nontrivial gap, this claim cannot be made. This result agrees with the two-period Bermudan upper bound, which is independent of the one-period exercise policy. A tighter upper bound implies a midpoint between lower and upper bounds is lower biased. The duality approach in option pricing has been extended in many ways. Several studies examine optimal dual bounds, use a multilevel approach, study dual bounds based on regression methods, derive upper bounds using linear and semidefinite programming, or use a pathwise-optimization approach that is less time consuming. In a novel extension, one study uses an information relaxation that nests the perfect-information assumption of the martingale approach, and also applies to other problems in operations research. We tailor duality results to our optimal recursive setting, which yields such tight bounds. Specifically, in the former case based on continuation values, we improve the upper bound by computing the continuation value only in the waiting region. In the latter case based on stopping times, the upper bound is both tight and efficient if we use fewer subsimulation paths, but a near-optimal exercise strategy as in the local least-squares Monte Carlo approach. In both ways, we bring the overall cost of the martingale approach in line with pathwise optimization or information relaxation. Moreover, the factorization of the dual martingales in terms of the components of the Bermudan process is mostly new. Our tight bounds are a useful benchmark for new methods that try to improve upper bounds in terms of accuracy or time effort. Section 2 reviews the local least-squares Monte Carlo method and explains the exercise policies and continuation values needed later for dual bounding; Section 3 shows the independence of the recursive upper bound on the next-stage exercise policy; Section 4 shows an upper bound based on a continuation-value function only needs this function in the waiting region; Section 5 provides the complexity analysis and examples; Section 6 concludes. Proofs are left to the Appendix.", "section": "Related Work", "doi": "10.1016/j.ejor.2019.07.031", "references": [122275331, 1866485205, 2017001821, 2062659568, 2089124841, 2112795785, 2120301464, 2120790358, 2137824542, 2151786492, 2156168464, 2164674975, 2295954716, 2611766878, 2617918452, 2782351573]}
{"paragraph": "Humans use the gaze to look at objects. This behavior can be used as a means to control interfaces in human–computer interaction by estimating the gaze point with the help of an eye tracker. Thanks to recent technological advancements and drop in price, eye tracking is no longer a niche technology only used in laboratories or by users with special needs. For example, with the price of an advanced game controller, players can enhance their gaming experience with eye tracking. A gaze-aware game knows where the player’s visual attention is at each moment and can offer optional input methods and enhanced gaming experience. At the same time, research on mobile eye tracking has been active. Simple eye-awareness is already included in some cell phone models so that the phone knows when the user is looking at it. Research on pervasive and mobile gaze interaction has demonstrated how eye tracking can enhance the interaction with mobile phones, tablets, smartwatches, smart glasses, and smart environments and public displays. Because the eye is primarily a perceptual organ, using gaze as an intentional control method poses challenges for interaction design. Most important, viewing should not be misinterpreted as a voluntary command. In gaze interaction literature, this problem is known as the Midas touch problem, where viewed objects are unintentionally acted on. Feedback plays an essential role in informing the user how the system is interpreting the gaze. Gazing at an object in real life naturally provides only visual feedback. But computers and smart devices can indicate if an object has been recognized as being pointed at, or being selected. Previous research has shown that visual and auditory feedback on gaze input significantly improve user performance and satisfaction. However, the effects of haptic feedback in gaze interaction have remained largely unknown. We assume haptic feedback could provide a useful alternative to, at least the audio, as auditory and haptic perception are known to share similarities. For example, participants could perceive auditory and tactile rhythms more accurately than visual rhythms. Auditory and haptic feedback can be perceived independently from the gaze location. Unlike the distal senses of vision and hearing, touch is a proximal sense that provides information of things close to or in contact with us. How would the interplay of a distal and proximal sense work? For instance, instead of seeing a button change its appearance, the user could feel the click of a button after selecting it with gaze. Could this novel combination of modalities provide some benefits compared to visual and auditory feedback, or is this unnatural combination of action and feedback perhaps incomprehensible? These were the questions that motivated us in the work reported in this article. Haptic feedback has become more common in consumer technology due to the emergence of mobile and wearable devices designed to be in contact with the skin. The most popular form of haptic stimulation in mobile and wearable devices is vibrotactile feedback. For example, continuous vibration is an effective way to notify of incoming calls with mobile phones. Shorter vibration bursts are used on phones and tablets to replace the tactile feel of pressing a physical key when typing with a virtual keyboard. This has been shown to improve typing speeds. Vibrotactile technology is also included in some smartwatches. In the Apple Watch, for instance, vibrotactile stimulation is used to mimic a heartbeat that can be sent to a close friend or family member. With multiple actuators, it is possible to create touch sensations that move on the wrist. To date, commercial smart glasses and other head-mounted devices have not utilized vibrotactile feedback. This is surprising because it is known that users can quite accurately localize which part of the head is stimulated with vibrotactile actuators. We were interested in studying how vibrotactile feedback could support gaze interaction. We conducted a series of experiments in which we focused on four main research questions: effectiveness of vibrotactile feedback, temporal limits between gaze events and vibrotactile feedback, effects of feedback location and spatial setup, and vibrotactile feedback in comparison to other modalities. Because our results are spread over more than 20 articles, this could make it difficult for future researchers to extract the main findings. The contribution of this article is to summarize the research results in a compact form and serve as a collection of pointers to more detailed work in the original publications. The goal is to add to our understanding of how the two modalities of haptics and gaze can be utilized effectively in human–computer interaction. The organization of the article is as follows. We first introduce gaze interaction and vibrotactile feedback. We then present results from the experiments before discussing lessons learned from the studies. We end with general discussion and present design guidelines based on our accumulated knowledge and insights.", "section": "Methodology", "doi": "10.1080/07370024.2017.1306444", "references": [180056178, 1492404201, 1520722479, 1538622809, 1590806791, 1976177666, 1982812012, 1986793049, 1988742375, 1990184244, 1994221098, 1995055145, 2000367546, 2002492663, 2004164216, 2005637512, 2006966072, 2010809371, 2014788716, 2016877042, 2018806117, 2020284712, 2022716189, 2025074021, 2030189835, 2034722664, 2040464349, 2041257003, 2042346957, 2046879541, 2053432308, 2057481637, 2060089136, 2075140915, 2076963239, 2079778212, 2088270376, 2090644126, 2091354492, 2093355879, 2094058125, 2094938076, 2099788879, 2102804490, 2103802786, 2110228188, 2134738924, 2138428053, 2144390195, 2144922371, 2150273910, 2156515914, 2156903065, 2164660958, 2167020116, 2176017127, 2285717100, 2293738675, 2295148469, 2298161945, 2344158693, 2402420151, 2420461153, 2484706745, 2537533530]}
{"paragraph": "In machine learning, data are usually described as points in a vector space. Nowadays, structured data are ubiquitous. The capability to capture the structural relationships among the data points can be particularly useful in improving the effectiveness of the models trained on them. To this aim, graphs are widely employed to represent this kind of information in terms of nodes or vertices and edges, including local and spatial information arising from data. For instance, consider a d-dimensional dataset representing points in space, a graph can be extracted by considering each point as a node, where edge connectivity and weights can be computed using a metric function. A new data representation is obtained, where there is a set which contains vertices and a set of weighted pairs of vertices representing edges. Applications in a graph domain can be usually divided into two main categories: vertex-focused and graph-focused. The former includes tasks where one performs classification or regression on the vertices of a graph, whereas in the latter one performs these tasks on the graph itself. For instance, object detection and image annotation are examples of vertex-focused applications, the former consists of finding whether an image contains a given object and its position, the latter consists of extracting a caption that describes an image. Another possible example could be web page classification, where the web is represented by a graph where nodes are the pages and edges are the hyperlinks between them, the aim being to exploit the web connectivity to classify pages in a set of topics. Instead, estimating the probability of a chemical compound to cause certain diseases can be seen as a graph-focused application. This is possible due to the fact that a chemical compound can be modeled by a graph where the nodes are the atoms and the edges the chemical bonds. For the sake of simplicity and without loss of generality, just the classification problem is considered. Under this setting, the vertex-focused applications are characterized by a set of labels, a dataset, and the related graph. Let us assume there is a subset of labeled nodes. The goal is to classify the unlabeled nodes by exploiting jointly the node features and the graph structure by means of a semi-supervised learning approach. Graph-focused applications are related to the goal of learning a function that maps different graphs to integer values by taking into account the features of the nodes of each graph. This task can be solved by supervised classification on the graphs. A number of research works are devoted to classification, both for vertex-focused and graph-focused applications. Nevertheless, there is a major limitation in existing studies: most of these research works are focused on static graphs. However, many real world graph-structured data are dynamic and nodes or edges in the graphs may change over time. In such dynamic scenarios, temporal information can also play an important role. For instance, the interactions between individuals inside a building in one day can be modeled as a sequence of graphs, each one describing a time window within the day, where the nodes are the people and the edges are the interactions occurring between them within a time frame. As another example, consider the classification of human activities from motion-capture data, where each frame can be modeled as a graph, where the vertices are the skeleton joints of the person in question. In the last decade, neural networks have shown their great power and flexibility by learning to represent the world as a nested hierarchy of concepts, achieving outstanding results in many different fields of application. It is important to underline that just a few research works have been devoted to encoding graph structures directly using a neural network model. Among them, to the best of the authors’ knowledge, no one is able to manage dynamic graphs. To exploit both graph-structured data and temporal information through the use of a neural network model, two novel approaches are introduced. They combine Long Short Term-Memory network and Graph Convolutional Network, which can be considered the two base elements of the proposed architectures. Both of them are able to deal with vertex-focused applications. Respectively, these techniques are able to capture temporal information and to properly manage graph-structured data. The approaches are also extended to deal with graph-focused applications. LSTMs are a special kind of Recurrent Neural Network, which are able to improve the learning of long term dependencies. All RNNs take the form of a chain of repeating modules of neural networks. Precisely, RNNs are artificial neural networks where connections among units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic sequential behavior. In standard RNNs, the repeating module is based on a simple structure, such as a single unit. LSTMs extend the repeating module by combining four interacting units. Specifically, it is based on: cell state, forget gate, input gate and output gate. The most important part of LSTMs is the cell state, which can be pictured as a conveyor belt. It runs straight down the entire chain, with some linear interactions. The first interaction decides what information is going to be removed from the cell state. This decision is made by a sigmoid layer, called the forget gate. The next interaction decides what new information will be stored in the cell state. This can be operationalized in two parts: a sigmoid layer, called the input gate decides which values should be updated, and another sigmoid layer creates a vector of new candidate values that could be added to the cell state. The final output is generated by combining the cell state with a sigmoid layer. A GCN is a neural network model that directly encodes graph structure, which is trained on a supervised target loss for all the nodes with labels. This approach is able to distribute the gradient information from the supervised loss and to learn representations exploiting both labeled and unlabeled nodes, thus achieving state-of-the-art results. The goal of a GCN is to learn a function of signals or features on a graph which takes as input a feature description for each node, summarized in a feature matrix, and a representative description of the graph structure in matrix form, typically in the form of an adjacency matrix. This model produces a node-level output, a feature matrix where the number of output features corresponds to each node. A GCN shares its underlying intuition with Convolutional Neural Networks, specialized kinds of neural networks that are highly successful in practical applications such as computer vision employing, within some of their layers, a convolutional operation in place of regular matrix multiplication. A convolutional layer in a CNN exploits the locality of information embedded in its input to extract local features. It does this by repeatedly looking at small patches of its input and by learning a mixing matrix of the size of the patches that is shared across all the patches, thus reducing the amount of parameters. Similarly, a GCN exploits the locality notion induced by the graph connectivity by looking at small patches of the graph, where the patches are all neighbourhood subgraphs of the original graph.", "section": "Introduction", "doi": "10.1016/j.patcog.2019.107000", "references": [145505233, 1522301498, 1662382123, 1869398109, 1968103943, 1968528416, 2004109568, 2004646046, 2051436535, 2064675550, 2065130322, 2086370422, 2100741829, 2116341502, 2117024994, 2124386111, 2139823104, 2143516773, 2146055337, 2154851992, 2158787690, 2173027866, 2244807774, 2366141641, 2468907370, 2519887557, 2557283755, 2610153490, 2740060125, 2754759618, 2771111398, 2950635152, 2953170998, 2963165299]}
{"paragraph": "The huge volume of check-in data from various location-based social networks enables studies on human mobility behavior on a large scale. Next POI recommendations is the task to predict the next POI a user will visit at a specific time point given her historical check-in data. This task has been studied extensively in recent years. Next POI recommendations is different from typical recommendation tasks such as movies, songs, or books because a wide range of contextual factors are related to the user's spatial behaviors. These auxiliary factors include the temporal context, sequential relations, geographical influence, and auxiliary meta-data information such as textual description and user friendship. However, these factors are heterogeneous in nature. While some relevant aspects are continuous values such as geographical distance and the time interval, others are in the form of discrete values such as friendship, textual words, or day of the week. Harnessing useful signals from all these heterogeneous factors to predict a user’s next move is not an easy task. Existing solutions based on matrix factorization and embedding learning techniques have delivered encouraging performances. These solutions project users and POIs and the associated context factors into a shared hidden space with dense vector representations. The preference score is then calculated directly based on these vectors through the inner product operation. However, shallow factor or embedding learning is too limited to express the complex knowledge underlying user spatial behaviors with multiple context factors. In existing methods, different context factors are often modeled separately. Then a simple combination is applied to derive the final recommendation score. That is, we need to devise an individual model for each context factor. This modeling methodology is complicated and the resultant solution would be inferior, since the different factors carry varying degrees of useful knowledge and their interactions could be much more complex. Some other methods incorporated multiple context factors as additional constraints to guide the learning process. For example, check-ins made at a specific time period are grouped together for dynamic feature learning. However, these constraints may not always be useful to match user-POI interactions. A single factor or embedding learning could inevitably incur information loss through a joint optimization of both preserving the constraints and matching user-POI interactions. One plausible solution is to enlarge the dimension number. However, given the sparsity nature of user-POI interaction data, it would easily result in data overfitting. The neural network with dense vector representation based techniques provide a new way of modeling these factors in a unified manner. This offers two benefits: By adding nonlinear transformations on top of the embeddings of users, POIs and their associated factors, we can separate the embedding learning and high-level spatial intent learning to better understand user spatial behavior, leading to a better recommendation accuracy. Specifically, we encode the semantic relatedness or constraints among POIs or users into the corresponding embeddings. For example, users at Golden Gate Overlook are likely to visit Baker Beach in San Francisco, and vice versa. Therefore, Golden Gate Overlook and Baker Beach are projected closer in the embedding space. Without the need to match user-POI interactions, the embedding learning would capture the latent features for users, POIs and the associated constraints to its fullness. Also, both new users and POIs may be covered partially by the associated auxiliary meta-data such as textual description or friendship. With dense vector representations, we can easily estimate the spatial intent from the associated meta-data for these cold-start cases. Since not all constraint information or latent features are useful for all user-POI interactions, the nonlinear transformation operation is adopted to learn how to extract high-level spatial intent for next POI recommendations. We can also devise factor-based nonlinear extractions to accommodate some specific context factors that are strongly relevant to spatial intent extraction. Through a unified framework with neural treatment and dense representations, the complex interactions among the context factors, users and POIs can be learnt smoothly without handcrafted modelling for each factor alone.", "section": "Related Work", "doi": "10.1007/s11704-018-8011-2", "references": [154472438, 179875071, 1500188831, 1546409232, 1614298861, 1924770834, 1971129545, 2009779426, 2044672016, 2047604680, 2059512502, 2059512573, 2064675550, 2070915285, 2073013176, 2084677224, 2087692915, 2094286023, 2101409192, 2110485445, 2110953678, 2118463056, 2137245235, 2137983211, 2146232090, 2154851992, 2156387975, 2205235818, 2242161203, 2250739653, 2408538552, 2471486255, 2511929605, 2512971201, 2515144511, 2531384334, 2534727297, 2536880093, 2539781657, 2567312369, 2572589325, 2575006718, 2605350416, 2762735242, 2788114581, 2949274928, 2950635152, 2962995178]}
{"paragraph": "As the Internet and mobile phones have evolved over the years, technology to improve remote communication has developed immensely. It is clear that mobile phone users prefer text messaging over phone calls, with the number of monthly text messages worldwide increasing from 395 billion in 2011 to 561 billion in 2014, and the number is still growing. In a computer-mediated communication such as text messaging, a cues-filtered-out perspective and social presence theory argue that computer-mediated communication lacks nonverbal communication cues and thus fails to communicate emotions and attitudes to receivers. Also, prior studies showed that viewing online text without emoticons caused misinterpretation of the nature of the message and the sender’s intent. There have also been studies that state that the emotional attitude of the text is more important when it comes to the interpretation, but a negative emoji can even change the interpretation of the message. For instance, when somebody sends a neutral text such as “That man came to see me again,” it is difficult to know whether the sender is annoyed or glad. Emojis can clarify these kinds of ambiguity and even show the intensity of the emotion, as well. Many researchers have suggested that written communication can be enhanced through visual cues in the same way visual or body language supports verbal communication. In computer-mediated communication, which has become so common today, users attach emoticons in their text messages to fill in the missing socioemotional context. Previous studies suggested that emoticons can provide the missing information and enhance computer-mediated communication. These visual cues have been identified as the primary way to express emotions in computer-mediated communication and as a way to replace nonverbal communication when face-to-face interaction is not possible. Emoticons have been defined as graphic representations of facial expressions that are embedded in electronic messages. The usual distinction between the terms emojis and emoticons is that the term emoticons is commonly used to refer to punctuation signs combined on the keyboard to convey nonverbal communication, such as the smiley face or a wink, whereas emojis are pictures that convey nonverbal information. This article uses the term emojis because the final tactile design examined here employed imagery rather than a combination of punctuation marks. Today, emojis are used worldwide in countries that use electronic devices to communicate. Since Apple created its Apple Color Emoji set, providing support for Unicode emojis, it has been used in all iOS and OS X devices and in many others. Japan and Korea, the countries with the largest mobile markets in the world, use Line and KakaoTalk for their messaging platforms. It is said that there are now more than 22 million emojis in KakaoTalk and Line, and KakaoTalk calculated that, of messages that contained emojis, emojis composed 15 percent of the conversation. In 2012, the daily sales of emojis totaled approximately 100 million Korean won. Throughout these many developments and changes in online communication, visually impaired individuals have often been left out. Although companies such as Apple, KakaoTalk, and Google are continuously developing technologies for people with visual impairments, most of these are based on audio guidance. Audio guidance is useful in its own way, but it has limitations; it cannot provide privacy without the use of headphones, which can create dangerous situations for the visually impaired, as their ears must be occupied. Also, audio guidance cannot be heard readily in a noisy environment. Studies on techniques for tactile delivery of emotional cues have also been progressing using vibration patterns, person-to-person haptic responses, and vibrotactile displays. However, as many of these were not developed for visually impaired people, they still do not provide the ultimate solution to limitations on visually disabled persons’ texting experience. The goal of this study, therefore, was to show that image-based tactile emojis can offer a solution for the future texting experience of individuals with visual impairments. Also, based on prior studies indicating that congenitally blind people are able to comprehend facial expressions, and research that implies face processing as a bimodal phenomenon where vision can be substituted by touch, this study examined the question of whether, with a short introduction, congenitally blind individuals, who have no prior visual knowledge, are capable of understanding emojis that sighted people use. To achieve these goals, the following steps were taken: We conducted a preliminary study to identify the connection between emotions and emojis. For the ten most commonly selected emojis for each emotion, we organized the facial features separately into eyes, eyebrows, and mouth and converted the images into textual descriptions. We ranked the separately organized facial features by weighting the features most often chosen. We created image-based tactile emoji designs based on the previously defined descriptions of the six emotions. We conducted interviews using the resultant apparatus and analyzed the results. The provision of image-based tactile emojis, when applied in the real world, will provide privacy for a visually impaired person’s texting service and equality in technology that has not previously existed. It will also improve textual communications for visually impaired people by reducing misunderstandings and making the texting experience more enjoyable.", "section": "Methodology", "doi": "10.1080/07370024.2017.1324305", "references": [40549020, 1978040282, 2012461879, 2020728888, 2024641146, 2062769641, 2096073495, 2100928276, 2101282523, 2107208519, 2118459787, 2129353167, 2133180260, 2144378002, 2146459173, 2160660844, 2161906378, 2243203301, 2296274542, 2519353570, 2574189006]}
{"paragraph": "Importance measures providing information about the importance of a component or a group of components on the system performance, such as reliability or availability, productivity, safety, or any performance metrics of interest, can help to identify design weaknesses or operation bottlenecks and to suggest optimal modifications for system upgrades and maintenances. In the literature, a large number of importance measures have been developed and successfully applied for various purposes. In risk analyses, importance measures are used in risk-informed decision-making. In reliability engineering, importance measures are used to prioritize components in a system for reliability improvement. Recently, importance measures have been applied for maintenance optimization and spare parts management. More specifically, Birnbaum structural importance measure is used to build a decision indicator for maintenance optimization of multi-component systems with complex structure. Differential importance measure is proposed to use in inventory management. More recently, the link between component importance and preventive maintenance policy has been discussed. In the framework of condition-based maintenance optimization, the current condition of components, such as failure state, working state, or deterioration level, is an important issue and needs to be taken into account in decision-making. However, very few existing importance measures allow incorporating the actual condition of components over time. Moreover, in practice, positive economic dependence, which implies that joint maintenance of several components is cheaper than performing maintenance on components separately, often exists and should be integrated in maintenance decision-making in the framework of maintenance optimization. To the best of our knowledge, no existing importance measure allows taking into account this kind of interaction between components. To face this issue, in this paper, a novel time-dependent importance measure based on the conditional reliability evaluation of the system, namely RIM measure, is proposed. At a given time and given the real condition of the components of a system, the proposed importance measure can be used to rank the components or groups of components according to their ability to improve the system reliability for a given mission. The proposed RIM measure is then extended to take into account the maintenance cost and the economic dependence between components. Indeed, the extended RIM measure is defined as the ratio of the cost benefit given by the maintenance of a component or group of components to its total maintenance cost. This indicator can help to identify the most cost-effective components or group of components for preventive maintenance before a given mission. This paper is organized as follows. Section 2 is devoted to the description of general assumptions and different reliability metrics. Different kinds of information on components at a given time are also discussed and integrated in the evaluation of reliability metrics. Section 3 focuses on the definition of the proposed time-dependent importance measure, namely RIM. The influence of information level on the RIM measure and RIM-based importance ranking is also investigated. An extension of RIM measure is developed in Section 4. Maintenance cost structures and economic dependence between components are also formulated and discussed. To illustrate the uses of RIM measure and its extension, a numerical of a 5-component system is introduced in Section 5. In addition, some numerical results are herein discussed. Finally, the last section presents the conclusions drawn from this work.", "section": "Introduction", "doi": "10.1016/j.ress.2019.106633", "references": [261935476, 1550064664, 1807378533, 1999069087, 2002699192, 2019006256, 2053885255, 2084648532, 2126484023, 2126967364, 2147664181, 2155326466, 2202654822, 2290696661, 2340663414, 2612742145, 2620514866]}
{"paragraph": "Constraint-based techniques, such as Integer Linear Program and SAT modulo theory solvers, play a key role in state-of-the-art approaches for solving challenging problems across a wide range of applications. In this work, we demonstrate how virtual data center allocation, a prominent and increasingly important problem arising in the operation of modern data centers, can be tackled using a pair of high-performance constraints solvers: Gurobi and MonoSAT. We obtain substantial improvements in performance and functionality over previous virtual data center allocation techniques. Central to our results is the formalization of virtual data center allocation in terms of multi-commodity flows, allowing us to exploit the efficient handling of network-commodity flow problems in Gurobi and MonoSAT. We are the first to demonstrate that constraint solvers can be successfully applied to this setting at full data center scales, while also improving on the state-of-the-art. A virtual data center consists of multiple communicating virtual machines, each with individual server resource requirements such as CPU or RAM, along with a virtual network of pair-wise bandwidth requirements between the virtual machines. The virtual data center allocation problem is to find a valid allocation of virtual machines to servers and links in the virtual network to links in the physical network. A valid allocation satisfies the compute, memory, and network bandwidth requirements of each virtual machine across the entire data center infrastructure, including servers, top-of-rack switches, and aggregation switches. The allocation is indicated with dashed lines. For example, the virtual machines for s1 and m are mapped to the same physical server and the virtual link s2 to m is allocated a multi-path route, in which each sub-path provides 1 Gbps. Support for end-to-end and multi-path are two characteristics that distinguish capabilities of our tool, Netsolver, from prior tools. In this work, we introduce Netsolver, a constraint-based virtual data center allocation procedure that is scalable, sound, and complete, with support for end-to-end, multi-path bandwidth guarantees across all the layers of the networking infrastructure, from servers to top-of-racks to aggregation switches to access routers. Netsolver efficiently allocates virtual data centers with a dozen or more virtual machines to full-size physical data centers with over a thousand servers, typically in seconds per allocation. Across a wide variety of data center topologies, Netsolver can allocate as many total virtual data centers to the same physical data center as state-of-the-art heuristic methods, such as SecondNet's allocation algorithm. Furthermore, Netsolver offers the flexibility and extensibility characteristics of a constraint-based approach. In real-world applications, data center operators often need to support additional constraints or optimization goals while allocating virtual machines, such as ensuring that certain virtual machines are placed together. We demonstrate that Netsolver can be easily extended to support additional virtual data center allocation constraints: virtual machine affinity constraints, minimization of the total number of utilized servers, and constraints to load-balance allocations and avoid hotspots. This paper extends results previously published with significant new capabilities, a new Integer Linear Program-based solver back end, and broadened experimental results. Additionally, we have improved the overall runtime performance of Netsolver through the use of automatic algorithm configuration, as well as upgrading the SMT solver from version 1.4 to version 1.6. We use Gurobi version 8.1.0 for all experiments.", "section": "Related Work", "doi": "10.1016/j.artint.2019.103196", "references": [60686164, 1480909796, 1572977974, 1710734607, 1770696206, 1857623778, 1968457703, 2001859357, 2002227246, 2006146932, 2010727402, 2017953449, 2022678927, 2027588582, 2074814670, 2077107611, 2077661716, 2094410510, 2096125134, 2097882016, 2107342126, 2112486185, 2114298221, 2116758077, 2121574037, 2123138012, 2126210439, 2130267070, 2132238781, 2134656724, 2137229314, 2142480021, 2142972529, 2152415706, 2154203494, 2157614013, 2157990152, 2161965229, 2257125174, 2295272781, 2803222079, 2949910966]}
{"paragraph": "Data envelopment analysis is a nonparametric method for measuring the efficiency of the decision-making units by a set of inputs and a set of outputs. DEA requires no assumption for the functional relationships between inputs and outputs and allows individual decision-making units to evaluate their efficiencies by choosing the most favorable input and output weights for themselves. It has been applied in many areas, such as resource reallocation, hospital assessment, and assessment of public finance. However, such an individual evaluation causes more than one unit to be efficient, leading to them being unable to be fully discriminated. In addition, the flexibility in weighting multiple inputs and outputs sometimes produces unrealistic weight schemes. To overcome these disadvantages, the cross-efficiency evaluation method has been developed to rank units based on cross-efficiency scores, which are linked to all units. Unfortunately, the cross-efficiency evaluations obtained from the original DEA are generally not unique because the optimal solution to the DEA linear programs is not unique. For this reason, secondary goals such as the aggressive and benevolent formulations have been proposed to address this issue. Additional models have introduced different secondary objective functions, which include minimizing the total deviation from the ideal point, minimizing the maximum efficiency score, and minimizing the mean absolute deviation. A neutral DEA model for cross-efficiency evaluation has also been proposed, which determines one set of input and output weights for each unit from its own point of view without being aggressive or benevolent toward others. Another approach generalized the original DEA cross-efficiency concept to game cross-efficiency, in which each unit seeks to maximize its own efficiency under the condition that the cross-efficiency of others does not deteriorate. A data DEA cross-efficiency aggregation method based on Shannon entropy has also been proposed. As a large number of zero weights may still exist among inputs and outputs, other models were developed to reduce the number of zero weights. Similar approaches estimate efficiency confidence intervals by using the bootstrap or other sub-sampling techniques. The weights for inputs and outputs of units are estimated from a random sample of input–output combinations, which are obtained from existing production units operating in the studied activity sector. Ratio-based efficiencies such as ranking intervals, dominance relations, and efficiency bounds have also been developed to compare the relative efficiencies of units for all feasible input–output weights. Some methods considered all possible weight sets in the weight space when computing the cross-efficiency, which produces an interval cross-efficiency for each unit. Other procedures were developed to carry out the cross-efficiency evaluation without the need to make any specific choice of weights. The proposed procedure takes into consideration all the possible choices of weights that all the units can make, and yields a range of the possible rankings for each unit instead of a single ranking. It is necessary to note that the interval efficiencies are different from the efficiency confidence intervals. The interval efficiencies contain all the possible efficiency scores of units, including the best and worst efficiency scores from all possible weights in the weight space. The efficiency confidence intervals are defined from a probability point of view, according to which of the most efficient units lie within an interval with a certain confidence level. Many classical decision analysis methods, including DEA, are based on assumptions of rationality and certainty. However, numerous studies have demonstrated that these classical methods cannot explain many phenomena in real applications. Therefore, researchers have explored decision-making theories based on behavior, and many behavioral decision theories have been developed, such as prospect theory, regret theory, and fairness theory. It has been noted that traditional decision and game theories rest on a fundamental assumption that players seek to maximize their individual utilities, but in some interactive decisions, it is observed and seems intuitively reasonable to maximize the utility of the group of players as a whole. Such phenomenon or thinking can be called team reasoning. Experimental evidence suggests that team reasoning predicts strategic choices more powerfully than orthodox game theory in some games. Some researchers commented on such an approach with varying mixtures of agreement and disagreement. In reply, it has been emphasized that decision makers sometimes act to maximize the collective payoff for a group, which cannot be explained in terms of standard social value orientations. Therefore, team reasoning is a distinctive and important mode of reasoning, which should be acknowledged in cognitive psychology and be added to the set of social value orientations used in social psychology. Furthermore, based on the experiments on how players use focal points to select equilibria in one-shot coordination games, two alternative explanations, namely, the cognitive hierarchy theory and team reasoning, were tested and strong support was found for team reasoning. In addition, some researchers considered all the individuals forming a team, and developed consensus models based on the minimum cost of the team. From the above literature review, we find that all the existing DEA approaches focus on the individual interest but ignore the team interest. Motivated by the idea of team reasoning, this paper investigates DEA models that put the team interest ahead of the individual interest. In DEA, all the decision-making units can be considered as a team, and the interest of the team can be reflected by the team's indexes, such as the overall efficiency of all the units, deviations of units, boundaries of units, and relationship between units in the team. The first three can be considered the external performance indexes of the team and expressed by interval values, whereas the fourth can be considered the internal performance index of the team, in which the relationship between units is expressed by the interval pairwise comparison matrix, where one value is the relative efficiency value between each pair of units. Based on such an idea, the following approach is proposed: First, the models are developed to estimate the values of the team indexes. Based on the obtained indexes, the decision makers can express their preferences about the indexes. Then, the individual models are developed to estimate the interval efficiencies of individual units under the condition that the team indexes of units are satisfied. Following the approach proposed above, the structure of the paper is as follows. Section 2 gives some preliminaries; Section 3 establishes the models based on the overall efficiency of all the units; Section 4 develops the models based on the variance of all the units; Section 5 focuses on the boundaries of all the units; in Section 6, the models based on the relationship between units are investigated; and Section 7 provides the conclusions.", "section": "Methodology", "doi": "10.1111/itor.12447", "references": [2006002090, 2011399838, 2022373133, 2051914789, 2061637245, 2078792730, 2116080612, 2136228439, 2141532588, 2145964516, 2153400179, 2163774646, 2179629619, 2298726707, 2474592240, 2554997024]}
{"paragraph": "The Shapley value and the prenucleolus are two well-known values for cooperative games. The Shapley value is an average of the contributions of an agent, while the prenucleolus is the value that minimizes the dissatisfaction of the worst-off coalitions. The nucleolus differs from the prenucleolus by only taking into account individually rational imputations. Coincidence between the Shapley value and the prenucleolus is uncommon and, in general, difficult to check without computing both values. A sufficient and necessary condition for this coincidence to hold has been provided, but it requires the computation of both the Shapley value and of a parametric family of sets, for which the computation mimics that of the prenucleolus. This characterization can be applied in order to identify the coincidence in some particular classes of games, such as airport games, bidder collusion games, and polluted river games. Coincidence is also found in some three-agent games based on bankruptcy problems. For general coalitional form games, we have coincidence if the game only has two agents or if all agents are symmetric within the normalized game. Some other games have also been proposed, all having in common that the value of a coalition is equal to the sum of the values created by the pairs composing that coalition. These games are called 2-additive games. The coincidence persists in games that satisfy a certain property. These games are such that the contributions of an agent to any coalition and its complement sum up to an agent-specific constant. A particular instance of such games has also been studied. Some researchers study the coincidence from a geometric point of view. Instead of providing classes of games where both values coincide, they study the properties that lead to this result in some already existing classes, such as certain types of games. A similar, yet different problem, is the invariance of the payoff assigned by an allocation rule to a specific player in two related games. In this paper, we present another family of games, called clique games, in which the Shapley value and the nucleolus coincide. The family can be described as follows: the set of agents is divided into cliques that cover it. A coalition creates value when it contains many agents belonging to the same clique, with the value increasing linearly with the number of agents in the same clique. Agents may belong to more than one clique, but the intersection of two cliques contains at most one agent. Finally, if two agents are not in the same clique, there exists at most one way to connect them through a chain of connected cliques. The family of clique games has a non-empty intersection with some other classes of games, and that intersection consists of 2-additive games. Some clique games are not in these other classes, and some of those games are not clique games. A clique game is convex, and hence its Shapley value is the average of the extreme points in its core. We thus obtain a link between three crucial concepts of cooperative game theory: the nucleolus, the core, and the Shapley value. Naturally, graph-induced games provide a fertile ground to apply our result. We first consider graph-restricted cooperative games. In these games, a coalitional value function is accompanied by a graph that summarizes the cooperation possibilities: a coalition cannot fully cooperate if some of its members have no path between them that uses only the vertices of agents in the coalition. When we consider a symmetric coalitional value function, assigning shares of the value created among agents is akin to defining centrality measures. We show that when the coalitional value function increases linearly with the number of agents in a coalition, we obtain coincidence of the Shapley value and the nucleolus for a family of graphs. Another graph-induced game that we study is the minimum coloring game, in which the graph represents conflicts between pairs of agents. We wish to assign agents to facilities, but cannot assign agents that are in conflict to the same facility. As facilities all have a cost of one, we wish to minimize the number of facilities used. Coincidence between the Shapley value and the nucleolus for a particular family of graphs can be explained by the fact that the graphs induce clique games. Our third example is the minimum cost spanning tree problem. This well-studied problem has agents connecting to a source through a network, with the cost of an edge being a fixed amount that is paid if the edge is used, regardless of the number of users of the edge. Any such problem has a non-empty core even though it may not be convex. Moreover, its Shapley value is not always in its core. When we consider elementary versions of this problem in which all edges have a cost of zero or one, the subset of cycle-complete problems generates clique games. Cycle-complete problems are such that if there exist multiple distinct free paths between a pair of agents the edge connecting them directly must also be free. Our result on clique games then applies, yielding that the nucleolus coincides with the Shapley value and the permutation-weighted average of extreme core allocations. The paper is divided as follows: preliminary definitions are in Section 2. Section 3 describes and illustrates clique games. Section 4 contains the coincidence results. Applications to graph-induced games are discussed in Section 5.", "section": "Introduction", "doi": "10.1016/j.mathsocsci.2019.10.002", "references": [1964733860, 1975617599, 1983125930, 1995443028, 1995559809, 2008131782, 2022418489, 2032441230, 2046116913, 2073225716, 2081334386, 2087121928, 2103267790, 2106417017, 2113370239, 2192499632, 2292459874, 2515623706, 2625101186, 2963187969]}
{"paragraph": "Recently, Vapnik and Vashist provided a new learning paradigm termed learning using privileged information, which is aimed at enhancing the generalization performance of learning models. Generally speaking, in classical supervised learning paradigm, the training data and test data must come from the same distribution. Although in this new learning paradigm the training data is also considered an unbiased representation for the test data, the LUPI provides a set of additional information for the training data during the training stage. The set of additional information is termed privileged information. Different from traditional supervised learning approaches, the LUPI based methods make use of a new kind of training data including privileged information during the training phase, but the privileged information is not available in the test stage. The authors note that the new learning paradigm is analogous to human learning process. In class, a teacher can provide some important and helpful information about this course for students, and these information provided by a teacher can help students acquire knowledge better. Therefore, a teacher plays an essential role in human leaning process. Likewise to the classroom teaching model, in general, the LUPI paradigm based methods can also achieve better generalization performance than traditional learning models. Vapnik and Vashist were the first to present a SVM algorithm with privileged information termed SVM+, which leverages the strength of the LUPI paradigm. A thorough theoretical analysis of the SVM+ was further illustrated in Pechyony and Vapnik and Vapnik and Izmailov. Previous works of the LUPI paradigm focus on two aspects: solving the LUPI based algorithms efficiently and incorporating the LUPI paradigm into various learning models. This paper focuses on the latter. The newly-derived RFVL+, however, has much milder optimization constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the new RFVL+, which naturally tackles the former. From the optimization perspective, the formulation of the SVM+ is a typical quadratic programming problem, and in general the QP problem can be solved by some optimization toolboxes. However, it is unnatural and inconvenient to train a learning model by some optimization toolboxes in real-world applications. For this reason, it is necessary to present an efficient approach to solve it. Pechyony et al. presented an SMO-style optimization approach for the SVM+. Li et al. further proposed two fast algorithms for linear and kernel SVM+, respectively. In addition to solving the SVM+ efficiently, the LUPI paradigm is incorporated into various learning algorithms. Nowadays, neural network is one of the most popular learning algorithms due to the wave of deep learning, and most of current deep learning methods are neural networks, including denoising auto-encoders, convolutional neural networks, deep belief networks and long short-term memory, etc. These neural network methods have achieved greatly successes in various real-world applications, including image classification and segmentation, speech recognition, natural language processing, etc. Therefore, it is very interesting to combine neural networks and the LUPI paradigm. The combined method is able to leverage the strengths of neural networks and the LUPI paradigm. The goal of this paper is to tackle this open problem and construct a bridge to link the LUPI paradigm and randomized neural networks. In this paper, the authors propose a novel random vector functional link network with privileged information called RVFL+. The random vector functional link network is a classical single layer feedforward neural network, which overcomes some limitations of SLFNs including slow convergence, over-fitting and trapping in a local minimum. Although the RVFL has achieved good generalization performance in some real-world tasks, in order to improve further its effectiveness, the authors incorporate the LUPI paradigm into the RVFL. Different from existing variants of RVFL, the RFVL+ may open a door towards alternative to the traditional learning paradigm for the RVFL in real-world tasks. In other words, the RVFL+ makes use of not only the labeled training data but also a set of additional privileged information during the training stage, which interprets the essential difference between the two learning paradigms. Moreover, following the kernel ridge regression, the authors further propose a kernel-based RVFL+ called KRVFL+ in order to handle highly complicated nonlinear relationships. The KRVFL+ has two major advantages over the RVFL+. On one hand, the random affine transform leading to unpredictability is eliminated in the KRVFL+. Instead, both the original and privileged features are mapped into a reproducing kernel Hilbert space. On the other hand, the KRVFL+ no longer considers the number of enhancement nodes, which is a key factor to affect its generalization ability. As a result, the performance of the KRVFL+ in terms of effectiveness and stability is significantly improved in most real-world tasks. Furthermore, the authors investigate the statistical property of the newly-derived RVFL+. The authors provide a tight generalization error bound based on the Rademacher complexity for the RVFL+. Our generalization error bound benefits from the advantageous property of the Rademacher complexity. The Rademacher complexity is a commonly-used powerful tool to measure the richness of a class of real-valued functions in terms of its inputs, and thus better capture the property of distribution that generates the date. In the RVFL+, the weights and biases between the input layer and enhancement nodes are generated randomly and are fixed, the output weights are then calculated by the Moore–Penrose pseudo-inverse or the ridge regression. Therefore, the RVFL+ is considered as a special linear learning model. The Rademacher complexity is an ideal choice for the analysis of this type of methods, and can provide a high-quality generalization error bound in terms of its inputs. In contrast to the previous work, the authors provide a more tight and general test error bound, and the novel bound is also appropriate for various versions of the RVFL including the newly-derived KRVFL+. Last but not least, the authors construct some competitive experiments on 14 real-world datasets to verify the effectiveness and efficiency of the newly-derived RVFL+ and KRVFL+. The experimental results illustrate that the novel RVFL+ and KRVFL+ outperform state-of-the-art comparisons. More importantly, recent existing works have illustrated that the cascaded multi-column RVFL+ and the cascaded kernel RVFL+ can obtain the best performance in terms of effectiveness for the single-modal neuroimaging-based diagnosis of Parkinson’s disease and the transcranial sonography based computer-aided diagnosis of Parkinson’s disease, respectively. Notice that both the RVFL+ and KRVFL+ are basic learners and play key roles in these two ensemble learning methods. The codes of RVFL+ and KRVFL+ are available. The contributions of this paper are summarized as follows. The authors propose a novel random vector functional link network with privileged information, called RVFL+. The RVFL+ bridges the gap between randomized neural networks and the LUPI paradigm. Different from existing variants of the RVFL, the newly-derived RVFL+ provides an alternative paradigm to train the RVFL. The authors extend the RVFL+ to the kernel version called KRVFL+, and the KRVFL+ enables handling effectively highly nonlinear relationships between high-dimensional inputs. The previous works of the LUPI focus on two aspects: deriving an efficient solver and combining the LUPI paradigm with different learning models. This paper focuses on the latter. However, from the optimization perspective, the authors find that the novel RVFL+ has sampler constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the RVFL+, which naturally tackles the former. This paper not only gives a comprehensive theoretical guarantee using the Rademacher complexity for the new RVFL+, but it also empirically verifies that the newly-derived RVFL+ and KRVFL+ outperform state-of-the-art methods on 14 real-world datasets. The remainder of this paper is organized as follows. The authors brief the related work of the RVFL in Section 2. In Section 3, the authors briefly explain the reason that the RVFL works well for most real-world tasks, and then introduce the newly-derived RVFL+ and KRVFL+. The authors study the statistical property of the RVFL+ and provide a novel tight generalization error bound based on the Rademacher complexity in Section 4. In Section 5, the authors build several experiments on 14 real-world datasets to evaluate the proposed RVFL+ and KRVFL+. This paper concludes in Section 6.", "section": "Related Work", "doi": "10.1016/j.neunet.2019.09.039", "references": [6533109, 176909285, 223114164, 1533861849, 1596717185, 1600810180, 1625958017, 1677182931, 1755117326, 1986278072, 1987618706, 1996640396, 2012638612, 2064675550, 2067562626, 2084251795, 2097998348, 2099579348, 2108544580, 2109574129, 2117138194, 2118181058, 2119821739, 2123223828, 2126885794, 2126942721, 2136331731, 2136922672, 2138383519, 2138580951, 2141225381, 2153232138, 2153635508, 2154209944, 2160354932, 2163605009, 2169408065, 2173379916, 2178357350, 2229668941, 2283039974, 2286961399, 2465334159, 2506404347, 2550643483, 2557283755, 2586160710, 2593382986, 2607586284, 2727059890, 2736488765, 2738226240, 2753648062, 2761362383, 2762962020, 2899470766, 2906146483, 2911964244, 2912162873, 2912934387, 2921936057, 2953267151]}
{"paragraph": "Telecom service provided by the service operator and mobile phone handset made by the handset manufacturer are indispensable and complementary elements when a consumer chooses a mobile phone from the mobile phone market. This complementarity has made it viable for firms to introduce the bundled channel to realize a bulk of sales, that is, consumers buy the contractual handset and the telecom service as a bundled package from either the manufacturer or the operator. At the same time, firms also sell handset models through an unlocked channel, that is, consumers get the unlocked handset from the manufacturer and subscribe to the telecom service from an arbitrary operator separately. In many countries, firms have deployed the dual sales channels to deliver both handset and service to end consumers. For example, in China, many manufacturers’ handset models such as Huawei Honer 7, Samsung Galaxy 7, and Apple iPhone 6s have been sold with subscription to China Unicom's telecom service, and these handset models also have been sold without any telecom service subscription. The dual sales channels will likely cause substitutable competition. It is, therefore, very likely that this phenomenon also occurs in the dual sales channels of the mobile phone industry. In the dual-channel distribution setting, manufacturer and operator provide essentially the same product and service, especially in the functional aspect. Consumers can get what they want from either channel. Thus, the unlocked and bundled channel may compete for the same group of consumers. On the other hand, the perceived experience difference between the unlocked and bundled channel induces consumers to choose the channel that is better suited to their needs. In the bundled channel, for example, operators require that the contractual handset should work exclusively with their own services within 12–36 months so as to gain the future subscriber revenues and offset the cost of subsidies, that is, consumers have to be tied up to the particular operator for a fixed period of time. Because exclusive arrangement is not convenient and the value of convenient plays the most important role in mobile phone usage, people looking for convenience will like the unlocked channel better. The substitutable competition between the unlocked and bundled channel poses vexing challenges for the manufacturer and operator. On one hand, although leveraging the bundled channel may enable greater market penetration, it may cannibalize the sales in the unlocked channel. In such a setting, how players decide their pricing strategies as well as the subsidy strategy to resolve the problem and maximize their profits will be an important factor in their success. On the other hand, the cannibalization of the dual sales channel leads to a more complex channel relationship and causes channel conflict. In general, the channel conflict tends to be a negative force that may hurt profits for all parties. For this, various contracts have been widely implemented for alleviating the channel conflict. For instance, subsidy contract is often implemented as a control mechanism to increase the sales volume as well as to stimulate the manufacturer and operator to collaborate with each other. Thus, how to manage coordinating and contracting problem among the manufacturers and operators becomes increasingly meaningful and challenging. To our knowledge, the extant literature has sparsely addressed the relevant problem for the rapid-developing mobile phone sector, even less effort has been devoted to examining the relevant problems to substitutable competition. This paper intends to contribute to fill this research gap by investigating a range of operational decisions and supply chain coordination issues. More specifically, we propose to answer the following research questions: How do the operator and the manufacturer make their optimal decisions as a whole on bundled and unlocked retail prices in a centralized system with substitutable competition? What are the operator's subsidy policy and the manufacturer's optimal pricing policy in a decentralized system with substitutable competition? Can a subsidy-only contract coordinate the competitive dual sales channels that possess both complementarity and substitutability? If not, what contractual terms should be designed to improve the overall performance? To address these questions, we will start with the demand model setting that can capture the features of the dual sales channels of the mobile phone industry. Then three scenarios are analyzed in a game theoretic framework: centralized scenario, which mainly as a benchmark for coordinating analysis; decentralized scenario, which is presented to verify the coordination failure of the subsidy-only contract; decentralized scenario with a two-way revenue sharing contract, which is presented for designing a properly and practical coordinating contract. Finally, after presenting equilibrium strategies and the relevant sensitivity and coordination analyses, we further analyze the impact of two-way revenue sharing contract and telecom service price on channel profit, as well as the impact of telecom service price on the two-way revenue sharing contract. Our key contributions to the literature are the following. First, this paper enriches the operations management studies of the mobile phone supply chain by analyzing three models in a Stackelberg game theoretical framework, as well as taking into consideration of the substitutable competition. We specifically investigate the channel members’ operational decisions for each model, which are well founded as well as practically relevant to real-world operations in some way. Second, this paper extends the supply chain coordination and contracting literature by modeling a subsidy-only contract and a two-way revenue sharing contract for the mobile phone supply chain. Our result implies that the widely used subsidy-only contract fails to coordinate the mobile phone supply chain due to the competition between the bundled and unlocked channel, thus we propose a two-way revenue sharing contract to achieve the optimal overall supply chain performance. The remainder of the paper is organized as follows. In Section 2, we provide a review of the related literature. Section 3 details the dual sales channels of the mobile phone industry. In Section 4, we present our models and analyses, and then focus on the coordination issues. We conduct the numerical analysis in Section 5, and provide two extensions in Section 6. The summary of our key findings and future research directions are presented in Section 7.", "section": "Methodology", "doi": "10.1111/itor.12451", "references": [1500626379, 1789954534, 1965007576, 1982862737, 2001330444, 2017767555, 2020907614, 2025887266, 2028308112, 2038476837, 2061660901, 2077800159, 2082011667, 2104052944, 2111107475, 2112482196, 2126741666, 2139753030, 2149035559, 2157406401]}
{"paragraph": "Nearest neighbor search is one of the most fundamental tools in many areas of computer science, such as image recognition, machine learning, and computational linguistics. For example, one can use nearest neighbor search on image descriptors such as MNIST to recognize handwritten digits, or one can find semantically similar phrases to a given phrase by applying the word2vec embedding and finding nearest neighbors. The latter can, for example, be used to tag articles on a news website and recommend new articles to readers that have shown an interest in a certain topic. In some cases, a generic nearest neighbor search under a suitable distance or measure of similarity offers surprising quality improvements. In many applications, the data points are described by high-dimensional vectors, usually ranging from 100 to 1000 dimensions. A phenomenon called the curse of dimensionality, the existence of which is also supported by popular algorithmic hardness conjectures, tells us that to obtain the true nearest neighbors, we have to use either linear time in the size of the dataset or time/space that is exponential in the dimensionality of the dataset. In the case of massive high-dimensional datasets, this rules out efficient and exact nearest neighbor search algorithms. To obtain efficient algorithms, research has focused on allowing the returned neighbors to be an approximation of the true nearest neighbors. Usually, this means that the answer to finding the nearest neighbors to a query point is judged by how close in some technical sense the result set is to the set of true nearest neighbors. There exist many different algorithmic techniques for finding approximate nearest neighbors. Classical algorithms such as kd-trees or M-trees can simulate this by terminating the search early, for example shown by Zezula et al. for M-trees. Other techniques build a graph from the dataset, where each vertex is associated with a data point, and a vertex is adjacent to its true nearest neighbors in the data set. Others involve projecting data points into a lower-dimensional space using hashing. A lot of research has been conducted with respect to locality-sensitive hashing, but there exist many other techniques that rely on hashing for finding nearest neighbors. We note that, in the realm of LSH-based techniques, algorithms guarantee sublinear query time, but solve a problem that is only distantly related to finding the k nearest neighbors of a query point. In practice, this could mean that the algorithm runs slower than a linear scan through the data, and counter-measures have to be taken to avoid this behavior. Given the difficulty of the problem of finding nearest neighbors in high-dimensional spaces and the wide range of different solutions at hand, it is natural to ask how these algorithms perform in empirical settings. Fortunately, many of these techniques already have good implementations. This means that a new variant of an existing algorithm can show its worth by comparing itself to the many previous algorithms on a collection of standard benchmark datasets with respect to a collection of quality measures. What often happens, however, is that the evaluation of a new algorithm is based on a small set of competing algorithms and a small number of selected datasets. This approach poses problems for everyone involved: The algorithm’s authors, because competing implementations might be unavailable, they might use other conventions for input data and output of results, or the original paper might omit certain required parameter settings, and even if these are available, exhaustive experimentation can take lots of CPU time. Their reviewers and readers, because experimental results are difficult to reproduce and the selection of datasets and quality measures might appear selective. This paper proposes a way of standardizing benchmarking for nearest neighbor search algorithms, taking into account their properties and quality measures. Our benchmarking framework provides a unified approach to experimentation and comparison with existing work. The framework has already been used for experimental comparison in other papers to refer to parameter choice of algorithms and algorithms have been contributed by the community, e.g., by the authors of NMSLib and FALCONN. An earlier version of our framework is already widely used as a benchmark referred to from other websites.", "section": "Introduction", "doi": "10.1007/978-3-319-68474-1_3", "references": [53016366, 73389528, 145388748, 1430582609, 1627400044, 1870428314, 1970647353, 2055839530, 2070572105, 2075547840, 2084434464, 2084732238, 2085937320, 2086179657, 2118323718, 2122196799, 2147717514, 2149684715, 2153579005, 2157092487, 2157169955, 2165558283, 2183176930, 2318810549, 2397770138, 2480086555, 2523268797, 2532189199, 2537425075, 2736701013, 2752891636, 2756637424, 2757117395, 2774705174, 2963671040]}
{"paragraph": "Clique-width. Every NP-hard graph problem becomes polynomial-time solvable after placing appropriate restrictions on the input. A general method in the design of graph algorithms for special graph classes is to decompose the vertex set of the graph into large sets of “similarly behaving” vertices and to exploit this decomposition algorithmically. An optimal vertex set decomposition gives us the “width” of the graph. Clique-width is one of the most studied width parameters. The vertex set decomposition of clique-width is defined via a graph construction based on vertex labellings. Starting from the empty graph, a graph G is built up vertex-by-vertex using four specific graph operations. These operations ensure that vertices labelled alike will keep the same label and thus “behave” identically. The clique-width of G is the minimum number of different labels needed to construct G in this way. A graph class has bounded clique-width if there exists a constant c such that every graph in the class has clique-width at most c. The algorithmic importance of having bounded clique-width follows from the existence of several meta-theorems which, when combined with an approximation result, ensure that many well-known NP-hard graph problems, such as Graph Colouring and Hamilton Cycle, become polynomial-time solvable for every graph class of bounded clique-width. Hence, there is a need to verify boundedness of clique-width of special graph classes, in particular when undertaking a systematic classification into the computational complexity of graph problems under input restrictions. Well-quasi-orderings. A graph class is well-quasi-ordered by a containment relation if for any infinite sequence of graphs in the class, there is a pair such that one is contained in the other. Just as is the case for having bounded clique-width, being well-quasi-ordered is a highly desirable property, which has been frequently discovered in the areas of discrete mathematics and theoretical computer science. To illustrate its importance, let us mention the seminal project of Robertson and Seymour on graph minors, which culminated in 2004 in the proof of Wagner's conjecture. Wagner's conjecture states that the set of all finite graphs is well-quasi-ordered by the minor relation, a graph H is a minor of a graph G if H can be obtained from G via a sequence of vertex deletions, edge deletions and edge contractions. As an algorithmic consequence, given a minor-closed graph class, it is possible to test in cubic time whether a given graph belongs to this class. To give some more examples, a result of Ding implies that every class of graphs with bounded vertex cover number is well-quasi-ordered by the induced subgraph relation, whereas Mader showed that every class of graphs with bounded feedback vertex number is well-quasi-ordered by the topological minor relation. Fellows, Hermelin and Rosamund simplified the proofs of these results and also showed that every class of graphs of bounded circumference is well-quasi-ordered by the induced minor relation. Furthermore, as “interesting applications” of these three results, they gave linear-time algorithms for recognizing graphs from any topological-minor-closed graph class of bounded feedback vertex number, any induced-minor-closed graph class of bounded circumference, and any induced-subgraph-closed graph class of bounded vertex cover number. Hereditary graph classes. Graph classes closed under taking induced subgraphs are said to be hereditary. Courcelle proved that the class of graphs obtained from graphs of clique-width 3 via one or more edge contractions has unbounded clique-width. This means that the clique-width of a graph can be much smaller than the clique-width of its minors. On the other hand, the clique-width of a graph is at least the clique-width of any of its induced subgraphs. Hence, it is natural to focus on determining boundedness of clique-width for hereditary graph classes. Research goals. Nothing in the definitions of clique-width and well-quasi-orderability suggests that there is anything in common between the two notions. However, as we will discuss, recent results for hereditary graph classes suggested an intriguing connection between them. This connection is not yet well understood, and as such, it remains to be further explored. Our underlying research goals are: to increase understanding of the relation between well-quasi-orderability by the induced subgraph relation and boundedness of clique-width for hereditary graph classes; and to obtain new results for both notions applied to hereditary graph classes. In a previous paper we showed that certain graph operations and graph constructions work equally well for bounded clique-width and well-quasi-orderability. In this paper we will explore common graph techniques further. Before discussing our results in detail, we first discuss a conjecture that motivated our research. Conjecture. We first note that the hereditary class of graphs of degree at most 2 is not well-quasi-ordered by the induced subgraph relation, as it contains the class of cycles, which form an infinite antichain. As every graph of degree at most 2 has clique-width at most 4, having bounded clique-width does not imply well-quasi-orderability by the induced subgraph relation. In 2010, Daligault, Rao and Thomassé asked about the reverse implication: does every hereditary graph class that is well-quasi-ordered by the induced subgraph relation have bounded clique-width? At WG 2015, Lozin, Razgon and Zamaraev gave a negative answer to this question. It is readily seen that a class of graphs is hereditary if and only if it can be characterised by a unique set of minimal forbidden induced subgraphs. As the set of minimal forbidden induced subgraphs in the counter-example is infinite, the question of Daligault, Rao and Thomassé remains open for finitely defined hereditary graph classes, that is, hereditary graph classes for which the set is finite.", "section": "Related Work", "doi": "10.1016/j.jcss.2019.09.001", "references": [52229496, 1546876109, 1581957840, 1785928421, 1967174286, 1977950997, 1988827593, 1993328543, 1994331970, 1996873316, 2005079828, 2007843697, 2008361472, 2020139369, 2026807383, 2052306562, 2053991811, 2063951771, 2067016370, 2070790034, 2076819776, 2097592506, 2097725094, 2110917011, 2115589427, 2116334057, 2122172628, 2147653456, 2148842617, 2167462019, 2623827933, 2733323637, 2762850984, 2962956065, 2963233231, 2963558861, 2963637237, 2964039793, 2964181817]}
{"paragraph": "In 2004 Web 2.0 was launched, marking the beginning of a new era in the history of the internet. This era is characterized by a shift from passive to more active internet consumption, allowing users to develop and disseminate their own content and to communicate. Social media constitutes the most popular platforms for these actions. These are internet-based applications that allow the creation and exchange of user generated content. These applications include blogs, social networking sites such as Facebook, content-sharing sites such as YouTube and more. Since the mid-2000s, social media joined other sources of health information by becoming a platform for posting, sharing or commenting on health-related content and for joining health-related groups. Yet research on health-related internet use has concentrated mostly on searching for online health information and much less on using the internet for other health-related purposes, such as online health services and health participation, which is the focus of this study. The notion of health participation is still poorly defined. Usually, active health-related participation refers to communication regarding health issues. On social media, however, it may include more actions. Three main types of health participation activities are mentioned in the literature: sharing personal experiences regarding chronic health conditions, discussing the work of health institutions, usually by posting of reviews on doctors, and posting or commenting on health-related content. The current study uses the lifestyle/exposure theoretical framework. According to this theory, the probability individuals will behave in a specific way is the result of their lifestyle and habits. Hence, differences in probabilities of engaging in a behavior reflect differences in lifestyle. Since lifestyle behaviors, like any other behavior, are a consequence of an individual's position in the social stratification system, differences in behaviors can be attributed to socioeconomic background. This theory has been tested in the context of offline and online victimization. Nevertheless, it has never been used to explain activities unrelated to victimization, especially those related to health participation via social media. Furthermore, gender differences in health participation have never been examined using this theoretical framework. The issue of gender in health-related use of the internet is well documented in studies in the fields of internet sociology and public health. The most stable finding across studies is that female users search for health information more than male users. Yet, due to the small body of knowledge on gender differences in health participation, the following research question emerges: What is the gender structure of health participation on social media? This question can be broken down into three alternatives: the known gender differences in searching for OHI are replicated in health participation behaviors, the absolute monopoly hypothesis; male users are more active health participants than female users, the areas of control hypothesis; there are no gender differences at all in health participation, the democracy hypothesis. According to the absolute monopoly hypothesis, women are expected to be dominant not only in searching for online health information, but also in the domain of active health participation. The absolute monopoly hypothesis reflects traditional gender role models, according to which women tend to take part in online health-related activities due to their family role of health caregivers and health managers. In contrast, according to the areas of control hypothesis, women and men will have separate domains of dominance in health-related internet use. Female users will be dominant in searching for health information, while male users are expected to be dominant in the health participation domain. This hypothesis is based on the finding that male users tend to be frequent contributors to online discussions, reflecting differences in socialization such that women, as compared to men, are socialized to more passive gender roles. Finally, the democracy hypothesis contends that there are no gender differences in health participation on social media. Some studies found no such differences with respect to communicating on health issues via the internet, thereby supporting this hypothesis. This study makes several contributions to the field. From the theoretical perspective, it extends the lifestyle/exposure theory to cover gender differences in online activities, which are unrelated to the field usually studied in the context of this theory. From the empirical perspective, this study employs a toolkit approach, according to which health participation on social media is considered multidimensional and therefore investigated using several outcomes. The article is organized as follows. First, we provide a general literature review, followed by a discussion of the research hypotheses and other factors that can affect health communication via social media. After that, we describe the methodology used in the study. Then we provide the descriptive, bivariate and multivariate results of the study. Finally, we discuss the results and outline the main conclusions.", "section": "Methodology", "doi": "10.1016/j.chb.2019.08.016", "references": [1961406311, 1969052573, 1973434331, 1981531558, 2026229375, 2028182940, 2029430065, 2029522813, 2029833201, 2054865483, 2071997882, 2092129161, 2110016748, 2131567188, 2169205257, 2399742221, 2559738839, 2765897036, 2775442000]}
{"paragraph": "Exam timetabling is a very important and time-critical task that faces registrars every semester in most educational institutions. This task is very complex and requires assigning a date, time, and room to every exam while ensuring that time, spatial, and other constraints are satisfied. With the increasing number of students, exams, and demands, solving the Exam Timetabling Problem manually is not a practical option and hence providing a computational solution for it attracted the attention of many researchers from the 1960s until recently. The ETP is known to be an NP-complete optimization problem, meaning it is unlikely to solve it optimally in polynomial time. Therefore, researchers tried and are still experimenting with many different ways that are based on mathematical models, heuristic techniques, or a combination of different algorithms to find quick and acceptable solutions for it. For example, in the survey presented in one study, the exam timetabling research was classified based on the techniques used to solve the problems such as: graph based, local search based, population based, and other methods. In summary, the proposed solutions compare to each other based on the used techniques, the hybridization of different methods, the decomposition of the problem into smaller sub-problems, and the hard and soft constraints taken into consideration. In this paper, a new technique is introduced to solve the ETP with emphasis on issues related to German Jordanian University and its set of rules and limiting constraints. The proposed method has three novelties. First, a feasible solution to the problem is found by segmenting it into three phases, which reduces the number of constraints to be considered in each phase and hence allows the optimizer to reach a desired solution in a quick manner with reduced memory demands. Noting that, an Integer Linear Programming based approach is used to find the solution for each phase. Second, a comprehensive set of hard constraints is considered to generate an exams schedule that is comfortable to all students and meets the needs of the different faculties at the university. Third, the same exam can be allocated to one or multiple rooms unlike most of the similar techniques. Similarly to the method in this paper, several other papers discussed integer programming based techniques to solve the ETP. However, to the best of our knowledge, most of those methods tried to solve the problem in one phase, which usually results in a system of equations with a large number of variables and hence solving it is very CPU and memory intensive. Unlike other methods, the hard constraint to prevent a student from having two or more exams in the same day as in this paper was not considered in some prior studies. In one approach, a hybrid adaptive decomposition approach was used to break the exams into difficult and easy sets before using an ILP approach to obtain a solution. Some of the approaches that did not use a mathematical ILP model to solve the ETP will be briefly discussed next. Such approaches either used graph coloring, metaheuristic, hybrid, or other methods to find a solution for the ETP. They can also be categorized into two groups based on whether all the hard constraints in this paper were considered or not. The first group of papers, the group that considered the set of hard constraints as in this paper, is discussed first. In one method, two column generation algorithms were used to solve the ETP. Another study utilized hyper-heuristic approaches. An adaptive linear combination of heuristics with a heuristic modifier under the framework of adaptive strategies was proposed. A search algorithm that consists of several phases is introduced. In the first construction phase, a complete solution is found using an iterative forward search algorithm. In the later phases, a local optimum is found using a combination of a hill climbing algorithm and great deluge technique. Another method hybridized bin packing heuristics to assign exams to time slots and rooms. In another study, the solution is based on graph coloring heuristics that were hybridized to generate four new low level heuristics. In one approach, the ETP was solved using a variable neighborhood search methodology. A random iterative graph based hyper-heuristic was used to produce a collection of heuristic sequences to construct solutions of different quality. Next, the second group of papers, that did not consider all hard constraints used in this paper, is presented. For example, a hybrid bee colony optimization approach was used. In another method, a sequential graph coloring with the largest enrolment first heuristic was used to construct a conflict-free examination timetable and then a simulated annealing heuristic was used to fit examinations into rooms, while satisfying the back-to-back constraint. In other studies, hill climbing and great deluge local search were used to solve the problem. A hybrid harmony search algorithm was used. Decomposition as well as a graph coloring heuristic was used. A hyper-heuristic approach was used. Heuristics and a stochastic algorithm called the roulette wheel graph coloring were used to solve the problem. In that method, the algorithm was also tested on the examination timetabling benchmark datasets. A graph-coloring-based method was utilized, but without considering the actual distribution of exam sessions to rooms. In one approach, the solution of the course-timetabling problem was used to construct an initial solution to the examination timetable. Finally, a hybrid two phase method was introduced to tackle the ETP. The rest of the paper is organized as follows. In Section 2, the hard constraints under consideration and an overview of the proposed method are provided. In Section 3, the adopted nomenclature, problem setup, and decomposing the ETP into three sub-problems are discussed. In Section 4, a simple example is used to show how the problem formulation is represented in AMPL format. In Section 5, the method is validated based on GJU registration information and real-world benchmark data. Finally in Section 6, conclusions and future works are presented.", "section": "Introduction", "doi": "10.1111/itor.12471", "references": [174805181, 178109621, 1526848900, 1964072303, 1966773610, 1969214516, 1977049223, 1982486082, 1984754775, 1989380511, 1995123252, 1996746644, 2010046755, 2014856631, 2031012845, 2038015627, 2047956445, 2056956492, 2074281851, 2082961839, 2092487519, 2121904912, 2293848204, 2586084087]}
{"paragraph": "Computing the Gröbner basis of an ideal with respect to a given term ordering is an essential step in solving systems of polynomials. Certain term orderings, such as the degree reverse lexicographic ordering, tend to make the computation of the Gröbner basis faster. This has been observed empirically since the 1980s and is now supported by theoretical results, at least for some nice families of inputs, such as complete intersections or certain determinantal systems. On the other hand, other orderings, such as the lexicographic ordering, make it easier to find the coordinates of the solutions, or to perform arithmetic operations in the corresponding residue class ring. For instance, for a zero-dimensional radical ideal in generic coordinates, the Gröbner basis of the ideal for the lexicographic ordering has a specific triangular form with all polynomials of degree less than a certain bound and the leading polynomial squarefree; this is known as the shape lemma. The points in the variety are then directly obtained from these polynomials. As a result, the standard approach to solve a zero-dimensional system by means of Gröbner basis algorithms is to first compute a Gröbner basis for a degree ordering and then convert it to a more exploitable output, such as a lexicographic basis. The latter step, while of polynomial complexity, can now be a bottleneck in practice. This paper focuses on this step; in order to describe our contributions, we first discuss previous work on the question. Let the input ideal be zero-dimensional. We assume that we know a monomial basis of the quotient ring together with the multiplication matrices of the variables in this basis. We denote by D the degree of the ideal, which is the vector space dimension of the quotient ring. Starting from a degree Gröbner basis, computing the multiplication matrices efficiently is not a straightforward task. Previous work showed how to do it efficiently, and more recent algorithms achieve faster bounds, at least for some favorable families of inputs. The FGLM algorithm computes the lexicographic Gröbner basis of the ideal using linear algebra operations. While the algorithm has an obvious relation to linear algebra, lowering the runtime was only recently achieved. Polynomials with the triangular shape mentioned earlier form a very useful data structure, but there is no guarantee that the lexicographic Gröbner basis has such a shape. When it does, we say the ideal is in shape position. As an alternative, one may use the Rational Univariate Representation algorithm to describe the zero-set by means of univariate rational functions, where the multiplicity of a root coincides with that of the ideal at the corresponding point. Using rational functions allows one to control precisely the bit-size of their coefficients when working over fields like the rationals. These algorithms rely on duality, which will also be at the core of our algorithms. They compute sequences of values involving trace forms and generic linear combinations of variables. From these values, one may then recover the output using structured linear algebra calculations. A drawback is that we need to know the trace of all elements of the basis, which is polynomial-time computable but not trivial. Randomization can alleviate this issue. It has been shown that computing values involving traces of random linear forms allows one to deduce a description of the variety by a set of rational functions with univariate polynomial denominators. The tuple computed by such algorithms is called a zero-dimensional parametrization of the variety. It generally differs from a Rational Univariate Representation, since the latter keeps track of multiplicities. Starting from such a parametrization, we can reconstruct the local structure of the ideal at its roots using additional algorithms. The most costly part of the randomized algorithm is computing the trace values; the rest involves standard univariate techniques. It was later pointed out that the multiplication matrices can often be expected to be sparse, and estimates on their sparsity have been given assuming the validity of certain conjectures. Based on this, sparse variants of FGLM have been designed. For example, if the ideal is in shape position, one can recover its lexicographic basis using trace values involving a linear form. For less favorable inputs, these algorithms fall back on more general methods. The techniques used are based on Krylov subspace methods and Berlekamp-Massey techniques, similar to those used for solving sparse linear systems and problems in integer factorization or discrete logarithm computations. Block versions of these methods allow for parallelization and were pioneered in that context. It is natural to adapt this strategy to Gröbner basis computations. One prior work showed how to compute the main univariate polynomial using such block techniques, assuming the base field is finite. The algorithm computes the roots of this polynomial and substitutes them into the system before computing a Gröbner basis for each root. Our first contribution is to give a block version of the randomized algorithm that extends the previous approach to compute all polynomials in the parametrization for essentially the same cost as computing the univariate polynomial. More precisely, the bottleneck of the algorithm is the computation of a block-Krylov sequence; once this sequence is computed, all polynomials in the zero-dimensional parametrization can be obtained. Unlike some previous algorithms, ours deals with any zero-dimensional ideal, not just those in shape position, though it assumes the base field has sufficiently large characteristic. The output is somewhat weaker than a Gröbner basis, as multiplicities are not computed. While we focus on sparse multiplication matrices, we also analyze the dense case. Our second contribution refines the first by trying to avoid computations with a generic linear form, motivated by the fact that its multiplication matrix is often denser. The refined algorithm first computes a zero-dimensional parametrization of a subset of the variety for which we can take a simple variable, and then applies the general algorithm to the rest. If the initial subset is large, this is expected to provide a speed-up. For experiments, our algorithms have been implemented in C++ using standard linear algebra and symbolic computation libraries. The paper is organized as follows. The next section reviews results on scalar and matrix recurrent sequences and gives an algorithm to compute a scalar numerator. Section 3 describes sequences arising in FGLM-like algorithms and proves some refined versions of earlier results. The main algorithm is given in Section 4, and the refinement is in Section 5. An appendix provides proofs of technical results on recurrent sequences. Complexity model: we count basic operations at unit cost. Most algorithms are randomized and involve selecting a vector of field elements; success is guaranteed if the vector avoids a certain hypersurface in parameter space. Suppose the input ideal is generated by a set of polynomials. Given a zero-dimensional parametrization found by our algorithms, one can always evaluate those polynomials at the parametrization, modulo the univariate polynomial. This allows us to check whether the output describes a subset of the variety, but not whether we have found all solutions. If the parametrization has the same dimension as the quotient ring, we can infer that we have all solutions and that the ideal is radical, so the output is correct. In what follows, we assume that the number of variables equals the dimension of the quotient ring, to simplify cost estimates. We use standard bounds for polynomial multiplication, and assume super-linearity of the time function. Then two matrices over the field with polynomials of degree less than a bound can be multiplied efficiently. Acknowledgments: We thank several researchers for helpful discussions and a reviewer for useful remarks. This research was partially supported by grants and institutional programs.", "section": "Related Work", "doi": "10.1016/j.jsc.2019.07.010", "references": [149384130, 1502078890, 1562183207, 1590894046, 1968561983, 1976677460, 1979462002, 1985924702, 1986421748, 1986905837, 1995653528, 2001224529, 2003767038, 2011693299, 2015497178, 2029891988, 2057341276, 2059454546, 2059522106, 2071483197, 2081523647, 2088713140, 2153077274, 2161330625, 2238531909, 2729305841]}
{"paragraph": "One of the central problems in elimination theory is the construction of determinantal formulae for the resultant. In this context there is also a special emphasis on exploiting the sparsity of the input, or in other words the support, of the involved polynomials. Among the various constructions the best we can hope for is a degree-one formula; that is a matrix whose non-zero entries are coefficients of the input polynomials, and whose determinant is equal to the resultant. The Sylvester-type formulae fall in this category. Unfortunately, such formulae do not always exist for given Newton polytopes. There are also Bézout-type formulae where the entries of the matrix are coefficients of the Bézoutian polynomial and thus they are high degree polynomials in the coefficients of the input polynomials. We call the matrices that have entries that are both Sylvester-type and Bézoutian-type, hybrid. We focus on resultants and discriminants for polynomial systems in two variables. A polynomial system is unmixed if all of its polynomials have the same Newton polytope, and mixed otherwise. Exact resultant formulae are mostly known for certain classes of unmixed systems, and very little is known for the general mixed case. We are interested in optimal degree-one formulae for the mixed resultant. Degree-one formulae are very convenient for both the analysis and the implementation of resultant methods, since their matrix expressions are simple to compute and have bitsize that matches the bitsize of the input. Common degree-one formulae are the Sylvester-type formulae; in this work we present a different one that expresses a second order Koszul map. Khetan presented explicit exact formulae for an arbitrary unmixed sparse bivariate polynomial system. His determinantal formula is a hybrid Sylvester and Bézout type. Also in the unmixed case there are necessary and sufficient conditions for the Dixon resultant formulation to produce the resultant. In the same context, Elkadi and Galligo proposed to use a variable substitution and two iterated resultants to compute the resultant polynomial. Regarding Sylvester-type formulae, matrices expressing optimally the resultant of unmixed bivariate polynomials with corner-cut support are found. Moreover, Sylvester formulae for more general unmixed bivariate systems were depicted. The proof of the main theorem makes use of tools from algebraic geometry, including sheaf cohomology on toric varieties and Weyman's resultant complex. There are also methods for constructing resultant matrices for bivariate polynomial systems that combine Sylvester type blocks with toric Jacobian blocks in the case where the Newton polytopes of the polynomials are scaled copies of a single polygon. The determinant of these matrices is a multiple of the sparse resultant, that is, the formula might not be optimal. Resultants are closely related to discriminants. Discriminants have many applications, ranging from singularity theory of partial differential equations to the computation of the Voronoi diagram of curved objects. Especially for the bivariate case there is work that relates the mixed discriminant with the sparse resultant and the toric Jacobian. Tensor-product systems fall in the general category of multihomogeneous systems. One finds the first expressions of the resultant of such systems as the determinant of a matrix. For unmixed multigraded systems Sturmfels and Zelevinski provided optimal Sylvester-type formulae. These formulae arise as certain choices of a Weyman complex of modules. Many, if not all, classical resultant matrices are instances of such complexes, including the projective resultant. There is a systematic exploration of possible determinantal complexes, and also a software package that produces formulae for unmixed and even scaled resultants. Interestingly, there is a plethora of hybrid resultant matrices that consist of Bézout-type and Sylvester-type blocks. The main contributions of this work are as follows. We present a determinantal degree-one formula for the resultant of arbitrary mixed bivariate tensor-product systems. The formula applies without any restrictions on the bidegree of the polynomials, it expresses a Koszul map and has degree one with respect to the coefficients of the system. Moreover, we prove that the univariate and the bivariate case are the only cases among tensor-product polynomials which admit an unconditional formula of degree one. We provide a constructive method to compute this matrix explicitly, by identifying the dual multiplication maps, therefore making our formula explicit. We call the matrix Koszul resultant matrix. The latter construction allows us to derive formulae for computing the discriminant of one or two bivariate tensor-product polynomials. Compared to the existing literature, we reduce significantly the degree of the extraneous factor which is involved in the computation. Another important aspect is that our formulae provided are free of nonzero multiplicative constant, so that they yield smoothness criteria that are valid in arbitrary characteristic. The rest of the paper is organized as follows. In the next section we present preliminary results that we need for our construction. In Section 3 we present the mixed resultant complex for the bivariate case and we derive the determinantal Koszul formula. Moreover, we show that universal degree-one formulae arise for at most two variables. In Section 4 we provide the algorithmic construction of the Koszul resultant matrix, by identifying the cohomology groups which appear in the complex. Finally, in Section 5 we tackle the problem of computing some related discriminants and mixed discriminants. Throughout the paper, several toy examples accompany the main results. A preliminary version of this paper appeared earlier. In the current final version the Sections 4 and 5 are expanded significantly, and in particular the computation of mixed discriminants is improved by relating them with mixed resultants of lower degree.", "section": "Methodology", "doi": "10.1016/j.jsc.2019.07.007", "references": [1857034947, 1963919699, 1966341842, 1968297701, 1971439379, 2025561788, 2027815348, 2048329955, 2058393311, 2061296216, 2066758283, 2067754443, 2088129367, 2108888104, 2149153582, 2618054456, 2952936986]}
{"paragraph": "Analyzing the image sequences captured from surveillance cameras has become an interesting research area in modern computer vision. The analysis of daytime images has been well studied in the past few years. However, researchers cannot use nighttime images to extract enough useful features and further information due to insufficient brightness. In fact, nighttime images have salient visual features which can be used to achieve many tasks. Repetitive window lights can be used to predict a planar surface; the light which changes over time reflects the laws of peoples’ work and rest habits; the spatial distribution of light can be used for recognition and classification of various locations. These salient light features make nighttime images distinctly different from daytime images, and we should fully excavate these rich data sources. Furthermore, the images captured from one static camera are nearly pixel-level aligned. The analyzed results from nighttime images can be used to verify those from daytime images, and a comprehensive analysis makes the result more reliable. In conclusion, the analysis of nighttime images has significance in computer vision. The results help expand the scope of visual information acquisition and open up a new door of thought for further big data analysis. In this paper, we focus on geometric analysis of a single nighttime image and explore the repetitive light structures on façades. These structures form regular grids or lattice structures. We apply these light cues to parse night scenes into façade planar surfaces in man-made scenes and estimate the surface depth based on the assumption of consistent storey heights. The method presented in this paper is an automatic algorithm to generate discrete planar surfaces. It includes three contributions to 3D parsing of a single-view night images in man-made environments, which mainly include: the first attempt to classify salient window lights in a nighttime image to detect planes; distinguish multiple façades of a single image effectively, and calculate their floor structures; an orientation estimation with low error is obtained without using any explicit geometric information, which is essential in other extraction methods applied to planar structures.", "section": "Introduction", "doi": "10.1007/s11704-017-6457-2", "references": [1500364503, 1521461170, 1795815886, 1989231886, 2037935589, 2042518273, 2063784584, 2077456032, 2095339704, 2102532099, 2116851763, 2122059483, 2124404372, 2131394160, 2155871590, 2160341214, 2162808897, 2167716896, 2203760849, 2244684328, 2906621894]}
{"paragraph": "Multi-view data are very common in some scientific data analytics problems such as computer video, social computing and environmental sciences, due to the use of different measuring methods or of different media, like text, video and audio. Multi-view clustering, which makes use of the complementary information embedded in multiple views to improve clustering performance, has attracted more and more attentions. In the existing methods, spectral clustering is a popular one for multi-view data because it represents multi-view data via graph structure and makes it possible to handle complex data such as high-dimensional and heterogeneous as well as it can easily use the pairwise constraint information provided by users. In some practical applications, particularly in the multimedia domain, a view is usually represented in a high-dimensional feature space. For high-dimensional data, the feature distribution is usually more sparse, the traditional similarity measurement methods based on distance measures become inapplicable. In order to solve this problem, the approaches based on low-rank representation try to learn a common low-dimensional subspace from the high-dimensional multi-view data and each object can be represented linearly by others objects in the same subspace, which contributes to reduce the computation cost and improves the robustness to noise corruptions. On the other hand, we find that a problem of the existing cluster analysis methods has not been addressed satisfactorily is the uncertain relationship between an object and a cluster. It is obviously that there are three relationships between an object and a cluster, namely, belong-to definitely, not belong-to definitely and uncertain. In most of the existing work, a cluster is represented by a single set, the set naturally divides the space into two regions. Objects belong to the cluster if they are in the set, otherwise they do not. Here, only two relationships are considered, no matter in hard clustering or in soft clustering. They are typically based on two-way decisions. Let us observe the third relationship, which means the object may or may not belong to the cluster. We just cannot make decisions based on the present obtained knowledge or information. We can make further certain decisions when we have further information. It is a typical idea of three-way decisions. Inspired by the theory of three-way decisions, a framework of three-way cluster analysis has been introduced. The previous results on three-way clustering provide us with a tool for studying the problem of clustering with uncertainty. In this paper, we focus on a general framework based on the theory of three-way decisions, which is appropriate for soft clustering or hard clustering. This three-way representation with two sets brings more insight into interpretation of clusters. Objects in the core region certainly belong to the cluster, objects in the trivial region definitively do not belong to the cluster, and objects in the fringe region maybe or may not belong to the cluster. Obviously, the two-way representation with a single set is a special case of three-way representation with two sets when fringe regions are empty. Compared with the supervised learning, clustering process lacks the user guidance or the class label information and may not produce the desired clusters. Thus, some semi-supervised clustering methods are proposed. These methods that use certain weak supervision form, such as pairwise constraints, can significantly improve the quality of unsupervised clustering. Pairwise constraints describe two objects whether they should be assigned to the same cluster or the different clusters. However, choosing the supervised information is random in most of existing methods, and it does not produce positive effect on improving the clustering result when the algorithm itself can find the prior information or there are amounts of noises in the prior information. Therefore, the active learning method is introduced to optimize the selection of the constraints for semi-supervised clustering. Hence, our work considers active learning of constraints in an iterative framework based on spectral clustering. In each iteration, we determine objects with the most important information toward improving the current clustering result and form queries accordingly instead of choosing the information randomly. The responses to the queries are then used to update the clustering. This process repeats until we reach a stable solution or we reach the maximum number of queries allowed. Such an iterative framework is widely used in active learning for semi-supervised clustering. The measurement of information is designed based on the entropy concept. Besides, to take the advantage of three-way representation of clustering, it is reasonable to choose pairwise constraints in fringe regions instead of the universe, which will improve the search efficiency. In this paper, we address the problem of clustering on multi-view data of high dimensionality. The main contributions are summarized as follows: A novel active three-way clustering method via low-rank matrices for multi-view data is proposed. It considers the diversity of multiple views and to improve the quality of clustering for multi-view data. A multi-view information fusion algorithm is presented via low-rank matrix representation for high-dimensional multi-view data, and the weights are adjusted adaptively on each view during solving the optimization problem of objective function. A three-way clustering representation is utilized to reflect the three relationships between an object and a cluster, namely, belong-to definitely, uncertain and not belong-to definitely. The three-way clustering approach provides us with a tool for studying the problem of clustering with uncertainty. An active three-way clustering algorithm is developed by taking the advantage of three-way representation of clustering, which can produce the three-way results as well as two-way results accordingly. The idea of farthest-first traversal scheme is used to construct the cores of clusters, then to expand cores to fringes by using the idea of k nearest neighbors, and the rules to adjust the consensus similarity matrix are also introduced. The proposed method is flexible for semi-supervised clustering as well as unsupervised clustering. We evaluate the proposed method with some other algorithms on seven real-world datasets. The results of comparative experiments demonstrate the effectiveness of the proposed method and show that it is appropriate for multi-view data of high dimensionality. The rest of the paper is organized as follows. In Section 2, we review existing clustering algorithms for multi-view data, clustering approaches for uncertain relationships and active learning approaches for clustering. In Section 3, we give a detailed description of the proposed method termed Active Three-way Clustering via Low-rank Matrices. In Section 4, we report on an extensive experimental evaluation of the proposed method. Finally, we summarize the present study in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.03.009", "references": [101345952, 201974436, 1041128124, 1511669739, 1558962159, 1596382552, 1670132599, 1965450189, 1997201895, 2004524042, 2013029404, 2016384870, 2023750115, 2029677459, 2056187338, 2070412788, 2077775960, 2079402740, 2080404663, 2085989833, 2088431846, 2103972604, 2105709960, 2135914502, 2137184539, 2142674578, 2153839362, 2154415691, 2159091719, 2165874743, 2166782149, 2169529055, 2210977594, 2294062232, 2297889545, 2405459681, 2512066509, 2518081091, 2527290711, 2716026328, 2951085447]}
{"paragraph": "In everyday life, we are surrounded by textual information: academic papers, technical reports, patents, business contracts, application forms for business procedures, presentation slides, emails, etc. Working with documents is ubiquitous in our work life, as well as our personal life. When working with documents, people frequently print them, not only for the ease of reading, but also for comparing information from different documents. New digital environments, such as tabletop systems, allow documents to be displayed in a similar way to a sheet of paper on a table. However, we need to understand how to optimize the reading and writing experience in tabletop systems for creating efficient and comfortable work environments. This work focuses on investigating preferred orientation of paper documents in relation to readers during reading, so readers can maintain a comfortable body posture and can easily handle documents while performing the reading as efficiently as possible. Informal observations of people reading physical paper documents show that right-handed people often rotate documents counter-clockwise and/or place documents to the right side of the body. When actively working with a document, such as writing, annotating or drawing, it is often rotated to a steeper angle. People rarely read documents placed at the central or median line of their body without any rotation. Scientific studies have reported on document positions, for instance, it was noted that readers rotated their working documents 0–20° when reading and 30–45° when writing. It was also observed that artists continuously rotated and adjusted the drawing bit-by-bit to keep the active drawing area in the same position relative to the hand. Another study reported that participants felt uncomfortable when reading on horizontal displays because the document was presented at an unnatural reading angle. It was observed that the orientation of documents or other objects, such as photos, could signal users’ intention to share them during collaborative work. For example, users indicated that they were not ready to share an object by placing it upside-down or at an odd angle. The document orientation can also serve as a barrier around a personal workspace, or as a prompt to others to view it. A few tabletop systems have specifically been developed to support reading or small group discussions around text documents. In these situations, being able to adjust a document rotation and location is important for creating a comfortable work environment. However, most of these tabletop systems assume that people prefer reading a document aligned to their median line. For instance, many systems provide a feature to automatically rotate documents to be perpendicular to the table’s closest edge. Some tabletop systems allow users to manually orient the documents. However, if the optimal document orientation is known, systems can automatically position documents according to users’ location and task. If reading and writing performance varies with document orientations, automatic positioning of documents can be an important feature for task efficiency. In this paper, we establish the optimal document orientation and investigate reasons for users’ preferences. We conducted five experiments to uncover both subjectively preferred and objectively optimal document orientation for left-handed and right-handed people. Although we do not use a tabletop device, our results can be applicable to tabletop applications as people’s activity with the document and their position relative to it remains the same. Therefore, we finally discuss how these results impact these systems.", "section": "Methodology", "doi": "10.1080/07370024.2018.1427584", "references": [1269801495, 1992306856, 1998408655, 2005855683, 2007580360, 2009220433, 2021346535, 2030533114, 2037001467, 2079876841, 2082591969, 2094567668, 2094982166, 2101082575, 2101532415, 2102773632, 2106779688, 2121172310, 2122624458, 2130334143, 2135718368, 2137092817, 2140706899, 2148969329, 2151942546, 2155811000, 2156317066, 2158165714, 2158707444, 2169463011]}
{"paragraph": "Pattern discovery has been widely studied in machine learning tasks such as image recognition and text analysis. A sequential pattern is a totally or partially ordered subsequence of a transactional dataset, symbolic sequence, numeric time series, or other data sequence. Sequential pattern discovery, also called pattern mining, refers to the discovery of all frequent sequential patterns from these data. For transactional datasets such as retailing, market-baskets, and planning, a pattern is a sequence of events, where each event is represented by an itemset. This type of representation has horizontal and vertical scalability, leading to general association rules. For symbolic sequences such as protein and text, a pattern is a subsequence of characters, also called a string. It can easily represent codons or keywords. For numeric time series such as stock prices and petroleum production, where the trend of fluctuation is essential to the stock holders and petroleum cooperation, a pattern is a subsequence of real values. One interesting research direction of symbolic sequential pattern discovery involves the generalization of patterns. A plain pattern is a subsequence that must be exactly matched. A wildcard, which is also called a motif, matches any character in the alphabet. A wildcard gap matches any subsequence within the length constraint. In this way, a pattern with wildcard gaps is able to handle noise or shifts. To enrich the semantics of the pattern, the alphabet is divided into weak and strong parts. A weak-wildcard gap matches a subsequence of weak characters. Consequently, a pattern with weak-wildcard gaps not only ignores weak characters, but also maintains strong ones. In this paper, we introduce a more general and flexible type of pattern called a tri-pattern that is inspired by three-way decisions. The alphabet is partitioned into three parts corresponding to strong, medium, and weak characters, respectively. In this situation, a tri-wildcard gap matches any sequence of medium or weak characters with a length between N and M. A tri-pattern is a sequence of strong or medium characters containing periodic tri-wildcard gaps, where the term periodic indicates that the gaps between any two adjacent characters are identical. The idea of a tri-partition has been widely adopted in many applications. Biologists partition the human amino acid alphabet into essential, conditional, and nonessential amino acids. Petroleum experts partition oil production fluctuation into significant, insignificant, and minor changes. The tri-partition of situations and actions also has attracted a large amount of research interest in data mining, especially three-way decisions. To the best of our knowledge, however, a tri-partition of an alphabet has not been considered in the three-way decisions research community. Tri-patterns are more general than the four existing types of patterns. Let tri-patterns be Type I patterns, plain patterns be Type II patterns, patterns with periodic wildcard gaps be Type III patterns, patterns with periodic weak-wildcard gaps be Type IV patterns, and strong patterns with periodic weak-wildcard gaps be Type V patterns. Types II through V are the special cases of Type I. Similar to existing frequent pattern discovery problems, a new problem for Type I patterns is defined. Using the Apriori property of the new problem, an Apriori algorithm is designed for efficient pattern discovery and tree pruning. We compare these five types of patterns in three application areas to reveal the universality of the new pattern. The first application is human protein sequence mining. Essential, conditional, and nonessential amino acids correspond to strong, medium, and weak characters, respectively. Twenty sequences were concatenated to discover frequent patterns. These patterns and their popularity in the original sequences were analyzed. Tri-patterns are the most meaningful type, while the other types, especially Type II, suffer from too many weak characters. For the pattern popularity, tri-patterns have the best minimal and second-best average performance. The second application is oil well daily production time series analysis. A coding table was designed to convert the numeric time series into a nominal sequence. Significant, insignificant, and minor changes correspond to strong, medium, and weak characters, respectively. We used two years of well data to discover frequent patterns, which are finally matched in the original time series. Compared with distance-based approaches that analyze numeric time series directly, the coding and mining approach handles different levels of fluctuations more easily. Observation on the original time series shows that tri-patterns match some similar subsequences with minor differences. In contrast, the four existing types of patterns either exclude some similar subsequences or include dissimilar ones. The third application is forged Chinese text mining. Notional words, function words, and special characters correspond to strong, medium, and weak characters, respectively. Four sets of text for news, novels, history, and law were collected. Forged Chinese text occurs when some text creators distribute advertisements for illegal items such as forged money or invoices. They may insert some characters into the text that makes the text still readable, but difficult for automated analysis. In this way, the text can avoid spam filters and similar blockers. Our purpose is to extract keywords from the forged text that are also keywords in the original text. This problem is closely related to fuzzy text matching and keyword extraction. With tri-patterns, the algorithm achieves the highest accuracy in mining keywords from forged text. The remainder of this paper is organized as follows. Section 2 reviews some related work. Section 3 defines the new types of pattern, analyzes the relationships among the five types, and presents the new pattern discovery problem. Section 4 demonstrates the Apriori property and proposes the new algorithm. Section 5 explains the experimental settings and results on three kinds of real-world data. Finally, the concluding remarks are discussed in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.04.013", "references": [1877823, 964455177, 1208197292, 1511669739, 1608194207, 1632950138, 1969463949, 1974461203, 1980120082, 1985716338, 1988757176, 1997362234, 2006873874, 2009418433, 2029677459, 2048472139, 2050385127, 2059429344, 2090509093, 2118376687, 2126452743, 2136634369, 2143507938, 2151077154, 2158454296, 2217596628, 2310435784, 2340020088, 2344564588, 2345465422, 2506225748, 2515543847, 2551757246, 2566882407, 2580172056, 2616721392, 2727405836, 2762194741]}
{"paragraph": "Compared to induction motor, the PMSM presents several advantages, such as high efficiency, high reliability, high power density and high torque to current ratio. Therefore, PMSM is used in wide applications, such as electric and hybrid vehicles, high-end white goods (refrigerators, washing machines, dishwashers, etc.), high-end pumps, fans and in other appliances. Despite its advantages, the PMSM remains complicated and difficult to control when good transient performance under all operating conditions is desired. This is due to the fact that the PMSM is a nonlinear, multivariable, time varying system subjected to unknown disturbances and variable parameters. Over the past decades, various control techniques have been developed in order to improve the PMSM performance. However, the widely used approach consists in using linear field oriented control theory. Another popular control technique, the DTC has a relatively simple control structure yet performs at least as good as the FOC technique. However, DTC involves hysteresis type, for both stator flux and torque magnitudes control, which introduces variable and uncontrollable switching frequency; which in turn produces large torque and flux ripples and high current distortion. Many artificial intelligence techniques and random search methods have been employed to improve the controller performances. Neural network, fuzzy system, and genetic algorithm have been widely applied to proper tuning of PID controller parameters. But all have some shortages. GA has a big computational complexity. Fuzzy system itself has many parameters to be optimized, the results of these experiments showed that fuzzy controllers perform better, or at least well as, adaptive controllers. Moreover, this technique offers the advantage of requiring only a simple mathematical model to formulate the algorithm, which can easily be implemented by a digital computer. These features are appreciated for nonlinear processes for which there is no reliable model and complex systems where the model is useless due to the large number of equations involved. Nevertheless, the main problem with fuzzy logic is that there is no systematic procedure for the design of fuzzy controller. The superiority of fuzzy controller, it can adapt his structure, acting on number of factors which constitute the internal configuration of this type of controller, such as: Fuzzification blocks, Fuzzy rules, block defuzzification and input, output gains. Additionally, it is possible to use the fuzzy logic to adjust or supervise the parameters of traditional PI regulator. Particle swarm optimization is a stochastic global optimization technique. The PSO technique can generate a high-quality solution within shorter calculation time and stable convergence characteristic than other stochastic methods. Because the PSO method is an excellent optimization methodology and a promising approach for solving the optimal PI controller parameters problem. In this paper, a direct torque controlled PMSM based on the AFLC and the PSO is presented. A comparison study between these techniques is also presented. To show the performances of the DTC of the PMSM based on the AFLC and the PSO, simulation results are presented. To validate the simulation results, these algorithms are implemented on a test bench around a DSPACE 1104. These techniques show high performances compared to the conventional DTC. Comparing these techniques, we see that the DTC based on the PSO is more efficient than the DTC based on the AFLC.", "section": "Related Work", "doi": "10.1016/j.matcom.2018.04.010", "references": [1854387882, 1968093301, 1969900902, 2008059784, 2016150415, 2030282996, 2032649063, 2032775349, 2045059098, 2049768223, 2051736125, 2062238863, 2064981932, 2072662494, 2081696533, 2086068525, 2099829287, 2115521895, 2153587688, 2180382463]}
{"paragraph": "Recent years have seen a growing interest in text mining applications aimed at uncovering public opinions and social trends. This is partially driven by the fact that the Web now holds a large number of opinionated documents, such as opinion pieces and product reviews, to name a few. An additional driver is that the language one uses to express opinion indicates one’s subjective viewpoints; this language can be used to understand and cluster people’s opinion based on belief, experience or emotion, rather than facts. Text mining methods are therefore desired for facilitating automatic discovery of subjective viewpoints present in such large amounts of opinionated documents. We define contrastive opinion mining as the discovery of opinion perspectives held by different individuals or groups, which are related to a given topic but opposite in terms of sentiments. The usefulness of contrastive opinion mining spans across many applications such as discovering the public’s stand on major socio-political events, observing heated debates over controversial issues where different sides defend their viewpoints with contrasting statements, as well as mining issues from product review sites that can serve as an important source of feedback to businesses. For example, there were heated discussions on the web about whether one should install the Mac OS X El Capitan soon after it was released to the public. People express highly controversial opinions after upgrading to the system, i.e., some experienced pleasant performance improvements while others witnessed a significant drop in speed. Considering the huge number of reviews available, it is highly desirable to acquire an overview of the major viewpoints from large amounts of text data automatically, allowing one to convert data into actionable knowledge and then make decisions in a timely manner. Recently, mining contrastive opinions has been applied to a variety of tasks, including analysing editorial differences between multiple media sources, extracting contrastive viewpoints from political debates, as well as examining cross-cultural differences with respect to language use on social media. However, these existing studies on contrastive opinion mining rely on an assumption that input data containing different opinion perspectives are separated into different collections beforehand. While this assumption might hold for some practical scenarios, quite often one needs to analyse contrastive opinion contained in a single collection such as text of streaming social media data. In addition, it is natural that debates on some topics are more prominent or controversial than others, which indicates the importance of the topic. Therefore, being able to understand the prominence of a topic and the levels of contrastiveness of sentiment will enable one to quickly identify information that needs immediate attention. Finally, existing models generally interpret contrastive opinions solely in terms of the extracted topic words, which are not adequate to help us accurately understand the opinions presented in the corpus since the topic words only express shallow semantics. Therefore, it would be illuminating to consider the dependency between the sentences in the corpus and the topic of discussion in order to better understand and interpret contrastive opinion. The representative sentences also help to clarify the coherence of the extracted topics. In this paper, we address the aforementioned issues by proposing a novel unified latent variable model for mining contrastive opinion from text collections. The proposed model makes several distinctive contributions, for it: can be trained flexibly under weakly-supervised or fully-supervised settings, depending on the type of supervision information available; automatically discovers contrastive opinion from both single and multiple text collections; quantifies the strength of opinion contrastiveness towards the topic of interest, which could allow one to swiftly flag issues that require immediate attention; and extracts sentences relevant to topics by adopting a strategy that makes sentiment-bearing topics clearer to users. Extensive experimental results show that our model outperforms several baseline models in terms of extracting coherent and distinctive sentiment-bearing topics which express contrastive opinions. The top sentences extracted by our approach further help us effectively understand and interpret sentiment-bearing topics. Lastly, we evaluate the performance of our model in the supervised sentiment and topic classification task, in which our model outperforms or gives comparable performance to five strong supervised baselines. The rest of the paper is organised as follows. We first review the related work in Section 2, followed by detailed discussion of our model in Section 3. Section 4 and Section 5 present the experimental setup and results, respectively. Finally we conclude the paper in Section 6.", "section": "Methodology", "doi": "10.1007/s11704-018-7073-5", "references": [137786571, 1054465322, 1536494821, 1569041844, 1570776870, 1969486090, 2018650704, 2023907142, 2045818565, 2061806977, 2062589306, 2098062695, 2102498033, 2103587173, 2106224734, 2108420397, 2112247328, 2123751690, 2126200466, 2130339025, 2147946282, 2159426623, 2250879034, 2251582277, 2329121264, 2463177446, 2573269214, 2620851716, 2774886230, 2949541494]}
{"paragraph": "Clustering is an unsupervised learning technique to find natural groups that are implicated in data. Its main task is to partition unlabeled patterns into subgroups such that the patterns in the same cluster are expected to have the highest similarities and the patterns between different clusters are expected to have the highest dissimilarities. In this manner, the data structure that is revealed by a clustering method is expected to reflect the natural geometry of the data as much as possible. Hard C-means is one of the foremost objective function-based clustering methods in which the degree of each pattern belonging to each cluster is 0 or 1. The validity of HCM will degenerate when dealing with patterns with overlapping areas. Fuzzy clustering, especially fuzzy C-means, as one extension of HCM, utilizes a partition matrix to evaluate the degree of each pattern belonging to each cluster, so that the overlapping partitions can be described effectively. The main challenge of FCM is the sensitivities to noisy patterns which may contaminate the calculations of the corresponding prototypes and membership degrees. Based on rough set theory, which aims at analyzing data involving uncertain, imprecise or incomplete information, a rough C-means clustering method was proposed. Each cluster is described not only by a prototype, but also with a pair of lower and upper approximations. Meanwhile, the boundary region is defined as the difference between the lower and upper approximations. The uncertainty and vagueness arising in the boundary region of each cluster can be captured well in RCM. Since no membership degrees are involved, the closeness of patterns to clusters cannot be detected. Rough sets and fuzzy sets, as two important paradigms of granular computing, are strongly complementary to each other. Incorporating with fuzzy membership degrees, a rough-fuzzy C-means clustering method was presented which integrated the merits of both fuzzy sets and rough sets. The lower and upper approximations are determined according to the membership degrees, rather than the individual absolute distances between a pattern and its neighbors. A robust rough-fuzzy C-means algorithm was further proposed that integrated both probabilistic and possibilistic memberships of fuzzy sets, which could handle overlapping clusters in noisy environments as well as the uncertainty and vagueness in cluster definitions due to involving rough sets. No matter which rough-fuzzy partitive clustering methods are used, some model parameters are involved: the weighted values that evaluate the contributions of lower approximations and boundary regions when calculating new prototypes; the threshold that determines the lower approximation and boundary region of each cluster; the value of fuzzifier parameter m that controls the shape of memberships. Since the contributions of lower approximations are considered more important than the contributions of boundary regions as computing the prototypes, the weighted value for lower approximations is much higher, and its complementarity is applied for the boundary regions. Meanwhile, through the combinational adjustments of the threshold and the fuzzifier, the influence caused by the weighted values can be reduced. In this case, these parameters will take over the effect of weighted values. The threshold that determines the approximation regions of each cluster is often selected depending on subjective tuning in the available research. Different strategies have chosen this value as the average value or the median of the difference between the highest and second highest fuzzy memberships of all the patterns. However, the same threshold is employed for all clusters though the sizes and the densities of the clusters may be significantly diverse. Additionally, the approximation regions are partitioned based on the absolute distances or membership degrees of individual patterns, not the global observation on data for a specific cluster, then the topology of data cannot be detected well with respect to this cluster. Shadowed sets, as a bridge between rough sets and fuzzy sets, provide a new scheme to granular computing. It is an example of three-way, three-valued, or three-region approximations of a fuzzy set according to the framework of three-way decisions which is another new paradigm of granular computing and can provide favorable semantical interpretation and generalized optimization methodology for determining partition threshold values. A shadowed set-based rough-fuzzy C-means method was introduced which gave a technique of automatic selection of the partition threshold. No matter which selection methods are used, an unreasonable threshold will result in undesired approximation region partitions, and then the prototype calculations may be spoiled. The value of fuzzifier parameter m is very important for the updating of prototypes and the corresponding partition matrix. A predefined value of m is often used in the rough set and fuzzy set-based clustering methods. However, it is difficult to express the uncertain notion of fuzziness in a given data set using a single fuzzifier value. To manage the uncertainty generated by the fuzzifier parameter m, an interval type-2 fuzzy C-means was proposed which extended a pattern set to interval type-2 fuzzy sets using a pair of fuzzifier values that created a footprint of uncertainty for the fuzzifier parameter. A general type-2 fuzzy C-means algorithm was further revealed via an alpha-plane representation theorem. However, the values of these fuzzifiers mainly depend on subjective selection or enumeration in the available studies and the results need more interpretations. Recently, the notion of multigranulation in granular computing is developed for solving human-centric problems and interpreting the obtained results from the perspective of multiple levels of granularity. This methodology provides a new insight into analyzing the uncertainties generated by the model parameters. The purpose of this paper is mainly to tackle the uncertainties associated with the two key parameters in rough set and fuzzy set-based clustering approaches, namely the threshold related to the approximation region partitions and the fuzzification coefficient m. The main objectives of this paper are to optimize the partition threshold for each cluster based on shadowed sets, which is obtained from the perspective of the global observation on data and will be used as a cornerstone for establishing the multi-levels of granularity for approximation regions; to capture the uncertainty generated by the fuzzifier parameter m in rough-fuzzy clustering methods via the variations in multigranulation approximation regions formed under multiple values of fuzzifier parameter with a partially ordered relation, rather than at a single level of granularity under a specific fuzzifier value; to update the prototypes by combining the intermediate results obtained at different levels of granularity. In this way, the prototypes calculated at a single level can be modified and then tend to their natural positions; to develop a multilevel degranulation mechanism according to granulation-degranulation philosophy based on which the quality of the clustering model can be evaluated. By integrating various granular computing technologies, i.e., fuzzy sets, rough sets, shadowed sets and the notion of multigranulation, the uncertain information in data, including overlapping partitions, the vagueness arising in boundary regions and the uncertainty produced by fuzzification coefficient, can be handled sufficiently. Experimental results with the use of synthetic and real-world data illustrate the improved performance of the proposed notion in terms of several validity indices, such as relative separation index, Davies–Bouldin index, Dunns index and PBM-index as well as the granulation-degranulation index. The rest of paper is organized as follows. Some rough set-based partitive clustering methods are reviewed in Section 2. The uncertainty generated by the fuzzifier parameter m is revealed in Section 3. Meanwhile, multigranulation approximate regions are formed based on which a new rough-fuzzy C-means method is introduced. In Section 4, the proposed granulation-degranulation mechanisms based on multiple levels of granularity are explained. Comparative experiments are presented in Section 5. Some conclusions are given in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.05.053", "references": [101345952, 1843766148, 1976670813, 1983753875, 1987149779, 1988695218, 1989577218, 1989907351, 1996747841, 1998965536, 2001692054, 2006873874, 2026489439, 2039742983, 2048404808, 2051224630, 2052608046, 2062696711, 2070813883, 2078757499, 2083449452, 2089923511, 2103535990, 2114832876, 2143451122, 2154437129, 2170755382, 2171975443, 2259316796, 2297889545, 2340020088, 2461653276, 2565881538, 2605693713, 2620114837]}
