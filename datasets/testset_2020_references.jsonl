{"paragraph": "Organizations need to innovate in response to changing customer demands and opportunities offered by technology and changing marketplaces, structures and dynamics. Joshi, Chi, Datta, and Han examine the relationship between IT and firm innovation focusing on knowledge capabilities that are enhanced through the use of IT, and demonstrate that IT plays a significant role in enhancing firm innovation. The combination of Big Data and Business Analytics represents one of the latest opportunities for organizations to change their practices by the use of IT. It is argued that organizations need to act swiftly to benefit from Big Data and Business Analytics by using them to create innovation and competitive advantage. The concept of Business Analytics is not new, but has recently re-emerged as an important area of study owing to its developing capabilities to handle Big Data. New IT processing technologies such as Hadoop and cloud services enable Business Analytics to deal with Big Data to provide descriptive, predictive and prescriptive analysis. Business Analytics thus clearly has commonalities with Operational Research. Ranyard, Fildes, and Hu refer to Business Analytics as apparently extending the scope of Operational Research practice, but the precise relationship between Business Analytics and Operational Research remains a matter of debate. Although Business Analytics is increasingly being used in organizations, there is a lack of theory linking analytics to innovation, and hence also a lack of practical guidance for managers. In particular, models of the innovation process do not usually include any explicit form of data acquisition, analysis or use. For example, Choi, Narasimhan, and Kim include only generating rates of product and process knowledge, the process of generation being unspecified, and Pan and Li similarly use only learning rate parameters. An exception is the work of Vidgen, Shaw, and Grant. One of the research questions they considered was how do organizations extract or create value from data. Their analysis - a Delphi study and three case studies - led to 21 recommendations, though there was no attempt to structure these into a causal model. Despite strong claims that Business Analytics can enhance innovation through product or service differentiation using Big Data, there remains a need for theory and empirical evidence to link Business Analytics and innovation. Many businesses are still struggling to figure out how, where and when to use Business Analytics to achieve a worthwhile return. Until the mechanisms underlying Business Analytics and its contribution to improved business performance are better understood, realizing desired outcomes, such as innovation, remains uncertain. It is notable that the research agenda for Operational Research in the analytics age set out by Mortenson et al. concentrates on research into Business Analytics itself rather than on links between Business Analytics and outcomes. Therefore, it is imperative to investigate and confirm if, how and to what extent Business Analytics contributes to innovation. This paper seeks to fill this research gap by proposing and validating a new model to explain the relationships between Business Analytics and product or service innovation. In so doing, it is important not to regard Business Analytics as just a technical development, but also one related to organizational culture. Like any technique, Business Analytics will always yield findings of some sort, but only if organizations choose to act on those findings can any innovation occur. Achieving competitive advantage as a result would be clear evidence that organizations have acted on the Business Analytics findings. An appropriate cultural focus when examining Business Analytics is the concept of data-driven culture. The term data-driven culture has been in use for many years, but with the emergence of Big Data, it has attracted much more attention from practitioners and researchers because they argue that to maximize the potential Business Analytics business value, a relevant organizational culture must be in place. Most Operational Research writers on Business Analytics acknowledge the importance of organizational culture, but few consider the acquisition of the data being analyzed, Hindle and Vidgen and Pape being notable exceptions. Yet the acquisition of data needs to be a purposeful activity – part of environmental scanning, which is a basic process of any organization to acquire and use data from the external environment to assist management in problem definition and decision making. As Big Data technologies enable organizations to acquire a vast array of data about their environments, the role of environmental scanning Big Data must be considered when studying Business Analytics' impact on innovation. To link analytics, data and culture, absorptive capacity theory thus appears highly relevant, because this theory relates to an organization's ability to recognize the value of new, external information, assimilate it and apply it to commercial ends. This is a crucial element of the path from Business Analytics to innovation. Yet as far as we are aware, ours is the first study to use it to help understand how Business Analytics affects innovation, and how managers might change their organizations to reap the benefits from Business Analytics. Therefore, this research aims to examine specifically the relationships between Business Analytics, data-driven culture, environmental scanning, new product or service innovation, and competitive advantage. To achieve this research aim, this study employs a deductive approach. A number of hypotheses are proposed from an information processing and use perspective, drawing on absorptive capacity theory. These hypotheses are integrated into a research model to explain how Business Analytics, working through environmental scanning and data-driven culture, contributes to new product or service innovation, and subsequently competitive advantage. To test the research model, a survey questionnaire is designed to collect quantitative data from UK commercial organizations. Survey data collected from 218 UK companies are used to test the research model. The remainder of this paper is structured as follows. Section 2 provides a literature review on the key concepts and theoretical considerations. Section 3 discusses the development of the research model. Section 4 explains the research method including research constructs, the associated measurements, and data collection process. Section 5 presents the data analysis and results. It is followed by discussion in section 6 and conclusion in Section 7.", "section": "Introduction", "doi": "10.1016/j.ejor.2018.06.021", "references": [92988379, 1419724723, 1542083448, 1555809017, 1721421031, 1920432947, 1969359483, 2013261784, 2013593749, 2034957137, 2038195563, 2041579435, 2070564502, 2081084322, 2096460314, 2107023540, 2121991049, 2122141202, 2126071066, 2132747867, 2136252976, 2141975087, 2164921310, 2264142365, 2281782812, 2284916459, 2304517173, 2498362352, 2590255024, 2626664234, 2769967834, 2803453471]}
{"paragraph": "Pricing Bermudan options in high dimensions requires Monte Carlo methods, and two simulation-based prices have been developed: lower and dual upper bounds. Specifically, Longstaff and Schwartz use a standard least-squares Monte Carlo approach to compute lower bounds. Likewise, upper bounds are also based on least squares and simulation. Although both bounds are widely used, upper bounds are hardly optimized, which is important because simulation is time consuming, demanding a smart approach. In this paper, we optimize recursive upper bounds and provide two new results. Lower and upper bounds generated by simulation depend on an exercise policy, whereby the upper bound is derived from a martingale based on this policy. First, we show a recursive upper bound is independent of the next-stage exercise decision and hence cannot be optimized. Therefore, we optimize the recursive lower bound, following a local least-squares Monte Carlo approach, and use its optimal recursive policy to evaluate the upper bound as well. We find these two bounds, which have a similar cost to the reciprocal bounds based on a policy estimated by the standard least-squares Monte Carlo method, are very tight. Second, we study separately an upper bound generated from a martingale based on continuation-value functions, a bound that is less time intensive yet more upper biased, and show how to reduce its bias as well. In our first approach, we consider a given family of exercise policies or stopping times. A local method maximizes a recursive lower bound with regard to this family at each exercise stage. An open question is which exercise strategy minimizes the upper bound. We show the exercise strategy that maximizes a recursive lower bound also minimizes not the recursive upper bound itself, but rather the gap between them. We provide a recursive expression for the gap, and show a recursive upper bound is independent of the next-stage exercise policy. Therefore, minimizing the gap is equivalent to maximizing the Bermudan price recursively. In the second approach, we consider a family of continuation-value functions. We show a recursive upper bound is independent of the next-stage continuation-value function as well. By factorizing the two martingales that are based on either stopping times or continuation values, the latter martingale includes a third error term, which ensures the process is actually a martingale yet implies more biased upper bounds. The other two terms of the martingale are those of the standard factorization of the American option into an early-exercise premium and the European counterpart. This third term, however, depends only on the option continuation value in the waiting or continuation region. The latter constraint is critical because Bermudan options are highly nonlinear near the exercise boundary but less so in the waiting region, and fitting a continuation-value function only in this region is easier. This new upper bound, based on a continuation value estimated only in the waiting region, is as accurate as an upper bound based on an exercise policy estimated by the least-squares Monte Carlo method, but in a fraction of the time. The new bound is especially accurate for at- or in-the-money options, which depend mostly on sample paths that cross the exercise region and that do not contribute to the martingale’s third error term. This dual waiting-region constraint is the reciprocal constraint of using in-the-money paths, and hence, the exercise region, to estimate the continuation value in the least-squares Monte Carlo or primal method. In the numerical exercise, we price up-and-out Bermudan max-options. The up-and-out barrier makes this option very sensitive to suboptimal exercise, providing a good test. From the local least-squares Monte Carlo method, we derive the optimal recursive exercise policy and compute the two bounds associated with this policy: the lower bound improves upon the reciprocal bounds based on the standard least-squares Monte Carlo and pathwise optimization by more than 100–200 cents; the upper bound yields a one-digit gap. This small gap implies the recursive policy and the associated martingale are near optimal and the two bounds are close to the true price. The local policy is so good that reducing the number of subsimulation paths by 20 decreases the time effort by a factor of 10, yet the upper bound increases only by a few cents. Notably, the upper bound based on the local least-squares Monte Carlo policy only changes marginally with the number of subsimulations and is robust to all refinements, implying the upper bound is tighter and closer to the true price than the lower bound. With other methods that yield a nontrivial gap, this claim cannot be made. This result agrees with the two-period Bermudan upper bound, which is independent of the one-period exercise policy. A tighter upper bound implies a midpoint between lower and upper bounds is lower biased. The duality approach in option pricing has been extended in many ways. Several studies examine optimal dual bounds, use a multilevel approach, study dual bounds based on regression methods, derive upper bounds using linear and semidefinite programming, or use a pathwise-optimization approach that is less time consuming. In a novel extension, one study uses an information relaxation that nests the perfect-information assumption of the martingale approach, and also applies to other problems in operations research. We tailor duality results to our optimal recursive setting, which yields such tight bounds. Specifically, in the former case based on continuation values, we improve the upper bound by computing the continuation value only in the waiting region. In the latter case based on stopping times, the upper bound is both tight and efficient if we use fewer subsimulation paths, but a near-optimal exercise strategy as in the local least-squares Monte Carlo approach. In both ways, we bring the overall cost of the martingale approach in line with pathwise optimization or information relaxation. Moreover, the factorization of the dual martingales in terms of the components of the Bermudan process is mostly new. Our tight bounds are a useful benchmark for new methods that try to improve upper bounds in terms of accuracy or time effort. Section 2 reviews the local least-squares Monte Carlo method and explains the exercise policies and continuation values needed later for dual bounding; Section 3 shows the independence of the recursive upper bound on the next-stage exercise policy; Section 4 shows an upper bound based on a continuation-value function only needs this function in the waiting region; Section 5 provides the complexity analysis and examples; Section 6 concludes. Proofs are left to the Appendix.", "section": "Related Work", "doi": "10.1016/j.ejor.2019.07.031", "references": [122275331, 1866485205, 2017001821, 2062659568, 2089124841, 2112795785, 2120301464, 2120790358, 2137824542, 2151786492, 2156168464, 2164674975, 2295954716, 2611766878, 2617918452, 2782351573]}
{"paragraph": "Humans use the gaze to look at objects. This behavior can be used as a means to control interfaces in human–computer interaction by estimating the gaze point with the help of an eye tracker. Thanks to recent technological advancements and drop in price, eye tracking is no longer a niche technology only used in laboratories or by users with special needs. For example, with the price of an advanced game controller, players can enhance their gaming experience with eye tracking. A gaze-aware game knows where the player’s visual attention is at each moment and can offer optional input methods and enhanced gaming experience. At the same time, research on mobile eye tracking has been active. Simple eye-awareness is already included in some cell phone models so that the phone knows when the user is looking at it. Research on pervasive and mobile gaze interaction has demonstrated how eye tracking can enhance the interaction with mobile phones, tablets, smartwatches, smart glasses, and smart environments and public displays. Because the eye is primarily a perceptual organ, using gaze as an intentional control method poses challenges for interaction design. Most important, viewing should not be misinterpreted as a voluntary command. In gaze interaction literature, this problem is known as the Midas touch problem, where viewed objects are unintentionally acted on. Feedback plays an essential role in informing the user how the system is interpreting the gaze. Gazing at an object in real life naturally provides only visual feedback. But computers and smart devices can indicate if an object has been recognized as being pointed at, or being selected. Previous research has shown that visual and auditory feedback on gaze input significantly improve user performance and satisfaction. However, the effects of haptic feedback in gaze interaction have remained largely unknown. We assume haptic feedback could provide a useful alternative to, at least the audio, as auditory and haptic perception are known to share similarities. For example, participants could perceive auditory and tactile rhythms more accurately than visual rhythms. Auditory and haptic feedback can be perceived independently from the gaze location. Unlike the distal senses of vision and hearing, touch is a proximal sense that provides information of things close to or in contact with us. How would the interplay of a distal and proximal sense work? For instance, instead of seeing a button change its appearance, the user could feel the click of a button after selecting it with gaze. Could this novel combination of modalities provide some benefits compared to visual and auditory feedback, or is this unnatural combination of action and feedback perhaps incomprehensible? These were the questions that motivated us in the work reported in this article. Haptic feedback has become more common in consumer technology due to the emergence of mobile and wearable devices designed to be in contact with the skin. The most popular form of haptic stimulation in mobile and wearable devices is vibrotactile feedback. For example, continuous vibration is an effective way to notify of incoming calls with mobile phones. Shorter vibration bursts are used on phones and tablets to replace the tactile feel of pressing a physical key when typing with a virtual keyboard. This has been shown to improve typing speeds. Vibrotactile technology is also included in some smartwatches. In the Apple Watch, for instance, vibrotactile stimulation is used to mimic a heartbeat that can be sent to a close friend or family member. With multiple actuators, it is possible to create touch sensations that move on the wrist. To date, commercial smart glasses and other head-mounted devices have not utilized vibrotactile feedback. This is surprising because it is known that users can quite accurately localize which part of the head is stimulated with vibrotactile actuators. We were interested in studying how vibrotactile feedback could support gaze interaction. We conducted a series of experiments in which we focused on four main research questions: effectiveness of vibrotactile feedback, temporal limits between gaze events and vibrotactile feedback, effects of feedback location and spatial setup, and vibrotactile feedback in comparison to other modalities. Because our results are spread over more than 20 articles, this could make it difficult for future researchers to extract the main findings. The contribution of this article is to summarize the research results in a compact form and serve as a collection of pointers to more detailed work in the original publications. The goal is to add to our understanding of how the two modalities of haptics and gaze can be utilized effectively in human–computer interaction. The organization of the article is as follows. We first introduce gaze interaction and vibrotactile feedback. We then present results from the experiments before discussing lessons learned from the studies. We end with general discussion and present design guidelines based on our accumulated knowledge and insights.", "section": "Methodology", "doi": "10.1080/07370024.2017.1306444", "references": [180056178, 1492404201, 1520722479, 1538622809, 1590806791, 1976177666, 1982812012, 1986793049, 1988742375, 1990184244, 1994221098, 1995055145, 2000367546, 2002492663, 2004164216, 2005637512, 2006966072, 2010809371, 2014788716, 2016877042, 2018806117, 2020284712, 2022716189, 2025074021, 2030189835, 2034722664, 2040464349, 2041257003, 2042346957, 2046879541, 2053432308, 2057481637, 2060089136, 2075140915, 2076963239, 2079778212, 2088270376, 2090644126, 2091354492, 2093355879, 2094058125, 2094938076, 2099788879, 2102804490, 2103802786, 2110228188, 2134738924, 2138428053, 2144390195, 2144922371, 2150273910, 2156515914, 2156903065, 2164660958, 2167020116, 2176017127, 2285717100, 2293738675, 2295148469, 2298161945, 2344158693, 2402420151, 2420461153, 2484706745, 2537533530]}
{"paragraph": "In machine learning, data are usually described as points in a vector space. Nowadays, structured data are ubiquitous. The capability to capture the structural relationships among the data points can be particularly useful in improving the effectiveness of the models trained on them. To this aim, graphs are widely employed to represent this kind of information in terms of nodes or vertices and edges, including local and spatial information arising from data. For instance, consider a d-dimensional dataset representing points in space, a graph can be extracted by considering each point as a node, where edge connectivity and weights can be computed using a metric function. A new data representation is obtained, where there is a set which contains vertices and a set of weighted pairs of vertices representing edges. Applications in a graph domain can be usually divided into two main categories: vertex-focused and graph-focused. The former includes tasks where one performs classification or regression on the vertices of a graph, whereas in the latter one performs these tasks on the graph itself. For instance, object detection and image annotation are examples of vertex-focused applications, the former consists of finding whether an image contains a given object and its position, the latter consists of extracting a caption that describes an image. Another possible example could be web page classification, where the web is represented by a graph where nodes are the pages and edges are the hyperlinks between them, the aim being to exploit the web connectivity to classify pages in a set of topics. Instead, estimating the probability of a chemical compound to cause certain diseases can be seen as a graph-focused application. This is possible due to the fact that a chemical compound can be modeled by a graph where the nodes are the atoms and the edges the chemical bonds. For the sake of simplicity and without loss of generality, just the classification problem is considered. Under this setting, the vertex-focused applications are characterized by a set of labels, a dataset, and the related graph. Let us assume there is a subset of labeled nodes. The goal is to classify the unlabeled nodes by exploiting jointly the node features and the graph structure by means of a semi-supervised learning approach. Graph-focused applications are related to the goal of learning a function that maps different graphs to integer values by taking into account the features of the nodes of each graph. This task can be solved by supervised classification on the graphs. A number of research works are devoted to classification, both for vertex-focused and graph-focused applications. Nevertheless, there is a major limitation in existing studies: most of these research works are focused on static graphs. However, many real world graph-structured data are dynamic and nodes or edges in the graphs may change over time. In such dynamic scenarios, temporal information can also play an important role. For instance, the interactions between individuals inside a building in one day can be modeled as a sequence of graphs, each one describing a time window within the day, where the nodes are the people and the edges are the interactions occurring between them within a time frame. As another example, consider the classification of human activities from motion-capture data, where each frame can be modeled as a graph, where the vertices are the skeleton joints of the person in question. In the last decade, neural networks have shown their great power and flexibility by learning to represent the world as a nested hierarchy of concepts, achieving outstanding results in many different fields of application. It is important to underline that just a few research works have been devoted to encoding graph structures directly using a neural network model. Among them, to the best of the authors’ knowledge, no one is able to manage dynamic graphs. To exploit both graph-structured data and temporal information through the use of a neural network model, two novel approaches are introduced. They combine Long Short Term-Memory network and Graph Convolutional Network, which can be considered the two base elements of the proposed architectures. Both of them are able to deal with vertex-focused applications. Respectively, these techniques are able to capture temporal information and to properly manage graph-structured data. The approaches are also extended to deal with graph-focused applications. LSTMs are a special kind of Recurrent Neural Network, which are able to improve the learning of long term dependencies. All RNNs take the form of a chain of repeating modules of neural networks. Precisely, RNNs are artificial neural networks where connections among units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic sequential behavior. In standard RNNs, the repeating module is based on a simple structure, such as a single unit. LSTMs extend the repeating module by combining four interacting units. Specifically, it is based on: cell state, forget gate, input gate and output gate. The most important part of LSTMs is the cell state, which can be pictured as a conveyor belt. It runs straight down the entire chain, with some linear interactions. The first interaction decides what information is going to be removed from the cell state. This decision is made by a sigmoid layer, called the forget gate. The next interaction decides what new information will be stored in the cell state. This can be operationalized in two parts: a sigmoid layer, called the input gate decides which values should be updated, and another sigmoid layer creates a vector of new candidate values that could be added to the cell state. The final output is generated by combining the cell state with a sigmoid layer. A GCN is a neural network model that directly encodes graph structure, which is trained on a supervised target loss for all the nodes with labels. This approach is able to distribute the gradient information from the supervised loss and to learn representations exploiting both labeled and unlabeled nodes, thus achieving state-of-the-art results. The goal of a GCN is to learn a function of signals or features on a graph which takes as input a feature description for each node, summarized in a feature matrix, and a representative description of the graph structure in matrix form, typically in the form of an adjacency matrix. This model produces a node-level output, a feature matrix where the number of output features corresponds to each node. A GCN shares its underlying intuition with Convolutional Neural Networks, specialized kinds of neural networks that are highly successful in practical applications such as computer vision employing, within some of their layers, a convolutional operation in place of regular matrix multiplication. A convolutional layer in a CNN exploits the locality of information embedded in its input to extract local features. It does this by repeatedly looking at small patches of its input and by learning a mixing matrix of the size of the patches that is shared across all the patches, thus reducing the amount of parameters. Similarly, a GCN exploits the locality notion induced by the graph connectivity by looking at small patches of the graph, where the patches are all neighbourhood subgraphs of the original graph.", "section": "Introduction", "doi": "10.1016/j.patcog.2019.107000", "references": [145505233, 1522301498, 1662382123, 1869398109, 1968103943, 1968528416, 2004109568, 2004646046, 2051436535, 2064675550, 2065130322, 2086370422, 2100741829, 2116341502, 2117024994, 2124386111, 2139823104, 2143516773, 2146055337, 2154851992, 2158787690, 2173027866, 2244807774, 2366141641, 2468907370, 2519887557, 2557283755, 2610153490, 2740060125, 2754759618, 2771111398, 2950635152, 2953170998, 2963165299]}
{"paragraph": "The huge volume of check-in data from various location-based social networks enables studies on human mobility behavior on a large scale. Next POI recommendations is the task to predict the next POI a user will visit at a specific time point given her historical check-in data. This task has been studied extensively in recent years. Next POI recommendations is different from typical recommendation tasks such as movies, songs, or books because a wide range of contextual factors are related to the user's spatial behaviors. These auxiliary factors include the temporal context, sequential relations, geographical influence, and auxiliary meta-data information such as textual description and user friendship. However, these factors are heterogeneous in nature. While some relevant aspects are continuous values such as geographical distance and the time interval, others are in the form of discrete values such as friendship, textual words, or day of the week. Harnessing useful signals from all these heterogeneous factors to predict a user’s next move is not an easy task. Existing solutions based on matrix factorization and embedding learning techniques have delivered encouraging performances. These solutions project users and POIs and the associated context factors into a shared hidden space with dense vector representations. The preference score is then calculated directly based on these vectors through the inner product operation. However, shallow factor or embedding learning is too limited to express the complex knowledge underlying user spatial behaviors with multiple context factors. In existing methods, different context factors are often modeled separately. Then a simple combination is applied to derive the final recommendation score. That is, we need to devise an individual model for each context factor. This modeling methodology is complicated and the resultant solution would be inferior, since the different factors carry varying degrees of useful knowledge and their interactions could be much more complex. Some other methods incorporated multiple context factors as additional constraints to guide the learning process. For example, check-ins made at a specific time period are grouped together for dynamic feature learning. However, these constraints may not always be useful to match user-POI interactions. A single factor or embedding learning could inevitably incur information loss through a joint optimization of both preserving the constraints and matching user-POI interactions. One plausible solution is to enlarge the dimension number. However, given the sparsity nature of user-POI interaction data, it would easily result in data overfitting. The neural network with dense vector representation based techniques provide a new way of modeling these factors in a unified manner. This offers two benefits: By adding nonlinear transformations on top of the embeddings of users, POIs and their associated factors, we can separate the embedding learning and high-level spatial intent learning to better understand user spatial behavior, leading to a better recommendation accuracy. Specifically, we encode the semantic relatedness or constraints among POIs or users into the corresponding embeddings. For example, users at Golden Gate Overlook are likely to visit Baker Beach in San Francisco, and vice versa. Therefore, Golden Gate Overlook and Baker Beach are projected closer in the embedding space. Without the need to match user-POI interactions, the embedding learning would capture the latent features for users, POIs and the associated constraints to its fullness. Also, both new users and POIs may be covered partially by the associated auxiliary meta-data such as textual description or friendship. With dense vector representations, we can easily estimate the spatial intent from the associated meta-data for these cold-start cases. Since not all constraint information or latent features are useful for all user-POI interactions, the nonlinear transformation operation is adopted to learn how to extract high-level spatial intent for next POI recommendations. We can also devise factor-based nonlinear extractions to accommodate some specific context factors that are strongly relevant to spatial intent extraction. Through a unified framework with neural treatment and dense representations, the complex interactions among the context factors, users and POIs can be learnt smoothly without handcrafted modelling for each factor alone.", "section": "Related Work", "doi": "10.1007/s11704-018-8011-2", "references": [154472438, 179875071, 1500188831, 1546409232, 1614298861, 1924770834, 1971129545, 2009779426, 2044672016, 2047604680, 2059512502, 2059512573, 2064675550, 2070915285, 2073013176, 2084677224, 2087692915, 2094286023, 2101409192, 2110485445, 2110953678, 2118463056, 2137245235, 2137983211, 2146232090, 2154851992, 2156387975, 2205235818, 2242161203, 2250739653, 2408538552, 2471486255, 2511929605, 2512971201, 2515144511, 2531384334, 2534727297, 2536880093, 2539781657, 2567312369, 2572589325, 2575006718, 2605350416, 2762735242, 2788114581, 2949274928, 2950635152, 2962995178]}
{"paragraph": "As the Internet and mobile phones have evolved over the years, technology to improve remote communication has developed immensely. It is clear that mobile phone users prefer text messaging over phone calls, with the number of monthly text messages worldwide increasing from 395 billion in 2011 to 561 billion in 2014, and the number is still growing. In a computer-mediated communication such as text messaging, a cues-filtered-out perspective and social presence theory argue that computer-mediated communication lacks nonverbal communication cues and thus fails to communicate emotions and attitudes to receivers. Also, prior studies showed that viewing online text without emoticons caused misinterpretation of the nature of the message and the sender’s intent. There have also been studies that state that the emotional attitude of the text is more important when it comes to the interpretation, but a negative emoji can even change the interpretation of the message. For instance, when somebody sends a neutral text such as “That man came to see me again,” it is difficult to know whether the sender is annoyed or glad. Emojis can clarify these kinds of ambiguity and even show the intensity of the emotion, as well. Many researchers have suggested that written communication can be enhanced through visual cues in the same way visual or body language supports verbal communication. In computer-mediated communication, which has become so common today, users attach emoticons in their text messages to fill in the missing socioemotional context. Previous studies suggested that emoticons can provide the missing information and enhance computer-mediated communication. These visual cues have been identified as the primary way to express emotions in computer-mediated communication and as a way to replace nonverbal communication when face-to-face interaction is not possible. Emoticons have been defined as graphic representations of facial expressions that are embedded in electronic messages. The usual distinction between the terms emojis and emoticons is that the term emoticons is commonly used to refer to punctuation signs combined on the keyboard to convey nonverbal communication, such as the smiley face or a wink, whereas emojis are pictures that convey nonverbal information. This article uses the term emojis because the final tactile design examined here employed imagery rather than a combination of punctuation marks. Today, emojis are used worldwide in countries that use electronic devices to communicate. Since Apple created its Apple Color Emoji set, providing support for Unicode emojis, it has been used in all iOS and OS X devices and in many others. Japan and Korea, the countries with the largest mobile markets in the world, use Line and KakaoTalk for their messaging platforms. It is said that there are now more than 22 million emojis in KakaoTalk and Line, and KakaoTalk calculated that, of messages that contained emojis, emojis composed 15 percent of the conversation. In 2012, the daily sales of emojis totaled approximately 100 million Korean won. Throughout these many developments and changes in online communication, visually impaired individuals have often been left out. Although companies such as Apple, KakaoTalk, and Google are continuously developing technologies for people with visual impairments, most of these are based on audio guidance. Audio guidance is useful in its own way, but it has limitations; it cannot provide privacy without the use of headphones, which can create dangerous situations for the visually impaired, as their ears must be occupied. Also, audio guidance cannot be heard readily in a noisy environment. Studies on techniques for tactile delivery of emotional cues have also been progressing using vibration patterns, person-to-person haptic responses, and vibrotactile displays. However, as many of these were not developed for visually impaired people, they still do not provide the ultimate solution to limitations on visually disabled persons’ texting experience. The goal of this study, therefore, was to show that image-based tactile emojis can offer a solution for the future texting experience of individuals with visual impairments. Also, based on prior studies indicating that congenitally blind people are able to comprehend facial expressions, and research that implies face processing as a bimodal phenomenon where vision can be substituted by touch, this study examined the question of whether, with a short introduction, congenitally blind individuals, who have no prior visual knowledge, are capable of understanding emojis that sighted people use. To achieve these goals, the following steps were taken: We conducted a preliminary study to identify the connection between emotions and emojis. For the ten most commonly selected emojis for each emotion, we organized the facial features separately into eyes, eyebrows, and mouth and converted the images into textual descriptions. We ranked the separately organized facial features by weighting the features most often chosen. We created image-based tactile emoji designs based on the previously defined descriptions of the six emotions. We conducted interviews using the resultant apparatus and analyzed the results. The provision of image-based tactile emojis, when applied in the real world, will provide privacy for a visually impaired person’s texting service and equality in technology that has not previously existed. It will also improve textual communications for visually impaired people by reducing misunderstandings and making the texting experience more enjoyable.", "section": "Methodology", "doi": "10.1080/07370024.2017.1324305", "references": [40549020, 1978040282, 2012461879, 2020728888, 2024641146, 2062769641, 2096073495, 2100928276, 2101282523, 2107208519, 2118459787, 2129353167, 2133180260, 2144378002, 2146459173, 2160660844, 2161906378, 2243203301, 2296274542, 2519353570, 2574189006]}
{"paragraph": "Importance measures providing information about the importance of a component or a group of components on the system performance, such as reliability or availability, productivity, safety, or any performance metrics of interest, can help to identify design weaknesses or operation bottlenecks and to suggest optimal modifications for system upgrades and maintenances. In the literature, a large number of importance measures have been developed and successfully applied for various purposes. In risk analyses, importance measures are used in risk-informed decision-making. In reliability engineering, importance measures are used to prioritize components in a system for reliability improvement. Recently, importance measures have been applied for maintenance optimization and spare parts management. More specifically, Birnbaum structural importance measure is used to build a decision indicator for maintenance optimization of multi-component systems with complex structure. Differential importance measure is proposed to use in inventory management. More recently, the link between component importance and preventive maintenance policy has been discussed. In the framework of condition-based maintenance optimization, the current condition of components, such as failure state, working state, or deterioration level, is an important issue and needs to be taken into account in decision-making. However, very few existing importance measures allow incorporating the actual condition of components over time. Moreover, in practice, positive economic dependence, which implies that joint maintenance of several components is cheaper than performing maintenance on components separately, often exists and should be integrated in maintenance decision-making in the framework of maintenance optimization. To the best of our knowledge, no existing importance measure allows taking into account this kind of interaction between components. To face this issue, in this paper, a novel time-dependent importance measure based on the conditional reliability evaluation of the system, namely RIM measure, is proposed. At a given time and given the real condition of the components of a system, the proposed importance measure can be used to rank the components or groups of components according to their ability to improve the system reliability for a given mission. The proposed RIM measure is then extended to take into account the maintenance cost and the economic dependence between components. Indeed, the extended RIM measure is defined as the ratio of the cost benefit given by the maintenance of a component or group of components to its total maintenance cost. This indicator can help to identify the most cost-effective components or group of components for preventive maintenance before a given mission. This paper is organized as follows. Section 2 is devoted to the description of general assumptions and different reliability metrics. Different kinds of information on components at a given time are also discussed and integrated in the evaluation of reliability metrics. Section 3 focuses on the definition of the proposed time-dependent importance measure, namely RIM. The influence of information level on the RIM measure and RIM-based importance ranking is also investigated. An extension of RIM measure is developed in Section 4. Maintenance cost structures and economic dependence between components are also formulated and discussed. To illustrate the uses of RIM measure and its extension, a numerical of a 5-component system is introduced in Section 5. In addition, some numerical results are herein discussed. Finally, the last section presents the conclusions drawn from this work.", "section": "Introduction", "doi": "10.1016/j.ress.2019.106633", "references": [261935476, 1550064664, 1807378533, 1999069087, 2002699192, 2019006256, 2053885255, 2084648532, 2126484023, 2126967364, 2147664181, 2155326466, 2202654822, 2290696661, 2340663414, 2612742145, 2620514866]}
{"paragraph": "Constraint-based techniques, such as Integer Linear Program and SAT modulo theory solvers, play a key role in state-of-the-art approaches for solving challenging problems across a wide range of applications. In this work, we demonstrate how virtual data center allocation, a prominent and increasingly important problem arising in the operation of modern data centers, can be tackled using a pair of high-performance constraints solvers: Gurobi and MonoSAT. We obtain substantial improvements in performance and functionality over previous virtual data center allocation techniques. Central to our results is the formalization of virtual data center allocation in terms of multi-commodity flows, allowing us to exploit the efficient handling of network-commodity flow problems in Gurobi and MonoSAT. We are the first to demonstrate that constraint solvers can be successfully applied to this setting at full data center scales, while also improving on the state-of-the-art. A virtual data center consists of multiple communicating virtual machines, each with individual server resource requirements such as CPU or RAM, along with a virtual network of pair-wise bandwidth requirements between the virtual machines. The virtual data center allocation problem is to find a valid allocation of virtual machines to servers and links in the virtual network to links in the physical network. A valid allocation satisfies the compute, memory, and network bandwidth requirements of each virtual machine across the entire data center infrastructure, including servers, top-of-rack switches, and aggregation switches. The allocation is indicated with dashed lines. For example, the virtual machines for s1 and m are mapped to the same physical server and the virtual link s2 to m is allocated a multi-path route, in which each sub-path provides 1 Gbps. Support for end-to-end and multi-path are two characteristics that distinguish capabilities of our tool, Netsolver, from prior tools. In this work, we introduce Netsolver, a constraint-based virtual data center allocation procedure that is scalable, sound, and complete, with support for end-to-end, multi-path bandwidth guarantees across all the layers of the networking infrastructure, from servers to top-of-racks to aggregation switches to access routers. Netsolver efficiently allocates virtual data centers with a dozen or more virtual machines to full-size physical data centers with over a thousand servers, typically in seconds per allocation. Across a wide variety of data center topologies, Netsolver can allocate as many total virtual data centers to the same physical data center as state-of-the-art heuristic methods, such as SecondNet's allocation algorithm. Furthermore, Netsolver offers the flexibility and extensibility characteristics of a constraint-based approach. In real-world applications, data center operators often need to support additional constraints or optimization goals while allocating virtual machines, such as ensuring that certain virtual machines are placed together. We demonstrate that Netsolver can be easily extended to support additional virtual data center allocation constraints: virtual machine affinity constraints, minimization of the total number of utilized servers, and constraints to load-balance allocations and avoid hotspots. This paper extends results previously published with significant new capabilities, a new Integer Linear Program-based solver back end, and broadened experimental results. Additionally, we have improved the overall runtime performance of Netsolver through the use of automatic algorithm configuration, as well as upgrading the SMT solver from version 1.4 to version 1.6. We use Gurobi version 8.1.0 for all experiments.", "section": "Related Work", "doi": "10.1016/j.artint.2019.103196", "references": [60686164, 1480909796, 1572977974, 1710734607, 1770696206, 1857623778, 1968457703, 2001859357, 2002227246, 2006146932, 2010727402, 2017953449, 2022678927, 2027588582, 2074814670, 2077107611, 2077661716, 2094410510, 2096125134, 2097882016, 2107342126, 2112486185, 2114298221, 2116758077, 2121574037, 2123138012, 2126210439, 2130267070, 2132238781, 2134656724, 2137229314, 2142480021, 2142972529, 2152415706, 2154203494, 2157614013, 2157990152, 2161965229, 2257125174, 2295272781, 2803222079, 2949910966]}
{"paragraph": "Data envelopment analysis is a nonparametric method for measuring the efficiency of the decision-making units by a set of inputs and a set of outputs. DEA requires no assumption for the functional relationships between inputs and outputs and allows individual decision-making units to evaluate their efficiencies by choosing the most favorable input and output weights for themselves. It has been applied in many areas, such as resource reallocation, hospital assessment, and assessment of public finance. However, such an individual evaluation causes more than one unit to be efficient, leading to them being unable to be fully discriminated. In addition, the flexibility in weighting multiple inputs and outputs sometimes produces unrealistic weight schemes. To overcome these disadvantages, the cross-efficiency evaluation method has been developed to rank units based on cross-efficiency scores, which are linked to all units. Unfortunately, the cross-efficiency evaluations obtained from the original DEA are generally not unique because the optimal solution to the DEA linear programs is not unique. For this reason, secondary goals such as the aggressive and benevolent formulations have been proposed to address this issue. Additional models have introduced different secondary objective functions, which include minimizing the total deviation from the ideal point, minimizing the maximum efficiency score, and minimizing the mean absolute deviation. A neutral DEA model for cross-efficiency evaluation has also been proposed, which determines one set of input and output weights for each unit from its own point of view without being aggressive or benevolent toward others. Another approach generalized the original DEA cross-efficiency concept to game cross-efficiency, in which each unit seeks to maximize its own efficiency under the condition that the cross-efficiency of others does not deteriorate. A data DEA cross-efficiency aggregation method based on Shannon entropy has also been proposed. As a large number of zero weights may still exist among inputs and outputs, other models were developed to reduce the number of zero weights. Similar approaches estimate efficiency confidence intervals by using the bootstrap or other sub-sampling techniques. The weights for inputs and outputs of units are estimated from a random sample of input–output combinations, which are obtained from existing production units operating in the studied activity sector. Ratio-based efficiencies such as ranking intervals, dominance relations, and efficiency bounds have also been developed to compare the relative efficiencies of units for all feasible input–output weights. Some methods considered all possible weight sets in the weight space when computing the cross-efficiency, which produces an interval cross-efficiency for each unit. Other procedures were developed to carry out the cross-efficiency evaluation without the need to make any specific choice of weights. The proposed procedure takes into consideration all the possible choices of weights that all the units can make, and yields a range of the possible rankings for each unit instead of a single ranking. It is necessary to note that the interval efficiencies are different from the efficiency confidence intervals. The interval efficiencies contain all the possible efficiency scores of units, including the best and worst efficiency scores from all possible weights in the weight space. The efficiency confidence intervals are defined from a probability point of view, according to which of the most efficient units lie within an interval with a certain confidence level. Many classical decision analysis methods, including DEA, are based on assumptions of rationality and certainty. However, numerous studies have demonstrated that these classical methods cannot explain many phenomena in real applications. Therefore, researchers have explored decision-making theories based on behavior, and many behavioral decision theories have been developed, such as prospect theory, regret theory, and fairness theory. It has been noted that traditional decision and game theories rest on a fundamental assumption that players seek to maximize their individual utilities, but in some interactive decisions, it is observed and seems intuitively reasonable to maximize the utility of the group of players as a whole. Such phenomenon or thinking can be called team reasoning. Experimental evidence suggests that team reasoning predicts strategic choices more powerfully than orthodox game theory in some games. Some researchers commented on such an approach with varying mixtures of agreement and disagreement. In reply, it has been emphasized that decision makers sometimes act to maximize the collective payoff for a group, which cannot be explained in terms of standard social value orientations. Therefore, team reasoning is a distinctive and important mode of reasoning, which should be acknowledged in cognitive psychology and be added to the set of social value orientations used in social psychology. Furthermore, based on the experiments on how players use focal points to select equilibria in one-shot coordination games, two alternative explanations, namely, the cognitive hierarchy theory and team reasoning, were tested and strong support was found for team reasoning. In addition, some researchers considered all the individuals forming a team, and developed consensus models based on the minimum cost of the team. From the above literature review, we find that all the existing DEA approaches focus on the individual interest but ignore the team interest. Motivated by the idea of team reasoning, this paper investigates DEA models that put the team interest ahead of the individual interest. In DEA, all the decision-making units can be considered as a team, and the interest of the team can be reflected by the team's indexes, such as the overall efficiency of all the units, deviations of units, boundaries of units, and relationship between units in the team. The first three can be considered the external performance indexes of the team and expressed by interval values, whereas the fourth can be considered the internal performance index of the team, in which the relationship between units is expressed by the interval pairwise comparison matrix, where one value is the relative efficiency value between each pair of units. Based on such an idea, the following approach is proposed: First, the models are developed to estimate the values of the team indexes. Based on the obtained indexes, the decision makers can express their preferences about the indexes. Then, the individual models are developed to estimate the interval efficiencies of individual units under the condition that the team indexes of units are satisfied. Following the approach proposed above, the structure of the paper is as follows. Section 2 gives some preliminaries; Section 3 establishes the models based on the overall efficiency of all the units; Section 4 develops the models based on the variance of all the units; Section 5 focuses on the boundaries of all the units; in Section 6, the models based on the relationship between units are investigated; and Section 7 provides the conclusions.", "section": "Methodology", "doi": "10.1111/itor.12447", "references": [2006002090, 2011399838, 2022373133, 2051914789, 2061637245, 2078792730, 2116080612, 2136228439, 2141532588, 2145964516, 2153400179, 2163774646, 2179629619, 2298726707, 2474592240, 2554997024]}
{"paragraph": "The Shapley value and the prenucleolus are two well-known values for cooperative games. The Shapley value is an average of the contributions of an agent, while the prenucleolus is the value that minimizes the dissatisfaction of the worst-off coalitions. The nucleolus differs from the prenucleolus by only taking into account individually rational imputations. Coincidence between the Shapley value and the prenucleolus is uncommon and, in general, difficult to check without computing both values. A sufficient and necessary condition for this coincidence to hold has been provided, but it requires the computation of both the Shapley value and of a parametric family of sets, for which the computation mimics that of the prenucleolus. This characterization can be applied in order to identify the coincidence in some particular classes of games, such as airport games, bidder collusion games, and polluted river games. Coincidence is also found in some three-agent games based on bankruptcy problems. For general coalitional form games, we have coincidence if the game only has two agents or if all agents are symmetric within the normalized game. Some other games have also been proposed, all having in common that the value of a coalition is equal to the sum of the values created by the pairs composing that coalition. These games are called 2-additive games. The coincidence persists in games that satisfy a certain property. These games are such that the contributions of an agent to any coalition and its complement sum up to an agent-specific constant. A particular instance of such games has also been studied. Some researchers study the coincidence from a geometric point of view. Instead of providing classes of games where both values coincide, they study the properties that lead to this result in some already existing classes, such as certain types of games. A similar, yet different problem, is the invariance of the payoff assigned by an allocation rule to a specific player in two related games. In this paper, we present another family of games, called clique games, in which the Shapley value and the nucleolus coincide. The family can be described as follows: the set of agents is divided into cliques that cover it. A coalition creates value when it contains many agents belonging to the same clique, with the value increasing linearly with the number of agents in the same clique. Agents may belong to more than one clique, but the intersection of two cliques contains at most one agent. Finally, if two agents are not in the same clique, there exists at most one way to connect them through a chain of connected cliques. The family of clique games has a non-empty intersection with some other classes of games, and that intersection consists of 2-additive games. Some clique games are not in these other classes, and some of those games are not clique games. A clique game is convex, and hence its Shapley value is the average of the extreme points in its core. We thus obtain a link between three crucial concepts of cooperative game theory: the nucleolus, the core, and the Shapley value. Naturally, graph-induced games provide a fertile ground to apply our result. We first consider graph-restricted cooperative games. In these games, a coalitional value function is accompanied by a graph that summarizes the cooperation possibilities: a coalition cannot fully cooperate if some of its members have no path between them that uses only the vertices of agents in the coalition. When we consider a symmetric coalitional value function, assigning shares of the value created among agents is akin to defining centrality measures. We show that when the coalitional value function increases linearly with the number of agents in a coalition, we obtain coincidence of the Shapley value and the nucleolus for a family of graphs. Another graph-induced game that we study is the minimum coloring game, in which the graph represents conflicts between pairs of agents. We wish to assign agents to facilities, but cannot assign agents that are in conflict to the same facility. As facilities all have a cost of one, we wish to minimize the number of facilities used. Coincidence between the Shapley value and the nucleolus for a particular family of graphs can be explained by the fact that the graphs induce clique games. Our third example is the minimum cost spanning tree problem. This well-studied problem has agents connecting to a source through a network, with the cost of an edge being a fixed amount that is paid if the edge is used, regardless of the number of users of the edge. Any such problem has a non-empty core even though it may not be convex. Moreover, its Shapley value is not always in its core. When we consider elementary versions of this problem in which all edges have a cost of zero or one, the subset of cycle-complete problems generates clique games. Cycle-complete problems are such that if there exist multiple distinct free paths between a pair of agents the edge connecting them directly must also be free. Our result on clique games then applies, yielding that the nucleolus coincides with the Shapley value and the permutation-weighted average of extreme core allocations. The paper is divided as follows: preliminary definitions are in Section 2. Section 3 describes and illustrates clique games. Section 4 contains the coincidence results. Applications to graph-induced games are discussed in Section 5.", "section": "Introduction", "doi": "10.1016/j.mathsocsci.2019.10.002", "references": [1964733860, 1975617599, 1983125930, 1995443028, 1995559809, 2008131782, 2022418489, 2032441230, 2046116913, 2073225716, 2081334386, 2087121928, 2103267790, 2106417017, 2113370239, 2192499632, 2292459874, 2515623706, 2625101186, 2963187969]}
{"paragraph": "Recently, Vapnik and Vashist provided a new learning paradigm termed learning using privileged information, which is aimed at enhancing the generalization performance of learning models. Generally speaking, in classical supervised learning paradigm, the training data and test data must come from the same distribution. Although in this new learning paradigm the training data is also considered an unbiased representation for the test data, the LUPI provides a set of additional information for the training data during the training stage. The set of additional information is termed privileged information. Different from traditional supervised learning approaches, the LUPI based methods make use of a new kind of training data including privileged information during the training phase, but the privileged information is not available in the test stage. The authors note that the new learning paradigm is analogous to human learning process. In class, a teacher can provide some important and helpful information about this course for students, and these information provided by a teacher can help students acquire knowledge better. Therefore, a teacher plays an essential role in human leaning process. Likewise to the classroom teaching model, in general, the LUPI paradigm based methods can also achieve better generalization performance than traditional learning models. Vapnik and Vashist were the first to present a SVM algorithm with privileged information termed SVM+, which leverages the strength of the LUPI paradigm. A thorough theoretical analysis of the SVM+ was further illustrated in Pechyony and Vapnik and Vapnik and Izmailov. Previous works of the LUPI paradigm focus on two aspects: solving the LUPI based algorithms efficiently and incorporating the LUPI paradigm into various learning models. This paper focuses on the latter. The newly-derived RFVL+, however, has much milder optimization constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the new RFVL+, which naturally tackles the former. From the optimization perspective, the formulation of the SVM+ is a typical quadratic programming problem, and in general the QP problem can be solved by some optimization toolboxes. However, it is unnatural and inconvenient to train a learning model by some optimization toolboxes in real-world applications. For this reason, it is necessary to present an efficient approach to solve it. Pechyony et al. presented an SMO-style optimization approach for the SVM+. Li et al. further proposed two fast algorithms for linear and kernel SVM+, respectively. In addition to solving the SVM+ efficiently, the LUPI paradigm is incorporated into various learning algorithms. Nowadays, neural network is one of the most popular learning algorithms due to the wave of deep learning, and most of current deep learning methods are neural networks, including denoising auto-encoders, convolutional neural networks, deep belief networks and long short-term memory, etc. These neural network methods have achieved greatly successes in various real-world applications, including image classification and segmentation, speech recognition, natural language processing, etc. Therefore, it is very interesting to combine neural networks and the LUPI paradigm. The combined method is able to leverage the strengths of neural networks and the LUPI paradigm. The goal of this paper is to tackle this open problem and construct a bridge to link the LUPI paradigm and randomized neural networks. In this paper, the authors propose a novel random vector functional link network with privileged information called RVFL+. The random vector functional link network is a classical single layer feedforward neural network, which overcomes some limitations of SLFNs including slow convergence, over-fitting and trapping in a local minimum. Although the RVFL has achieved good generalization performance in some real-world tasks, in order to improve further its effectiveness, the authors incorporate the LUPI paradigm into the RVFL. Different from existing variants of RVFL, the RFVL+ may open a door towards alternative to the traditional learning paradigm for the RVFL in real-world tasks. In other words, the RVFL+ makes use of not only the labeled training data but also a set of additional privileged information during the training stage, which interprets the essential difference between the two learning paradigms. Moreover, following the kernel ridge regression, the authors further propose a kernel-based RVFL+ called KRVFL+ in order to handle highly complicated nonlinear relationships. The KRVFL+ has two major advantages over the RVFL+. On one hand, the random affine transform leading to unpredictability is eliminated in the KRVFL+. Instead, both the original and privileged features are mapped into a reproducing kernel Hilbert space. On the other hand, the KRVFL+ no longer considers the number of enhancement nodes, which is a key factor to affect its generalization ability. As a result, the performance of the KRVFL+ in terms of effectiveness and stability is significantly improved in most real-world tasks. Furthermore, the authors investigate the statistical property of the newly-derived RVFL+. The authors provide a tight generalization error bound based on the Rademacher complexity for the RVFL+. Our generalization error bound benefits from the advantageous property of the Rademacher complexity. The Rademacher complexity is a commonly-used powerful tool to measure the richness of a class of real-valued functions in terms of its inputs, and thus better capture the property of distribution that generates the date. In the RVFL+, the weights and biases between the input layer and enhancement nodes are generated randomly and are fixed, the output weights are then calculated by the Moore–Penrose pseudo-inverse or the ridge regression. Therefore, the RVFL+ is considered as a special linear learning model. The Rademacher complexity is an ideal choice for the analysis of this type of methods, and can provide a high-quality generalization error bound in terms of its inputs. In contrast to the previous work, the authors provide a more tight and general test error bound, and the novel bound is also appropriate for various versions of the RVFL including the newly-derived KRVFL+. Last but not least, the authors construct some competitive experiments on 14 real-world datasets to verify the effectiveness and efficiency of the newly-derived RVFL+ and KRVFL+. The experimental results illustrate that the novel RVFL+ and KRVFL+ outperform state-of-the-art comparisons. More importantly, recent existing works have illustrated that the cascaded multi-column RVFL+ and the cascaded kernel RVFL+ can obtain the best performance in terms of effectiveness for the single-modal neuroimaging-based diagnosis of Parkinson’s disease and the transcranial sonography based computer-aided diagnosis of Parkinson’s disease, respectively. Notice that both the RVFL+ and KRVFL+ are basic learners and play key roles in these two ensemble learning methods. The codes of RVFL+ and KRVFL+ are available. The contributions of this paper are summarized as follows. The authors propose a novel random vector functional link network with privileged information, called RVFL+. The RVFL+ bridges the gap between randomized neural networks and the LUPI paradigm. Different from existing variants of the RVFL, the newly-derived RVFL+ provides an alternative paradigm to train the RVFL. The authors extend the RVFL+ to the kernel version called KRVFL+, and the KRVFL+ enables handling effectively highly nonlinear relationships between high-dimensional inputs. The previous works of the LUPI focus on two aspects: deriving an efficient solver and combining the LUPI paradigm with different learning models. This paper focuses on the latter. However, from the optimization perspective, the authors find that the novel RVFL+ has sampler constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the RVFL+, which naturally tackles the former. This paper not only gives a comprehensive theoretical guarantee using the Rademacher complexity for the new RVFL+, but it also empirically verifies that the newly-derived RVFL+ and KRVFL+ outperform state-of-the-art methods on 14 real-world datasets. The remainder of this paper is organized as follows. The authors brief the related work of the RVFL in Section 2. In Section 3, the authors briefly explain the reason that the RVFL works well for most real-world tasks, and then introduce the newly-derived RVFL+ and KRVFL+. The authors study the statistical property of the RVFL+ and provide a novel tight generalization error bound based on the Rademacher complexity in Section 4. In Section 5, the authors build several experiments on 14 real-world datasets to evaluate the proposed RVFL+ and KRVFL+. This paper concludes in Section 6.", "section": "Related Work", "doi": "10.1016/j.neunet.2019.09.039", "references": [6533109, 176909285, 223114164, 1533861849, 1596717185, 1600810180, 1625958017, 1677182931, 1755117326, 1986278072, 1987618706, 1996640396, 2012638612, 2064675550, 2067562626, 2084251795, 2097998348, 2099579348, 2108544580, 2109574129, 2117138194, 2118181058, 2119821739, 2123223828, 2126885794, 2126942721, 2136331731, 2136922672, 2138383519, 2138580951, 2141225381, 2153232138, 2153635508, 2154209944, 2160354932, 2163605009, 2169408065, 2173379916, 2178357350, 2229668941, 2283039974, 2286961399, 2465334159, 2506404347, 2550643483, 2557283755, 2586160710, 2593382986, 2607586284, 2727059890, 2736488765, 2738226240, 2753648062, 2761362383, 2762962020, 2899470766, 2906146483, 2911964244, 2912162873, 2912934387, 2921936057, 2953267151]}
{"paragraph": "Telecom service provided by the service operator and mobile phone handset made by the handset manufacturer are indispensable and complementary elements when a consumer chooses a mobile phone from the mobile phone market. This complementarity has made it viable for firms to introduce the bundled channel to realize a bulk of sales, that is, consumers buy the contractual handset and the telecom service as a bundled package from either the manufacturer or the operator. At the same time, firms also sell handset models through an unlocked channel, that is, consumers get the unlocked handset from the manufacturer and subscribe to the telecom service from an arbitrary operator separately. In many countries, firms have deployed the dual sales channels to deliver both handset and service to end consumers. For example, in China, many manufacturers’ handset models such as Huawei Honer 7, Samsung Galaxy 7, and Apple iPhone 6s have been sold with subscription to China Unicom's telecom service, and these handset models also have been sold without any telecom service subscription. The dual sales channels will likely cause substitutable competition. It is, therefore, very likely that this phenomenon also occurs in the dual sales channels of the mobile phone industry. In the dual-channel distribution setting, manufacturer and operator provide essentially the same product and service, especially in the functional aspect. Consumers can get what they want from either channel. Thus, the unlocked and bundled channel may compete for the same group of consumers. On the other hand, the perceived experience difference between the unlocked and bundled channel induces consumers to choose the channel that is better suited to their needs. In the bundled channel, for example, operators require that the contractual handset should work exclusively with their own services within 12–36 months so as to gain the future subscriber revenues and offset the cost of subsidies, that is, consumers have to be tied up to the particular operator for a fixed period of time. Because exclusive arrangement is not convenient and the value of convenient plays the most important role in mobile phone usage, people looking for convenience will like the unlocked channel better. The substitutable competition between the unlocked and bundled channel poses vexing challenges for the manufacturer and operator. On one hand, although leveraging the bundled channel may enable greater market penetration, it may cannibalize the sales in the unlocked channel. In such a setting, how players decide their pricing strategies as well as the subsidy strategy to resolve the problem and maximize their profits will be an important factor in their success. On the other hand, the cannibalization of the dual sales channel leads to a more complex channel relationship and causes channel conflict. In general, the channel conflict tends to be a negative force that may hurt profits for all parties. For this, various contracts have been widely implemented for alleviating the channel conflict. For instance, subsidy contract is often implemented as a control mechanism to increase the sales volume as well as to stimulate the manufacturer and operator to collaborate with each other. Thus, how to manage coordinating and contracting problem among the manufacturers and operators becomes increasingly meaningful and challenging. To our knowledge, the extant literature has sparsely addressed the relevant problem for the rapid-developing mobile phone sector, even less effort has been devoted to examining the relevant problems to substitutable competition. This paper intends to contribute to fill this research gap by investigating a range of operational decisions and supply chain coordination issues. More specifically, we propose to answer the following research questions: How do the operator and the manufacturer make their optimal decisions as a whole on bundled and unlocked retail prices in a centralized system with substitutable competition? What are the operator's subsidy policy and the manufacturer's optimal pricing policy in a decentralized system with substitutable competition? Can a subsidy-only contract coordinate the competitive dual sales channels that possess both complementarity and substitutability? If not, what contractual terms should be designed to improve the overall performance? To address these questions, we will start with the demand model setting that can capture the features of the dual sales channels of the mobile phone industry. Then three scenarios are analyzed in a game theoretic framework: centralized scenario, which mainly as a benchmark for coordinating analysis; decentralized scenario, which is presented to verify the coordination failure of the subsidy-only contract; decentralized scenario with a two-way revenue sharing contract, which is presented for designing a properly and practical coordinating contract. Finally, after presenting equilibrium strategies and the relevant sensitivity and coordination analyses, we further analyze the impact of two-way revenue sharing contract and telecom service price on channel profit, as well as the impact of telecom service price on the two-way revenue sharing contract. Our key contributions to the literature are the following. First, this paper enriches the operations management studies of the mobile phone supply chain by analyzing three models in a Stackelberg game theoretical framework, as well as taking into consideration of the substitutable competition. We specifically investigate the channel members’ operational decisions for each model, which are well founded as well as practically relevant to real-world operations in some way. Second, this paper extends the supply chain coordination and contracting literature by modeling a subsidy-only contract and a two-way revenue sharing contract for the mobile phone supply chain. Our result implies that the widely used subsidy-only contract fails to coordinate the mobile phone supply chain due to the competition between the bundled and unlocked channel, thus we propose a two-way revenue sharing contract to achieve the optimal overall supply chain performance. The remainder of the paper is organized as follows. In Section 2, we provide a review of the related literature. Section 3 details the dual sales channels of the mobile phone industry. In Section 4, we present our models and analyses, and then focus on the coordination issues. We conduct the numerical analysis in Section 5, and provide two extensions in Section 6. The summary of our key findings and future research directions are presented in Section 7.", "section": "Methodology", "doi": "10.1111/itor.12451", "references": [1500626379, 1789954534, 1965007576, 1982862737, 2001330444, 2017767555, 2020907614, 2025887266, 2028308112, 2038476837, 2061660901, 2077800159, 2082011667, 2104052944, 2111107475, 2112482196, 2126741666, 2139753030, 2149035559, 2157406401]}
{"paragraph": "Nearest neighbor search is one of the most fundamental tools in many areas of computer science, such as image recognition, machine learning, and computational linguistics. For example, one can use nearest neighbor search on image descriptors such as MNIST to recognize handwritten digits, or one can find semantically similar phrases to a given phrase by applying the word2vec embedding and finding nearest neighbors. The latter can, for example, be used to tag articles on a news website and recommend new articles to readers that have shown an interest in a certain topic. In some cases, a generic nearest neighbor search under a suitable distance or measure of similarity offers surprising quality improvements. In many applications, the data points are described by high-dimensional vectors, usually ranging from 100 to 1000 dimensions. A phenomenon called the curse of dimensionality, the existence of which is also supported by popular algorithmic hardness conjectures, tells us that to obtain the true nearest neighbors, we have to use either linear time in the size of the dataset or time/space that is exponential in the dimensionality of the dataset. In the case of massive high-dimensional datasets, this rules out efficient and exact nearest neighbor search algorithms. To obtain efficient algorithms, research has focused on allowing the returned neighbors to be an approximation of the true nearest neighbors. Usually, this means that the answer to finding the nearest neighbors to a query point is judged by how close in some technical sense the result set is to the set of true nearest neighbors. There exist many different algorithmic techniques for finding approximate nearest neighbors. Classical algorithms such as kd-trees or M-trees can simulate this by terminating the search early, for example shown by Zezula et al. for M-trees. Other techniques build a graph from the dataset, where each vertex is associated with a data point, and a vertex is adjacent to its true nearest neighbors in the data set. Others involve projecting data points into a lower-dimensional space using hashing. A lot of research has been conducted with respect to locality-sensitive hashing, but there exist many other techniques that rely on hashing for finding nearest neighbors. We note that, in the realm of LSH-based techniques, algorithms guarantee sublinear query time, but solve a problem that is only distantly related to finding the k nearest neighbors of a query point. In practice, this could mean that the algorithm runs slower than a linear scan through the data, and counter-measures have to be taken to avoid this behavior. Given the difficulty of the problem of finding nearest neighbors in high-dimensional spaces and the wide range of different solutions at hand, it is natural to ask how these algorithms perform in empirical settings. Fortunately, many of these techniques already have good implementations. This means that a new variant of an existing algorithm can show its worth by comparing itself to the many previous algorithms on a collection of standard benchmark datasets with respect to a collection of quality measures. What often happens, however, is that the evaluation of a new algorithm is based on a small set of competing algorithms and a small number of selected datasets. This approach poses problems for everyone involved: The algorithm’s authors, because competing implementations might be unavailable, they might use other conventions for input data and output of results, or the original paper might omit certain required parameter settings, and even if these are available, exhaustive experimentation can take lots of CPU time. Their reviewers and readers, because experimental results are difficult to reproduce and the selection of datasets and quality measures might appear selective. This paper proposes a way of standardizing benchmarking for nearest neighbor search algorithms, taking into account their properties and quality measures. Our benchmarking framework provides a unified approach to experimentation and comparison with existing work. The framework has already been used for experimental comparison in other papers to refer to parameter choice of algorithms and algorithms have been contributed by the community, e.g., by the authors of NMSLib and FALCONN. An earlier version of our framework is already widely used as a benchmark referred to from other websites.", "section": "Introduction", "doi": "10.1007/978-3-319-68474-1_3", "references": [53016366, 73389528, 145388748, 1430582609, 1627400044, 1870428314, 1970647353, 2055839530, 2070572105, 2075547840, 2084434464, 2084732238, 2085937320, 2086179657, 2118323718, 2122196799, 2147717514, 2149684715, 2153579005, 2157092487, 2157169955, 2165558283, 2183176930, 2318810549, 2397770138, 2480086555, 2523268797, 2532189199, 2537425075, 2736701013, 2752891636, 2756637424, 2757117395, 2774705174, 2963671040]}
{"paragraph": "Clique-width. Every NP-hard graph problem becomes polynomial-time solvable after placing appropriate restrictions on the input. A general method in the design of graph algorithms for special graph classes is to decompose the vertex set of the graph into large sets of “similarly behaving” vertices and to exploit this decomposition algorithmically. An optimal vertex set decomposition gives us the “width” of the graph. Clique-width is one of the most studied width parameters. The vertex set decomposition of clique-width is defined via a graph construction based on vertex labellings. Starting from the empty graph, a graph G is built up vertex-by-vertex using four specific graph operations. These operations ensure that vertices labelled alike will keep the same label and thus “behave” identically. The clique-width of G is the minimum number of different labels needed to construct G in this way. A graph class has bounded clique-width if there exists a constant c such that every graph in the class has clique-width at most c. The algorithmic importance of having bounded clique-width follows from the existence of several meta-theorems which, when combined with an approximation result, ensure that many well-known NP-hard graph problems, such as Graph Colouring and Hamilton Cycle, become polynomial-time solvable for every graph class of bounded clique-width. Hence, there is a need to verify boundedness of clique-width of special graph classes, in particular when undertaking a systematic classification into the computational complexity of graph problems under input restrictions. Well-quasi-orderings. A graph class is well-quasi-ordered by a containment relation if for any infinite sequence of graphs in the class, there is a pair such that one is contained in the other. Just as is the case for having bounded clique-width, being well-quasi-ordered is a highly desirable property, which has been frequently discovered in the areas of discrete mathematics and theoretical computer science. To illustrate its importance, let us mention the seminal project of Robertson and Seymour on graph minors, which culminated in 2004 in the proof of Wagner's conjecture. Wagner's conjecture states that the set of all finite graphs is well-quasi-ordered by the minor relation, a graph H is a minor of a graph G if H can be obtained from G via a sequence of vertex deletions, edge deletions and edge contractions. As an algorithmic consequence, given a minor-closed graph class, it is possible to test in cubic time whether a given graph belongs to this class. To give some more examples, a result of Ding implies that every class of graphs with bounded vertex cover number is well-quasi-ordered by the induced subgraph relation, whereas Mader showed that every class of graphs with bounded feedback vertex number is well-quasi-ordered by the topological minor relation. Fellows, Hermelin and Rosamund simplified the proofs of these results and also showed that every class of graphs of bounded circumference is well-quasi-ordered by the induced minor relation. Furthermore, as “interesting applications” of these three results, they gave linear-time algorithms for recognizing graphs from any topological-minor-closed graph class of bounded feedback vertex number, any induced-minor-closed graph class of bounded circumference, and any induced-subgraph-closed graph class of bounded vertex cover number. Hereditary graph classes. Graph classes closed under taking induced subgraphs are said to be hereditary. Courcelle proved that the class of graphs obtained from graphs of clique-width 3 via one or more edge contractions has unbounded clique-width. This means that the clique-width of a graph can be much smaller than the clique-width of its minors. On the other hand, the clique-width of a graph is at least the clique-width of any of its induced subgraphs. Hence, it is natural to focus on determining boundedness of clique-width for hereditary graph classes. Research goals. Nothing in the definitions of clique-width and well-quasi-orderability suggests that there is anything in common between the two notions. However, as we will discuss, recent results for hereditary graph classes suggested an intriguing connection between them. This connection is not yet well understood, and as such, it remains to be further explored. Our underlying research goals are: to increase understanding of the relation between well-quasi-orderability by the induced subgraph relation and boundedness of clique-width for hereditary graph classes; and to obtain new results for both notions applied to hereditary graph classes. In a previous paper we showed that certain graph operations and graph constructions work equally well for bounded clique-width and well-quasi-orderability. In this paper we will explore common graph techniques further. Before discussing our results in detail, we first discuss a conjecture that motivated our research. Conjecture. We first note that the hereditary class of graphs of degree at most 2 is not well-quasi-ordered by the induced subgraph relation, as it contains the class of cycles, which form an infinite antichain. As every graph of degree at most 2 has clique-width at most 4, having bounded clique-width does not imply well-quasi-orderability by the induced subgraph relation. In 2010, Daligault, Rao and Thomassé asked about the reverse implication: does every hereditary graph class that is well-quasi-ordered by the induced subgraph relation have bounded clique-width? At WG 2015, Lozin, Razgon and Zamaraev gave a negative answer to this question. It is readily seen that a class of graphs is hereditary if and only if it can be characterised by a unique set of minimal forbidden induced subgraphs. As the set of minimal forbidden induced subgraphs in the counter-example is infinite, the question of Daligault, Rao and Thomassé remains open for finitely defined hereditary graph classes, that is, hereditary graph classes for which the set is finite.", "section": "Related Work", "doi": "10.1016/j.jcss.2019.09.001", "references": [52229496, 1546876109, 1581957840, 1785928421, 1967174286, 1977950997, 1988827593, 1993328543, 1994331970, 1996873316, 2005079828, 2007843697, 2008361472, 2020139369, 2026807383, 2052306562, 2053991811, 2063951771, 2067016370, 2070790034, 2076819776, 2097592506, 2097725094, 2110917011, 2115589427, 2116334057, 2122172628, 2147653456, 2148842617, 2167462019, 2623827933, 2733323637, 2762850984, 2962956065, 2963233231, 2963558861, 2963637237, 2964039793, 2964181817]}
{"paragraph": "In 2004 Web 2.0 was launched, marking the beginning of a new era in the history of the internet. This era is characterized by a shift from passive to more active internet consumption, allowing users to develop and disseminate their own content and to communicate. Social media constitutes the most popular platforms for these actions. These are internet-based applications that allow the creation and exchange of user generated content. These applications include blogs, social networking sites such as Facebook, content-sharing sites such as YouTube and more. Since the mid-2000s, social media joined other sources of health information by becoming a platform for posting, sharing or commenting on health-related content and for joining health-related groups. Yet research on health-related internet use has concentrated mostly on searching for online health information and much less on using the internet for other health-related purposes, such as online health services and health participation, which is the focus of this study. The notion of health participation is still poorly defined. Usually, active health-related participation refers to communication regarding health issues. On social media, however, it may include more actions. Three main types of health participation activities are mentioned in the literature: sharing personal experiences regarding chronic health conditions, discussing the work of health institutions, usually by posting of reviews on doctors, and posting or commenting on health-related content. The current study uses the lifestyle/exposure theoretical framework. According to this theory, the probability individuals will behave in a specific way is the result of their lifestyle and habits. Hence, differences in probabilities of engaging in a behavior reflect differences in lifestyle. Since lifestyle behaviors, like any other behavior, are a consequence of an individual's position in the social stratification system, differences in behaviors can be attributed to socioeconomic background. This theory has been tested in the context of offline and online victimization. Nevertheless, it has never been used to explain activities unrelated to victimization, especially those related to health participation via social media. Furthermore, gender differences in health participation have never been examined using this theoretical framework. The issue of gender in health-related use of the internet is well documented in studies in the fields of internet sociology and public health. The most stable finding across studies is that female users search for health information more than male users. Yet, due to the small body of knowledge on gender differences in health participation, the following research question emerges: What is the gender structure of health participation on social media? This question can be broken down into three alternatives: the known gender differences in searching for OHI are replicated in health participation behaviors, the absolute monopoly hypothesis; male users are more active health participants than female users, the areas of control hypothesis; there are no gender differences at all in health participation, the democracy hypothesis. According to the absolute monopoly hypothesis, women are expected to be dominant not only in searching for online health information, but also in the domain of active health participation. The absolute monopoly hypothesis reflects traditional gender role models, according to which women tend to take part in online health-related activities due to their family role of health caregivers and health managers. In contrast, according to the areas of control hypothesis, women and men will have separate domains of dominance in health-related internet use. Female users will be dominant in searching for health information, while male users are expected to be dominant in the health participation domain. This hypothesis is based on the finding that male users tend to be frequent contributors to online discussions, reflecting differences in socialization such that women, as compared to men, are socialized to more passive gender roles. Finally, the democracy hypothesis contends that there are no gender differences in health participation on social media. Some studies found no such differences with respect to communicating on health issues via the internet, thereby supporting this hypothesis. This study makes several contributions to the field. From the theoretical perspective, it extends the lifestyle/exposure theory to cover gender differences in online activities, which are unrelated to the field usually studied in the context of this theory. From the empirical perspective, this study employs a toolkit approach, according to which health participation on social media is considered multidimensional and therefore investigated using several outcomes. The article is organized as follows. First, we provide a general literature review, followed by a discussion of the research hypotheses and other factors that can affect health communication via social media. After that, we describe the methodology used in the study. Then we provide the descriptive, bivariate and multivariate results of the study. Finally, we discuss the results and outline the main conclusions.", "section": "Methodology", "doi": "10.1016/j.chb.2019.08.016", "references": [1961406311, 1969052573, 1973434331, 1981531558, 2026229375, 2028182940, 2029430065, 2029522813, 2029833201, 2054865483, 2071997882, 2092129161, 2110016748, 2131567188, 2169205257, 2399742221, 2559738839, 2765897036, 2775442000]}
{"paragraph": "Exam timetabling is a very important and time-critical task that faces registrars every semester in most educational institutions. This task is very complex and requires assigning a date, time, and room to every exam while ensuring that time, spatial, and other constraints are satisfied. With the increasing number of students, exams, and demands, solving the Exam Timetabling Problem manually is not a practical option and hence providing a computational solution for it attracted the attention of many researchers from the 1960s until recently. The ETP is known to be an NP-complete optimization problem, meaning it is unlikely to solve it optimally in polynomial time. Therefore, researchers tried and are still experimenting with many different ways that are based on mathematical models, heuristic techniques, or a combination of different algorithms to find quick and acceptable solutions for it. For example, in the survey presented in one study, the exam timetabling research was classified based on the techniques used to solve the problems such as: graph based, local search based, population based, and other methods. In summary, the proposed solutions compare to each other based on the used techniques, the hybridization of different methods, the decomposition of the problem into smaller sub-problems, and the hard and soft constraints taken into consideration. In this paper, a new technique is introduced to solve the ETP with emphasis on issues related to German Jordanian University and its set of rules and limiting constraints. The proposed method has three novelties. First, a feasible solution to the problem is found by segmenting it into three phases, which reduces the number of constraints to be considered in each phase and hence allows the optimizer to reach a desired solution in a quick manner with reduced memory demands. Noting that, an Integer Linear Programming based approach is used to find the solution for each phase. Second, a comprehensive set of hard constraints is considered to generate an exams schedule that is comfortable to all students and meets the needs of the different faculties at the university. Third, the same exam can be allocated to one or multiple rooms unlike most of the similar techniques. Similarly to the method in this paper, several other papers discussed integer programming based techniques to solve the ETP. However, to the best of our knowledge, most of those methods tried to solve the problem in one phase, which usually results in a system of equations with a large number of variables and hence solving it is very CPU and memory intensive. Unlike other methods, the hard constraint to prevent a student from having two or more exams in the same day as in this paper was not considered in some prior studies. In one approach, a hybrid adaptive decomposition approach was used to break the exams into difficult and easy sets before using an ILP approach to obtain a solution. Some of the approaches that did not use a mathematical ILP model to solve the ETP will be briefly discussed next. Such approaches either used graph coloring, metaheuristic, hybrid, or other methods to find a solution for the ETP. They can also be categorized into two groups based on whether all the hard constraints in this paper were considered or not. The first group of papers, the group that considered the set of hard constraints as in this paper, is discussed first. In one method, two column generation algorithms were used to solve the ETP. Another study utilized hyper-heuristic approaches. An adaptive linear combination of heuristics with a heuristic modifier under the framework of adaptive strategies was proposed. A search algorithm that consists of several phases is introduced. In the first construction phase, a complete solution is found using an iterative forward search algorithm. In the later phases, a local optimum is found using a combination of a hill climbing algorithm and great deluge technique. Another method hybridized bin packing heuristics to assign exams to time slots and rooms. In another study, the solution is based on graph coloring heuristics that were hybridized to generate four new low level heuristics. In one approach, the ETP was solved using a variable neighborhood search methodology. A random iterative graph based hyper-heuristic was used to produce a collection of heuristic sequences to construct solutions of different quality. Next, the second group of papers, that did not consider all hard constraints used in this paper, is presented. For example, a hybrid bee colony optimization approach was used. In another method, a sequential graph coloring with the largest enrolment first heuristic was used to construct a conflict-free examination timetable and then a simulated annealing heuristic was used to fit examinations into rooms, while satisfying the back-to-back constraint. In other studies, hill climbing and great deluge local search were used to solve the problem. A hybrid harmony search algorithm was used. Decomposition as well as a graph coloring heuristic was used. A hyper-heuristic approach was used. Heuristics and a stochastic algorithm called the roulette wheel graph coloring were used to solve the problem. In that method, the algorithm was also tested on the examination timetabling benchmark datasets. A graph-coloring-based method was utilized, but without considering the actual distribution of exam sessions to rooms. In one approach, the solution of the course-timetabling problem was used to construct an initial solution to the examination timetable. Finally, a hybrid two phase method was introduced to tackle the ETP. The rest of the paper is organized as follows. In Section 2, the hard constraints under consideration and an overview of the proposed method are provided. In Section 3, the adopted nomenclature, problem setup, and decomposing the ETP into three sub-problems are discussed. In Section 4, a simple example is used to show how the problem formulation is represented in AMPL format. In Section 5, the method is validated based on GJU registration information and real-world benchmark data. Finally in Section 6, conclusions and future works are presented.", "section": "Introduction", "doi": "10.1111/itor.12471", "references": [174805181, 178109621, 1526848900, 1964072303, 1966773610, 1969214516, 1977049223, 1982486082, 1984754775, 1989380511, 1995123252, 1996746644, 2010046755, 2014856631, 2031012845, 2038015627, 2047956445, 2056956492, 2074281851, 2082961839, 2092487519, 2121904912, 2293848204, 2586084087]}
{"paragraph": "Computing the Gröbner basis of an ideal with respect to a given term ordering is an essential step in solving systems of polynomials. Certain term orderings, such as the degree reverse lexicographic ordering, tend to make the computation of the Gröbner basis faster. This has been observed empirically since the 1980s and is now supported by theoretical results, at least for some nice families of inputs, such as complete intersections or certain determinantal systems. On the other hand, other orderings, such as the lexicographic ordering, make it easier to find the coordinates of the solutions, or to perform arithmetic operations in the corresponding residue class ring. For instance, for a zero-dimensional radical ideal in generic coordinates, the Gröbner basis of the ideal for the lexicographic ordering has a specific triangular form with all polynomials of degree less than a certain bound and the leading polynomial squarefree; this is known as the shape lemma. The points in the variety are then directly obtained from these polynomials. As a result, the standard approach to solve a zero-dimensional system by means of Gröbner basis algorithms is to first compute a Gröbner basis for a degree ordering and then convert it to a more exploitable output, such as a lexicographic basis. The latter step, while of polynomial complexity, can now be a bottleneck in practice. This paper focuses on this step; in order to describe our contributions, we first discuss previous work on the question. Let the input ideal be zero-dimensional. We assume that we know a monomial basis of the quotient ring together with the multiplication matrices of the variables in this basis. We denote by D the degree of the ideal, which is the vector space dimension of the quotient ring. Starting from a degree Gröbner basis, computing the multiplication matrices efficiently is not a straightforward task. Previous work showed how to do it efficiently, and more recent algorithms achieve faster bounds, at least for some favorable families of inputs. The FGLM algorithm computes the lexicographic Gröbner basis of the ideal using linear algebra operations. While the algorithm has an obvious relation to linear algebra, lowering the runtime was only recently achieved. Polynomials with the triangular shape mentioned earlier form a very useful data structure, but there is no guarantee that the lexicographic Gröbner basis has such a shape. When it does, we say the ideal is in shape position. As an alternative, one may use the Rational Univariate Representation algorithm to describe the zero-set by means of univariate rational functions, where the multiplicity of a root coincides with that of the ideal at the corresponding point. Using rational functions allows one to control precisely the bit-size of their coefficients when working over fields like the rationals. These algorithms rely on duality, which will also be at the core of our algorithms. They compute sequences of values involving trace forms and generic linear combinations of variables. From these values, one may then recover the output using structured linear algebra calculations. A drawback is that we need to know the trace of all elements of the basis, which is polynomial-time computable but not trivial. Randomization can alleviate this issue. It has been shown that computing values involving traces of random linear forms allows one to deduce a description of the variety by a set of rational functions with univariate polynomial denominators. The tuple computed by such algorithms is called a zero-dimensional parametrization of the variety. It generally differs from a Rational Univariate Representation, since the latter keeps track of multiplicities. Starting from such a parametrization, we can reconstruct the local structure of the ideal at its roots using additional algorithms. The most costly part of the randomized algorithm is computing the trace values; the rest involves standard univariate techniques. It was later pointed out that the multiplication matrices can often be expected to be sparse, and estimates on their sparsity have been given assuming the validity of certain conjectures. Based on this, sparse variants of FGLM have been designed. For example, if the ideal is in shape position, one can recover its lexicographic basis using trace values involving a linear form. For less favorable inputs, these algorithms fall back on more general methods. The techniques used are based on Krylov subspace methods and Berlekamp-Massey techniques, similar to those used for solving sparse linear systems and problems in integer factorization or discrete logarithm computations. Block versions of these methods allow for parallelization and were pioneered in that context. It is natural to adapt this strategy to Gröbner basis computations. One prior work showed how to compute the main univariate polynomial using such block techniques, assuming the base field is finite. The algorithm computes the roots of this polynomial and substitutes them into the system before computing a Gröbner basis for each root. Our first contribution is to give a block version of the randomized algorithm that extends the previous approach to compute all polynomials in the parametrization for essentially the same cost as computing the univariate polynomial. More precisely, the bottleneck of the algorithm is the computation of a block-Krylov sequence; once this sequence is computed, all polynomials in the zero-dimensional parametrization can be obtained. Unlike some previous algorithms, ours deals with any zero-dimensional ideal, not just those in shape position, though it assumes the base field has sufficiently large characteristic. The output is somewhat weaker than a Gröbner basis, as multiplicities are not computed. While we focus on sparse multiplication matrices, we also analyze the dense case. Our second contribution refines the first by trying to avoid computations with a generic linear form, motivated by the fact that its multiplication matrix is often denser. The refined algorithm first computes a zero-dimensional parametrization of a subset of the variety for which we can take a simple variable, and then applies the general algorithm to the rest. If the initial subset is large, this is expected to provide a speed-up. For experiments, our algorithms have been implemented in C++ using standard linear algebra and symbolic computation libraries. The paper is organized as follows. The next section reviews results on scalar and matrix recurrent sequences and gives an algorithm to compute a scalar numerator. Section 3 describes sequences arising in FGLM-like algorithms and proves some refined versions of earlier results. The main algorithm is given in Section 4, and the refinement is in Section 5. An appendix provides proofs of technical results on recurrent sequences. Complexity model: we count basic operations at unit cost. Most algorithms are randomized and involve selecting a vector of field elements; success is guaranteed if the vector avoids a certain hypersurface in parameter space. Suppose the input ideal is generated by a set of polynomials. Given a zero-dimensional parametrization found by our algorithms, one can always evaluate those polynomials at the parametrization, modulo the univariate polynomial. This allows us to check whether the output describes a subset of the variety, but not whether we have found all solutions. If the parametrization has the same dimension as the quotient ring, we can infer that we have all solutions and that the ideal is radical, so the output is correct. In what follows, we assume that the number of variables equals the dimension of the quotient ring, to simplify cost estimates. We use standard bounds for polynomial multiplication, and assume super-linearity of the time function. Then two matrices over the field with polynomials of degree less than a bound can be multiplied efficiently. Acknowledgments: We thank several researchers for helpful discussions and a reviewer for useful remarks. This research was partially supported by grants and institutional programs.", "section": "Related Work", "doi": "10.1016/j.jsc.2019.07.010", "references": [149384130, 1502078890, 1562183207, 1590894046, 1968561983, 1976677460, 1979462002, 1985924702, 1986421748, 1986905837, 1995653528, 2001224529, 2003767038, 2011693299, 2015497178, 2029891988, 2057341276, 2059454546, 2059522106, 2071483197, 2081523647, 2088713140, 2153077274, 2161330625, 2238531909, 2729305841]}
{"paragraph": "One of the central problems in elimination theory is the construction of determinantal formulae for the resultant. In this context there is also a special emphasis on exploiting the sparsity of the input, or in other words the support, of the involved polynomials. Among the various constructions the best we can hope for is a degree-one formula; that is a matrix whose non-zero entries are coefficients of the input polynomials, and whose determinant is equal to the resultant. The Sylvester-type formulae fall in this category. Unfortunately, such formulae do not always exist for given Newton polytopes. There are also Bézout-type formulae where the entries of the matrix are coefficients of the Bézoutian polynomial and thus they are high degree polynomials in the coefficients of the input polynomials. We call the matrices that have entries that are both Sylvester-type and Bézoutian-type, hybrid. We focus on resultants and discriminants for polynomial systems in two variables. A polynomial system is unmixed if all of its polynomials have the same Newton polytope, and mixed otherwise. Exact resultant formulae are mostly known for certain classes of unmixed systems, and very little is known for the general mixed case. We are interested in optimal degree-one formulae for the mixed resultant. Degree-one formulae are very convenient for both the analysis and the implementation of resultant methods, since their matrix expressions are simple to compute and have bitsize that matches the bitsize of the input. Common degree-one formulae are the Sylvester-type formulae; in this work we present a different one that expresses a second order Koszul map. Khetan presented explicit exact formulae for an arbitrary unmixed sparse bivariate polynomial system. His determinantal formula is a hybrid Sylvester and Bézout type. Also in the unmixed case there are necessary and sufficient conditions for the Dixon resultant formulation to produce the resultant. In the same context, Elkadi and Galligo proposed to use a variable substitution and two iterated resultants to compute the resultant polynomial. Regarding Sylvester-type formulae, matrices expressing optimally the resultant of unmixed bivariate polynomials with corner-cut support are found. Moreover, Sylvester formulae for more general unmixed bivariate systems were depicted. The proof of the main theorem makes use of tools from algebraic geometry, including sheaf cohomology on toric varieties and Weyman's resultant complex. There are also methods for constructing resultant matrices for bivariate polynomial systems that combine Sylvester type blocks with toric Jacobian blocks in the case where the Newton polytopes of the polynomials are scaled copies of a single polygon. The determinant of these matrices is a multiple of the sparse resultant, that is, the formula might not be optimal. Resultants are closely related to discriminants. Discriminants have many applications, ranging from singularity theory of partial differential equations to the computation of the Voronoi diagram of curved objects. Especially for the bivariate case there is work that relates the mixed discriminant with the sparse resultant and the toric Jacobian. Tensor-product systems fall in the general category of multihomogeneous systems. One finds the first expressions of the resultant of such systems as the determinant of a matrix. For unmixed multigraded systems Sturmfels and Zelevinski provided optimal Sylvester-type formulae. These formulae arise as certain choices of a Weyman complex of modules. Many, if not all, classical resultant matrices are instances of such complexes, including the projective resultant. There is a systematic exploration of possible determinantal complexes, and also a software package that produces formulae for unmixed and even scaled resultants. Interestingly, there is a plethora of hybrid resultant matrices that consist of Bézout-type and Sylvester-type blocks. The main contributions of this work are as follows. We present a determinantal degree-one formula for the resultant of arbitrary mixed bivariate tensor-product systems. The formula applies without any restrictions on the bidegree of the polynomials, it expresses a Koszul map and has degree one with respect to the coefficients of the system. Moreover, we prove that the univariate and the bivariate case are the only cases among tensor-product polynomials which admit an unconditional formula of degree one. We provide a constructive method to compute this matrix explicitly, by identifying the dual multiplication maps, therefore making our formula explicit. We call the matrix Koszul resultant matrix. The latter construction allows us to derive formulae for computing the discriminant of one or two bivariate tensor-product polynomials. Compared to the existing literature, we reduce significantly the degree of the extraneous factor which is involved in the computation. Another important aspect is that our formulae provided are free of nonzero multiplicative constant, so that they yield smoothness criteria that are valid in arbitrary characteristic. The rest of the paper is organized as follows. In the next section we present preliminary results that we need for our construction. In Section 3 we present the mixed resultant complex for the bivariate case and we derive the determinantal Koszul formula. Moreover, we show that universal degree-one formulae arise for at most two variables. In Section 4 we provide the algorithmic construction of the Koszul resultant matrix, by identifying the cohomology groups which appear in the complex. Finally, in Section 5 we tackle the problem of computing some related discriminants and mixed discriminants. Throughout the paper, several toy examples accompany the main results. A preliminary version of this paper appeared earlier. In the current final version the Sections 4 and 5 are expanded significantly, and in particular the computation of mixed discriminants is improved by relating them with mixed resultants of lower degree.", "section": "Methodology", "doi": "10.1016/j.jsc.2019.07.007", "references": [1857034947, 1963919699, 1966341842, 1968297701, 1971439379, 2025561788, 2027815348, 2048329955, 2058393311, 2061296216, 2066758283, 2067754443, 2088129367, 2108888104, 2149153582, 2618054456, 2952936986]}
{"paragraph": "Analyzing the image sequences captured from surveillance cameras has become an interesting research area in modern computer vision. The analysis of daytime images has been well studied in the past few years. However, researchers cannot use nighttime images to extract enough useful features and further information due to insufficient brightness. In fact, nighttime images have salient visual features which can be used to achieve many tasks. Repetitive window lights can be used to predict a planar surface; the light which changes over time reflects the laws of peoples’ work and rest habits; the spatial distribution of light can be used for recognition and classification of various locations. These salient light features make nighttime images distinctly different from daytime images, and we should fully excavate these rich data sources. Furthermore, the images captured from one static camera are nearly pixel-level aligned. The analyzed results from nighttime images can be used to verify those from daytime images, and a comprehensive analysis makes the result more reliable. In conclusion, the analysis of nighttime images has significance in computer vision. The results help expand the scope of visual information acquisition and open up a new door of thought for further big data analysis. In this paper, we focus on geometric analysis of a single nighttime image and explore the repetitive light structures on façades. These structures form regular grids or lattice structures. We apply these light cues to parse night scenes into façade planar surfaces in man-made scenes and estimate the surface depth based on the assumption of consistent storey heights. The method presented in this paper is an automatic algorithm to generate discrete planar surfaces. It includes three contributions to 3D parsing of a single-view night images in man-made environments, which mainly include: the first attempt to classify salient window lights in a nighttime image to detect planes; distinguish multiple façades of a single image effectively, and calculate their floor structures; an orientation estimation with low error is obtained without using any explicit geometric information, which is essential in other extraction methods applied to planar structures.", "section": "Introduction", "doi": "10.1007/s11704-017-6457-2", "references": [1500364503, 1521461170, 1795815886, 1989231886, 2037935589, 2042518273, 2063784584, 2077456032, 2095339704, 2102532099, 2116851763, 2122059483, 2124404372, 2131394160, 2155871590, 2160341214, 2162808897, 2167716896, 2203760849, 2244684328, 2906621894]}
{"paragraph": "Multi-view data are very common in some scientific data analytics problems such as computer video, social computing and environmental sciences, due to the use of different measuring methods or of different media, like text, video and audio. Multi-view clustering, which makes use of the complementary information embedded in multiple views to improve clustering performance, has attracted more and more attentions. In the existing methods, spectral clustering is a popular one for multi-view data because it represents multi-view data via graph structure and makes it possible to handle complex data such as high-dimensional and heterogeneous as well as it can easily use the pairwise constraint information provided by users. In some practical applications, particularly in the multimedia domain, a view is usually represented in a high-dimensional feature space. For high-dimensional data, the feature distribution is usually more sparse, the traditional similarity measurement methods based on distance measures become inapplicable. In order to solve this problem, the approaches based on low-rank representation try to learn a common low-dimensional subspace from the high-dimensional multi-view data and each object can be represented linearly by others objects in the same subspace, which contributes to reduce the computation cost and improves the robustness to noise corruptions. On the other hand, we find that a problem of the existing cluster analysis methods has not been addressed satisfactorily is the uncertain relationship between an object and a cluster. It is obviously that there are three relationships between an object and a cluster, namely, belong-to definitely, not belong-to definitely and uncertain. In most of the existing work, a cluster is represented by a single set, the set naturally divides the space into two regions. Objects belong to the cluster if they are in the set, otherwise they do not. Here, only two relationships are considered, no matter in hard clustering or in soft clustering. They are typically based on two-way decisions. Let us observe the third relationship, which means the object may or may not belong to the cluster. We just cannot make decisions based on the present obtained knowledge or information. We can make further certain decisions when we have further information. It is a typical idea of three-way decisions. Inspired by the theory of three-way decisions, a framework of three-way cluster analysis has been introduced. The previous results on three-way clustering provide us with a tool for studying the problem of clustering with uncertainty. In this paper, we focus on a general framework based on the theory of three-way decisions, which is appropriate for soft clustering or hard clustering. This three-way representation with two sets brings more insight into interpretation of clusters. Objects in the core region certainly belong to the cluster, objects in the trivial region definitively do not belong to the cluster, and objects in the fringe region maybe or may not belong to the cluster. Obviously, the two-way representation with a single set is a special case of three-way representation with two sets when fringe regions are empty. Compared with the supervised learning, clustering process lacks the user guidance or the class label information and may not produce the desired clusters. Thus, some semi-supervised clustering methods are proposed. These methods that use certain weak supervision form, such as pairwise constraints, can significantly improve the quality of unsupervised clustering. Pairwise constraints describe two objects whether they should be assigned to the same cluster or the different clusters. However, choosing the supervised information is random in most of existing methods, and it does not produce positive effect on improving the clustering result when the algorithm itself can find the prior information or there are amounts of noises in the prior information. Therefore, the active learning method is introduced to optimize the selection of the constraints for semi-supervised clustering. Hence, our work considers active learning of constraints in an iterative framework based on spectral clustering. In each iteration, we determine objects with the most important information toward improving the current clustering result and form queries accordingly instead of choosing the information randomly. The responses to the queries are then used to update the clustering. This process repeats until we reach a stable solution or we reach the maximum number of queries allowed. Such an iterative framework is widely used in active learning for semi-supervised clustering. The measurement of information is designed based on the entropy concept. Besides, to take the advantage of three-way representation of clustering, it is reasonable to choose pairwise constraints in fringe regions instead of the universe, which will improve the search efficiency. In this paper, we address the problem of clustering on multi-view data of high dimensionality. The main contributions are summarized as follows: A novel active three-way clustering method via low-rank matrices for multi-view data is proposed. It considers the diversity of multiple views and to improve the quality of clustering for multi-view data. A multi-view information fusion algorithm is presented via low-rank matrix representation for high-dimensional multi-view data, and the weights are adjusted adaptively on each view during solving the optimization problem of objective function. A three-way clustering representation is utilized to reflect the three relationships between an object and a cluster, namely, belong-to definitely, uncertain and not belong-to definitely. The three-way clustering approach provides us with a tool for studying the problem of clustering with uncertainty. An active three-way clustering algorithm is developed by taking the advantage of three-way representation of clustering, which can produce the three-way results as well as two-way results accordingly. The idea of farthest-first traversal scheme is used to construct the cores of clusters, then to expand cores to fringes by using the idea of k nearest neighbors, and the rules to adjust the consensus similarity matrix are also introduced. The proposed method is flexible for semi-supervised clustering as well as unsupervised clustering. We evaluate the proposed method with some other algorithms on seven real-world datasets. The results of comparative experiments demonstrate the effectiveness of the proposed method and show that it is appropriate for multi-view data of high dimensionality. The rest of the paper is organized as follows. In Section 2, we review existing clustering algorithms for multi-view data, clustering approaches for uncertain relationships and active learning approaches for clustering. In Section 3, we give a detailed description of the proposed method termed Active Three-way Clustering via Low-rank Matrices. In Section 4, we report on an extensive experimental evaluation of the proposed method. Finally, we summarize the present study in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.03.009", "references": [101345952, 201974436, 1041128124, 1511669739, 1558962159, 1596382552, 1670132599, 1965450189, 1997201895, 2004524042, 2013029404, 2016384870, 2023750115, 2029677459, 2056187338, 2070412788, 2077775960, 2079402740, 2080404663, 2085989833, 2088431846, 2103972604, 2105709960, 2135914502, 2137184539, 2142674578, 2153839362, 2154415691, 2159091719, 2165874743, 2166782149, 2169529055, 2210977594, 2294062232, 2297889545, 2405459681, 2512066509, 2518081091, 2527290711, 2716026328, 2951085447]}
{"paragraph": "In everyday life, we are surrounded by textual information: academic papers, technical reports, patents, business contracts, application forms for business procedures, presentation slides, emails, etc. Working with documents is ubiquitous in our work life, as well as our personal life. When working with documents, people frequently print them, not only for the ease of reading, but also for comparing information from different documents. New digital environments, such as tabletop systems, allow documents to be displayed in a similar way to a sheet of paper on a table. However, we need to understand how to optimize the reading and writing experience in tabletop systems for creating efficient and comfortable work environments. This work focuses on investigating preferred orientation of paper documents in relation to readers during reading, so readers can maintain a comfortable body posture and can easily handle documents while performing the reading as efficiently as possible. Informal observations of people reading physical paper documents show that right-handed people often rotate documents counter-clockwise and/or place documents to the right side of the body. When actively working with a document, such as writing, annotating or drawing, it is often rotated to a steeper angle. People rarely read documents placed at the central or median line of their body without any rotation. Scientific studies have reported on document positions, for instance, it was noted that readers rotated their working documents 0–20° when reading and 30–45° when writing. It was also observed that artists continuously rotated and adjusted the drawing bit-by-bit to keep the active drawing area in the same position relative to the hand. Another study reported that participants felt uncomfortable when reading on horizontal displays because the document was presented at an unnatural reading angle. It was observed that the orientation of documents or other objects, such as photos, could signal users’ intention to share them during collaborative work. For example, users indicated that they were not ready to share an object by placing it upside-down or at an odd angle. The document orientation can also serve as a barrier around a personal workspace, or as a prompt to others to view it. A few tabletop systems have specifically been developed to support reading or small group discussions around text documents. In these situations, being able to adjust a document rotation and location is important for creating a comfortable work environment. However, most of these tabletop systems assume that people prefer reading a document aligned to their median line. For instance, many systems provide a feature to automatically rotate documents to be perpendicular to the table’s closest edge. Some tabletop systems allow users to manually orient the documents. However, if the optimal document orientation is known, systems can automatically position documents according to users’ location and task. If reading and writing performance varies with document orientations, automatic positioning of documents can be an important feature for task efficiency. In this paper, we establish the optimal document orientation and investigate reasons for users’ preferences. We conducted five experiments to uncover both subjectively preferred and objectively optimal document orientation for left-handed and right-handed people. Although we do not use a tabletop device, our results can be applicable to tabletop applications as people’s activity with the document and their position relative to it remains the same. Therefore, we finally discuss how these results impact these systems.", "section": "Methodology", "doi": "10.1080/07370024.2018.1427584", "references": [1269801495, 1992306856, 1998408655, 2005855683, 2007580360, 2009220433, 2021346535, 2030533114, 2037001467, 2079876841, 2082591969, 2094567668, 2094982166, 2101082575, 2101532415, 2102773632, 2106779688, 2121172310, 2122624458, 2130334143, 2135718368, 2137092817, 2140706899, 2148969329, 2151942546, 2155811000, 2156317066, 2158165714, 2158707444, 2169463011]}
{"paragraph": "Pattern discovery has been widely studied in machine learning tasks such as image recognition and text analysis. A sequential pattern is a totally or partially ordered subsequence of a transactional dataset, symbolic sequence, numeric time series, or other data sequence. Sequential pattern discovery, also called pattern mining, refers to the discovery of all frequent sequential patterns from these data. For transactional datasets such as retailing, market-baskets, and planning, a pattern is a sequence of events, where each event is represented by an itemset. This type of representation has horizontal and vertical scalability, leading to general association rules. For symbolic sequences such as protein and text, a pattern is a subsequence of characters, also called a string. It can easily represent codons or keywords. For numeric time series such as stock prices and petroleum production, where the trend of fluctuation is essential to the stock holders and petroleum cooperation, a pattern is a subsequence of real values. One interesting research direction of symbolic sequential pattern discovery involves the generalization of patterns. A plain pattern is a subsequence that must be exactly matched. A wildcard, which is also called a motif, matches any character in the alphabet. A wildcard gap matches any subsequence within the length constraint. In this way, a pattern with wildcard gaps is able to handle noise or shifts. To enrich the semantics of the pattern, the alphabet is divided into weak and strong parts. A weak-wildcard gap matches a subsequence of weak characters. Consequently, a pattern with weak-wildcard gaps not only ignores weak characters, but also maintains strong ones. In this paper, we introduce a more general and flexible type of pattern called a tri-pattern that is inspired by three-way decisions. The alphabet is partitioned into three parts corresponding to strong, medium, and weak characters, respectively. In this situation, a tri-wildcard gap matches any sequence of medium or weak characters with a length between N and M. A tri-pattern is a sequence of strong or medium characters containing periodic tri-wildcard gaps, where the term periodic indicates that the gaps between any two adjacent characters are identical. The idea of a tri-partition has been widely adopted in many applications. Biologists partition the human amino acid alphabet into essential, conditional, and nonessential amino acids. Petroleum experts partition oil production fluctuation into significant, insignificant, and minor changes. The tri-partition of situations and actions also has attracted a large amount of research interest in data mining, especially three-way decisions. To the best of our knowledge, however, a tri-partition of an alphabet has not been considered in the three-way decisions research community. Tri-patterns are more general than the four existing types of patterns. Let tri-patterns be Type I patterns, plain patterns be Type II patterns, patterns with periodic wildcard gaps be Type III patterns, patterns with periodic weak-wildcard gaps be Type IV patterns, and strong patterns with periodic weak-wildcard gaps be Type V patterns. Types II through V are the special cases of Type I. Similar to existing frequent pattern discovery problems, a new problem for Type I patterns is defined. Using the Apriori property of the new problem, an Apriori algorithm is designed for efficient pattern discovery and tree pruning. We compare these five types of patterns in three application areas to reveal the universality of the new pattern. The first application is human protein sequence mining. Essential, conditional, and nonessential amino acids correspond to strong, medium, and weak characters, respectively. Twenty sequences were concatenated to discover frequent patterns. These patterns and their popularity in the original sequences were analyzed. Tri-patterns are the most meaningful type, while the other types, especially Type II, suffer from too many weak characters. For the pattern popularity, tri-patterns have the best minimal and second-best average performance. The second application is oil well daily production time series analysis. A coding table was designed to convert the numeric time series into a nominal sequence. Significant, insignificant, and minor changes correspond to strong, medium, and weak characters, respectively. We used two years of well data to discover frequent patterns, which are finally matched in the original time series. Compared with distance-based approaches that analyze numeric time series directly, the coding and mining approach handles different levels of fluctuations more easily. Observation on the original time series shows that tri-patterns match some similar subsequences with minor differences. In contrast, the four existing types of patterns either exclude some similar subsequences or include dissimilar ones. The third application is forged Chinese text mining. Notional words, function words, and special characters correspond to strong, medium, and weak characters, respectively. Four sets of text for news, novels, history, and law were collected. Forged Chinese text occurs when some text creators distribute advertisements for illegal items such as forged money or invoices. They may insert some characters into the text that makes the text still readable, but difficult for automated analysis. In this way, the text can avoid spam filters and similar blockers. Our purpose is to extract keywords from the forged text that are also keywords in the original text. This problem is closely related to fuzzy text matching and keyword extraction. With tri-patterns, the algorithm achieves the highest accuracy in mining keywords from forged text. The remainder of this paper is organized as follows. Section 2 reviews some related work. Section 3 defines the new types of pattern, analyzes the relationships among the five types, and presents the new pattern discovery problem. Section 4 demonstrates the Apriori property and proposes the new algorithm. Section 5 explains the experimental settings and results on three kinds of real-world data. Finally, the concluding remarks are discussed in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.04.013", "references": [1877823, 964455177, 1208197292, 1511669739, 1608194207, 1632950138, 1969463949, 1974461203, 1980120082, 1985716338, 1988757176, 1997362234, 2006873874, 2009418433, 2029677459, 2048472139, 2050385127, 2059429344, 2090509093, 2118376687, 2126452743, 2136634369, 2143507938, 2151077154, 2158454296, 2217596628, 2310435784, 2340020088, 2344564588, 2345465422, 2506225748, 2515543847, 2551757246, 2566882407, 2580172056, 2616721392, 2727405836, 2762194741]}
{"paragraph": "Compared to induction motor, the PMSM presents several advantages, such as high efficiency, high reliability, high power density and high torque to current ratio. Therefore, PMSM is used in wide applications, such as electric and hybrid vehicles, high-end white goods (refrigerators, washing machines, dishwashers, etc.), high-end pumps, fans and in other appliances. Despite its advantages, the PMSM remains complicated and difficult to control when good transient performance under all operating conditions is desired. This is due to the fact that the PMSM is a nonlinear, multivariable, time varying system subjected to unknown disturbances and variable parameters. Over the past decades, various control techniques have been developed in order to improve the PMSM performance. However, the widely used approach consists in using linear field oriented control theory. Another popular control technique, the DTC has a relatively simple control structure yet performs at least as good as the FOC technique. However, DTC involves hysteresis type, for both stator flux and torque magnitudes control, which introduces variable and uncontrollable switching frequency; which in turn produces large torque and flux ripples and high current distortion. Many artificial intelligence techniques and random search methods have been employed to improve the controller performances. Neural network, fuzzy system, and genetic algorithm have been widely applied to proper tuning of PID controller parameters. But all have some shortages. GA has a big computational complexity. Fuzzy system itself has many parameters to be optimized, the results of these experiments showed that fuzzy controllers perform better, or at least well as, adaptive controllers. Moreover, this technique offers the advantage of requiring only a simple mathematical model to formulate the algorithm, which can easily be implemented by a digital computer. These features are appreciated for nonlinear processes for which there is no reliable model and complex systems where the model is useless due to the large number of equations involved. Nevertheless, the main problem with fuzzy logic is that there is no systematic procedure for the design of fuzzy controller. The superiority of fuzzy controller, it can adapt his structure, acting on number of factors which constitute the internal configuration of this type of controller, such as: Fuzzification blocks, Fuzzy rules, block defuzzification and input, output gains. Additionally, it is possible to use the fuzzy logic to adjust or supervise the parameters of traditional PI regulator. Particle swarm optimization is a stochastic global optimization technique. The PSO technique can generate a high-quality solution within shorter calculation time and stable convergence characteristic than other stochastic methods. Because the PSO method is an excellent optimization methodology and a promising approach for solving the optimal PI controller parameters problem. In this paper, a direct torque controlled PMSM based on the AFLC and the PSO is presented. A comparison study between these techniques is also presented. To show the performances of the DTC of the PMSM based on the AFLC and the PSO, simulation results are presented. To validate the simulation results, these algorithms are implemented on a test bench around a DSPACE 1104. These techniques show high performances compared to the conventional DTC. Comparing these techniques, we see that the DTC based on the PSO is more efficient than the DTC based on the AFLC.", "section": "Related Work", "doi": "10.1016/j.matcom.2018.04.010", "references": [1854387882, 1968093301, 1969900902, 2008059784, 2016150415, 2030282996, 2032649063, 2032775349, 2045059098, 2049768223, 2051736125, 2062238863, 2064981932, 2072662494, 2081696533, 2086068525, 2099829287, 2115521895, 2153587688, 2180382463]}
{"paragraph": "Recent years have seen a growing interest in text mining applications aimed at uncovering public opinions and social trends. This is partially driven by the fact that the Web now holds a large number of opinionated documents, such as opinion pieces and product reviews, to name a few. An additional driver is that the language one uses to express opinion indicates one’s subjective viewpoints; this language can be used to understand and cluster people’s opinion based on belief, experience or emotion, rather than facts. Text mining methods are therefore desired for facilitating automatic discovery of subjective viewpoints present in such large amounts of opinionated documents. We define contrastive opinion mining as the discovery of opinion perspectives held by different individuals or groups, which are related to a given topic but opposite in terms of sentiments. The usefulness of contrastive opinion mining spans across many applications such as discovering the public’s stand on major socio-political events, observing heated debates over controversial issues where different sides defend their viewpoints with contrasting statements, as well as mining issues from product review sites that can serve as an important source of feedback to businesses. For example, there were heated discussions on the web about whether one should install the Mac OS X El Capitan soon after it was released to the public. People express highly controversial opinions after upgrading to the system, i.e., some experienced pleasant performance improvements while others witnessed a significant drop in speed. Considering the huge number of reviews available, it is highly desirable to acquire an overview of the major viewpoints from large amounts of text data automatically, allowing one to convert data into actionable knowledge and then make decisions in a timely manner. Recently, mining contrastive opinions has been applied to a variety of tasks, including analysing editorial differences between multiple media sources, extracting contrastive viewpoints from political debates, as well as examining cross-cultural differences with respect to language use on social media. However, these existing studies on contrastive opinion mining rely on an assumption that input data containing different opinion perspectives are separated into different collections beforehand. While this assumption might hold for some practical scenarios, quite often one needs to analyse contrastive opinion contained in a single collection such as text of streaming social media data. In addition, it is natural that debates on some topics are more prominent or controversial than others, which indicates the importance of the topic. Therefore, being able to understand the prominence of a topic and the levels of contrastiveness of sentiment will enable one to quickly identify information that needs immediate attention. Finally, existing models generally interpret contrastive opinions solely in terms of the extracted topic words, which are not adequate to help us accurately understand the opinions presented in the corpus since the topic words only express shallow semantics. Therefore, it would be illuminating to consider the dependency between the sentences in the corpus and the topic of discussion in order to better understand and interpret contrastive opinion. The representative sentences also help to clarify the coherence of the extracted topics. In this paper, we address the aforementioned issues by proposing a novel unified latent variable model for mining contrastive opinion from text collections. The proposed model makes several distinctive contributions, for it: can be trained flexibly under weakly-supervised or fully-supervised settings, depending on the type of supervision information available; automatically discovers contrastive opinion from both single and multiple text collections; quantifies the strength of opinion contrastiveness towards the topic of interest, which could allow one to swiftly flag issues that require immediate attention; and extracts sentences relevant to topics by adopting a strategy that makes sentiment-bearing topics clearer to users. Extensive experimental results show that our model outperforms several baseline models in terms of extracting coherent and distinctive sentiment-bearing topics which express contrastive opinions. The top sentences extracted by our approach further help us effectively understand and interpret sentiment-bearing topics. Lastly, we evaluate the performance of our model in the supervised sentiment and topic classification task, in which our model outperforms or gives comparable performance to five strong supervised baselines. The rest of the paper is organised as follows. We first review the related work in Section 2, followed by detailed discussion of our model in Section 3. Section 4 and Section 5 present the experimental setup and results, respectively. Finally we conclude the paper in Section 6.", "section": "Methodology", "doi": "10.1007/s11704-018-7073-5", "references": [137786571, 1054465322, 1536494821, 1569041844, 1570776870, 1969486090, 2018650704, 2023907142, 2045818565, 2061806977, 2062589306, 2098062695, 2102498033, 2103587173, 2106224734, 2108420397, 2112247328, 2123751690, 2126200466, 2130339025, 2147946282, 2159426623, 2250879034, 2251582277, 2329121264, 2463177446, 2573269214, 2620851716, 2774886230, 2949541494]}
{"paragraph": "Clustering is an unsupervised learning technique to find natural groups that are implicated in data. Its main task is to partition unlabeled patterns into subgroups such that the patterns in the same cluster are expected to have the highest similarities and the patterns between different clusters are expected to have the highest dissimilarities. In this manner, the data structure that is revealed by a clustering method is expected to reflect the natural geometry of the data as much as possible. Hard C-means is one of the foremost objective function-based clustering methods in which the degree of each pattern belonging to each cluster is 0 or 1. The validity of HCM will degenerate when dealing with patterns with overlapping areas. Fuzzy clustering, especially fuzzy C-means, as one extension of HCM, utilizes a partition matrix to evaluate the degree of each pattern belonging to each cluster, so that the overlapping partitions can be described effectively. The main challenge of FCM is the sensitivities to noisy patterns which may contaminate the calculations of the corresponding prototypes and membership degrees. Based on rough set theory, which aims at analyzing data involving uncertain, imprecise or incomplete information, a rough C-means clustering method was proposed. Each cluster is described not only by a prototype, but also with a pair of lower and upper approximations. Meanwhile, the boundary region is defined as the difference between the lower and upper approximations. The uncertainty and vagueness arising in the boundary region of each cluster can be captured well in RCM. Since no membership degrees are involved, the closeness of patterns to clusters cannot be detected. Rough sets and fuzzy sets, as two important paradigms of granular computing, are strongly complementary to each other. Incorporating with fuzzy membership degrees, a rough-fuzzy C-means clustering method was presented which integrated the merits of both fuzzy sets and rough sets. The lower and upper approximations are determined according to the membership degrees, rather than the individual absolute distances between a pattern and its neighbors. A robust rough-fuzzy C-means algorithm was further proposed that integrated both probabilistic and possibilistic memberships of fuzzy sets, which could handle overlapping clusters in noisy environments as well as the uncertainty and vagueness in cluster definitions due to involving rough sets. No matter which rough-fuzzy partitive clustering methods are used, some model parameters are involved: the weighted values that evaluate the contributions of lower approximations and boundary regions when calculating new prototypes; the threshold that determines the lower approximation and boundary region of each cluster; the value of fuzzifier parameter m that controls the shape of memberships. Since the contributions of lower approximations are considered more important than the contributions of boundary regions as computing the prototypes, the weighted value for lower approximations is much higher, and its complementarity is applied for the boundary regions. Meanwhile, through the combinational adjustments of the threshold and the fuzzifier, the influence caused by the weighted values can be reduced. In this case, these parameters will take over the effect of weighted values. The threshold that determines the approximation regions of each cluster is often selected depending on subjective tuning in the available research. Different strategies have chosen this value as the average value or the median of the difference between the highest and second highest fuzzy memberships of all the patterns. However, the same threshold is employed for all clusters though the sizes and the densities of the clusters may be significantly diverse. Additionally, the approximation regions are partitioned based on the absolute distances or membership degrees of individual patterns, not the global observation on data for a specific cluster, then the topology of data cannot be detected well with respect to this cluster. Shadowed sets, as a bridge between rough sets and fuzzy sets, provide a new scheme to granular computing. It is an example of three-way, three-valued, or three-region approximations of a fuzzy set according to the framework of three-way decisions which is another new paradigm of granular computing and can provide favorable semantical interpretation and generalized optimization methodology for determining partition threshold values. A shadowed set-based rough-fuzzy C-means method was introduced which gave a technique of automatic selection of the partition threshold. No matter which selection methods are used, an unreasonable threshold will result in undesired approximation region partitions, and then the prototype calculations may be spoiled. The value of fuzzifier parameter m is very important for the updating of prototypes and the corresponding partition matrix. A predefined value of m is often used in the rough set and fuzzy set-based clustering methods. However, it is difficult to express the uncertain notion of fuzziness in a given data set using a single fuzzifier value. To manage the uncertainty generated by the fuzzifier parameter m, an interval type-2 fuzzy C-means was proposed which extended a pattern set to interval type-2 fuzzy sets using a pair of fuzzifier values that created a footprint of uncertainty for the fuzzifier parameter. A general type-2 fuzzy C-means algorithm was further revealed via an alpha-plane representation theorem. However, the values of these fuzzifiers mainly depend on subjective selection or enumeration in the available studies and the results need more interpretations. Recently, the notion of multigranulation in granular computing is developed for solving human-centric problems and interpreting the obtained results from the perspective of multiple levels of granularity. This methodology provides a new insight into analyzing the uncertainties generated by the model parameters. The purpose of this paper is mainly to tackle the uncertainties associated with the two key parameters in rough set and fuzzy set-based clustering approaches, namely the threshold related to the approximation region partitions and the fuzzification coefficient m. The main objectives of this paper are to optimize the partition threshold for each cluster based on shadowed sets, which is obtained from the perspective of the global observation on data and will be used as a cornerstone for establishing the multi-levels of granularity for approximation regions; to capture the uncertainty generated by the fuzzifier parameter m in rough-fuzzy clustering methods via the variations in multigranulation approximation regions formed under multiple values of fuzzifier parameter with a partially ordered relation, rather than at a single level of granularity under a specific fuzzifier value; to update the prototypes by combining the intermediate results obtained at different levels of granularity. In this way, the prototypes calculated at a single level can be modified and then tend to their natural positions; to develop a multilevel degranulation mechanism according to granulation-degranulation philosophy based on which the quality of the clustering model can be evaluated. By integrating various granular computing technologies, i.e., fuzzy sets, rough sets, shadowed sets and the notion of multigranulation, the uncertain information in data, including overlapping partitions, the vagueness arising in boundary regions and the uncertainty produced by fuzzification coefficient, can be handled sufficiently. Experimental results with the use of synthetic and real-world data illustrate the improved performance of the proposed notion in terms of several validity indices, such as relative separation index, Davies–Bouldin index, Dunns index and PBM-index as well as the granulation-degranulation index. The rest of paper is organized as follows. Some rough set-based partitive clustering methods are reviewed in Section 2. The uncertainty generated by the fuzzifier parameter m is revealed in Section 3. Meanwhile, multigranulation approximate regions are formed based on which a new rough-fuzzy C-means method is introduced. In Section 4, the proposed granulation-degranulation mechanisms based on multiple levels of granularity are explained. Comparative experiments are presented in Section 5. Some conclusions are given in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.05.053", "references": [101345952, 1843766148, 1976670813, 1983753875, 1987149779, 1988695218, 1989577218, 1989907351, 1996747841, 1998965536, 2001692054, 2006873874, 2026489439, 2039742983, 2048404808, 2051224630, 2052608046, 2062696711, 2070813883, 2078757499, 2083449452, 2089923511, 2103535990, 2114832876, 2143451122, 2154437129, 2170755382, 2171975443, 2259316796, 2297889545, 2340020088, 2461653276, 2565881538, 2605693713, 2620114837]}
{"paragraph": "Granular computing (GrC) is an emerging computing paradigm of information processing with multiple granularity levels, and its studies are closely related to several existing fields such as rough sets. GrC has a basic issue regarding the information granule number and its interpretation, and three-way decisions (3WD) utilize three parts or granules for problem solving. To enrich the studies of GrC and 3WD, this paper mainly focuses on attribute reduction of rough sets by introducing information theory, and we will particularly establish three-way information reducts within the basic framework of three-way granular structures. By virtue of GrC, attribute reduction serves as a fundamental topic of rough sets, and it utilizes effective simplification or feature selection to implement the classification and reasoning in information processing. Attribute reduction usually needs a basic data background of a decision table, and the three-layer granular structures of a decision table were revealed. The mainstream attribute reducts remain at the Macro-Top, and relevant classification-based reducts adopt basic research pathways of the rule region, discernibility matrix, and information measure; moreover, there are various other approaches of reduct construction such as incremental, dynamic, parallel, approximate, hierarchical reducts. The traditional classification-based reducts depend on the set operation and region structure, and thus they adhere to the so-called algebra viewpoint. In contrast, the classification-based reducts have in-depth investigations via the introduction of the information theory, so they also have the information viewpoint. For example, one study offered the informational representation of knowledge reduction and decision reduction, where the entropy and mutual information are highlighted; another study adopted the conditional entropy to develop heuristic reduction algorithms; another used the conditional entropy to define approximate reducts; moreover, another presented the relative decision entropy to propose a feature selection algorithm. In particular, aiming at the classification-based reducts, one study conducted a comparative study from the algebra and information viewpoints, where the conditional entropy acts as the main information tool, so the algebra and information types gain their explanation and difference. The classification-based attribute reducts implement complete optimization to, on average, fit all of the decision classes, but their compromise might not necessarily be suitable for specific optimization with regard to every decision class. Thus, one study first introduced a notion of attribute reducts for a decision class based on the positive region of the decision class; in particular, another study deeply analyzed three relevant blind spots of classification-based reducts and further stood at the Meso-Middle with only one decision class to establish the class-specific attribute reducts, mainly from the viewpoint of the positive region or acceptance rules; recently, another study enriched the class-specific reducts from the 3WD perspective, mainly by adding the negative region or negative rules and positive-and-negative region or rule-out rules. The class-specific reducts become a new sort of attribute reduction, and their difference from the local attribute reducts has also been clarified. Regarding practical applications, the class-specific reducts can utilize the concrete decision class to simplify the class-pattern recognition or class-decision rules and further integratedly complete the entire classification and reasoning tasks in a low-dimensional space. However, the class-specific reducts currently offer only the algebra interpretation on regions, so their fundamental information construction and further comparison between the algebra and information types become novel and valuable. This paper mainly sets about these relevant works, and the search for appropriate information measures becomes a starting point. The information theory provides an effective approach for uncertainty analyses. Now, relevant information measures have been introduced into the rough set theory for uncertainty representation and measurement. In particular, as mentioned above, the classification-based reducts depended on information measures to gain the information interpretation. There, the information theory directly generated the entropy, conditional entropy, and mutual information, but the three information measures are located at the Macro-Top to develop only the usual classification-based reducts. In our previous research, the three-way informational measures based on the three-layer granular structures were systematically and hierarchically established from the rough set itself based on the decision table and attribute reduction; as a result, the previous information measures at the Macro-Top achieved their hierarchical construction mechanisms and Bayes-based systematic relationships, and three relevant measures with uncertainty measurement also emerged at the Meso-Middle, i.e., the likelihood, prior, and posterior weighted entropies. Clearly, the three-way weighted entropies can be fully utilized to underlie our discussion on the information class-specific reducts here. The above background is clarified in a figure. Targeting the class-specific attribute reducts, this paper mainly establishes three-way information types by using the three-way weighted entropies with uncertainty measurement, and it further compares these new information types with the existing algebra type. The two basic research contents are also marked by two question marks in the figure, and they are actually related to GrC and 3WD, except the above information theory. GrC has a trialistic framework to act as a fundamental structural methodology and can effectively process hierarchical information. GrC basically concerns information granulation and is extensively applied in rough sets; moreover, the granulation monotonicity in knowledge coarsening plays an important role in attribute reduction. In particular, one study utilized GrC to define orthopairs and a hierarchy on them, and thus three levels determined by orthopairs were discussed to give a kind of three-level granular structures. Herein, the class-specific reducts are related to the three-layer granular structures, so they adhere to GrC. The class-specific attribute reduction also concerns a basic GrC process of knowledge coarsening, and the relevant granulation monotonicity and equality condition are particularly required. In contrast, the GrC methodology of multi-granule, multi-level, and multi-view can be fully utilized to probe the concerned information measures and constructed multiple reducts. 3WD serves as an important decision-making methodology with extensive applications, especially in knowledge discovery and management. 3WD was first introduced in rough sets, and it gradually moved to a more general tri-partition framework. At present, 3WD has already become a research hotspot to exhibit in-depth results, and these recent studies show that 3WD can be viewed in a broad sense as thinking in threes. In particular, one study basically discussed generalized and special 3WD; furthermore, another rationally established the more general 3WD theory to embrace multiple ideas including rough sets, and thus the essential 3WD thinking is through trisecting and acting. As a result, the three-level analysis falls into the 3WD category, so our three-layer granular structures are related to 3WD. Moreover, our three-way information class-specific reducts and their systematic relationships will be explored by the three-way weighted entropies and three-way regions, and thus these relevant studies also become a good example for the enrichment of 3WD. According to the three-way weighted entropies and their granulation monotonicity, three-way information class-specific reducts can be naturally established by preserving the weighted entropy value, and this preservation reduction target is essential to maintain a kind of certainty optimization, which is initially reached by the entire condition attributes. In fact, the existing algebra class-specific reduct also aims to obtain a sort of optimization, mainly regarding the positive region. Therefore, mining and comparing the optimization preservation conditions of the three-way weighted entropies and the positive region become the research key and difficulty to reveal class-specific reducts’ relationships from the information and algebra viewpoints. In this core process, we would effectively utilize the related GrC technology, and three auxiliary probability conditions for granular merging are also provided, i.e., the LPE-Condition, IP0-Condition, and IPS-Condition. Against the above content and thought, the research developed in this paper is concretely presented as follows. The algebra class-specific reduct is reviewed by the positive region. Then, the likelihood, prior, and posterior class-specific reducts are systematically and respectively established by the likelihood, prior, and posterior weighted entropies. Optimization preservation conditions of the positive region and weighted entropy are deeply mined, and they are effectively represented by the granular merging and three-way regions. All four types of class-specific reducts gain their strong-weak relationships, consistency or inconsistency degeneration, and information systematicness. The algebra and information class-specific reducts and their relationships are illustrated by a consistency example and an inconsistency example. In summary, the four types of class-specific reducts adopt distinctive algebra and information viewpoints to present different emphases and mutual relationships. This study provides an in-depth insight into GrC-based attribute reduction, mainly by the information theory and 3WD theory, and this contribution is also clarified in the figure. The remainder of this paper is organized as follows. Section 2 provides preliminaries to review the three-layer granular structures, the three-way informational measures, and the algebra class-specific reducts. Section 3 constructs the three-way information class-specific reducts via the three-way weighted entropies. Section 4 first mines optimization preservation conditions of the positive region and weighted entropy and then analyzes multiple relationships of class-specific reducts. Section 5 provides an illustrative example via a consistent and an inconsistent decision table. Finally, Section 6 concludes this paper.", "section": "Related Work", "doi": "10.1016/j.ins.2018.06.001", "references": [101345952, 831516024, 1798724659, 1861960977, 1971515969, 1974313046, 1994743372, 2040753112, 2045810864, 2049204733, 2059228994, 2070416356, 2070860530, 2074056315, 2082272625, 2098093602, 2130367040, 2131956306, 2162755671, 2206296687, 2270306881, 2297889545, 2346184948, 2483885539, 2517012834, 2551396410, 2557802193, 2560804083, 2568228263, 2568232622, 2588487610, 2612102978, 2619471640, 2707389940, 2745134596, 2753804762, 2760880172, 2770194470, 2792691650, 2793566013, 2794856943]}
{"paragraph": "The digital age is already upon us, and we have started reading and writing on digital devices in place of a physical piece of paper. However, the coexistence of printed and digital documents is ensured by factors such as ubiquitousness, ease of use, cost and security of documents. One of the important forensic aspects of a printed document is the knowledge related to the source of that document. The knowledge of printer used to print a document can not only help in criminal investigations but also safeguard the use of paper for legal, administrative and other official records. Source attribution of printed documents using digital techniques has gained significant importance in recent times. Traditional methods use chemical or microscopic techniques which are time-consuming, costly, may even be intrusive and require an expert examiner. On the contrary, working on the scanned image of a printed document converts it into a classical pattern recognition problem involving feature extraction and classification. So, in digital techniques, only an office scanner and computer are required. These techniques involve the use of an extrinsically embedded signature or watermark before or during the printing process as well as detection of source printer based on intrinsic signatures. The early methods based on intrinsic signatures relied on printed documents scanned at high resolutions which would be a costly and time-consuming affair. A relatively new intrinsic approach is based on geometric distortions induced by a printer. However, these either require the original soft copy of the printed document or a way to generate the reference soft copy from the printed document which would be troublesome and/or inaccurate in most real-life applications. There is another category of intrinsic signature which is based on texture analysis. Most of the successful methods in this category extract features from all occurrences of a specific letter type, font type and font size. Then a classifier is trained to predict labels of the test data. Thus, extracting information from all letters require training multiple classifiers, one for each letter differing by letter type, font type, or font size. Also, each of those classifiers would require enough amount of input letters for proper training. Further, extraction process for each of those letters such as use of optical character recognition limits the classification process. An attempt was made recently to extract information from all letters by use of a single classifier. The feature extraction pipeline gave promising results when letters printed using a specific font were used to train as well as test. However, in many practical scenarios, testing data might consist of only a particular font or language, not present in the training data. Building on the approach, this work presents a new feature descriptor for source classification of printed documents with the aim of addressing the cross font problem, a scenario where font of letters in test data is different from font of letters present in the train data. The printer’s signature related to texture is a result of non-uniform and unintentional imperfections in toner developed on a printed page. The local texture features around a pixel in a small neighborhood would be directly dependent on the way this toner appears or spreads in that neighborhood. Thus, we introduce a new printer specific local texture descriptor to capture textures on the scanned image of a printed document. The proposed method makes no assumption about the type of the letter. Following are the major contributions of this paper: Introduction of a novel set of printer specific local binary pattern based features which can capture printer generated local texture patterns. Demonstration that encoding and regrouping strategy based on the orientation of intensity and gradient-based small linear structures makes the method a very powerful discriminative feature. Evaluation of the proposed system comprehensively on the publicly available dataset, showing that, given sufficient amount of train data, it outperforms existing methods. Also, it reduces confusion between printers of the same brand and model. Encouraging results on cross font dataset, outperforming state-of-the-art methods by large margins. The remainder of this paper is organized as follows. Section II briefly describes existing intrinsic signature based techniques for classifying the source printer of printed text documents. The details of our proposed feature descriptor have been mentioned in Section III. The proposed system is described in Section IV. A series of experiments have been conducted to test the efficacy of the proposed method. Their description and results have been discussed in Section V. Finally, we present the conclusions that can be derived from this work along with the pending challenges for future work in Section VI.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2919869", "references": [60318, 1480586119, 1481922366, 1566219896, 1820514791, 1976499261, 1977737593, 1981119885, 1987392461, 2021549453, 2029832855, 2078765864, 2079317077, 2089575713, 2117487104, 2127206948, 2131081720, 2131978991, 2135392152, 2138584058, 2145947562, 2147141800, 2149286313, 2153635508, 2156436243, 2163352848, 2165553585, 2606661584, 2682476753, 2798033509, 2800424369, 2801323926, 2963123309]}
{"paragraph": "Clean drinking water is a critical resource for the health and well-being of all humans. However, water quality is easily deteriorated because of a malicious attack or accidental incident. For example, in 2000, an outbreak of waterborne disease epidemic in Walkerton, Ontario, Canada affected 2300 people because of exposure to contaminated drinking water. The poisoning of water supply affected 71 people in Ruyang city, Henan Province, China in 2007. According to the Global Risks Report, water crises, i.e., water shortage and contamination, have consistently featured among the top-ranked global risks. Deploying water quality sensor networks is considered as a promising approach for detecting contamination incidents in a drinking water distribution system. Owing to the high cost of quality sensors and the high cost of installation and maintenance, optimization methods have been developed to help utility companies choose optimal locations for the placement of water quality sensors aiming at effective detection. Thus far, the problem of determining the placement of water quality sensors and identifying the contaminant sources within WDS for enhancing the monitoring and security capabilities has been widely investigated over the last decade by utility companies and research community in the field of WDSs. The mainstream method is to establish an optimization model of water quality sensor placement, and subsequently apply single-objective and multi-objective optimization algorithms to obtain the optimal layout of sensors. The overall goal of deploying water quality sensors is to monitor potential pollution incidents in order to minimize the risk of contamination intrusion. At present, there are several competing design objectives for sensor placement. The widely accepted objective is the minimization of public health impact, which includes the number of individuals exposed to a contaminant, extent of contamination in a water supply network, detection time, and percentage of contamination incidents not detected. Although a few significant optimization objectives have been proposed, to the best of our knowledge, a many-objective optimization algorithm for sensor placement in WDS has not been proposed yet. Herein, for the first time, we attempt to propose a modified NSGA-III for the deployment of water quality sensors. By employing two typical water distribution networks from the battle of the water sensor networks, comprehensive experiments are conducted and sensitivity analyses with different sizes of WDN are explored. Specifically, the contributions of our work are as follows: We investigate the optimization model of sensor placement in WDS, theoretically analyze this problem, and prove that it is NP-hard. Inspired by the connectivity degree in graphics, we propose a modified NSGA-III for many-objective sensor placement optimization. By employing two typical WDNs from the BWSN, the benefits of the proposed algorithm are illustrated, and some critical factors are also addressed. The rest of the paper is organized as follows. Section 2 reviews the state-of-art works from the aspects of single- and multi-objective optimization. Section 3 focuses on the problem formulation and theoretical analysis. Section 4 presents the modified algorithm. Section 5 discusses the simulation results. Section 6 concludes the paper and presents some issues that should be further investigated.", "section": "Introduction", "doi": "10.1016/j.ins.2018.06.055", "references": [61339854, 1418108976, 1968219458, 1991356461, 1996228877, 2022485595, 2152045847, 2344916885, 2501190883, 2521918431, 2546584549, 2558150887, 2560305685, 2570235502, 2589278106, 2598797402, 2617298469, 2648950690, 2739365735, 2740758288, 2757662287, 2769746177, 2790368141, 2802576758]}
{"paragraph": "Detecting line segments in images is a long standing problem in computer vision. A classical approach to the problem relies on a global Hough transform. More efficient methods implement in various ways the principles of perceptual grouping, by grouping local clues, which are usually obtained from the gradient of the image. Recently, it is proposed to combine a global Hough transform and local grouping. The most recent methods rely on deep learning, but to the best of our knowledge, none of these methods addresses the point of line segment detection in images with strong noises. One domain where the levels of noise are extremely high is Synthetic Aperture Radar imaging, where images are impacted by very strong speckle noise, a noise that is inherent to all coherent imaging systems. While the detection of linear features in SAR images has received a lot of attention, typically in view of the detection of road networks, reliably detecting line segments is still an open problem. Nevertheless, line segments are very important features in SAR images, mostly because many man-made objects like buildings, farmlands or airports can be described by line segments. Besides, most geometric structures can be approximated by line segments. In addition, line segments can be extracted as low level features and then be used for tasks such as image registration and target recognition. Due to the strong speckle noise, methods that are effective for optical images cannot be straightforwardly applied to SAR images. First, the usual assumption that noise is additive and Gaussian is wrong. Second, and more importantly, the strong level of noise encountered in SAR images makes most optical approaches inefficient. Taking the logarithm of the amplitude or intensity of SAR images can change multiplicative noise to additive noise but this does not allow the plain application of optical methods, as we will see in the experimental section in the case of the LSD detector and a recent line segment detector AFM which is based on deep learning. The usual way to detect line segments in SAR images is global and relies on the Hough transform. First, a constant false alarm rate edge detector is applied to the image, followed by a Hough transform to detect lines. Then, post processing steps are applied to localize Hough lines into line segments. Many methods of this kind have been proposed for SAR images following the early work, in the context of different applications. In some cases, line segments are extracted by the Hough transform and then used to reconstruct buildings from meter-resolution multi-aspect SAR images. An optical-to-SAR image registration method has been proposed, relying on line segments that are detected using a ratio-based gradient and the Hough transform. The same idea was previously explored. In other cases, edge detection using phase symmetry and wavelet correlations is followed by a Hough transform in order to detect ship wakes. A common limitation of these approaches is that the performance of the Hough transform critically relies on both a preliminary edge detection and on the selection of parameters. The input of the Hough transform is usually a binary edge map. Many dedicated methods have been proposed for SAR images to compute gradients, but extracting a binary edge map necessitates a difficult compromise between suppressing false alarms due to speckle and preserving edges of low contrast. Besides, the corresponding threshold choices are strongly image-dependent. An interesting approach, which was recently proposed, detects lines from the magnitude field instead of a binary edge map, but the subsequent detection tasks still require non-trivial parameter tunings. The LSD detector, which is based on the a contrario model and the Helmholtz principle, is considered to be one of the most celebrated line segment detector. The goal of the present paper is to develop a LSD-like line segment detector for SAR images, which results in a very challenging task. Indeed the LSD detector, as most a contrario approaches, relies on a null hypothesis against which segments are detected. Unfortunately, this null hypothesis is completely inadequate for SAR images. More precisely, local orientations, each of which being defined as the direction perpendicular to the gradient orientation, are grouped against the hypothesis that they are uniformly distributed and mutually independent. Both these hypotheses appear to be structurally wrong in SAR images. First, classical ways to compute the gradient in SAR images yield a non uniform distribution of the orientation, even in the absence of geometrical structures. Second, the speckle noise imposes the use of strong filtering schemes, implying strong structural dependencies between nearby orientations. In the proposed LSDSAR approach, we replace the gradient computation with a ratio based method, which yields robust and unbiased local orientations at each pixel. Further, we replace the crucial independence hypothesis between local orientations by a first order Markov chain modeling, which in practice is enough to counterbalance the effect of filtering and yields an efficient control of the number of false detections. The result is a generic line-segment detector adapted to the specific structure of SAR images. This paper is organized as follows: in Section 2, we give a description of the original LSD algorithm. In Section 3, the new line segment detector is provided. In particular, the computation of the gradient is detailed, as well as the use of a first order Markov chain in the a contrario model. Line segment detection results on both synthetic and real SAR images are given in Section 4 and are compared to the results of state-of-the-art line segment detectors. We finally conclude in Section 5 and present some perspectives.", "section": "Related Work", "doi": "10.1016/j.patcog.2019.107034", "references": [1504108853, 1983184501, 1990200755, 1995376165, 2004491626, 2021278996, 2030233108, 2038563371, 2055568811, 2070041365, 2095905764, 2101853874, 2113654450, 2119764380, 2121796624, 2135049578, 2138055993, 2138106400, 2139381235, 2140753314, 2142572935, 2144506334, 2144572173, 2145023731, 2145158371, 2156953558, 2160072137, 2167566599, 2294634632, 2307048845, 2737353725, 2752347180, 2798943056, 2902517736, 2913455019]}
{"paragraph": "The unmanned aerial vehicle, an aircraft designed for missions that are too dull, dirty or dangerous for humans, has attracted the widespread attention of researchers. The future UAV will possess distributed autonomous airborne abilities to complete a four-step loop independently: observe, orient, decide, and act. The improvement of distributed airborne abilities will trigger changes in future mission modes where small and smart UAVs will execute a task in a group or a swarm. A great number of problems exist in the course of the realization of the future mission mode, such as how to coordinate a group of agents to move in a formation under complex obstacle environments. In the literature, several strategies have been proposed for navigation problems of UAVs flying in a formation in an obstacle environment, with considerations of computation time and control stability. Some works have proposed bio-inspired stabilization approaches for control and navigation of large teams of UAVs along predefined paths through complex environments, although UAV flocking can be unsteady due to lack of strict velocity alignment. Other methods have achieved multi-robot formation control among obstacles using sophisticated algorithms based on local interaction information. In our previous work, a flocking control algorithm based on pigeon interaction mode switch behavior, which relies on the prior information of obstacles, is proposed to coordinate a heterogeneous UAV swarm to fly through obstacle environments. This paper aims to put forward a UAV flocking control algorithm, which can form a stable formation among obstacles without depending on the number of UAVs, other information of neighbors except positions and velocities, or prior information of environments. To implement the stable and collision-free collective motion of a UAVs swarm under obstacle environments, optimal control decisions need to be made in the presence of many conflicting criteria. These criteria depict trade-offs not only between individual flocking control and obstacle avoidance control but also between individual control and other individual control. Therefore, the UAV flocking centralized control problem with obstacle avoidance is a many-objective optimization problem where the number of objectives scales with the number of UAVs. In this paper, a distributed UAV flocking control optimization frame is designed to render the many-objective optimization problem into a multi-objective optimization problem to be solved by each UAV. The objectives of each UAV are summarized as follows: If obstacles are detected, pass through obstacles as soon as possible; if not, keep expected states; maintain a presupposed distance with neighbors steadily; keep a safe distance with obstacles; avoid collisions with neighbors. The above objectives are divided into soft constrains and hard ones. Each UAV aims to minimize the soft and satisfy the hard. However, considering the restrictions of onboard computing, the time requirement for the multi-objective optimization algorithm computations is very strict. Pigeon-inspired optimization, a novel bio-inspired computing algorithm, was proposed to solve mono-objective optimization problems and has proven its worth in applications such as image restoration and UAV path planning. However, the superiority of multi-objective pigeon-inspired optimization is not apparent in multi-objective optimization problems compared with the modified non-dominated sorting genetic algorithm. In this paper, a hierarchical learning behavior discovered in pigeon flocks is adopted to modify the basic multi-objective pigeon-inspired optimization. By the designed distributed UAV flocking control optimization frame and modified algorithm, a UAV distributed flocking control algorithm is proposed to coordinate UAVs to fly in a stable formation under complex obstacle environments. The rest of the paper is organized as follows. Section 2 gives a brief review of the principle of basic pigeon-inspired optimization. After building the UAV model, self-propelled flocking model, and obstacle avoidance model, Section 3 formulates the multi-objective optimization problem to be solved in this paper. Section 4 presents modified pigeon-inspired optimization based on the hierarchical learning behavior in pigeon flocks. Section 5 proposes a distributed UAV flocking control algorithm with obstacle avoidance based on the modified algorithm in Section 4. Comparative simulation validations are elaborated in Section 6, and our concluding remarks are drawn in Section 7.", "section": "Methodology", "doi": "10.1016/j.ins.2018.06.061", "references": [1756033412, 1795672577, 1898410413, 2082829278, 2097326205, 2122032069, 2126105956, 2187224205, 2409707557, 2428303113, 2526479336, 2560634206, 2596192431, 2735519414, 2792792517]}
{"paragraph": "Neighborhood Systems were proposed through extending the strategies of the nearest neighbors. In a neighborhood system, an object is associated with its neighborhood rather than its nearest neighbors. The classifications based on neighborhoods were proven to be more efficient than the classifications based on nearest-neighbor search. The space of neighborhoods was also investigated to approximate global data distribution. From the view of topology, it has been demonstrated that the neighborhood spaces are more general than the data-level spaces. This indicates that transforming original data into neighborhood systems will facilitate the data generalization. Through extending Rough Sets with neighborhoods, Neighborhood Rough Sets were proposed to construct approximations of data space. Different from the equivalence classes defined by symbols in the classic rough sets, the basic granules in neighborhood rough sets are the neighborhoods in numerical or nominal data spaces, which makes the model represent the mixed-type data well. Formulating data space with neighborhood rough sets, data distributions can be approximated by Neighborhood Covering, which consists of a group of homogeneous neighborhoods, i.e. all the data samples in a neighborhood belonging to the same class. Neighborhood covering provides us an effective way to represent data distributions on neighborhood level. Moreover, to obtain the concise representation of data distribution, Neighborhood Covering Reduction methods were used to remove the redundant neighborhoods from the initial neighborhood coverings. Based on the neighborhood coverings of data distributions, the learning methods can be implemented for classification and feature selection. Comparing with other kinds of learning methods, NC-based methods require no parameter setting and are robust to complex data. For the classification with neighborhood covering, the existing methods directly classify an unknown sample into a class according to its nearest neighborhood. However, this certain classification strategy strictly classifies the uncertain data and may lead to serious classification mistakes. Because of the unavoidable inconsistency between the training data and the unknown world, there generally exist uncertain cases in data classification. Thus it is required to design a cautious NC-based classification for uncertain data to reduce the classification risk. To implement the uncertain classification with neighborhood covering, we expect to construct a possibilistic measure of the belongingness of neighborhood coverings and thereby design a three-way classification strategy. This solution originates from the methodologies of Three-Way Decisions. In the process of three-way decision making, decision rules are extracted from the data with uncertainty through tri-partitioning data space into Positive, Negative and Boundary regions. From the view of classification, the three regions correspond to the cases of certainly belonging to a class, certainly beyond a class and non-commitment, i.e. uncertain case. In the light of the superiority of fuzzy sets for the learning tasks on uncertain data, we adopt fuzzy membership functions to measure the possibilities of data samples belonging to neighborhoods. Based on the neighborhood memberships of data samples, we apply tri-partitioning methodology to reformulate the neighborhood-based classification and propose a Three-Way Classification method with Fuzzy Neighborhood Covering. The proposed method involves two parts: fuzzy extension of neighborhood coverings and three-way classification with fuzzy neighborhood coverings. Different from the traditional covering model formed by the union of neighborhoods, the fuzzy neighborhood covering consists of a group of neighborhood membership functions which are integrated to form the membership distribution of neighborhood coverings. The membership distribution of neighborhood coverings of different classes induces a soft partition of data space. According to the memberships of neighborhood coverings, data samples are classified into certain classes and uncertain case. The contributions of this paper are summarized as follows. Extend neighborhood covering to Fuzzy Neighborhood Covering. Fuzzy neighborhood covering consists of a group of neighborhood membership functions and forms an uncertain measure of neighborhood covering belongingness. In contrast to the set-level approximation of neighborhood coverings, the fuzzy neighborhood covering provides a membership-level approximation of data distributions. Propose Three-Way Classification with Fuzzy Neighborhood Covering. Based on the fuzzy neighborhood covering of a class, data samples are classified into Positive, Negative and Uncertain cases according to their memberships. The three-way strategy separates uncertain cases to reduce the classification risk. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 describes the entire workflow of the proposed three-way classification method. Section 4 introduces the strategy of the fuzzy extension of neighborhood coverings and also presents the three-way classification algorithms with fuzzy neighborhood coverings. In Section 5, experimental results validate the effectiveness of the proposed three-way method for uncertain data classification. The work conclusion is given in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.07.065", "references": [167016754, 837378864, 938113424, 1021182246, 1533530351, 1585743408, 1597852916, 1834402812, 1973892392, 1989393769, 1990116832, 2006873874, 2015932775, 2024638676, 2024646871, 2025542264, 2033857698, 2041182276, 2053036794, 2059429344, 2070813883, 2088779313, 2089923511, 2093329961, 2110183025, 2134866037, 2158633287, 2163952039, 2200382070, 2217596628, 2340020088, 2551757246, 2609279172, 2616721392, 2699869760, 2912707296]}
{"paragraph": "In 2009, from Pawlak’s rough sets, decision-theoretic rough sets on the basis of Bayesian decision theory and some problems in model selection, environmentally precautionary decision-making, statistical process control, suitable rough set model selection for data analysis, information filtering model on the Web and Web-Based Support Systems, Yao put the concept of three-way decisions such that the existing models become the particular examples of this formulation. Three-way decisions, as an extension of the classical two-way decisions, possess three sorts of decision rules, that is, acceptance rules, rejection rules and uncertainty rules. And then, for any object in a universe, it can be assigned into one of the three regions, that is, positive region determined by acceptance rules, negative region determined by rejection rules and boundary region determined by uncertainty rules. It is in recent years that three-way decisions have had a fast development both in theory and applications. In theory, in 2010, Yao discussed three-way decision rules in the classical rough set model and the decision-theoretic rough set model. In 2011, Yao pointed out that under certain circumstances when taking into account the costs of different kinds of miss-classifications, probabilistic three-way decisions are superior to probabilistic two-way decisions and qualitative three-way decisions of the standard rough set model. In 2012, Yao showed an outline of the theory of three-way decisions through investigating its basic ingredients, interpretations and relationships to other theories. In 2014, based on the decision-theoretic rough set model, Liang and Liu took the interval-valued loss function into account and investigated its new decision mechanisms. They also obtained the criteria for opting an appropriate method to three-way decisions with interval-valued decision-theoretic rough sets. Deng and Yao proposed decision-theoretic three-way approximations of fuzzy sets. At the same time, after a systematically research to all classes of rough sets, especially to probabilistic rough sets, Hu introduced axiomatic definitions for decision measurement, decision condition and decision evaluation function, and proposed the theory of three-way decision spaces. The author also gave a lot of three-way decisions on three-way decision spaces such that the existing three-way decisions become the peculiar examples of three-way decisions spaces discussed. Xiao et al. investigated three-way decisions of type-2 fuzzy sets and interval-valued type-2 fuzzy sets based on partially ordered sets with involutive negations. At the same time, Hu gave out an aggregation method from multiple three-way decisions spaces to a single three-way decision space by an axiomatic complement-preserving aggregation function. Le and Hu offered a summary of investigation on the three-way decisions based on covering rough sets through applying the approach of decision-theoretic rough sets. Li and Wang discussed approximate concept construction with three-way decisions and attribute reduction in incomplete contexts. Hu introduced the concept of semi-decision evaluation functions and proposed three-way decisions based on semi-three-way decision spaces. Hu et al. also investigated a new type of three-way decisions in three-way decision spaces and discussed some related properties. Yao et al. presented a general framework to research three-way or three-valued approximations of fuzzy sets. Li et al. investigated generalized three-way decision models based on subset evaluation. Li et al. discussed generalized matroids based on three-way decision models. Qiao and Hu proposed the transformation methods from semi-three-way decision spaces to three-way decision spaces based on triangular norms and triangular conorms. In applications, Liu et al. obtained a profit-based three-way approach to investment decision-making based on decision-theoretic rough set model. Yang and Yao studied a multi-agent decision-theoretic rough set model and expressed it in the form of three-way decisions. Liang et al. obtained a certain class of novel three-way decisions based on the Bayesian decision process by thinking of the two kinds of parameters, which are used in the three-way decisions with linguistic evaluation. They also gave an adaptive algorithm to amend the inconsistency of multi-attribute group decision-making under linguistic evaluation. In addition, Yao and Azam generalized the game-theoretic rough set model to investigate uncertainty referred to medical decision-making. Liu et al. proposed a novel three-way decision model based on incomplete information system. Li et al. introduced a sequential three-way decision method for cost-sensitive face recognition. Peters and Ramanna established a framework for proximal three-way decisions based on Delaunay triangulations in the study of socio-spatial properties of location-based social networks. Yu et al. studied a novel tree-based incremental overlapping clustering method using the three-way decision theory. Liang et al. introduced group decision-making into three-way decisions with decision-theoretic rough sets and proposed group decision-making based three-way decisions. Savchenko investigated an application of sequential three-way decisions and granular computing to the problem of multi-class statistical recognition of the objects, which can be represented as a sequence of independent homogeneous segments. Chen et al. showed the multi-granular three-way decision algorithm to reduce the boundary regions. Lang et al. introduced the notions of probabilistic conflict, neutral and allied sets of conflicts by using decision-theoretic rough sets and presented an algorithm for computing the probabilistic conflict, neutral and allied sets in information systems. At first, we recall the definition of decision evaluation function in order to explain the motivation of this research more precisely. In the following Definition 1.1, we assume that and are two partially ordered sets with involutive negations and respectively. In addition, let U be a nonempty universe to make a decision on it, called decision universe and V be a nonempty universe where condition function is defined, named condition universe. Let U be a decision universe and V be a condition universe. Then a mapping is called a decision evaluation function of U, if it satisfies the following axioms. On the one hand, it has been pointed out by many discussions that, in three-way decisions, decision evaluation functions play a pretty important role in decision-making. On the other hand, in realistic decision-making problems, decision-makers always rely on decision evaluation functions to obtain decision results. Different evaluation functions determine different decision results. For example, we have stated in our previous work that if we consider Example 4.1, then, we have the following two different decision-making results according to different decision evaluation functions. Then three-way decisions are as follows. From the above two cases, we can see that the final grade of student x6 is uncertain whether he or she is excellent or fail at one function. But, from the other function, we can judge accurately that the final grade of student x6 is fail. Thus, there arise many discussions investigating the construction methods of decision evaluation functions from the theoretical point of view. In addition, there arises a key problem as follows: Question: How to obtain decision evaluation functions as much as possible? To solve this problem, we can consider the following two ways. The first way is to consider more general domain of decision evaluation functions. Since hesitant fuzzy sets, interval-valued hesitant fuzzy sets and other generalized meaningful structures from fuzzy sets cannot compose fuzzy lattices in general, it follows that the theory of three-way decision spaces introduced on fuzzy lattice imposes restrictions on extensive applications of this theory to practical decision-marking problems. Therefore, Hu extended measurement on decision conclusion in three-way decision spaces from fuzzy lattices to partially ordered sets with involutive negations. Moreover, as an application, the author systematically investigated three-way decision spaces and three-way decisions based on hesitant fuzzy sets and interval-valued hesitant fuzzy sets and got a great deal of useful decision evaluation functions. The second way is to generalize the axiomatic definition of decision evaluation functions. On the one hand, we have stated in Definition 1.1 above that decision evaluation function is defined by three axioms, that is, minimum element axiom, monotonicity axiom and complement axiom. In addition, minimum element axiom and monotonicity axiom can be easily satisfied by many functions while complement axiom cannot be naturally satisfied. On the other hand, many existing discussions show that the complement axiom is very important and pretty essential for three-way decisions. Thus, the concept of semi-decision evaluation function without complement axiom was introduced and the transformation methods from semi-three-way decision spaces to three-way decision spaces were given. However, in Definition 1.1 above, notice that the complement axiom uses the involutive negations. Meanwhile, it is well known that negations are more general than involutive negations and it has been proposed and applied in many aspects. Therefore, in this paper, we attempt to use negations to define complement axiom of decision evaluation functions, which can be regarded as another way to extend the axiomatic definition of decision evaluation functions different from the work proposed previously. By this way, the existing decision evaluation functions become the special cases of the generalized decision evaluation functions discussed in this paper and we can obtain more decision evaluation functions from the theoretical point of view. Moreover, from the application point of view, it provides convenience for decision-makers to choose more suitable decision evaluation functions to make decisions in practical decision-making problems. This is also the motivation of this research. This paper is structured as follows. In Section 2, we give the measures of generalized three-way decisions based on partially ordered sets with negations and the axiomatic definition of generalized decision evaluation functions, and we also establish generalized three-way decision spaces based on the partially ordered sets with negations. In Section 3, we propose the theory of generalized three-way decisions on generalized three-way decision spaces based on partially ordered sets with negations, which includes generalized three-way decisions, the lower and upper approximations induced by generalized three-way decisions, generalized three-way decisions of multiple generalized three-way decision spaces and so on. In Section 4, some new types of decision evaluation functions and generalized three-way decisions are established, such as generalized decision evaluation functions and generalized three-way decisions based on fuzzy sets, interval-valued fuzzy sets, fuzzy relations, shadowed sets and hesitant fuzzy sets. In Section 5, we show a practical application of the results obtained in this paper. Finally, our researches are concluded.", "section": "Related Work", "doi": "10.1016/j.ins.2018.07.032", "references": [101345952, 964455177, 1021182246, 1041128124, 1567021454, 1606022329, 1704544815, 1834402812, 1861960977, 1883715000, 1969535228, 2006873874, 2023750115, 2025295306, 2029526940, 2048472139, 2050237791, 2060952501, 2070813883, 2080404663, 2089923511, 2131390763, 2167293341, 2174369037, 2176795090, 2217596628, 2252128398, 2272142993, 2272554295, 2340020088, 2499835732, 2551396410, 2560804083, 2561843635, 2580172056, 2605693713, 2620114837, 2740407498, 2774054418, 2912565176]}
{"paragraph": "A shadowed set, proposed by Pedrycz, maps the membership grades of objects in the universe to a set {0, [0, 1], 1} based on a pair of thresholds. The membership grade of an object indicates the degree of the concept is applicable to that object. The objects whose membership grades are between the thresholds constitute shadowed areas. That is, a shadowed set uses a set of three values as membership grades. Yao proposed a theory of three-way decisions based on the philosophy of thinking in threes. Three-way decisions have been widely applied in frequent pattern mining, clustering, classifications, social network analysis, and the collective knowledge exploitation, just to name a few. The trisecting-and-acting framework consists of two parts, trisecting and acting. Trisecting or trisection or a tripartition works on dividing a whole into three parts. Acting designs the most effective strategies for processing the three parts. Various theories, such as rough sets, fuzzy sets, shadowed sets, interval sets, have been used to construct three-way decisions. A shadowed set is a three-way approximation of a fuzzy set. A shadowed set based three-way approximation is constructed by choosing a single number in the unit interval as the membership grades of elements in the shadowed areas. The selected single number replaces the unit interval to represent a membership grade of the highest uncertainty. Thus a shadowed set based three-way approximation is defined as a mapping from the universe to the three-value set {0, σ, 1}. The membership grades are elevated or reduced to 1, σ, and 0 by the elevation and reduction operations. These two operations produce the elevation and reduction errors which show the difference between the original membership grades and the corresponding elevated or reduced values. Recently a generalized framework of three-way approximations using a set of three values was proposed. Shadowed set based three-way approximations are viewed as a case of the generalized framework. Three-way approximations can be converted to three-way decisions, and vice versa. One of the fundamental issues of applying three-way approximations is the determination and interpretation of the pair of thresholds. A general optimization-based framework for interpreting and determining the thresholds was introduced. Three principles were summarized: a principle of uncertainty invariance, a principle of minimum distance, and a principle of least cost, used to determine the thresholds. A framework for constructing a shadowed set according to a principle of uncertainty invariance was also proposed. A symmetric model was used to compute thresholds by minimizing the difference between the shadowed area and the sum of the elevated and reduced areas. Analytical formulas to calculate the thresholds based on a measure of fuzziness of a fuzzy set were proposed. Based on the principle of minimum distance, the nearest interval approximation of a fuzzy number using a distance measure was explored. Various distance-based three-way approximations of fuzzy sets have also been examined. A decision-theoretic approach to calculate the thresholds by minimizing decision costs was proposed, which obtains the thresholds according to the principle of least cost. The error-based model was derived by considering a loss function satisfying additional properties. Another shadowed set approximation works in the opposite way, that is, constructing shadowed sets using the α-cuts of a given fuzzy number. We apply a principle of tradeoff with games in this research to determine the thresholds of three-way approximations in the shadowed set context. Let’s examine the elevation and reduction errors produced in shadowed set based three-way approximations. When the thresholds change, the elevation and reduction errors change correspondingly. The increases of thresholds cause the decrease of the elevation errors and the increase of the reduction errors. The decreases of thresholds cause the increase of the elevation errors and the decrease of the reduction errors. We desire a shadowed set model with the minimal elevation errors and the minimal reduction errors. However, the elevation and reduction errors are not able to decrease simultaneously. The decrease of elevation errors results in the increase of reduction errors, and vice versa. The aim of this research is to find a pair of thresholds which define a shadowed set based three-way approximation based on a tradeoff between the elevation and reduction errors. The game-theoretic shadowed sets are proposed. They apply a game mechanism to determine and interpret the thresholds of shadowed sets according to a principle of tradeoff with games. The contradiction between the elevation and reduction errors are formulated as the competitive games in which the increase of one player’s payoff may cause the decrease of the other player’s payoff. The players gradually approach a compromise by repeatedly modifying the thresholds in games and then finding equilibria of games. The resulting thresholds can be induced from the game result. A shadowed set based three-way approximation defined by the resulting thresholds is able to represent a tradeoff between the elevation and reduction errors. The paper is organized as follows. We first analyze the errors produced by the shadowed set based three-way approximations. When the thresholds change, the changes of elevation and reduction errors are investigated. We then discuss game-theoretic shadowed sets, including the game formulation and repetition learning mechanism. The settings of game players, the strategies performed by each player, payoff functions, game equilibrium, repetition of games, and stopping criteria are explained in detail. Finally, we apply game-theoretic shadowed sets in an example to show how they work.", "section": "Methodology", "doi": "10.1016/j.ins.2018.07.058", "references": [101345952, 1021182246, 1549841259, 1963024134, 1988695218, 1998965536, 2001258251, 2006136802, 2006873874, 2010360766, 2030827420, 2052715156, 2052722125, 2059429344, 2066111511, 2078757499, 2080404663, 2091336043, 2111589951, 2114832876, 2120260965, 2154437129, 2284529841, 2297889545, 2407310659, 2551396410, 2560804083, 2568228263, 2588487610, 2620114837, 2752073986, 2792234535, 2796438895, 2799813871, 2804489692, 2806445482]}
{"paragraph": "Negation is a linguistic phenomenon present in any human language, either written or spoken. It has the ability to transform an affirmative statement into its opposite meaning. The a wide range of NLP applications. Negation detection is especially important in the biomedical domain, where various linguistic forms are used widely to express impressions, hypothesized explanations of experimented results or negative findings. In tasks such as information retrieval, sometimes, only the positive information of an event should be considered. For example, while searching for patients with pneumonia, we should not include a patient who has a clinical report saying No radiographic abnormality seen of the chest. Negation detection is also important in other NLP tasks, such sentiment analysis, where detecting the negation is also a critical process, as it may change the polarity of text and result in wrong predictions. In a sentence, the presence of negation is determined by the presence of a negation cue. A negation cue is a lexical element that carries negation meaning. The scope of negation is the sequence of words in a sentence that is affected by the negation cue. In example 1, the word “not” represents the cue and the continuous word sequence “not retinoic acid treatment of the U937 cells” forms the scope. Example 1 PMA treatment, and not retinoic acid treatment of the U937 cells acts in inducing NF-KB expression in the nuclei. Prior works on negation detection focused on rule-based algorithms to find negated terms in a sentence. Later, with the release of the Bioscope corpus, negation detection problem received a special attention, where most of the research integrated machine learning techniques to solve the task, and considered it as classification or a sequence labeling problem. These methods have all been proved to be effective in solving the negation scope detection problem. However, this success depends heavily on the use of an appropriate feature set, which often requires a lot of engineering efforts for each task in hand. Deep learning methods are alternative approaches that can automatically learn latent features and minimize the feature-engineering process. These techniques proved recently to be very effective in various NLP tasks, such as machine translation, relation extraction, and sentiment analysis and outperformed most of machine learning techniques, without relying on any hand-crafted feature, or just a few. Recently, neural networks methods were also introduced for negation scope detection. Lazib et al. first used different recurrent neural network models over real-valued vector representations of negated sentences to find the scope, and Qian et al. used a CNN model over syntactic information to detect the part of the sentence that is affected by the cue. These methods both outperformed the existing feature-rich approaches. Motivated by the success of neural networks, we propose in this paper a novel neural network method to solve the negation scope detection problem that improves the performance of previous works by adding additional information to the existing models without relying on hand-crafted features. We develop a syntactic path-based hybrid neural network model that combines two types of neural networks; BiLSTM and CNN, to model both the context of a sequence and the syntactic information. Bi-LSTM model is a two-way Recurrent Neural Network that can learn both the preceding and the following context information of each word in a sentence. CNN model is another neural network that can extract semantic representations and capture salient features from flat structures, such as the shortest syntactic path between the candidate token and the cue. Syntactic paths have been proved in previous works to hold salient information that can help in solving sequence labeling problems such as the scope detection issue. Taking advantage of the context and the syntactic information, our model can get rid of the hand-crafted feature engineering process, and thus be easily adapted to different out-of-domain texts. We evaluate our model for negation scope detection on the bioscope corpus. On the abstract sub-corpus, our approach achieved an F-score of 90.82% outperforming the results obtained by manually encoded features systems. These results prove that relying only on neural network models and syntactic information is a powerful alternative to get rid of the time-consuming feature engineering methods. The remainder of the paper is structured as follows. Section 2 gives an overview of the previous work. Then, Section 3 describes our hybrid neural network model. In addition, Section 4 illustrates the experimental settings and reports the different experimental results and analysis. Finally, Section 5 concludes the paper and present our perspective for future work.", "section": "Introduction", "doi": "10.1007/s11704-018-7368-6", "references": [13254153, 179875071, 1540017823, 1574147753, 1750263989, 1782003667, 1940872118, 1982464493, 2035703356, 2064675550, 2079735306, 2097606805, 2102113734, 2105827650, 2115915625, 2123207900, 2131774270, 2135932125, 2136566870, 2139865360, 2142786899, 2147880316, 2158899491, 2172140247, 2250521169, 2250966211, 2251135946, 2251826083, 2293634267, 2402268235, 2491832692, 2508309896, 2511964075, 2513222501, 2566847560, 2751776087, 2776461464, 2964217331]}
{"paragraph": "In finance, credit scoring is one of the crucial challenges and a core responsibility for lenders in their risk management. For example, the financial crisis of 2007/2008 impressively shows possible impacts of faulty risk management. Started as local sub-prime crisis in the housing sector in the U.S., the crisis spread around the world and severely infected the global economy as a whole. Even a decade later, the aftermaths of the crisis are still virulent, challenging economies around the world. As a lesson of the financial crisis, the rules for the assessment of credit ratings and the capital requirements for financial institutions have been tightened all over the world. Credit scoring ranges from the rating of countries and global international companies to ratings of small enterprises applying for credits. Furthermore, individuals are regularly assessed regarding their creditworthiness, whether they are applying for a large home loan or just seeking for a small mobile phone contract. The impacts of credit scoring are significant. On the one hand, approving credits to clients who fail to repay, may lead to affordable losses in the best case but may also jeopardize a lender’s business or even have severe impacts on macroeconomic levels like in the financial crisis in the worst case. On the other hand, strict rules for credit approvals may damage an economy that could be strengthened by affording credits. Consumer credits may increase the demand for goods whereas investment credits may support the supply side of an economy. When performing credit scoring three results are normally obtained. Some clients are solvent at the first sight while some other clients instantly fail to be creditworthy. The remaining clients need to be assessed in more detail before a final decision on the approval or disapproval of the credit can be made. More generally, the intensities of assessment required are not constant for all clients but they vary from client to client. They depend on the amount of information that is needed for a particular decision. The principles of such three-way decisions are widely applied in a diverse range of areas, such as medical diagnosis, computer vision, or recommender systems. However, only recently Yao proposed a formal framework for three-way decisions that is derived from probabilistic rough sets. Yao proposed the theory of three-way decisions to obtain cost efficient categorizations of objects into such three classes; the positive and negative ones and the objects whose final decision should or must be postponed. The objective of this paper is to propose a methodology for credit scoring that minimizes the decision-relevant costs using three-way decisions with probabilistic rough sets. Furthermore, we underline this methodology’s potential via a real-world application, where we analyze a data set that was collected by a Chilean bank. It consists of more than 7000 credit applications from small and micro-companies and their evaluations. In our analysis, we take the perspective of financial accounting as it is relevant for banking regulation. In Section 2, we review the theory of three-way decisions and present some of their applications. In Section 3, we develop our framework for credit scoring using three-way decisions and design decision rules to take the final deterministic decision for the case of credit granting. The subsequent section contains the results of applying this framework to the before-mentioned data. The paper concludes with a summary in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.08.001", "references": [1877823, 95749022, 101345952, 110592617, 112052679, 154229132, 837378864, 1004203976, 1041128124, 1511669739, 1537207111, 1756509722, 1861960977, 1973948212, 1975251650, 1982120517, 2001692054, 2007929615, 2009653394, 2036349451, 2050237791, 2060952501, 2067145099, 2070813883, 2080404663, 2090687467, 2137919530, 2159875572, 2191555075, 2217596628, 2278789126, 2310435784, 2549504134, 2600072788, 2792234535, 2793566013]}
{"paragraph": "The concept of three-way decisions was originally introduced by Yao to describe the three regions of rough sets. As a matter of fact, the theory of three-way decisions goes beyond rough sets and is related to three-valued sets, three-valued logics, shadowed sets, and orthopairs. Yao formalized a more general framework of three-way decisions called the trisecting-and-acting model, which divides a universal set into three pair-wise disjoint parts and performs effective strategies on some or all of the parts. The ideas of three-way decisions have inspired many three-way approaches and applications, for example, three-way classification, three-way analysis, three-way clustering, three-way recommendation, three-way fuzzy matroids, and three-way approximations. In three-way classification, the three regions are known as the positive, boundary, and negative regions, and can be interpreted in terms of three types of decision rules, namely rules for acceptance, rules for non-commitment, and rules for rejection, respectively. Objects satisfying acceptance rules are put into the positive region, objects satisfying non-commitment rules are put into the boundary region, and objects satisfying rejection rules are put into the negative region. The decision-theoretic rough set model, proposed by Yao, is one possible way to construct the three regions of three-way decisions. The decision-theoretic rough set model systematically calculates thresholds from loss functions using the well-known Bayesian decision procedure. Researchers have studied three-way decisions and decision-theoretic rough set models in various types of situations. For example, some generalized loss functions with triangular numbers, interval numbers, point operators, dual hesitant fuzzy elements, and established the triangular fuzzy decision-theoretic rough set model, interval-valued decision-theoretic rough set model, intuitionistic fuzzy decision-theoretic rough set model, and dual hesitant fuzzy decision-theoretic rough set model, respectively. Within the framework of fuzzy probabilistic approximation space and interval-valued fuzzy probabilistic approximation space, some generalized classical relations to fuzzy relations and interval-valued fuzzy relations, and proposed the fuzzy decision-theoretic rough set model and interval-valued fuzzy decision-theoretic rough set model. Other researchers studied three-way decisions in the framework of three-way decision spaces in an attempt to provide a solid mathematical foundation. The results of three-way decisions and decision-theoretic rough set models have been successfully applied to many fields, such as software defect prediction, cluster analysis, face recognition, government decisions, decision-making, attribute reduction, pattern discovery, credit scoring, etc. A multiset is a collection of elements in which elements may occur more than once. The idea of multiple instances of the same element has existed throughout the development of mathematics. For example, the prime factorization of an integer greater than zero is a multiset whose elements are primes, for example the number 360 has the prime factorization which gives the multiset 2, 2, 2, 3, 3, 5. Every monic polynomial over the complex numbers corresponds in a natural way to a multiset of its roots, for example the roots of the polynomial constitute the multiset 1, 1, 3, 3, 3. Multisets are not only of interest in mathematics, computer science, physics, but are also very common in our daily life. For example, decision results for a paper from a group of reviewers, grades for interviewees from a group of interviewers, and evaluation results from a group of customers are all multisets. Suppose that four doctors make their decisions of whether a patient has a disease. The probabilities of having the disease, as given by four doctors, are 0.9, 0.9, 0.9, and 0.1, respectively. We have two means of representing the results: 0.9, 0.1 and 0.9, 0.9, 0.9, 0.1. The former is a classic set which deletes all duplicate elements, while the latter is a multiset. The former cannot provide full information about the result, while the latter causes a person to believe that the patient has the disease. In this paper, we generalize the decision-theoretic rough set model to deal with multiset-valued data. We develop two generalized models known as the multiset-decision-theoretic rough set model and multiset-fuzzy-decision-theoretic rough set model. The remainder of this paper is organised as follows. Section 2 reviews basic concepts of a multiset, three-way decisions, and the decision-theoretic rough set model. Several new operations of multisets are introduced and their corresponding properties are discussed. Section 3 investigates three-way decisions in multiset-valued information tables. The multiset-decision-theoretic rough set model is established, which generalizes loss functions using multiset values. By integrating the multiset-decision-theoretic rough set model with the fuzzy decision-theoretic rough set model, the multiset-fuzzy-decision-theoretic rough set model is created for multiset-valued information tables. The conclusion follows in the last section.", "section": "Methodology", "doi": "10.1016/j.ins.2018.08.024", "references": [101345952, 147108263, 837378864, 881110586, 1041128124, 1756509722, 1883715000, 1969463949, 1969535228, 1977880445, 2023750115, 2048472139, 2052715156, 2070416356, 2070813883, 2074056315, 2089923511, 2114832876, 2176795090, 2199918042, 2217596628, 2272142993, 2297889545, 2499835732, 2515748156, 2519715111, 2549504134, 2560804083, 2561843635, 2586356223, 2588487610, 2600072788, 2607909519, 2609279172, 2620114837, 2740407498, 2763102103, 2772794611, 2774054418, 2792234535, 2793566013, 2799813871, 2806445482, 2887596728]}
{"paragraph": "When data originate from different but related tasks, it is possible to improve generalization performance by discovering intrinsic relationships among tasks, and multitask learning is applicable in this setting. The multitask learning framework has been successfully applied in many real-world applications: bioinformatics, computer vision, and speech recognition. Tasks can be related in different ways, and various multitask learning techniques have been developed based on different assumptions. One important assumption of multitask learning is that all tasks have shared common structures, and several algorithms have been developed based on this assumption. However, this may be too restrictive in real-world applications due to the reason that some tasks may be less correlated with others. To overcome this limitation, a number of methods have been proposed by adopting different assumptions. For example, some methods assume all tasks are clustered into different disjoint groups, and model parameters for the tasks in the same group should be shared. The approach assumes that the task within each group should lie in a low dimensional subspace. These methods are suitable for the case in which tasks can be easily clustered. On the other hand, imposing a hard partitioning of tasks may lead to some weakly relevant tasks being assigned to the same group, which may lead to unsatisfactory performance. In addition, feature selection is not taken into consideration. Feature selection techniques are an effective way to avoid the limitation due to high-dimensional data. Most feature selection methods are based on the assumption that each task is learned individually. Unlike conventional model learning, multitask feature selection algorithms choose relevant features shared by related tasks. Many multitask feature selection methods are developed based on the assumption that models for all tasks can be jointly learned in the same feature subspace. Note that the objective of this paper is to identify relevant shared features for the related tasks during model learning, which is also called multitask feature learning. For instance, one method proposed a regularization-based multitask feature selection method to select a common set of features for all tasks. Another model assumes that all tasks share some common features and a few task-specific features. In order to identify the task outliers, the robust multitask feature learning model has been developed. However, these assumptions are still too restrictive. For example, in a document classification task, each topic represents a task and each keyword is a feature. Some methods select a subset of keywords for representing all topics. This is not effective since there may be some overlap between related topics, and different topics should be associated with different discriminative keywords. In this paper, we offer an effective solution to the above problem by analyzing the relevance among tasks, and selecting the appropriate features for related tasks. As shown in a figure, the three topics, education, computer, and sciences, are related, and similar keywords are selected for representing them. In this paper, we consider a more flexible multitask feature selection or learning setting: tasks are jointly learned based on a similarity graph. Different from existing clustering methods in which a hard partitioning is imposed on the tasks, a graph is more suitable for representing the pairwise relevance among tasks. Moreover, the proposed model is able to identify task-specific features for each task. Inspired by previous models, the model matrix is decomposed into two components, and different regularization terms are applied to these components. Specifically, a graph-guided regularization term and a sparsity-inducing regularization term are used to constrain the sparsity of the solution at both the task level and the feature level. A variant of the smooth proximal gradient method is proposed to solve the corresponding optimization problem. The proposed approach is able to simultaneously capture the structure of tasks, and select discriminative features for related tasks. As a result, related tasks share similar features. On the other hand, a number of task-specific features can also be selected for each task. As a result, the proposed method can be applied to high-dimensional data for exploring feature-task relationships. The main contributions of this paper include the following. A flexible multitask feature selection approach which is able to simultaneously capture the task grouping structure, and perform feature selection by adopting a graph-guided regularization framework. A variant of the smooth proximal gradient method which was developed to effectively solve the related optimization problem. A theoretical analysis of the proposed model is also provided. An evaluation of the proposed method on several benchmark multitask data sets has been performed, and experimental results verify the effectiveness and demonstrate the superiority of the proposed model when compared with the standard multitask learning methods. The rest of this paper is organized as follows. In Section II, we review related multitask learning approaches. We introduce the proposed robust multitask feature selection with graph-clustered feature sharing in Section III and develop smooth proximal gradient optimization method in Section IV. We provide a theoretical analysis in Section V. Experimental results on synthetic and real-world datasets are described in Section VI. In Section VII, we draw the conclusion of this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864107", "references": [123755266, 577139289, 1939941161, 1942758450, 1970304186, 1977617632, 1999140058, 2005292390, 2018096278, 2031250362, 2032612424, 2065180801, 2080995863, 2097451239, 2106115875, 2107077531, 2110332320, 2110994494, 2118099552, 2119387367, 2119479037, 2135069544, 2137401668, 2143104527, 2144567071, 2151386286, 2153086202, 2156200251, 2166721725, 2186054958, 2188379175, 2244253939, 2322020277, 2561587488, 2914746235]}
{"paragraph": "In the last few years, digital image forensics has been drawing an ever increasing attention in the scientific community and beyond. With cheap and powerful cameras available to virtually anyone in the world, and the ubiquitous diffusion of social networks, images and videos have become a dominant source of information. Unfortunately, they are used not only for innocent purposes, but more and more often to shape and distort people’s opinion for commercial, political or even criminal aims. In this context, image and video manipulations are becoming very common, and increasingly dangerous for individuals and society as a whole. Driven by these phenomena, in the last decade, a large number of methods have been proposed for forgery detection and localization or camera identification. Some of them rely on semantic or physical inconsistencies, but statistical methods, based on pixel-level analyses of the data, are by far the most successful and widespread. Mostly, they exploit the fact that any acquisition device leaves on each captured image distinctive traces, much like a gun barrel leaves peculiar striations on any bullet fired by it. Statistical methods can follow both a model-based and a data-driven approach. Methods of the first class try to build mathematical models of some specific features and exploit them for forensic purposes. Popular targets of such analyses are lens aberration, camera response function, color filter array or JPEG artifacts. Having models to explain the available evidence has an obvious appeal, but also a number of shortcomings, first of all their usually narrow scope of application. As an alternative, one can rely on data-driven methods, where models are mostly abandoned, and the algorithms are trained on a suitably large number of examples. Most data-driven methods work on the so-called noise residual, that is, the noise-like signal which remains once the high-level semantic content has been removed. A noise residual can be obtained by subtracting from the image its clean version estimated by means of denoising algorithms, or by applying some high-pass filters in the spatial or transform domain. Noise residuals can be also used in a blind context to reveal local anomalies that indicate possible image tampering. Among all methods based on noise residuals, those relying on the photo-response non-uniformity noise deserve a special attention for their popularity and performance. It was observed that each individual device leaves a specific mark on all acquired images, the PRNU pattern, due to imperfections in the device manufacturing process. Because of its uniqueness, and stability in time, the PRNU pattern can be regarded as a device fingerprint, and used to carry out multiple forensic tasks. PRNU-based methods have shown excellent performance for source identification and for image forgery detection and localization. Note that they can find any type of forgeries, irrespective of their nature, since the lack of PRNU is seen as a possible clue of manipulation. The main drawbacks of PRNU-based methods are the need of a large number of images taken from the camera to obtain good estimates and the low power of the signal of interest with respect to noise, which impacts heavily on the performance. In particular, the prevailing source of noise is the high-level image content, which leaks in the PRNU due to imperfect filtering. The latter often overwhelms the information of interest, especially in the presence of saturated, dark or textured areas. This latter is a typical problem of all the methods based on noise residuals. In this work, to overcome these problems we propose a new method to extract a noise residual. Our explicit goal is to improve the rejection of semantic content and, at the same time, emphasize all the camera-related artifacts, which contribute to the digital history of an image. To this end, we follow a data driven approach and exploit deep learning. A suitable architecture is designed, inspired by Siamese networks, and trained on a large dataset which includes pristine images from many different camera models. During training we only need to know whether the extracted patches come from the same camera and position or not. Therefore, we take advantage of hidden spatial dependencies, also with the help of a suitable spectral loss. Once the training is over, the network is freezed, and can be used with no further supervision on images captured by any camera model, both inside and outside the training set. To any single image the network associates a noise residual, called noiseprint from now on, which shows clear traces of camera artifacts. Therefore, it can be regarded as a camera model fingerprint, much like the PRNU pattern represents a device fingerprint. It can also happen that image manipulations leave traces very evident in the noiseprint, such to allow easy localization even by direct inspection. As an example, two images subject to a splicing forgery can be easily detected by visual inspection of their noiseprints. It is worth observing that these artifacts cannot be spotted so clearly using other noise residuals. In the rest of the paper, we first analyze related work on noise residuals to better contextualize our proposal, then describe the proposed architecture and its training, carry out a thorough comparative performance analysis of a noiseprint-based algorithm for forgery localization, provide ideas and examples on possible uses of noiseprints for further forensic tasks, and eventually draw conclusions.", "section": "Related Work", "doi": "10.1109/TIFS.2019.2916364", "references": [1515615411, 1536019179, 1968164921, 1983586459, 1985729543, 1994743750, 2006028575, 2009130368, 2012203653, 2018377917, 2019716459, 2025328853, 2034556443, 2046180645, 2049771774, 2055745001, 2066923536, 2068575457, 2068734928, 2071794886, 2080264113, 2087361987, 2089818991, 2096076665, 2096754397, 2097506049, 2098594597, 2099013489, 2104116677, 2107461197, 2110598603, 2111374353, 2113267006, 2136492908, 2156238933, 2164990255, 2168126647, 2170343586, 2410977227, 2412509443, 2469175162, 2479919622, 2508457857, 2514123796, 2541922885, 2557414982, 2560026874, 2572561073, 2574852830, 2590464616, 2603123944, 2734722035, 2747268660, 2752015292, 2762911267, 2777769595, 2787755382, 2801560392, 2802179504, 2802701183, 2888817253, 2888922181, 2907295878, 2962958939, 2963678090, 2964146055]}
{"paragraph": "Non-orthogonal multiple access techniques offer promising solutions to spectrum scarcity and congestion problems in next-generation wireless networks, attributed to its efficient utilization of available resources serving multiple users simultaneously, as opposed to conventional orthogonal multiple access techniques. Owing to the broadcast nature of wireless transmissions, securing transmitted data from potential eavesdroppers or untrusted nodes in the network is a critical system design aspect that needs careful consideration. Physical layer security is a powerful tool to achieve the goal of provably unbreakable, secure communications by exploiting the inherently different physical communication channels between different nodes in the network. In this work, we design secure transmission schemes for a downlink single-input single-output non-orthogonal multiple access system considering two possible unsecure environments: the first is when there is an external eavesdropper, for which we use trusted cooperative relays to enhance security, and the second is when communication occurs through an untrusted relay node. There have been a number of recent works in the literature that study physical layer security for non-orthogonal multiple access systems. Secrecy sum rate maximization of single-input single-output systems has been studied. Using tools from stochastic geometry, some works study security measures for large-scale systems in the downlink and the uplink, respectively. Multicast-unicast streaming has also been studied, where secure rates for unicast transmission using non-orthogonal multiple access is shown to outperform conventional orthogonal schemes. Other work considers a multiple-input multiple-output two-user setting with an external eavesdropper and designs beamforming signals that maximize the secrecy sum rate. This approach is also considered in multiple-input single-output and multiple-input multiple-output scenarios in a two-user setting, with the assumption that one user is entrusted and the other is the potential eavesdropper. The impact of transmit antenna selection strategies on the secrecy outage probability has been investigated. Transmit power minimization and minimum secrecy rate maximization subject to a secrecy outage constraint have also been considered. Different from the previous works, in this work we investigate the advantages of using trusted cooperative relays to secure messages from an external eavesdropper, and also study the impact of having an untrusted relay on achievable secrecy rates in the context of non-orthogonal multiple access. Our work on using trusted cooperative relays is most closely related to single-receiver wiretap channel works in which half-duplex relays are employed to enhance security. Other works use similar ideas as well with a focus on full-duplex relays using mixed decode-and-forward and cooperative jamming strategies. Information-theoretic analysis of communication systems with untrusted relay nodes has been considered, including settings of deaf relays that are ignorant of the source’s transmitted signal in the presence of external eavesdroppers, and scenarios that involve cooperative jamming and noise forwarding schemes. Some studies investigate two-hop scenarios with an untrusted relay and provide achievable secrecy rates for a single source-destination pair and for a multi-terminal setting, respectively, with the help of cooperative jamming signals from the destinations. In another general untrusted relay channel scenario, positive secrecy rates are shown to be achievable if the source-relay channel is orthogonal to the relay-destination channel via a compress-and-forward scheme at the relay. Similar to these previous works, in this work we also use information-theoretic tools to derive achievable secrecy rate regions in the context of non-orthogonal multiple access, with trusted and untrusted relays. In the first part of this paper, we extend existing ideas to work in the context of a two-user downlink single-input single-output system with an external eavesdropper. We employ multiple trusted cooperative half-duplex relays to enhance the achievable secrecy rate region through various relaying schemes: cooperative jamming, decode-and-forward, and amplify-and-forward. For each scheme, we design secure beamforming signals at the relays that benefit the users and/or hurt the eavesdropper. Under a total system power constraint, that is divided between the base station and the relays, an achievable secrecy rate region for each relaying scheme is derived and analyzed. In general, the results in this case show that the best relaying scheme highly depends on the system parameters, in particular the distances between nodes, and that the relatively simple cooperative jamming scheme performs better than the other schemes when the relays are close to the eavesdropper. In the second part of this paper, we consider a different scenario in which an untrusted half-duplex relay node is available to assist with the base station’s transmission. Applications of this scenario are when, for example, the relay has a lower security clearance relative to the end users, and hence transmission schemes should be designed in such a way that the relay can only forward the data without revealing its actual contents. We derive achievable secrecy rate regions in this case under two relaying schemes: compress-and-forward and amplify-and-forward. We also consider two modes of operations: passive user mode and active user mode. In the passive user mode, the users receive data from both the base station and the relay and combine them efficiently to decode their messages. In the active user mode, the users transmit a cooperative jamming signal simultaneously with the base station’s transmission to further confuse the relay, and hence, since the focus is on half-duplex nodes, they cannot receive the base station’s transmission and rely solely on the data forwarded to them through the relay. We derive, analyze, and compare the achievable secrecy rate regions for each relaying scheme and operating mode under a total system power constraint, that is divided between the base station, the relay, and the users if operating in the active mode. As in the first part of the paper, the results also show in this case that the best relaying scheme and operating mode depends, in particular, on the distances between the nodes, with a general superiority of the active user mode over the passive user mode.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2911162", "references": [2011755800, 2026537412, 2088654965, 2091682073, 2097480027, 2130684118, 2137552535, 2142102521, 2148672861, 2171006634, 2277419020, 2302349451, 2551126468, 2566155463, 2582744858, 2605451877, 2620915951, 2743190787, 2761665166, 2920559781]}
{"paragraph": "Over the past decades, exoskeleton robots have been developed for human power augmentation and rehabilitation training. One of the most critical issues in controlling a robotic exoskeleton is to enable the robots to learn the human’s experiences and skills so that the robots could actively cooperate with the human subject. In this paper, a hierarchical control scheme is proposed, which includes a high-level learning model for interaction that can learn human experience and skills from demonstration, and low-level robot motion control that enables the robot to be back drivable. One way to achieve successful and safe human–robot cooperative manipulation is to make the robot possess the ability of estimating human intention. There are multiple types of information that a robot could use to predict human intention, including motion, partner posture, and interaction force between the human and environment among others. For example, electromyography signals, generated by motor neuron impulses that activate the muscle fibers, can be correlated with the torque at the joint level produced by muscles, which contains the human intention information. The electromyography-based intention estimation methods are related to complex calibrations that are necessary for accurate and reliable modeling. The relation between electromyography signals and torques involves several nonlinearities and strongly depends on the placement of electrodes. Compared to model-based methods, electromyography-based techniques do not require a dynamic model of the limb interaction with the environment. An alternative to estimating human intention is to enable the robot to learn from demonstrations and transfer the human experience and skills from demonstrations to the robot. Until now, there are many different approaches investigated for learning from demonstration. For example, learning algorithms such as hidden Markov models and Gaussian mixture models have been used. A prediction framework based on hidden Markov models was proposed to estimate the human’s intended trajectory, which employs a position-based impedance controller to map the haptic observations to the estimated trajectory. Online incremental learning of full body motion primitives was proposed, which partitioned human motion trajectory into motion primitives and then used a hidden Markov model to encode each motion primitive. Several sets of trajectories were modeled by a library of Gaussian mixture models, and the human intention inference algorithm was developed using unsupervised Gaussian mixture models. The application of a statistical framework was presented, which endows a robot with the ability to perform a cooperative manipulation task with a human subject. A comprehensive survey of robot learning from demonstration, a technique that develops policies from example state to action mappings, has been presented. Dynamical movement primitives for modeling attractor behaviors of autonomous nonlinear dynamical systems have also been presented. However, the above works seldom consider the variability of human–robot cooperative manipulation among multiple demonstrations or exploit it in the reproduction phase. When a human subject interacts with a robot, one goal is to model the human behavior which is recorded during demonstration, and to determine the underlying features and constraints of the interaction. During demonstration, a demonstrator teaches a robot how to execute the task successfully and allows it to analyze the behavior recorded by using statistical approaches. In this paper, a strategy for learning human skills from demonstration that utilizes Gaussian mixture models for high-level interaction is presented. In human–robot cooperative manipulation, the human subject should offer a minimum impedance required to complete a cooperative task. But due to variation in the interaction force caused by environmental disturbances, the motion of the robot end-effector would deviate far from the desired, which may lead to the failure of the task. The proposed strategy for learning human skills from demonstration compares the impedance behavior of a demonstrator in those successful trials with that of the subjects who want to perform the same task. First during demonstration, the demonstrator performs the tasks individually and successfully. The underlying features and constraints of the interaction are statistically analyzed and learned using Gaussian mixture models based on logged human–robot interaction data. Then, during reproduction, the robot predicts the interaction force similar to those in demonstrations by using Gaussian mixture regression, to assist the subjects with external assistance to be sure that the task can be completed successfully. To encourage the subjects to actively participate into the cooperative task, provision of assistance to the subject is adjusted according to the variability of the demonstrator’s behavior observed in the demonstration phase. Therefore, the presented learning-based strategy transfers the underlying characteristics and constraints of the given impedance-based task to the exoskeleton robot, which leads to cooperative interaction between the human subject and the robot, where the robot provides assistance only when needed. Furthermore, we extended the learning model to task-parameterized Gaussian mixture models in order to reproduce motions with different targets. On the other hand, low-level motion control of the robot needs to be designed to achieve comfortable and safe interaction with the robot. In order to design an admittance control when the model of the exoskeleton robot is unavailable, a model-free PID-type admittance control has been proposed. In order to achieve the goal of the operator feeling as if a wearable robot is a natural extension of the body, a novel admittance control for a wearable robot has been developed. However, these works do not consider the physical constraints of an upper limb exoskeleton. Various physical constraints such as safety specifications and physical limits normally exist in many robotic systems. Violation of constraints may degrade the performance of the system. For example, the maximum value of a robotic manipulator’s joint position is constrained by its physical configuration and it needs to avoid violating these physical limits when it operates. Without taking constraints into account, the control design would lead to the failure of control. Therefore, it is a challenge to deal with the constraint as one of the control objectives. Many methods have been proposed to handle the constraints problem for various mechanical systems. Contractive model predictive control was proposed for constrained nonlinear systems. To handle the constraints in robotic systems, a barrier Lyapunov function was developed. A systematic control design for single-input–single-output nonlinear systems with output constraints was presented. Barrier Lyapunov function is used for the single-input–single-output output-constrained nonlinear system, and it is used for the nonlinear system with time-varying constraints. A novel integral barrier Lyapunov function is designed for the control of a single-input–single-output nonlinear system with state constraints. As we can see from the above works, the barrier Lyapunov function method has been proven to be an effective method in the control of robotic system with constraints. However, none of these works considers human–robot interaction for a robotic exoskeleton, nor the use of asymmetric barrier Lyapunov function. For low-level robot motion control, this paper proposed an admittance control scheme with an inner position controller. Admittance control provides an established mechanism to specify a dynamic relationship between position and force in a system, and provides definable degrees of compliance. The admittance control uses the interaction force as input and outputs the position of the robot end-effector in task space, and it requires an inner position controller to track this position. In order to avoid singularities in task space so that the robot can operate without violating asymmetric physical constraints, and to improve the tracking performance, we designed an asymmetric barrier Lyapunov function-based adaptive neural network controller, which allows the tracking errors to remain within the constraints. Asymmetric barrier Lyapunov function is used to cope with asymmetric physical constraints while adaptive neural network is used to approximate the unknown model parameters of the robotic system. In this paper, we present a learning-based hierarchical control scheme which consists of a high-level learning model that is capable of characterizing cooperative impedance-based manipulation tasks, and a low-level admittance controller with an inner position controller that is able to deal with asymmetric constraints. The robot first observes and learns the impedance-based behavior of the demonstrator in the demonstration phase, and then predicts the movements the subject should have based on recorded demonstrated motion data so that it can assist the subject by compensating for the differences between predicted movements and the subject’s actual movements during reproduction. The contributions of this paper are threefold: a newly designed asymmetric barrier Lyapunov function-based adaptive neural network controller that achieves asymptotic tracking without violating asymmetric output constraints and its application in an exoskeleton robot; a learning-based control scheme which guides exoskeleton robot to aid human with assistance only when needed; extension of task-parameterized Gaussian mixture model into human–robot cooperative manipulation for exoskeleton robot.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864784", "references": [1682895868, 1977013839, 1986014385, 1986024064, 1992171281, 1997610562, 2001709653, 2021756757, 2023172481, 2032277247, 2034214945, 2056996905, 2062373349, 2074809255, 2076024315, 2079025554, 2091914309, 2102393714, 2113698995, 2115123877, 2122447324, 2130301822, 2136719407, 2139595765, 2140546948, 2142572589, 2165575781, 2200632918, 2210365848, 2322447463, 2549092026, 2561515991, 2565780995, 2588114174, 2609675074, 2738231311, 2962848549]}
{"paragraph": "These days, in real-world applications, many generated data sets comprise multiple representations or views. For example, in person re-identification, we may obtain photos of the same person from nonoverlapping cameras distributed at different physical locations. In document clustering, the same document may be translated into multiple different languages. In web retrieval, webpages contain not only the context information but also the hyperlink information. In general, these different views share some consistency and complementarity. Therefore, by integrating such information, we can significantly improve the performance of machine learning tasks compared with only using single-view information. To date, many multiview learning methods have been proposed, such as co-training-based methods, multiple kernel learning-based methods, and subspace learning-based methods. With the development of deep neural networks, many deep multiview learning methods are also proposed, such as deep restricted Boltzmann machine, deep autoencoder, and deep recurrent neural network. Although these multiview learning methods have been successfully applied in many scenarios such as computer vision, natural language processing, computational biology, web retrieval, and so on, a major problem is that most of these methods can not address the unpaired multiview data problem in which the pairwise relations of some instances in different views are not given and the incomplete multiview data problem in which some instances in one view miss corresponding instances in other views. However, it is often the case that in practice the available multiview data are unpaired or incomplete. For example, some but not all news articles published on one website may be also simultaneously published on other websites by different publishers. In the audio–video system, different sampling frequencies of sensors acquiring data may result in nonsynchronicity between signals from different channels. In the Alzheimer’s disease neuroimaging initiative, almost half of the patients lack cerebrospinal fluid measurements and many other patients may suffer from the lack of fluorodeoxyglucos positron emission tomography scans. Such unpaired and incomplete cases in all views make it difficult to learn better models for most of the above-mentioned methods. Thus, it is necessary to develop special multiview learning methods for both of these cases. Recently, some attempts have been made to address the unpaired and the incomplete multiview data learning problems, respectively. In this paper, we provide a taxonomy of these existing methods and roughly divide them into the following four main categories. Instances Imputation Methods: These methods are mainly designed for incomplete multiview data learning problems. They first fill in the missing instances with certain algorithms and then train the existing multiview learning algorithms on the imputative data sets. For example, some methods first fill in the missing samples in each incomplete view with the average feature values and then use the weighted non-negative matrix factorization with regularization for shared subspace learning. Other works embedded the view imputation step into the learning process and then simultaneously learn the missing instances and model parameters. Some proposed incomplete multiview clustering methods by using the kernel matrix in one view to fill in the missing kernel matrices of other views, instead of filling in the missing instances. Instance-Level Constraint Methods: These methods use the specially designed algorithms to ignore the missing information and just constrain the paired or nonmissing instances in each view to follow certain consistent assumptions. For example, some propose the incomplete multiview data clustering methods based on subspace learning by constraining the same instances in different views to have the same latent representations in the subspace. Others also used a similar constraint and propose a unified dimensionality reduction framework for partially paired and semi-supervised multiview data by utilizing both the global structural information captured from the unlabeled data and the local discriminative information captured from the limited labeled data. Another method proposed an incomplete multiview data classification method based on multitask learning by constraining the instances observed in each combination of views to belong to the same task. Further work proposed a transfer learning method for incomplete multiview data learning by constraining the different data sets and views to share a common subspace. Group-Level Constraint Methods: These methods carry out multiview learning by constraining the instance groups in different views to follow the consistent assumptions, and these groups can be obtained based on the label or cluster information. For example, one method proposed the group-level constraint method for a weakly paired multiview problem in which the full pairing between data samples may be not known, while pairing of a group of samples from one view to a group of samples in another view is known by jointly learning the projection matrices in each view and the pairing matrices. Another proposed a group-level constraint-based multiview dictionary learning method to simultaneously learn the dictionary and the pairing matrix. Co-Training: These methods train the single-view learning algorithms alternatively on different views and maximize the mutual agreement among these views. For example, one method first learns a base classifier for each view and then estimates the missing prediction scores by using certain missing value estimation algorithms which can be considered as maximizing the mutual agreement step of the co-training method. Another proposed a partially paired multiview data clustering method by using the co-EM algorithm to iteratively learn the clustering models in each individual view and then transferring the constraints across views. As described before, although the unpaired and the incomplete multiview data learning methods have many similarities, in general, the incomplete multiview data learning is more challenging than the unpaired multiview data learning, since the information loss of the former case where the instances in some view are missing is much more than that of the latter case where just the pairwise relations are unobserved. Especially, when dealing with multiview data which contain a relatively large number of missing instances, these above-mentioned methods often lead to poor results because of the less shared information caught by the models. To the best of our knowledge, only one work takes into consideration such circumstance by exploring the global structure over the entire training instances; however, it can only be used in two-view data and its performance is also not good enough, both of which limit its application scope. Recently, the Gaussian process-based latent variable models have been proposed to learn the complex low-dimensional manifolds and demonstrate their superiorities in nonlinear learning, uncertainty quantification, and so on. Furthermore, as a probabilistic nonparametric method, it needs no assumption on the concrete form of projection function and, thus, much flexibility is preserved, particularly in comparison to parametric methods, such as canonical correlation analysis and non-negative matrix factorization and so on. Apparently, it seems to be fairly straightforward to extend this model to the multiview learning case for gaining the merits of two worlds, that is, Gaussian process and multiview learning. But in fact, such a trivial extension of the previous Gaussian process-based multiview learning models just by using the instance-level constraint cannot obtain results as desired. In order to address this problem, in this paper, we propose a shared Gaussian process latent variable model for incomplete multiview data clustering. The key idea of this model is that, apart from using the instance-level constraint, we also learn a set of intentionally aligned representative auxiliary points among individual views to not only compensate for missing instances but also implement the group-level constraint. Thus, the shared information among these views can be explicitly built into the model in the form of a group-level constraint to improve the performance of incomplete multiview data learning. In the learning step, we use the variational inference methods to derive a lower bound for the joint posterior distribution of latent variables and then learn all of the hyper-parameters and the auxiliary points simultaneously by maximizing the lower bound. Compared with the existing methods, this model naturally inherits the properties of Gaussian processes and mainly has the following advantages. It inherits the flexibility of Gaussian processes in modeling without explicitly defining the concrete form of the projection function. It can elegantly carry out nonlinear learning by using the nonlinear kernel function in the Gaussian process. It not only learns more representative latent variables of multiview data for clustering but also gives the uncertainty measure of predictions that can be utilized by the follow-up tasks. It is also straightforwardly extended to cases with more than two views without adding any complexity in formulation. In the experiments, we compare our proposed method with the state-of-the-art methods for incomplete multiview data clustering. The results on four small and two large multiview data sets demonstrate the superiorities of our method.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2863790", "references": [66306528, 87822204, 137285897, 146815106, 154472438, 309263341, 1501486674, 1541825479, 1550614472, 1670132599, 1775792793, 1777124189, 1869602175, 1879834137, 1883346539, 1907775068, 1947481528, 1988569689, 1991311318, 1998450142, 2010755682, 2035299679, 2041262647, 2050997020, 2083196737, 2085789144, 2088801701, 2089494389, 2099680562, 2100560442, 2101324110, 2104563967, 2108193616, 2108502868, 2111440402, 2115601375, 2115936765, 2120340025, 2120405375, 2129625650, 2131144984, 2136111243, 2142674578, 2142742813, 2144304835, 2147070561, 2161638512, 2169410692, 2178987369, 2179793346, 2181159407, 2184188583, 2396665160, 2405459681, 2546561608, 2563908351, 2572663323, 2586899202, 2611879059, 2613146443, 2748391982, 2788125892]}
{"paragraph": "Rough set theory is regarded as a useful tool for analyzing various types of data. The main contributions of this theory are set approximation and attribute reduction. However, the classical rough set model has two limitations. First, this model is sensitive to noisy data. Second, this model is incapable of directly handling real data based on equivalence relation. Decision-theoretic rough sets, which were introduced by Yao et al., result from the integration of Bayesian decision theory and rough set theory. DTRSs use conditional probability to measure the degree overlap between two sets and provide a generalized framework to solve the first limitation. Yao et al. defined three quantitative probabilistic regions by introducing a pair of thresholds. The required thresholds for these regions can be systematically computed and interpreted. In the past 20 years, DTRSs have gained increasing attention from researchers across several fields and have become increasingly popular in various theoretical and practical fields, thereby producing many comprehensive results. As an extension of classical rough sets, the DTRS model cannot directly deal with numerical data. Different types of extended DTRSs have been proposed, including DTRSs with fuzzy sets, interval sets, fuzzy interval sets, interval fuzzy sets, intuitionistic fuzzy sets, and hesitant fuzzy sets, which defined all types of relations instead of equivalence relations in classical DTRSs, to overcome this limitation. In all types of real data, the intuitionistic fuzzy value is regarded as an intuitively direct extension of the fuzzy set value. Rough sets and intuitionistic fuzzy sets capture particular facets of the same notion of imprecision. Studies that combine intuitionistic fuzzy sets and rough sets have been considered a positive approach to rough set theory. For instance, some researchers presented an intuitionistic fuzzy rough set and discussed the covering reduction method in intuitionistic fuzzy graded covering systems. Although these generalized DTRSs can be utilized to handle real data with noise, their target concepts are approximated by lower and upper approximations through a single information table. On the one hand, Qian et al. considered lower and upper approximations through multiple relations, proposed the so-called multi-granulation rough sets, and discussed the use of multi-granulation DTRSs in knowledge acquisition within the context of multiple information tables. Since then, MG-DTRSs have become an important topic in rough set theory. Despite multiple meanings in granular computing, based on Qian et al.’s viewpoint, the multi-granulation means multiple relations or information tables in this study. On the other hand, in many real-life problems, an object can take on many values given that multiple scales are available under the same attribute in an information table. For instance, the English examination results of students can be recorded as real numbers between 0 to 100 and can also be classified into five grades: “Excellent,” “Good,” “Moderate,” “Bad,” and “Unacceptable.” If necessary, the grades may be further classified into two values: “Pass” and “Fail.” Hence, knowledge representation and discovery in hierarchically organized information granulations are two important tasks in real-life data mining. Some researchers investigated fuzzy concept lattice representation in fuzzy information systems. Others introduced the notion of multi-scale information tables and analyzed knowledge acquisition in multi-scale decision tables under different granulation levels. Two optimal scale selection algorithms for multi-scale information tables were presented. Another group discussed a local approach to rule induction in multi-scale decision tables. Some used sequential three-way decisions to investigate the optimal scale selection problem in a dynamic multi-scale decision table. Others considered a dominance-based rough set approach by applying an incremental learning technique for multi-scale information tables. The domains of DTRSs, intuitionistic fuzzy rough sets, and MGRS have been comprehensively described with the advancements in related studies. However, few studies have been conducted on their combination in multi-scale intuitionistic fuzzy information tables. For instance, in some practical applications illustrated as an example, we can see that the context of multi-scale in intuitionistic fuzzy backgrounds and intuitionistic fuzzy presentations is reasonable in reality. However, as illustrated in the example, three basic problems on MG-DTRSs have been encountered in multi-scale intuitionistic fuzzy information tables. The method for addressing DTRSs and MG-DTRSs should be examined in multi-scale intuitionistic fuzzy information tables. Optimal scale selection should be investigated from the viewpoint of MG-DTRSs in multi-scale intuitionistic fuzzy information tables. In an optimal scale, the use of fewer intuitionistic fuzzy relations should be investigated from the perspective of MG-DTRSs to achieve the same approximation results. Five candidates are applying for a faculty position at a university. The similarities among these candidates are characterized by three objective evaluation indices: “Comprehensive Ability,” “Scientific Research Achievement,” and “Costs”. Each of these five evaluation indices has two scales, which are originally recorded as real numbers between 0 to 1 and three grades: “Equivalent,” “Similar,” and “Different,” respectively. Furthermore, these real numbers and grades can be further evaluated by some experts and represented by intuitionistic fuzzy values. For instance, on the second scale of Comprehensive Ability, 20 percent of the experts thought that two candidates were equivalent, 60 percent of them considered that they were totally different with each other, and the other did not give any opinions. A subjective evaluation intuitionistic fuzzy set of the five candidates can be obtained on the basis of some interviewers’ opinions, where a tuple like ⟨candidate, 0.7, 0.2⟩ means that 70 percent of the experts thought that the candidate should pass the interview, 20 percent of them opposed, and the other did not give any opinion. Thereafter, three problems arise. How to present the relations between the objective and subjective evaluations? Which scale of objective evaluations is optimal for describing the candidates? Which candidate should be chosen? On the basis of the aforementioned analysis, although the use of MG-DTRSs in information analysis is significant, MG-DTRSs cannot handle intuitionistic fuzzy information systems and exhibit limitations in processing multi-scale information tables. Therefore this study aims to explore MG-DTRSs in multi-scale intuitionistic fuzzy information tables. In consideration of this objective, we study inclusion measure-based MG-DTRSs in multi-scale intuitionistic fuzzy information tables. The main contribution of this study is the construction of MG-DTRSs and the further exploration of their optimal scale selection and reduction processes in multi-scale intuitionistic fuzzy information tables. The rest of this paper is organized as follows. Section 2 briefly introduces the preliminary notions considered in this study. Section 3 presents the concept of the inclusion measure-based DTRSs in multi-scale intuitionistic fuzzy information tables. Section 4 proposes the inclusion measure-based optimistic and pessimistic MG-DTRSs in multi-scale intuitionistic fuzzy information tables. Section 5 discusses optimal scale selection based on MG-DTRSs and presents two optimal scale selection algorithms for multi-scale intuitionistic fuzzy information tables. Section 6 defines the concept of optimal scale reduct based on MG-DTRSs in multi-scale intuitionistic fuzzy information tables and develops their discernibility function-based reduction methods. Section 7 shows possible further generalizations related to the inclusion measure-based MG-DTRSs in multi-scale intuitionistic fuzzy information tables. Finally, Section 8 concludes this study.", "section": "Methodology", "doi": "10.1016/j.ins.2018.08.061", "references": [964455177, 984339528, 1131999718, 1470723969, 1969463949, 1977268486, 1977880445, 1986839581, 1992915331, 1997362234, 2013300239, 2023750115, 2031345712, 2048472139, 2071255876, 2075471994, 2079438262, 2122937613, 2131468036, 2159457601, 2162755671, 2170755382, 2172368975, 2175162522, 2215524969, 2330216853, 2340020088, 2345465422, 2409322758, 2516473705, 2519715111, 2519971059, 2549504134, 2555871690, 2588487610, 2588572297, 2601091105, 2605772523, 2707976390, 2763102103, 2767869620, 2800399611]}
{"paragraph": "With a burgeoning number of IoT devices penetrating into all aspects of our lives, privacy-related issues are attracting increasing interest. Users of these devices worry about being watched, listened to, or tracked by wearable devices and smart home appliances. Sensitive personal data like lifestyle preferences and location information may be abused for unwanted advertisement purposes or for more nefarious objectives like unauthorized surveillance. In China, for example, over 20 million surveillance cameras equipped with artificial intelligence have been installed. The new surveillance systems have raised fears among citizens that the technology is being used to monitor their daily lives. To put consumers at ease and to encourage adoption of IoT technologies, privacy consideration should be incorporated into the core design of IoT solutions, with users being given more control over what information can be shared with the service providers. We model an IoT network consisting of multiple devices using the decentralized detection framework. In this model, IoT devices or sensors make observations, where each is the local observation of sensor. Each sensor then maps its local observation to a new value and sends that to a fusion center or service provider. The fusion center uses these mapped values to infer a hypothesis of interest, which is also called a public hypothesis. However, since the sensor information may also be used by the fusion center to infer other hypotheses that the user may not have given authorization for (which are known as private hypotheses), sending unsanitized sensor information to the fusion center may result in privacy leakage. For example, on-body wearables containing accelerometers and gyroscopes can be used to measure a person’s movements and pose in order to detect falls. However, the sensor information may also be used to detect other activities performed by the person, leading to loss of privacy. In this paper, we investigate the case where the fusion center is authorized to infer a public hypothesis based on sensor information that is sanitized in such a way that makes it difficult for the fusion center to infer an infinite set of private hypotheses. To define the set of private hypotheses, we consider an uncertainty set defined around a nominal private hypothesis. This is because in most applications, one can define a specific public hypothesis, but it may be difficult for a user to specify multiple private hypotheses that she wants to protect. Our goal is to design local privacy mappings at each sensor to protect the information privacy of private hypotheses that are “close” in a specific sense to the nominal hypothesis. We can interpret this framework as providing robust privacy for the nominal private hypothesis, in the same spirit as robust hypothesis testing.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2916650", "references": [1601684460, 1958009659, 2009816033, 2017613761, 2031533839, 2037759417, 2038194220, 2053801139, 2054365984, 2082639035, 2085472312, 2092214308, 2096870293, 2098145033, 2102931639, 2109426455, 2115915204, 2119874464, 2130118845, 2152926062, 2162408301, 2182112800, 2394738186, 2398203045, 2399072287, 2435473771, 2520571093, 2556397940, 2556626427, 2566050141, 2576765166, 2583220279, 2592694684, 2610910029, 2744172881, 2749457063, 2769615563, 2776458542, 2783793345, 2802013709, 2808012592, 2810524107, 2896658168, 2963446403, 2963535017, 2963634943]}
{"paragraph": "Many location-based services, such as recommending tourism resources based on travelers’ locations, require that their applications should work in real time and give priority to location relevance, for example, a tourist being in traveling is more likely to accept those proposed tourism resources that are nearby and recommended in time. In above applications, spatial objects should be found out firstly before further matching with users. Range queries are often conducted to find a group of nearby things for a specific object. The query efficiency on a large number of spatial objects will have a great influence on the service quality of these applications. Since queries must be completed as soon as possible on big datasets even many query requests are submitted simultaneously, spatial object storage management should be given some novel design considerations, which should be helpful for further query optimization to satisfy access requirements of location-related services. Data storage model is a key factor for query performance. For big datasets, distributed data storage model is popular. HDFS can be used for storing all kinds of data and HBase prefers to structured data by a key-value schema. The original intention of both HDFS and HBase are only to provide a storage solution for big data, optimization mechanisms for various kinds of data are not fully considered, especially for location-related data. Usually, when storage model is concrete, various indexes will be designed to improve query performance, both the storage model and data type decide which kind of index can be applied and is more efficient. For example, R-tree series indexes are often the first choice for centralized spatial data queries. But considering extra storage space caused by indexes and the access of a set of continuous objects in a specific space, this storage model needs still a new design. It is a very valuable problem to study the storage model of spatial objects by giving consideration to both its data management ability and the interface for further query optimization. This paper discusses a new perspective of managing spatial objects by integrating encoding mechanism with key-value structure, the proposed model performs well on both storage ability and index interface compatibility for query optimization.", "section": "Related Work", "doi": "10.1007/s11704-018-7030-3", "references": [1979919780, 1981420413, 2000053898, 2041549331, 2081894886, 2087946700, 2088630449, 2089617543, 2118269922, 2151135734, 2153704625, 2173213060, 2426261211, 2436533802, 2438792749, 2547319955, 2548049805, 2548613432, 2548945327]}
{"paragraph": "In many engineering systems, various failures are often encountered due to aging, wear, and other instability factors, such as sensor failures, actuator failures, uncertain internal failures, and so on. As we all know, the failures always affect the normal operation and even destroy the performance of the systems, where actuator faults play a pivotal role in these faults since they have a direct impact on the systems. Therefore, it is of both theoretical and practical importance to explore actuator fault-tolerant control strategies. Many scholars have devoted themselves to the study of this problem and have achieved some fruitful results. Over the past decades, adaptive technology has been greatly developed and a set of systematic theory has been gradually formed and widely used to solve various control problems. For example, adaptive fault compensation protocols are proposed for a class of uncertain nonlinear systems subject to actuator failures with measurable states. The problem of adaptive disturbance suppression for generalized uncertain nonlinear systems is solved by combining adaptive technique with adding a power integrator method. A distributed observer approach is proposed with adaptive technique for multiagent systems. Some works deal with actuator faults for unknown uncertain nonlinear systems with unmeasurable states by using adaptive neural network or fuzzy technologies. With the rapid development of networking and information technology, it has become a trend to connect control systems and plants through network communication channels in recent years. Network-based controls are widely used in various practical engineering systems, such as oil exploration on offshore platforms, smart grids, multiagent systems, and so on. Unfortunately, all the aforementioned references about the actuator failures require real-time transmission of control signals to the actuators, which increases the communication burden and transmission costs. Therefore, how to reduce the required computation cost and save communication resources has attracted extensive attention of scholars and a tremendous effort has been made into it. For this reason, event-triggered control is rapidly evolving with its unique advantages in conserving communication resources and maintaining competitive control performance. Compared with the conventional periodic sampling technique, the event-triggered control takes the behavior of the systems into consideration rather than relies on the real-time state solely. Such a new control method produces the samplings only when the state vector of the systems or output deviate from a certain predetermined threshold, so it has more advantages in saving energy resources and reducing computational cost. To our best knowledge, the research about the event-based control for linear systems have been relatively perfect and some significant results have been achieved. Things become particularly challenging when we attempt to achieve the same control effect for nonlinear systems. Fortunately, many outstanding achievements on the event-based control for nonlinear systems have been emerging in recent years. More specifically, approximation-based event-triggered control for multi-input multi-output continuous time systems has been proposed. An event-based adaptive control strategy for a class of nonlinear systems was presented with the trigger mechanism in the controller-to-actuator channels, while the trigger mechanism was intercalated in the sensor-to-controller channels. Some works consider the event-based control for the strict-feedback nonlinear systems with actuator faults. Furthermore, an event-driven stochastic adaptive dynamic programming technique was introduced for nonlinear systems which ensured ultimate boundedness of the systems. It is worth mentioning that the main limitation of those works is to assume that the states of the systems are measurable completely. However, it is not realistic to obtain all the states of the system in the practice systems. It is not a trivial matter to design the event-triggered controller only by utilizing the output instead of all the states of the systems. The exciting thing is that event-based output feedback control to nonlinear systems has been proposed by introducing a novel nonlinear observer and constructing dynamic controllers, respectively. Nevertheless, some works need to assume input-to-state stability for the measurement errors, while others need to presuppose that the dynamic controllers render the system uniformly asymptotically stable in the absence of a network. Therefore, the object of this paper is to devise the event-based output feedback control law for unknown nonlinear systems with the actuator failures in absence of such assumptions. As far as we know, there are no results about the event-based output feedback control for unknown nonlinear systems with actuator failures so far. The reasons may come from the inherent difficulties of designing the event-triggered output feedback control law for the unknown nonlinear systems with the uncertainty of actuator faults, including unknown occurrence time and unknown mode. In this paper, by constructing neural network state observer and combining adaptive technique, the event-triggered output-feedback controller for the uncertain nonlinear systems with the actuator faults is designed, which ensures that the output signal track the desired signal, and all the signals of the closed-loop systems are bounded. The main contributions or difficulties of this paper are highlighted as follows. This paper is the first to address the problem of event-triggered output feedback control for unknown nonlinear systems with actuator failures. Unlike earlier articles that only deal with actuator failures, the setting of the event triggering mechanism for the system with actuator failures makes the control design not a trivial matter. The controller is finally designed by using the properties of the hyperbolic tangent function to handle the event-triggered errors and designing adaptive compensation mechanisms to compensate for the actuator faults. The system to be considered is more general than those in relevant literature. The reason lies in that this system contains unknown nonlinearities, unknown external disturbances, unknown control gain, and unpredictable actuator faults. Another challenge is to design the state observer with adaptive compensation. Compared with prior work, the coupling of the unknown control direction and the loss of effectiveness fault makes the design of the state observer more painful. A new method to design the parameter adaptive law by estimating the product of the loss of control effectiveness rate and unknown control direction is proposed.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868169", "references": [1970540608, 1972107234, 1978921618, 2023230804, 2028579885, 2030629074, 2064710808, 2078804574, 2081079276, 2119493620, 2142072309, 2150535417, 2332899049, 2343782047, 2344109271, 2417709453, 2479280896, 2514514867, 2515370279, 2525747818, 2560013550, 2597767335, 2613419728, 2747580379, 2765965898, 2768680573, 2768779900, 2770213313, 2808419467, 2963713359]}
{"paragraph": "Internet of Things has quietly entered a multiple intelligence industry. Today, in advocating energy savings and environmental protection, the configuration optimization of IoT resources has become an urgent problem to be solved. There are many artificial intelligence algorithms for resource scheduling, which, however, were developed for specific applications and not well suited for solving IoT service problems studied in this paper. As a service, we will regard the entire layout of IoT as a service system. The solution to the resources optimization allocation problem of service-oriented networked collaborative equipment is a very complex issue, which belongs to a typical NP-hard combinatorial optimization problem. The question is: How to minimize the consumption of resources, and shorten the service time? In other words, how can multiple optimal services in enormous candidate sets be selected to meet the above objectives? So it is going to be a challenging multiobjective optimization problem. Many researchers have attempted to solve the problems of multiobjective service selection in web services. Some presented the first approximation scheme for multiobjective quality-driven service selection. Others launched research on multiobjective optimization of quality of service. They introduced Pareto set model for QoS-aware service composition. One approach supported decision makers in finding robust, QoS optimized service compositions using clustering. In our previous research work, we implemented adaptive web service composition inspired by the neuroendocrine-immune system. However, the above work focused on web service composition based on QoS. IoT services, whose features are large-scale, heterogeneity, unreliability, and dynamic in nature, are different from web services. An important challenge to address in the domain of IoT services composition is the development of efficient services selection algorithms for optimal management of both energy and QoS. This issue becomes crucial in the case of large-scale IoT environments composed of thousands of distributed entities. Some stated that IoT is a paradigm in which real-world physical things can be connected to the Internet and provide services through the computing devices attached. A three-layer QoS scheduling model for service-oriented IoT was proposed. The sensing as a service model is expected to be built on top of the IoT infrastructure and services. Then, services were assigned to interfaces with heterogeneous resources and produced optimal solutions for this computationally hard problem. From the analysis of the above literature, other approaches dealing with services selection were mostly unaware of energy issues, or they noted the minimum energy consumption but only as a unilateral goal. There are very few studies on multiobjective optimization in the context of IoT service, especially considering the equipment energy consumption and service time. It is necessary to develop a multiobjective optimization algorithm for IoT services, which can offer more practical value, for example, intelligentized facility agriculture and industrial manufacture. Evolutionary multiobjective optimization has become one of the mainstream research directions in the field of evolutionary computation. Some made a comprehensive review of the modern multiobjective evolutionary algorithms. Typical multiobjective optimization algorithms based on artificial immune systems include multiobjective immune algorithm, constrained multiobjective immune algorithm, an artificial immune network for multiobjective optimization called vector immune system, nondominated neighbor immune algorithm, and so on. Besides, in terms of vaccine, immune genetic algorithm and strategies of selecting vaccines and constructing an immune operator were proposed. Others emulated a biological notion in vaccines to promote exploration in the search space. In recent years, a novel immune clonal algorithm for multiobjective optimization was proposed. A degeneration recognizing clonal selection algorithm for multimodal optimization was designed. A new multiclass clustering method based on maximum margin clustering algorithm and immune evolutionary algorithm was proposed. In addition, some focused on local search strategies. An auxiliary local improvement operator convergence acceleration operator was introduced, and hill climber with sidestep was designed for the local search. A new multiobjective optimization framework based on nondominated sorting and local search was introduced. A novel ranking strategy called global margin ranking was adopted which deployed the position information of individuals in objective space to gain the margin of dominance throughout the population. Furthermore, in order to consider the coordination between the population and environment, and population and population in the evolutionary process, coevolutionary mechanism has been introduced into the immune optimization algorithm, and good results have been obtained for solving combinatorial optimization problems. The competition model and cooperative model are two important models in the coevolutionary multiobjective optimization algorithm. The coevolutionary algorithm based on cooperative model has achieved great success in solving single objective optimization problem. A cooperative coevolutionary algorithm for multiobjective optimization was presented, which was capable of maintaining archive diversity by dynamic sharing and extending operator. A competitive-cooperation coevolutionary algorithm was also proposed. Multiple subpopulations, respectively, optimized the part of decision variables. The difference is that the mapping relationship between each subpopulation and the decision variables are not fixed but determined by competitive results. Some other works also employed coevolutionary technique and multiple populations for multiobjective optimization. Based on immune system model, several subpopulations evolved using different evolutionary strategies. An immune coevolutionary algorithm with two stages was designed to search the optimal balanced partitions. A coevolutionary immune algorithm for garment matching problem was proposed, introducing dominance affinity and distance affinity. Cooperative coevolutionary algorithms for multiobjective capacitated arc routing problem were proposed. Two subpopulations that were cooperatively coevolved using the coevolutionary algorithm were employed to achieve a better global optimality for the estimated radial basis function neural network. A novel coevolutionary mechanism based on elite strategy was proposed, where elite individuals were used to guide the search. A framework named hyper multiobjective evolutionary algorithm was proposed. The size of subpopulation was adjusted according to the corresponding algorithm’s performance. Multiple subpopulations were adopted, and clustering and statistical methods were used to guide the generation of new population and the local search. Moreover, some researchers decomposed a multiobjective optimization problem in a collaborative manner. In addition, inspired by the mammalian endocrine system, an artificial endocrine controller for power management was designed in robotic systems. The endocrine mechanism was introduced to regulate cooperative coevolution among the particles. The aforementioned research work usually adopts multipopulation to implement coevolution; nevertheless it is rare to organize the population in a hierarchical way. Meanwhile, little has been done to embed the endocrine regulation mechanism into the evolution of subpopulations in multiobjective immune algorithm, while there is a natural synergy between the immune system and the endocrine system. Thus, the main motivation of this paper is to simulate the functions of immune-endocrine system and map these mechanisms to the coevolution of multiple populations, so as to more efficiently solve the multiobjective optimization of IoT service problems. Inspired by the existing achievements and the human immune-endocrine mechanism, we propose an immune-endocrine system inspired hierarchical coevolutionary multiobjective optimization algorithm in this paper. This algorithm employs the hierarchical structure, i.e., foundation layer and top layer, which evolves and learns from the ideas similar to previous elite strategy but not the same. It can provide optimal nondominated decision-making for service-oriented resource optimization allocation problem in IoT systems. Experimental results demonstrate the proposed algorithm is efficient to minimize the consumption of resources and shorten the service time. The main contributions of this paper are as follows. Inspired by the endocrine regulation mechanism, an endocrine-based strategy is designed and embedded in the subpopulation evolution process, which can guide efficient cooperative interactions among subpopulations and assist the top population toward global optimal solutions. The human forgetting memory mechanism is introduced into the evolution of the top population, which successfully solves the choice problem of nondominated solutions. Using clustering and statistical method during the evolution process, different components operated on the x-axis and y-axis are proposed, which can make the operations more directionally and purposefully. The rest of this paper is organized as follows. Section defines the multiobjective optimization model of IoT service. Section proposes the algorithm and details search mechanisms and strategies. The performances of the proposed approaches are evaluated and discussed in Section. Section concludes this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2866527", "references": [1150848239, 1511346087, 1965040190, 1967981290, 1968164389, 1970356115, 1980186244, 1988242323, 1994969405, 1999218527, 2011587419, 2017299308, 2020320008, 2021135079, 2034392538, 2035292227, 2035960005, 2039298419, 2040331308, 2046491030, 2053855431, 2055142708, 2077098488, 2095771532, 2098907614, 2102785645, 2106334424, 2121429049, 2122340689, 2126105956, 2137165245, 2147573707, 2156262512, 2169574584, 2193409108, 2219491737, 2271389764, 2272065637, 2275596639, 2323358965, 2336681276, 2338772919, 2344331011, 2401044109, 2517051814, 2517908245, 2586316225, 2911992235]}
{"paragraph": "Gaze tracking is the process of determining one’s point-of-gaze as a temporal sequence of coordinates. Spatially registering the points-of-gaze to a visual stimulus displayed on a monitor requires a calibration which depends on several factors related to the subject such as physiological properties, presence of sight correction apparatus or the environment such as illumination and position of the monitor. While controlled cross-subject and cross-device comparative studies of gaze tracking accuracy are available, these do not provide any information on whether the accuracy may degrade in time. In addition, vendor-provided performance information is an estimate assuming a specific use context such as desktop-based human–computer interaction. Especially from the biomedical engineering perspective, there are several motivations for exploring human perception via gaze tracking. Namely, understanding the medical image perception process by assessment of visual search strategies of clinicians, studying human visual expertise during image acquisition and interpretation for potential skill assessment, improving and designing graphical visualizations and interfaces for HCI in medical settings, and developing computer-based image analysis methods, such as automatic detection of medical image contents and computational modeling of human visual attention. First, the holistic understanding of expert perceptive and cognitive learning processes in terms of visual scan, search and recognition strategies, and decision-making has been of considerable interest to the scientific community. Studies in this direction include the evaluation of visual search tasks in radiology, computed tomography, mammography, magnetic resonance imaging, pathology, and ultrasound imaging. Second, variations in visual expertise and behavior between experts, trainees, and novices have been analyzed in different clinical settings, such as endoscopic surgery, radiology, mammography, pathology, computed tomography, pediatric neurology, dermatology, and ultrasound-guided anesthesia. Third, gaze tracking has been widely explored for HCI and usability research. The information inferred from eye tracking devices can be utilized to determine areas of interest and visual search patterns in an interface, and to evaluate the visibility, usefulness, and position of its elements. This has been shown to be helpful in improving interface design for more efficient system interaction, for example, reduced overall visual clutter and cognitive workload. An example of an HCI study for medical imaging is where gaze tracking was used to analyze visual search paths of radiologists to provide useful insights for designing efficient radiology workstations. Finally, recently there has been an emergence of interest in the image analysis literature in gaze tracking to understand its role in constraining image interpretation. For example, applications of gaze tracking-based computer vision include object recognition, action recognition, and caption generation. Likewise, expert human knowledge has been leveraged to design advanced medical image analysis algorithms for automatic detection and classification of image contents in mammography, retinal images, MRI, and ultrasound images. From the computer vision point of view, gaze tracking has informed saliency-based visual attention methods, leading to a number of computational visual saliency models. Correspondingly, prediction of visual attention in medical images has been performed using gaze data as ground truth in radiology and retinal images, endoscopy, and ultrasound images. However, the construction of meaningful models of medical image perception requires a large amount of real-world data to account for the natural variability of the task as well as the natural variability in human perception. This involves the acquisition of gaze data across an extended period of time potentially several months. In order to facilitate the analysis and interpretation of gaze data, it is important that the eye tracker is accurately calibrated for each observer, and that the calibration does not drift with time. Therefore, we wanted to understand how the accuracy of a commercial eye tracker varies in time, and whether a regular recalibration is necessary. Longitudinal stability of gaze tracking accuracy has previously been reported at timescales of the order of minutes. Some evaluated the temporal stability of different eye tracking algorithms for webcams in a single continuous session, that is, the stability of measurements across consecutive camera frames. Others compared the accuracy of two eye trackers with nine participants and at four different instances, separated by a pause of 2 min. The authors reported that the effect of time elapsed since calibration was not statistically significant at this timescale. Hence, to the best of our knowledge, there has not been a longitudinal study assessing performance at larger timescales. However, long-term temporal factors could have an impact on the accuracy of gaze tracking. Changes in the external environment, such as head position or illumination. Changes in the appearance of the user. This is relevant as most eye trackers use image-based tracking algorithms to take measurements from the user’s face and eyes. In this paper, we report the results of a longitudinal study of gaze tracking performance conducted over a period of one month. The aim of this paper was to evaluate whether the accuracy of gaze tracking is stable over this time interval after a single initial calibration for each user. This issue has been studied in related areas such as biometric recognition. For instance, it has been suggested that the accuracy of iris recognition may decrease in the very long term. A recent study reported a diminution of face recognition accuracy with years. However, accuracy over several years is not relevant to the range of applications of interest in biomedical image analysis. Indeed, the purpose of having a temporally robust gaze tracking system is to acquire data on human visual behavior while demanding minimum time and effort, that is, a minimal number of calibrations from the studied subjects who are busy clinical professionals working in clinics. Our investigation was divided into two studies. The first was a desktop study with 13 participants. It was intended as a reference for image viewing on a desktop monitor. The second is an in situ study, which specifically looked at accuracy in the context of cart-based ultrasonography, a cart-based scanner is the most commonly used device for ultrasound exams. The environment of an ultrasound exam is different from the desktop setting in which gaze tracking is usually performed. First, the amplitude of motion of a sonographer is typically larger than that of someone sitting at a desk in front of a computer, and the variability of head positioning is also larger due to the flexibility of the cart-based ultrasound scanner. Second, the ultrasound exam is performed in the dark, with the monitor as the main source of luminosity. Since the head position and room illumination are two important factors impacting eye tracking quality, it is of interest to estimate how an eye tracker performs in these conditions. Note that we purposefully did not constrain the movement of the participants, because the objective was to evaluate the temporal evolution of gaze tracking performance in a real use situation rather than the performance under optimal conditions.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2866274", "references": [67472587, 1980711281, 1997741316, 2032339067, 2043033468, 2045480547, 2048474912, 2064909625, 2071555787, 2080541453, 2088032731, 2089820506, 2097493241, 2100379672, 2116576293, 2204666881, 2462177648, 2465528855, 2488748484, 2506096269, 2520271149, 2572045081, 2731219627, 2743391672, 2750736663, 2807476029]}
{"paragraph": "The study focuses on Apriori algorithm based rule generation from tables with uncertainty and its actual application system. This field of research is closely related to rough sets, granular computing, data mining, and information incompleteness. Each field of research is related to others and we consider a review of the previous literature based on six classes, ranging from I to VI, as shown in Table 1. Our study belongs to Class VI. In the Introduction, we first review existing studies based on Table 1. We then describe the purpose of the study. In Table 1, the vertical heading is considered as information incompleteness and we consider exact data and inexact data. The horizontal heading is related to the purpose of the research. We consider information retrieval, model and approach for rules, and rule generation system. First, we review Class I information retrieval with exact data in Table 1. Marek and Pawlak clarified the mathematical framework of information retrieval. The study appears to correspond to the origin of rough sets and table data analysis. The definability of a set which is the basic concept of rough sets was employed in the mathematical framework. Codd also proposed relational algebra for table data management, and SQL systems were developed. We subsequently focus on Class II information retrieval with inexact data. Lipski employed non-deterministic information to handle information incompleteness and investigated a question-answering system based on possible world semantics. A query is transformed into a normal form for the evaluation by possible worlds. Lipski proved that the set of axioms for the transformation corresponds to the system S4 in modal logic, and the set also corresponds to sound a transformed query becomes the normal form and complete any query is transformed into the normal form. The property theoretically ensures the validity of the system. For example, the SLD-resolution algorithm in logic programs and deductive databases is sound and complete for logical consequences. We agree with the property, and we introduce the property into rule generation, i.e., an algorithm generates a rule τ if and only if τ is defined as a rule. A rule generation system with soundness and completeness is rare. With respect to Prolog, Zadeh’s fuzzy theory was also applied to fuzzy databases and fuzzy Prolog, and Sakai proposed a framework of logic programs with non-deterministic values. Furthermore, Orłowska and Pawlak proposed many valued information systems and nondeterministic information systems to handle information incompleteness. We follow the systems and address their systems as Non-deterministic Information Systems. We correspondingly term a table without information incompleteness as Deterministic Information Systems. We focus on Class III model and approach for rules in exact data as listed in Table 1. In the 1980s, the research trend appeared to shift from information retrieval to data mining and rule generation. Pawlak proposed rough set theory that affords a mathematical framework of table data analysis. In rough set theory, lower and upper approximations of a target set X of objects in a table are generated via equivalence classes, and rules are obtained as a side effect. Several other related models and approaches to consider rules are investigated. Skowron et al. proposed the discernibility matrix and discernibility function and proved that the problems of generating minimal relative reducts and of generating minimal dependencies are NP-hard. Greco and Słowiński proposed a framework of dominance-based rough sets and handled rough sets for tables in which the attribute values are ordered. Ziarko extended rough set models to variable precision rough set models. Komorowski et al. surveyed the framework of rough sets. Tsumoto applied rough set-based rule generation to medical data analysis. Yao extended rough sets to three-way decisions with probabilistic rough sets, and investigated rough set models in multi-granulation spaces. Ciucci investigated lower and upper approximations for tables with rational numbers. Leung et al. extended rough sets in tables to those in interval-valued information systems. Qian et al. considered rules with a disjunctive decision part and termed the framework as multi-granulation rough sets. Zhu proposed topological approaches to covering rough sets. Information incompleteness is an extremely attractive issue, and thus it is natural that models and approaches for rules in exact data are extended to those in inexact data. We move to Class IV model and approach for rules in inexact data as shown in Table 1. Several other important models and approaches for rules in inexact data are investigated. Kryszkiewicz characterised rules in incomplete information systems where incomplete attribute values are introduced into Deterministic Information Systems. Extended similarity relations obtained via the value are applied to calculate the lower and upper approximations of a set X. Nakata et al. followed Lipski’s incomplete information and proposed rule generation based on possible world semantics. Stefanowski et al. considered the relationship between incomplete information tables and rough classification. Wu et al. investigated incomplete fuzzy information systems via a rough set approach. Yang et al. examined the relationship between the dominance-based rough set approach and the incomplete interval-valued information system. In the studies in Class III and Class IV, the main problem involves the characterization of rules via lower and upper approximations. Most studies focus on models and approaches. In order to discriminate the research on implementations from that on models and approaches, we consider Class V rule generation system in exact data and Class VI rule generation system in inexact data. Predki et al. developed a Rough Set Data Explorer for decision support, and Bazan et al. created a Rough Set Exploration System, which is applicable to data exploration, classification support, and knowledge discovery. Grzymała-Busse realised Learning from Examples based on Rough Sets. In this system, a set X is covered by sets termed as blocks, and rules are generated as a side effect. Ślęzak et al. considered the property of the distribution of attribute values and proposed the concept of packing in SQL. The technology is termed as infobright. In order to handle medical data sets, Tsumoto generated the PRIMEROSE system. Recently, Riza et al. developed a rough set-based package in R that employs rough set theory and fuzzy rough set theory. Finally, we consider studies in Class VI. Although information incompleteness is extremely attractive, there is a paucity of studies on Class VI. Grzymała-Busse introduced missing values into table data and extended the LERS system. It employs a few assumptions for the definition of blocks that assume a role similar to that of equivalence classes in rough sets. After defining blocks, a covering algorithm is applied to a set X of objects. We also dealt with studies in Class VI. We follow the framework of Lipski’s incomplete information databases and Orłowska’s Nondeterministic Information Systems and propose a NIS-Apriori-based rule generation. The framework is related to rough sets in applying equivalence classes although the definition of lower and upper approximations is slightly different. Therefore, certain rules and possible rules in our framework are not investigated in Class IV. We develop the NIS-Apriori algorithm that corresponds to an adjusted Apriori algorithm for the NIS case. The Apriori algorithm was proposed to obtain association rules from transaction data, and it is currently a representative algorithm for data mining. We briefly reviewed related studies based on Class I through Class VI in Table 1. We now describe the purpose of the study. Our study belongs to Class VI, and the purpose involves realizing systems that handle tables with inexact data. More specifically, we describe the following. We clarify the difference between Rough Set-based Rule Generation and Apriori-based Rule Generation. Although they both handle rules from tables, the characteristics of the obtainable rules are different. We reconsider the theory of NIS-Apriori-based rule generation with respect to the aspect of aforementioned point. Thus, we solve a computational problem. Without the solution, it is difficult to address the rules from NISs. We present a prototype system in SQL that simulates the NIS-Apriori algorithm. We propose a few unsolved new research topics related to NIS-Apriori-based rule generation. As a topic, a plausible method to estimate the actual Deterministic Information System from Nondeterministic Information System is considered. By presenting all these points, we demonstrate that NIS-Apriori-based rule generation is a significantly new framework and that it extends the research area of three-way decisions, rough sets, and granular computing. The study is organised as follows. Rough Set-based and Apriori-based rule generation in Deterministic Information Systems are reviewed in Section 2. The rule generation in Deterministic Information Systems are extended to Nondeterministic Information Systems and the difference between the two approaches is clarified in Section 3. This section also addresses the computational problem. In Section 4, the Apriori algorithm in Deterministic Information Systems is adjusted to that in Nondeterministic Information Systems. This modified algorithm is termed as the NIS-Apriori algorithm. In Section 5, a prototype system in SQL powered by the NIS-Apriori algorithm is presented. In Section 6, unsolved new topics related to NIS-Apriori-based rule generation are introduced. In Section 7, a software tool to estimate the actual Deterministic Information System from Nondeterministic Information System is presented. Finally, in Section 8 the conclusions of the study are presented.", "section": "Methodology", "doi": "10.1016/j.ins.2018.09.008", "references": [15698242, 84883688, 184146824, 198358702, 200823476, 1479674096, 1506285740, 1557533423, 1605948227, 1843766148, 1872996274, 1966699200, 1967807191, 1979029381, 1992107931, 1994974007, 1997362234, 2003017562, 2009184503, 2010855867, 2049234232, 2070813883, 2078505173, 2082266564, 2086785573, 2087740657, 2098034130, 2102720558, 2103514965, 2110582769, 2151101158, 2153676086, 2165467455, 2170755382, 2293888039, 2340020088, 2524598441, 2643203814]}
{"paragraph": "Trices are structures with three binary operations that are a generalization of lattices in triangular situations. Trices were firstly defined and developed in subsequent works. In this paper, we investigate their role as the membership values structure for fuzzy sets. Historically, lattice-valued fuzzy sets following the usage of the unit interval were introduced by Goguen. Then there were generalizations, mostly by using lattices with additional operations as a co-domain, the most important is a residuated lattice. Later, there were other generalizations, like semilattice and bi-semilattice valued fuzzy sets. Trice-valued fuzzy sets were firstly introduced as an attempt to generalize intuitionistic fuzzy sets. An additional reason for introducing trice-valued fuzzy sets, connected to applications, appears in dealing with complex orders or movements of objects in the space. Namely, if an object in the two or three dimensions is influenced by several forces, then its movements could not be modeled by a partial order, hence neither by a lattice. For such situations, multi-semilattices are suitable algebraic structures. Moreover, even residuated lattices, which are known as the main membership values structure for fuzzy sets, are a special case of multi-adjoint lattices, in which the underlying poset has a complete lattice structure. A particular case of these structures, suitable for applications in two or three-dimensional spaces are trices, mentioned above. Still, the main motivation for our research is to provide a suitable mathematical model for the theory of three-way decisions. This approach to decision making has been developed recently in the rough set theory by Yao. The main reason was to deal scientifically, using rough sets, with problem solving and information processing. The focus was on dividing a whole into three main parts and then developing strategies to act on each of these parts. A framework for investigating three-way decisions may further be a concept analysis, granular or cognitive computing. For more approaches to three-way decisions mostly in the framework of rough sets, Liang et al. provided relevant studies. Investigating three-way decisions in an axiomatic framework, Hu introduced decision measurement, decision condition and decision evaluation function and established three-way decision spaces. He also investigated several approaches to three-way decisions based on fuzzy sets, interval-valued fuzzy sets, interval sets, shadowed sets, rough sets, and introduced three-way decision spaces based on posets. A particular semi-three-way spaces approach was presented also by Hu. In the mentioned papers there is an extensive literature concerning several aspects of three-way and other multi-criteria decisions. Within the fuzzy set theory multicriteria decision making has also been investigated, see the extensive presentation by Abdullah and others including Hong and Choi. Finally, a contribution to three-way decisions are shadowed sets, introduced by Pedrycz and applied to decision procedures. Thresholds appearing in applications of shadowed sets have recently been investigated by Zhang and Yao. In one hand, this paper should establish a theoretical background for trice-valued fuzzy sets, and in the other it should provide arguments for applications in multi criteria decisions, in particular in three-way decisions, using also ideas of shadowed sets. Concerning the theoretical aspect, we investigate trice-valued fuzzy sets from the point of view of cut sets. These fuzzy sets possess three families of cuts, connected by particular equivalence relations. We determine set-theoretic properties of cuts, proving that they form a centralized and also a semi-closure system. At the end of this part, we present theorems of synthesis of trice-valued fuzzy sets by families of subsets which should be their cuts. In Section 5, we explain an idea of using trice-valued fuzzy sets in three-way decisions, providing also an appropriate example. Our approach to three-way decisions has two levels. The first level is based on the fact that in many decisions there are three criteria A, B, and C, where C is some middle option between A or B, or it is the only remaining solution etc., not necessarily excluding each other. The items attributes, properties, characteristics upon which decisions are made are evaluated under an order for example from good to bad with respect to each of these three criteria. Thus we get three not necessarily linear, neither generally independent orderings, and an ordering structure dealing with such a situation is a trice, presented in this paper. In this way constructed trice T is the membership values structure for a fuzzy set on the collection U of individuals or objects to which the decisions are related. In the second level we use the known procedure of three-way decisions by fuzzy sets, applied on each of the three orders in a trice. Then we combine this approach with the techniques of shadowed sets.", "section": "Introduction", "doi": "10.1016/j.ins.2018.09.007", "references": [95749022, 101345952, 1511669739, 1660981353, 1883715000, 1969921454, 1979092190, 1998965536, 2048472139, 2064232703, 2114832876, 2140146517, 2297889545, 2475810011, 2519715111, 2560804083, 2588487610, 2620114837, 2884615103]}
{"paragraph": "Sensor technology has experienced important advances lately, allowing us to measure various aspects of the objects on the surface of Earth. Remotely sensed hyperspectral images provide wealthy spectral information to uniquely discriminate various materials of interest, leading to finer classification of land-cover classes. However, in certain circumstances, it may be necessary to resort to different source to complement the information provided solely by hyperspectral instruments for further improving and or refining classification. For this purpose, a series of approaches have been investigated in the literature for fusion of data collected from different sources. Light detection and ranging data, which provide elevation information about the surveyed area, are very useful source for complementing the information provided solely by hyperspectral images. Collaborative classification of hyperspectral images and LiDAR has been extensively employed in various applications, such as complex area classification, forest fire management, etc., due to its fine behavior. Numerous studies have indicated that classification performance can be improved after integrating hyperspectral and LiDAR data. For example, LiDAR was used for the scene segmentation and hyperspectral data for classifying the segmented regions; morphological extinction-profiles were exploited to extract both hyperspectral and LiDAR features; and extinction profiles were utilized for joint feature extraction, followed by total variation component analysis for further fusion. On the other hand, it was emphasized that simple concatenation or stacking of features such as morphological attribute profiles may contain redundant information, and despite the simplicity of such feature fusion methods, the fusion systems may not perform better or even worse than using a single type of features. This is due to the fact that the element values of different features may be significantly unbalanced, and the information contained by different features is not equally represented or measured. It was further pointed out that the dimensional increasement of the stacked features and the limited number of labeled samples may cause the issue of the curse of dimensionality. Thus, a decision-fusion method for hyperspectral and LiDAR data classification was presented; besides, linear and nonlinear features were also combined through a decision fusion strategy. Although these decision fusion-based researches have shown excellent performance on classification task, they cannot be a feasible solution to deal with limited training samples, merely avoiding the features extraction process with more demand for training samples. How to extract joint features containing complete information of hyperspectral and LiDAR data, without suffering from Hughes effect, still faces great challenges. Traditionally, human-engineered features depending on the experts’ experience and parameter setting have been the main workhorse for classification tasks; nevertheless, it was further pointed out that it is difficult to find appropriate parameters to generate features for different classification tasks. Recently, deep learning-based methods have broadly replaced hand-engineered approaches in many domains, and have aroused wide attention for their capabilities of automatically extracting robust and high-level features, which are known to be generally invariant to most local changes of the input, at deeper layers. Saliency features in remote sensing scenes were explored to build networks for scene classification. The general way for constructing deep networks of remotely sensing images has been systematically analyzed, which attempted to evaluate the effectiveness of all the state-of-the-art deep learning methods on remote sensing images. For the sake of extracting high-level features in hyperspectral images, a deep learning architecture with multilayer stacked auto-encoder was constructed through an unsupervised manner. The convolutional neural network, which needs fewer parameters than fully connected networks with the same number of hidden units, has drawn increasing attention in image analysis. For example, CNN was exploited to extract features for hyperspectral image classification and obtained excellent performance. In addition, a set of improved methods based on CNN were used for remote sensing classification tasks and yielded excellent performance. CNN-based methods can yield promising results only when a sufficient supply of labeled training samples is ensured; unfortunately, only a small number of labeled samples is available for training in practical situations, especially for remote sensing data. In other words, the supervised CNN generally suffers from either limited training samples or imbalanced data sets. Meanwhile, deep models are also trained to seek feature representations by means of unsupervised learning methods. Unsupervised feature learning, which has a quick access to arbitrary amounts of unlabeled data, has become the focus of concern in both academia and industry. In general, the chief aim of unsupervised feature learning is to extract useful features from unlabeled data, detect and eliminate input redundancies, and preserve only essential aspects of the data in robust and discriminative representations. A pioneer work moving from the supervised CNN to unsupervised CNN for learning spectral–spatial features was proposed, which was based on sparse learning to estimate the network weights. However, this model was trained in a greedy layer-wise fashion, i.e., not an end-to-end network. The image-to-image translation networks, belonging to an end-to-end network, mapped an image from one domain to another domain for learning the translation function. Moreover, feature representation with sufficient information can be effectively explored in an automatic learning process of the end-to-end network, auto-encoder, by minimizing the reconstruction error between an input sample and its reconstruction. In this paper, an intuitive yet effective procedure for the joint feature extraction of hyperspectral and LiDAR data via a patch-to-patch CNN is investigated. The proposed framework refers to an unsupervised feature extraction framework via a patch-to-patch CNN to learn joint features between hyperspectral and LiDAR data without labeled data considerations. Patch-to-patch CNN is based on the so-called encoder–decoder translation architecture. Specifically, the input source hyperspectral data is first mapped into a hidden subspace via an encoding path encoder, and then reversed to reconstruct the output source LiDAR data by a decoding path decoder. Meanwhile, the hidden representation within the translation procedure can be deemed as fused features of hyperspectral and LiDAR data. After that, features derived from different hidden layers of patch-to-patch CNN are integrated by the hierarchical fusion module, afterward converted into a stacked vector and fed into three fully connected layers to produce the final classification map. A multiscale patch-to-patch CNN is proposed for feature extraction, which consists of three tunnels covering a convolutional filter bank with well-designed structure. Learning such an end-to-end network for joint feature extraction of hyperspectral and LiDAR data has not been studied yet to the best of our knowledge. Different hidden nodes in the network are capable of precisely capturing attributes in different information levels, which ensures convenient access to multiscale joint features. The hierarchical fusion module can optimally exploit joint features extracted by the patch-to-patch CNN for classification. Through the module, initial spatial–spectral features obtained from the multiscale filter bank are then combined together to form a joint spatial–spectral feature map. The feature map, representing rich spectral and spatial properties of hyperspectral and LiDAR data, is then fed into a convolution-based block for multilayer block concatenation operation. The neural weights learning process of the designed patch-to-patch CNN is completely unsupervised, which is independent of labeled samples. In the proposed patch-to-patch CNN, training patches are first collected by adopting a sliding window from the hyperspectral and LiDAR data, and the learning process based on these patches can ensure the perfectibility of features even with small-size labeled samples. The remainder of this paper is organized as follows. Some related works are introduced in Section II. The proposed methodology is described in Section III. The experimental results are discussed in Section IV. The conclusion is summarized in Section V.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2864670", "references": [1497089125, 1836465849, 1901129140, 1903029394, 2011085793, 2029316659, 2029992428, 2067874135, 2086866337, 2097117768, 2127152713, 2147800946, 2165796970, 2179290474, 2320738207, 2320846209, 2518815253, 2548791488, 2565258258, 2572303978, 2592141703, 2598997103, 2606929568, 2611655888, 2614256707, 2719511702, 2757208835, 2764205729, 2765739551, 2768537477, 2791928749, 2963446712]}
{"paragraph": "Hierarchical classification is to classify a given data belonging to classes that are organized into a class hierarchy. The class hierarchy is usually represented by a tree or directed acyclic graph. In machine learning, the need for hierarchical classification has been raised and attempts have been made to solve. Hierarchical classification has been applied to several domains, including text classification, functional genomics, and image annotation. In the existing studies, a variant of support vector machine, which is one of the well-known algorithms in classification, or a method of classification based on a decision tree, is widely used. Neural networks have also been used for hierarchical classification. Classification is one of the most important problems in machine learning. Especially in human and agent interaction, classification is needed when the agent recognizes the surroundings and chooses a reaction in a certain situation. The interaction takes place in real time and the surrounding situation changes frequently. The agent should be able to learn new knowledge incrementally and reflect it in response to the changing environment. When a new data that belongs to a new class is given, the existing classification methods should be retrained for all data including the new data. It means the new data cannot be classified in real time. Therefore, incremental class learning is required to be able to classify the data to a new class without retraining for all the data. Although several methods have been proposed to enable incremental class learning in flat classification that classifies input data into classes without considering the class hierarchy, they have not been attempted in hierarchical classification. In hierarchical classification, the existing methods should have a predefined class hierarchy, or even if the hierarchy can be built incrementally, the classes that make up the hierarchy should be predetermined. That is, the given data is classified into predefined hierarchical classes under the assumption that the class information is known in advance. If data corresponding to new classes is additionally given, the methods should be retrained for all the existing data and the new data. In this context, a hierarchical classification method capable of incremental class learning is needed. In order to enable incremental class learning in multiclass classification, some existing algorithms form several unit classification modules that make up an ensemble. In contrast, other algorithms, including fuzzy adaptive resonance theory-supervised predictive mapping and class incremental extreme learning machine, are variants of the existing classification algorithms that add the functionality to a single module. Fuzzy ARTMAP is a classification network that is based on a clustering network, fuzzy adaptive resonance theory. Fuzzy ARTMAP performs classification through the map field module that matches the clustered category node from a fuzzy ART network for the input vector and the clustered category node from another fuzzy ART network for the label vector. CIELM is an algorithm that enables incremental class learning in extreme learning machine. It uses the least-squares solution and Moore–Penrose inverse to find the weighted connections between input nodes and category nodes online. Since CIELM includes the multiplication of a matrix with a size proportional to the size of the training dataset, a large amount of memory is required for learning a large dataset. In this sense, in this paper, fuzzy ARTMAP is adopted as a base network. Our proposed ARTMAP for hierarchical classification network is composed of hierarchically stacked modules, and each module incorporates two fuzzy ARTMAPs. The first one determines whether additional classification for a higher level class should be performed or not. The other one classifies the corresponding class for a certain level depending on the given input. ARTMAP-HC is capable of incremental class learning so that it can classify data belonging to a new class for each class hierarchy level. As the level in class hierarchy is increased, a new module is added and stacked on the lower level module hierarchically. In this way, regardless of the level of the class hierarchy and the number of classes for each level, ARTMAP-HC is able to incrementally learn data belonging to new classes. We also apply additional processes of normalization and prior labels appending to overcome the limitations that occur when fuzzy ARTMAP networks are hierarchically built for classification. First, a novel online normalization process is adopted in ARTMAP-HC. In fuzzy ARTMAP networks, continuous inputs with attribute values between 0 and 1 could be used. It means that the value range of all data should be assumed to be known beforehand to normalize the data by the maximum value. Our ARTMAP-HC, however, is able to classify a new input data without prior knowledge of the maximum value of the dataset. We expand the dimension of an input vector and then normalize the vector using its maximum absolute value. In the expanded dimension, the scale component is preserved even after normalization. Second, the process of prior labels appending is adopted to reflect the class dependency between levels. When a module is added for the next level, the elements of the prior input and the label of the previous level are concatenated and input to the additional module. The prior label information is included in the appended input, thereby the class consistency can be maintained by selecting the label that is a child node of the previously classified parent node. ARTMAP-HC is compared to the existing hierarchical classification methods, which are not capable of incremental class learning, for three different tasks: text categorization; image annotation; and protein function prediction. Our ARTMAP-HC shows comparable performance and even outperforms other methods. The incremental class learning ability is also validated through experiments by training the network for training data instances sequentially one by one and testing after each training. ARTMAP-HC shows improved performance even when an input data instance belonging to a new class is added for training. After analyzing the performance of the proposed ARTMAP-HC, ARTMAP-HC is applied to a multimedia recommendation system for digital storytelling. The system provides appropriate media during a conversation between a user and an agent that is embedded in a smartphone. By providing media related to the dialogue, the meaning of the dialogue can be conveyed more effectively. Since media can be classified by its type and then its specific genre hierarchically, ARTMAP-HC can be applied. When the keywords of the dialogue and context information of the user are given, ARTMAP-HC can select the related type of media. Then, the selected media is recommended and provided in the smartphone during the dialogue. The multimedia recommendation system verifies the applicability of ARTMAP-HC. The rest of this paper is organized as follows. Section II briefly describes hierarchical classification and fuzzy ARTMAP which is the basic network of the proposed ARTMAP-HC. Section III proposes the ARTMAP-HC. Section IV presents the experimental results and the multimedia recommendation system, respectively, to validate and demonstrate the effectiveness and applicability of ARTMAP-HC. Finally, concluding remarks follow in Section V.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2866869", "references": [190228827, 1563942368, 1620204465, 1768675166, 1968259359, 1980179845, 1991020953, 1999954155, 2012611887, 2026401830, 2038906069, 2061453990, 2087347434, 2101234009, 2119821739, 2121526711, 2123096872, 2150766729, 2155440340, 2156465113, 2157438458, 2158054309, 2166280719, 2171066191, 2495774670, 2520368209, 2556889907, 2677572839, 2735629075, 2743276272]}
{"paragraph": "With the popularity of mobile Internet and mobile devices, images and videos need to be played in different resolutions. In this way, devices in special resolution need to resize the images to meet the requirements. However, the quality of image retargeting results affect the quality of experience. Currently, many image retargeting method have been presented, all of which are not universal for application situations. Therefore, it is necessary to design a universal retargeting method. In this process, how to evaluate the results of image retargeting becomes a critical issue, which will be the main purpose for this paper. Presently, there have been many research achievements in image-quality evaluation. Wang used structural similarity to measure the error visibility for image-quality evaluation. Sheikh and Bovik made use of the relationship between image information and visual quality. A novel feature similarity method designed for image assessment was proposed by Zhang. Later, the image-quality evaluation was gradually developed for video evaluation. Based on these typical algorithms, we can arrive at a primary conclusion. In addition, some video-quality assessment metrics were proposed. Traditional full reference image-quality assessment often makes use of the subtraction between distortion image and reference image. In early research on image retargeting evaluation, many researchers directly applied the traditional image-quality evaluation algorithms on image retargeting quality assessment. However, there is a big difference between them. The most obvious problem is that image retargeting pays attention to the geometrical shape and contextual matching after resolution changes, which is difficult to solve by traditional image-quality evaluation algorithms. Based on these issues, many special evaluation algorithms for image retargeting were designed. Simakov proposed a principled approach based on optimization of the well-defined similarity measure. The problem it considered is retargeting of image or video data into smaller sizes. Liu presented an objective metric simulating the human vision system. Different from traditional objective assessment methods that work in a bottom-up manner, it used a reverse order that organizes image features from global to local viewpoints. Inspired by Wang, Fang proposed an effective but simple image retargeting quality assessment method which can be called IR-SSIM. In the assessment process, the scale-invariant feature transform flow is used to find the pixel correspondence. And an SSIM map is computed to measure the preserved structure information in the retargeted image. Zhang made another breakthrough by developing an aspect ratio similarity metric. In the computing process, local block-quality changes are used. Rubinstein first set up a database for evaluating the retargeting image named RetargetMe. They present the first comprehensive perceptual study and analysis on image retargeting. Ma put forward another database designed as a diverse independent public database with corresponding subjective scores, and they also give an effective evaluation method. Based on perceptual geometric distortion and information loss, Hsu presented a new objective-quality assessment method for image retargeting. At the same time, they also built another database called NRID. Jiang conducted research on image retargeting quality assessment through learning sparse representation. They focused on finding the potentiality of sparse presentation based on distortion-sensitive features. Ma resorted to the pairwise rank learning approach to discriminate the perceptual quality between the retargeted image pairs. Liang considered five different key factors for image retargeting, such as salient regions, influence of artifacts, the global structure of the image, well-established aesthetics rules, and preservation of symmetry. Liu put forward image retargeting quality assessment based on four quality factors and support vector regression. They accounted quality factors into two categories: shape distortions and visual content changes. Although the above algorithms have achieved some good results, they still have a lot of problems. These problems can be summarized in three aspects. In image retargeting quality assessment, most of the evaluation methods use simple regression methods, which do not correspond with the perception simulation in the human vision system. Based on this consideration, the deep learning method will greatly benefit the accuracy of the evaluation algorithm. Although there are some methods considering geometrical shape or contextual matching separately, the relationship between the features based on the two different parts has not been fully studied. If this problem can be solved, the evaluation result will improve. Most of the image retargeting quality assessment methods only inherit traditional image evaluation framework. And how to design a framework designed especially for image retargeting quality assessment is very important. The cross database experiments are ignored in previous research. In the proposed algorithm, the following contributions of this paper can be summarized to improve the performance of the image retargeting quality assessment algorithm. Segmented stacked autoencoder based on image representations is used to simulate the retargeting image perception process in the human vision system. Especially, we propose a deep quality evaluator for image retargeting based on the connections of two modules: image representations and segmented stacked autoencoder. On the one hand, the image representations are used for image information exaction, which can simulate the first image perception step from eyes to brains in the visual pathway. On the other hand, segmented stacked autoencoder makes use of a greedy training method layer by layer to train each layer of the network sequentially. The process corresponds with the retargeting image perception in human brains in the visual pathway. Based on the above consideration, we choose it to finish the final assessment of the retargeting images. The proposed method overcomes overfitting and finds the complementary image features for the whole framework. Presently, the method of deep learning has promoted the ability of the pattern recognition algorithm, but encounters an overfitting problem in the traditional image-quality evaluation. In order to solve the overfitting problem, we introduce regularization as the solution to make the deep model more accurate in the test stage. In addition, we choose the network input by considering two complementary parts: geometrical shape and contextual matching. And we make a deep study in the relationship between the two categories. Experiments show that the geometrical shape and content matching can actually provide a more reliable feature group for deep learning on image retargeting quality assessment. Cross database experiments have been performed in this paper and it can promote the development in practical applications. In previous research on image retargeting quality assessment, cross database experiments are always ignored during the different building principles. In order to improve the practical value of image retargeting evaluation, we put forward the cross database experiments. On RetargetedMe and NRID, we set virtual differential mean opinion score for quality of every retargeted image based on the preferred number in the original paired comparison methodology, which can correspond with CUHK. Through a comprehensive validation, the proposed metric correlates well with subjective observations and it can be used for a general quality evaluator in image retargeting quality assessment. The rest of this paper is organized as follows. Section II will illustrate the background and motivation based on related work. In Section III, the special algorithm framework will be given. In Sections IV and V, the experimental design and experimental results are shown. Finally, the conclusion will be given in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864158", "references": [1566135517, 1574818812, 1581984155, 1755382400, 1963884485, 1967046884, 1974647172, 1977246677, 1985949187, 1994197834, 2018377917, 2022164110, 2046119925, 2070322626, 2080008265, 2084363474, 2085431493, 2085889771, 2089947415, 2090518410, 2102285609, 2102591820, 2102817897, 2109506130, 2110528268, 2115273023, 2121809595, 2133665775, 2135306627, 2136211010, 2139144545, 2141983208, 2155175457, 2161907179, 2161927477, 2162692770, 2166554908, 2167322721, 2168397407, 2235636382, 2241147999, 2340803990, 2347050905, 2463319845, 2463322449, 2511113797, 2527269634, 2539033431, 2543770934, 2606880804]}
{"paragraph": "Multicast is an efficient way to disseminate the same content from a single source node to multiple destination nodes. It has been widely used to conserve the network bandwidth and reduce the load on the source node. Multicast has a lot of successful applications in IPTV networks, enterprise networks, and datacenter networks. To support a multicast session, we usually construct a multicast tree, along which the source node can efficiently deliver data to each destination node. Compared with separately delivering data along a set of independent unicast paths to those destinations, multicast delivers data towards all destinations along a general multicast tree and avoids unnecessary duplicated transmissions. To minimize the total cost of used links in a multicast tree, researchers have designed many approximation mathods for the Steiner minimal tree, an NP-hard problem. Such existing methods, however, are mostly designed for the traditional multicast with only one fixed source node. This type of multicast is also called deterministic multicast. Unfortunately, in many cases, the source node of a multicast is unnecessary to be in a specific location. That is, a multicast may have more than one source node, due to the content replica design. For example, to improve the robustness and efficiency, the content distribution networks and the datacenter networks usually deploy multiple replicas for each file at different locations. If a multicast transfer aims to deliver one file to a set of destinations, those replica nodes of that file can indeed serve as the potential source nodes. Thus, each destination node can be served by any source node as long as certain constraints are satisfied. Such kind of multicast with flexible source nodes is referred to as uncertain multicast. As mentioned in our prior work, the routing structure for an uncertain multicast is usually a forest, consisting of multiple isolated trees. In such a forest, each destination node is served by one source node but it is unnecessary that all source nodes appear in the resultant forest. Thus, the uncertain multicast problem cannot be simply reduced as the deterministic multicast problem. Although efficient methods have been proposed to build a minimal cost forest for any uncertain multicast, the serious impact of the dynamic behaviors on the uncertain multicast still lacks attention. Many multicast applications require the networks to support dynamic multicast sessions, where the membership of the multicast group often changes. For example, in the process of a live broadcast, viewers may join and leave the multicast transmission frequently. Additionally, the set of content nodes which can serve as source nodes is also dynamic. The existing minimal cost forest should be updated when any group member changes in an uncertain multicast. An intrinsic method is to completely rebuild a minimal cost forest after each change so as to achieve the minimal cost. This mechanism, however, causes huge computation overhead, and may cause unaccepted disruptions to the flow transmitting along the existing minimal cost forest. Thus, an efficient and practical method requires the ability to just incrementally update the existing forest to tackle those new joined or leaved nodes in an uncertain multicast. Consider that an uncertain multicast usually faces frequent dynamic behaviors. Moreover, it is necessary to find the balance between minimizing the forest cost and minimizing modifications to the existing forest. In this paper, we study the building and maintaining problem of a minimal cost forest for any uncertain multicast with dynamic behaviors. It is defined as the dynamic minimal cost forest problem, an NP-hard problem. We propose an approximate method, a-MCF method, to build the initial minimal cost forest. To avoid re-compute the entire minimal cost forest for each change in multicast group, we propose d-MCF method to alter minimal cost forest with local modifications. Note that, a number of local modifications may lead to the lost of global optimality. So we also design a rearrangement method, r-MCF method, to monitor the accumulated performance degradation and trigger two types of rearrangement when necessary. The major contributions of this paper are summarized as follows: We define the dynamic minimal cost forest problem, and formulate it as a mixed integer linear programming. The sequence of dynamic behaviors is represented as a request vector, where each element is a modification request to add or delete a single source node or destination node. We propose an approximation method, a-MCF, to completely rebuild a minimal cost forest for each request. It aims at maintaining the minimal total cost and is applicable to the situations where dynamic behaviors is not frequent. We propose d-MCF method to balance the the minimal cost and minimal modifications. Instead of rebuilding the entire minimal cost forest, the d-MCF method responds to each request with local modifications, which significantly improves the responding speed. We propose r-MCF method to monitor the accumulated degradation caused by the local modifications. A rearrangement will be triggered if necessary. That is, to rebuild the entire or partial minimal cost forest with global algorithm. It is a supplement to the local d-MCF method. The reminder of this paper is organized as follows. Section 2 summarizes the related multicast methods. In Section 3, we formulate the problem of dynamic uncertain multicast and present its model. An approximation method is proposed to build minimal cost forest. Section 4 presents a method to modify the existing minimal cost forest for each request. Section 5 discusses the rearrangement method. Section 6 evaluates the performance of proposed methods and Section 7 concludes this paper.", "section": "Related Work", "doi": "10.1007/s11704-018-7429-x", "references": [1914921536, 1987575016, 2016234215, 2039298646, 2069853034, 2087304736, 2095788461, 2103992755, 2104954161, 2105808107, 2119565742, 2121041130, 2138336874, 2149288610, 2152950844, 2179603265, 2399307553]}
{"paragraph": "As is well known, the Markovian jump systems have been widely employed to model many practical engineering systems, such as manufacturing systems, power systems, economic systems, and communication systems. As such, it is not surprising that the control problem for the Markovian jump systems has become a tremendous hot topic. For the purpose of dealing with the uncertainties, a number of control schemes have been reported, such as the H-infinity control schemes, disturbance-observer-based control schemes, and sliding mode control schemes. However, it should be noticed that the mentioned literature restricts its attention on the Markovian jump systems without nonlinearities or with the nonlinearities obeying the Lipschitz condition. To handle this, very recently, several adaptive sliding mode control schemes have been designed for the non-Lipschitz Markovian jump nonlinear systems. Note that in these works, the mismatched or unmatched disturbances have not been considered which is pervasive in engineering practice. To overcome such a limitation, the so-called backstepping approach has been put forward. Despite the progress, in the aforementioned literature, it has been implicitly assumed that the unknown parameters never vary with the switching modes. Clearly, for the Markovian jump nonlinear systems with randomly jumping unknown parameters and Markovian switching unknown functions, which are encountered more often in practice, the control approaches are not capable of achieving desired control performances. Moreover, it should be pointed out that, to the best of the author’s knowledge, the Markovian jump nonlinear systems in strict-feedback form have never been investigated. On the other hand, the actuator failures are also often encountered in a wealth of realistic applications, such as robotic systems, spacecrafts, and hypersonic vehicle systems. It has been well recognized that the actuator failures are one of the main causes which could degrade the control performances. To solve this problem, a lot of effective control methods have been proposed in the past few decades. Several passive fault-tolerant control structures have been developed according to robust control theory. Furthermore, to achieve the adaptive capability for actuator and sensor faults, a number of active fault-tolerant controllers, which possess reconfigurable structures, have been proposed. Note that in the aforementioned results, the randomly changing actuator failures which often exist in practice have been rarely investigated. Most recently, two fault-tolerant schemes have been developed for Markovian jumping actuator faults. However, the parameters and the structures of the controlled systems in these works are never allowed to vary with the Markovian modes. Therefore, the designed controllers in these studies are not capable of achieving the desired performances for the concerned Markovian jump nonlinear systems with randomly switching actuator failures. Furthermore, to obtain the wanted control performances, the unmodeled dynamics has to be adequately handled. As is well known, the unmodeled dynamics, also denoted as the ignored dynamics or dynamic uncertainties, widely exists in engineering practice, which might lead to the instability of the controlled systems. In recent years, a great deal of literature has been reported on control of systems with unmodeled dynamics. However, up to now, the control schemes have not been investigated for the dynamic uncertainties with Markovian switching parameters and randomly jumping structures. In this paper, a novel adaptive fuzzy control scheme is developed for a class of strict-feedback Markovian jump nonlinear systems with randomly varying uncertainties and Markovian jumping actuator failures. The main challenges stem from the immanent randomly jumping characteristics of the parameters and structures in the considered system. It is worth noting that it is difficult to estimate the unknown parameters which keep randomly changing and few results have been reported. Additionally, the mismatched and random features of the uncertainties bring about more challenges in the controller design. In this paper, by estimating the upper bounds of the unknown parameters, the difficulties caused by the Markovian jumping unknown parameters and actuator failures are circumvented. Meanwhile, the unknown nonlinearities are handled by the fuzzy logic systems. An adaptive fuzzy control approach is finally synthesized in the context of the adaptive backstepping technique. Compared with the existing literature, the main contributions of the proposed method are highlighted as follows. The established control method first solves the tracking control problem of the strict feedback Markovian jump nonlinear systems with randomly changing actuator faults and Markovian switching dynamic uncertainties. As far as the authors know, it is also the first attempt to tackle the tracking control problem for the pure Markov jumping strict-feedback systems in the sense that the parameters and structures of the Markovian jump nonlinear systems are permitted to be Markovian switching. By using the proposed method, the transition rate matrix is allowed to be completely unknown. The structure of this paper is provided herein. Section II provides the problem description and several imperative definitions. In Section III, the adaptive fault-tolerant tracking controller is designed. The convergence analysis is also given in this section. The simulation results are shown in Section IV. In this paper, AT represents the transpose of matrix A, and the norm represents the Euclidean norm. The probability space is represented by the sample space, the sigma-algebra of subsets of the sample space, and the probability measure on the sigma-algebra, respectively. The expectation operator represents the mathematical expectation, and the probability operator represents the probability.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2865677", "references": [1564089118, 1842211470, 1967964192, 1971410285, 1984285111, 2010108030, 2024677911, 2034320245, 2054500645, 2066374800, 2069516953, 2083601177, 2132179668, 2133085788, 2217539471, 2341465751, 2343288771, 2344237528, 2411793894, 2462438590, 2512467232, 2560451446, 2603089571, 2619461977]}
{"paragraph": "Face hallucination, which can be seen as a domain-specific super-resolution technology, is a technique to infer a high-resolution face image, along with increasing the detailed face features, from low-resolution face images. It has numerous applications for face recognition, 3-D face modeling, criminal detection, and so on. From the pioneering work, many issues of face hallucination have been increasingly studied. Generally speaking, these methods all try to explore the implicit or explicit transformation between the low-resolution and high-resolution spaces with an additional training set with low-resolution and high-resolution face image pairs. Most methods in the literature fall into two main categories: statistical model-based global face methods and patch prior-based local face methods. Statistical model-based global face methods leverage the face statistical models to model the face image and execute face hallucination globally. They can well maintain the global structure of the human face. However, their results lack the detailed local face features and suffer from ghosting artifacts. Considering that the human face structure is a significant prior, many face hallucination methods try to exploit the prior knowledge present in smaller patches. Among them, position-patch-based methods have gained widespread attention in recent years. The common idea of these methods is to divide the global face into many small patches with a predefined patch size and overlap, and use the training patches with the same position as the input one to construct the input patch. In this paper, our work is mainly concerned with this type of approach. The least square representation method is one of the representative position-patch-based methods. To address the problem that the solution of least square representation is unstable, sparse representation-based models have been developed by incorporating the sparsity regularization. However, sparse representation methods overemphasize sparsity and neglect local similarity among the training samples, which is essential for exploiting the intrinsic nonlinear manifold of the training sample space. A locality-constrained representation model was developed which simultaneously adds the sparsity and locality constraints to the patch representation objective function, obtaining stable and reasonable representation coefficients. In order to alleviate the inconsistency of the low-resolution and high-resolution spaces, some works have been proposed to iteratively obtain the patch representation and perform neighbor embedding or learn the mapping in correlation spaces. Based on locality-constrained representation, recently, the low-rank and self-similarity priors are also introduced to regularize patch representation. Gradient information of the face image was incorporated to further regularize the patch representation. In addition to face hallucination, the locality-constrained representation algorithm has been also used to deal with pose and illumination problems in face hallucination and synthesis. However, aforementioned local patch treatments mainly focus on the small patches and do not take into account the global nature, which has been verified to be beneficial to image description, image denoising, and retrieval tasks. To model the global nature of local patch-based methods, the most direct way to incorporate the contextual information is to extend the patch. The most extreme situation is to treat the entire face as a whole, using a global face-based approach. Another possible solution is to introduce a global reconstruction constraint in the image patch-based method. However, when the training sample size is fixed, it will become much more formidable to reconstruct a large patch or infer the global face image. In other words, because the training sample size should grow exponentially with the size of the image patch, it becomes impractical to present a too large image patch. More recently, to reconstruct the latent high-resolution image locally while thinking globally, deep neural networks, especially convolutional neural networks, have been applied to construct the mapping relationship between the low-resolution images and their high-resolution counterparts and have shown strong learning capability and accurate prediction of high-resolution images. For example, a general image super-resolution method was developed based on a deep neural network. This is the very first attempt to use deep learning tools for image super-resolution reconstruction. Another approach proposes introducing the domain expertise to design a sparse coding-based network. Recently, several approaches are the most competitive approaches for face hallucination. They utilized very deep networks to model the relationship between the low-resolution images and their high-resolution correspondings, and verified that deeper networks can produce better results due to the large receptive field, which means considering more contextual information, that is, very large image regions. To utilize the contextual information without enlarging the patch size, this paper proposes simultaneously considering all patches in a large window centered at the observation patch that is called context-patch and develops a context-patch-based face hallucination framework. It inherits the merits of predicting with small local patches, while having the benefits of working with large patches. Based on thresholding locality-constrained representation, the stability of representation and reconstruction accuracy can be improved. Observing that the reconstruction performance will be improved if there are some similar samples in the training set, we further advance an enhancement scheme via reproducing learning, which puts reconstructed high-resolution samples back to the training set and makes it easier to reconstruct the input image. For a testing patch, on the input low-resolution face image, we first extract the low-resolution context patches from the low-resolution training set. Then, we calculate the distance between the input low-resolution patch and the context patches, and choose the nearest neighbor patches to reconstruct the input low-resolution patch. Lastly, the output high-resolution patch can be predicted by combining the corresponding high-resolution context patches with the representation coefficient obtained in the low-resolution training set. To promote the performance, we add back the hallucinated high-resolution image to the training set, which can simulate the case that the high-resolution version of the input low-resolution face is present in the training set, and repeat the thresholding-representation-prediction steps to iteratively enhance the final hallucination result. In summary, the main contributions of this paper are three-fold. We introduce the concept of context patch to expand the receptive field of the patch representation model. It not only inherits the merits of predicting with small patches but also has the benefits of working with large patches. In addition, we combine the low-level pixel values and high-level position information to represent the image patch, thus further exploiting the contextual information. We develop a novel and robust image patch reconstruction method based on thresholding locality-constrained representation. It is inherited from the locality-constrained representation method but has the advantages of accurate patch representation and low computational complexity. We propose a face hallucination improvement strategy via reproducing learning. The estimated high-resolution face image is iteratively reconstructed with a reproduced training set through adding the hallucinated high-resolution image and its degenerated version to the training set. Experiments demonstrate its superiority for some state-of-the-arts in terms of both objective assessment and visual quality, especially when confronted with misalignment or the small sample size problem. The research reported in this paper is an extension of our preliminary work. We highlight the significant differences between this paper and the earlier one as follows. To exploit much more contextual information of the patch images, we extend the pixel intensity-based representation to the combination of low-level pixel values and high-level position prior, which can be seen as the contextual information. The earlier approach focuses only on controllable conditions. However, this paper extends the application of the thresholding locality-constrained representation and reproducing learning algorithm from controllable conditions to more intricate conditions, including both very limited training sample size and real-world image reconstruction. This paper gives deep analysis on the motivations and advantages of introducing the contextual information, thresholding strategy, and reproducing learning, leading to a better understanding of why and how our method works. The rest of this paper is organized as follows. The notations and formulation of position-patch-based methods are presented first. Then, the details of the face hallucination method are given followed by the improvement strategies of thresholding-based patch representation and iterative estimation. Finally, experimental evaluations and comparisons with some competitive algorithms are reported, and the paper concludes with a summary and future directions.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868891", "references": [1885185971, 1919542679, 1967482855, 1972002222, 1985436611, 1987017523, 1997462613, 1999457380, 2001393004, 2015497428, 2027325144, 2027922120, 2031349574, 2033133094, 2037133587, 2054515210, 2069165391, 2069201803, 2070038402, 2073795739, 2078301312, 2114712988, 2118963448, 2121058967, 2133665775, 2134563198, 2141631520, 2143619097, 2145096794, 2160021903, 2164307510, 2295477204, 2307888739, 2345557152, 2404650843, 2428551659, 2461349148, 2507235960, 2509704168, 2518224564, 2520930090, 2545685597, 2559478892, 2566721764, 2569520361, 2570189907, 2611646129, 2618553632, 2642849260, 2740535357, 2752773224, 2790508633, 2790630725, 2792302760, 2800324071, 2809795042, 2963035013, 2963393566, 2963676087]}
{"paragraph": "Type-2 fuzzy logic, which is a generalization of ordinary type-1 fuzzy logic, has made a significant breakthrough in the area of computational intelligence. It has been shown in several studies that type-2 fuzzy logic systems, especially interval type-2 fuzzy logic systems, have the capability to handle or represent uncertainties in a better way due to their extra degree of freedom provided by the footprint of uncertainty in comparison to their type-1 counterparts. Generally, the footprint of uncertainty parameters have been handled as design parameters and, thus, they are tuned in order to reduce the effect of uncertainties and nonlinearities on system performance. In this context, theoretical studies have been presented on how the size or shape of the footprint of uncertainty affects the interval type-2 fuzzy mappings. Researchers have also focused on defining interpretable relationships between the footprint of uncertainty sizes and the output of the interval type-2 fuzzy logic systems to use the benefits of the linguistic meaning of type-2 fuzzy sets and fuzzy logic systems. As a result of these studies, the deployment of interval type-2 fuzzy logic systems has become popular in real-world applications where uncertainties and complexities are high such as control system design, outlet temperature control, flight control, computing with words, decision making, and games. Games are gaining more attention as they are challenging testbeds for computational and artificial intelligence methods. One of the most pervasive real-world game types is the pursuit-evasion game. The pursuit-evasion games have been handled in many studies since many other robotic problems can be seen as a variant of pursuit-evasion games such as obstacle avoidance, leader-following, and path-planning. In literature, there are different game scenarios developed for pursuit-evasion games depending on the game environment, number of the agents, limitation of the moving capabilities of the agents, and definition of the capture. Various methods such as game theory, dynamic programming, and fuzzy logic have been successfully employed to solve this challenging problem. In this paper, we will present a systematic and interpretable design approach to generate pursuing strategies with type-2 fuzzy logic and their deployment to a real-world pursuit-evasion game. First, we will present a novel type-2 fuzzy strategy planner that is composed of two single input interval type-2 fuzzy logic systems. Then, through detailed theoretical investigations, we will show that it is possible to design a linguistic strategy which defines both the pursuer’s approaching behavior and approaching side to the evader by simply tuning the footprint of uncertainty sizes of the interval type-2 fuzzy sets. We will not only present a theoretical way to tune the footprint of uncertainty sizes such that to generate pursuing strategies but also provide an interpretable relationship between the footprint of uncertainty sizes and the linguistic pursuing strategies. Furthermore, we will present a type-1 fuzzy decision making mechanism to tune the footprint of uncertainty sizes of the type-2 fuzzy strategy planner in an online manner. Thus, the resulting pursuing strategy, named as intelligent pursuing strategy, can be seen as an online-generated type-2 fuzzy logic-based pursuing strategy that is capable of transition between various pursuing strategies. This paper will also present a real-world pursuit-evasion game which will act as a platform to evaluate the developed type-2 fuzzy logic-based pursuing strategies and type-1 fuzzy decision making mechanism. Several experimental results will be presented to first validate that the type-2 fuzzy strategy planner has an interpretable relationship between its footprint of uncertainty sizes and output. Then, the performances of the proposed pursuing strategies will be examined against a human controlled evader to show that the resulting performances are satisfactory in the dynamic pursuit-evasion game. The results will demonstrate that the type-1 fuzzy decision making tuned type-2 fuzzy strategy planner, which generates the intelligent pursuing strategy, has improved the pursuing performance as it has the capability to adjust its pursuing strategy in real-time. Section II will briefly introduce the pursuit-evasion games. In Section III, the type-2 fuzzy strategy planner and the novel type-2 fuzzy logic-based pursuing strategies will be presented. In Section IV, the type-1 fuzzy decision making mechanism will be given. Section V will provide the experimental results and Section VI will present the conclusions and future works. The widely used abbreviations are given in a table for the convenience of the reader.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868405", "references": [40650588, 1961697688, 1964569793, 1970974975, 1978838873, 1989434135, 2063615461, 2092965146, 2093051821, 2121235060, 2125223024, 2125631815, 2145416734, 2150879558, 2223927396, 2308507486, 2410041922, 2413454671, 2466197628, 2521651672, 2550759882, 2551462992, 2553405792, 2559820313, 2588293788, 2748314524, 2759567813, 2765770996, 2771998355]}
{"paragraph": "Leader–follower multiagent systems that coordinate and cooperate over an information exchange network have been increasingly applied in science and engineering. Typical applications of leader–follower systems include distributed coordination in robotic networks, formation and propagation of opinions in social networks, and analysis of biochemical reaction in biological networks. In such applications, agents are classified as either leaders or followers, where leaders are a small subset of the agents tasked to direct the overall network behavior, while the remaining agents, i.e., followers, are influenced by the leaders via the underlying connectivity of the network to perform desired tasks. The success of these applications relies on the capability of driving the network to a desired state by external controls via selected leaders, i.e., network controllability. Consequently, network controllability must be ensured in network design to enable leader–follower control. However, network controllability is deeply coupled with agent dynamics and the underlying network topology whose interactions are still largely unexplored. Based on the interactions among agents, multiagent networks can be classified as either cooperative or competitive networks. Cooperative networks are commonly modeled as unsigned graphs containing only positive edge weights, where positive weights indicate cooperative relationships between agents. Average consensus is a typical example of cooperative networks, where agents positively value information collected from neighboring agents and achieve group consensus via collaboration. If a graph allows to admit negative edge weights, it is called signed graph. Signed graphs are widely used to represent networks with antagonistic interactions. For instance, a positive or negative weight in signed graphs can be used to model friend or adversary relationship in social networks and collaborative or competitive relationship in multiagent systems. Controllability on cooperative networks has been extensively studied in the literature. Leader–follower controllability was considered for the first time, where the network controllability was characterized based on the spectral analysis of the system matrix. Graph theoretic approaches were then explored to provide characterizations of network controllability. For instance, it was established that symmetry with respect to a single leader can potentially lead to uncontrollability. Graphical and topological characterizations of network controllability were investigated. Graph-distance-based lower bounds on the rank of the controllability matrix were developed. Sufficient and necessary conditions for network controllability were developed for tree topologies, grid graphs, and path and cycle graphs. Other than graphical characterizations of network controllability, structural properties of cooperative networks were also exploited from matrix-theoretical perspectives to reveal the connections between network controllability and underlying graphs. Other representative works that investigate network controllability include additional results. Besides characterizing network controllability, various methods, e.g., combinatorial and heuristic selection methods, were developed to select leaders to ensure controllability of given networks. Leader selection in complex networks was investigated, where, in addition to ensure network controllability, control energy was also taken into account when selecting leaders. Leaders were selected with either minimum number or minimum energy cost to ensure the controllability of dynamic networks. Other representative works regarding leader selection for network controllability were presented. However, all of the aforementioned results focus on the characterization of network controllability and leader selection on cooperative networks, without considering networks with potential antagonistic interactions. In addition, due to the existence of negative weights in signed networks, most existing analysis tools and leader selection approaches are no longer applicable to signed networks. Recent emergence of the control and analysis of signed graphs with applications in social networks, brain networks, and complex networks motivates the research on controllability problems, where such networks often need to be driven to desirable states by external inputs via selected control nodes within the network. For instance, the controllability of signed graphs were partially studied via structural balance in several works. As a variant of controllability, herdability over signed and directed graphs was considered, which investigated the reachability of a specific set, rather than the whole state space as in controllability. A submodular optimization-based leader selection approach was developed to ensure leader–follower consensus in signed networks. However, network design in terms of leader group selection to ensure the controllability of the signed network remains largely unattended in the literature. Therefore, this paper is particularly motivated to study the leader group selection to ensure controllability of signed networks. In this paper, leader–follower controllability on signed networks is investigated. Specifically, we consider signed noncooperative networks, which admit both positive and negative edges. The signed network is capable of representing a variety of practical network applications, such as social network, fault tolerant networks, and secure networks, where the network may have both friendly and adversarial interactions. Motivated by the broad range of potential applications, it is of particular interest in this paper to identify a small subset of nodes in the signed network, such that the selected nodes are able to drive the network to a desired behavior, even in the presence of antagonistic interactions. In other words, this paper focuses on leader selection to ensure the controllability of signed networks. In particular, based on the classic controllability notations, graph-inspired topological characterizations of the leader–follower controllability of signed networks are first developed. Such characterizations investigate the interaction between the underlying network topology and agent dynamics and pave the way for leader selection on signed networks. As the signed path and cycle graphs are basic building blocks for a variety of networks, the revealed topological characterizations are then exploited to develop leader selection methods for signed path and cycle graphs, where topological properties are explored to extend existing controllability analysis from unsigned to signed networks. Along with illustrative examples, heuristic algorithms are developed showing how leader selection methods developed for path and cycle graphs can be potentially extended for more general signed networks. The contributions of this paper are multifold. First, controllability ensured leader group selection on signed networks is considered. Despite extensive study of controllability of unsigned networks, relatively few research effort focuses on signed networks. To the best of our knowledge, this paper is one of the first attempts to consider leader group selection on signed networks from graphical perspective. Specifically, we develop leader selection rules for signed path and cycle networks, which provides constructive approaches to select leaders for network controllability. Since most networks can be considered as a combination of path and cycle networks, the developed leader selection rules on path and cycle graphs can be potentially extended to more complex and sophisticated networks. Second, the developed leader group selection approaches are generic, in the sense that they not only hold for signed graphs but also for unsigned graphs, since unsigned graphs are a particular case of signed graphs that only consider positive edges. For instance, the leader selection approaches developed in this paper are consistent with the results developed for unsigned graphs, while the leader selection approaches developed for unsigned graphs are not generally applicable to signed graphs. Third, in contrast to most existing matrix-theoretical approaches to characterize network controllability, graph-inspired understandings of network controllability are realized in this paper. Specifically, we investigate the relationship between the network controllability and the underlying topology and characterize how leader-to-leader and leader-to-follower connections affect the controllability of a signed network with Laplacian dynamics. Such graphical characterizations of network controllability are able to provide more intuitive and effective means in selecting leaders for network controllability. As a result, the leader group selection methods developed for signed path and cycle graphs in this paper can be conveniently employed without requiring any matrix-theoretical analysis.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868470", "references": [1963646571, 1963649370, 1966437309, 1967509640, 1977594883, 2003949298, 2014706340, 2020809274, 2021807663, 2043052736, 2055340995, 2056735670, 2060176024, 2077076780, 2080131446, 2081632029, 2082273344, 2088760612, 2100145912, 2111409074, 2111725629, 2120791740, 2128928412, 2140623975, 2159924869, 2160643434, 2167516879, 2168109446, 2231268124, 2257788506, 2315383458, 2404649140, 2472069939, 2511127296, 2518887685, 2587622940, 2608101173, 2734772652, 2789372517, 2796759283, 2801653436, 2962951410, 2963398368]}
{"paragraph": "Social network analysis is the process of studying social structures using networks and graph theory. It is used to characterize network or graph structures in terms of nodes and ties, edges, or links that connect them. These nodes can be people, actors, groups, organizations, nations, and products of human activity or cognition such as web sites or semantic concepts. Signed graphs are frequently used to represent the relationships between nodes and signed edges can represent more relationships than unsigned edges, such as like or dislike, trust or distrust or for or against. Signed graphs are often used in social network analysis, so partitioning signed graphs is an important task in social network analysis. It can be used to understand group structures and to analyze group behavior with its social structure. Clustering is an important tool for social network analysis. Clustering methods are used to partition nodes in signed graphs into different clusters using various criteria. Clustering on social networks that is measured using empirical observations usually exhibits homophily, whereby people tend to have most interactions with homogeneous individuals. This tendency has observable characteristics, such as race, gender, age, income, education and other demographics. There is a long history of clustering methods for social network analysis, such as the algorithm CONCOR that was used to discover the block model in human relationship networks. Clustering for signed graphs was first defined as partitioning the point set into subsets such that each positive line joins two points in the same subset, and each negative line joins two points from different subsets. A signed graph is balanced in social network analysis if the point set can be exactly divided into two mutually hostile subsets. It is clusterable if the point set can be divided into subsets, where any two subsets are mutually hostile. Using the balance theorem, an algorithm was proposed that takes the amount of imbalance as criterion by using a locally optimal partitioning procedure to partition the structure of signed graphs in as balanced a method as possible. Both balanced and clusterable are termed ‘k-balanced’ in social network analysis. Moreover, the balance theory had been extended to blockmodeling and generalized blockmodeling that were used in clustering signed graphs in social network analysis. Blockmodeling methods transform networks into blockmodels that represent the structure of networks. Various clustering methods for partitioning networks that use blockmodels have been proposed in the literature. As well as structural balance theory, blockmodeling and generalized blockmodeling, there are various ways for clustering signed graphs, such as correlation clustering, community detection and spectral clustering. Correlation clustering, called CC, divides graphs by minimizing the inconsistent amount that is similar to the amount of imbalance or by maximizing the consistent amount that is similar to the amount of balance. In CC methods, KwickCluster is applicable to signed networks. Fusion moves have been used for the CC method, called FusionCC, in which these points are fused with positive edges into new points and the general CC algorithms are run. A so-called FEC method has been proposed for community mining of signed networks. A criterion function similar to previous methods was proposed and then a spectral approach was used to achieve final partitioning. In social network analysis, we expect to get clusters that have as few imbalances as possible. Using CC methods for signed graphs, the term “inconsistence” is used rather than “imbalance.” However, the two terms have the same meaning. The goal of CC is to achieve clusters that have a minimum value of inconsistence. The objective of community detection is to find clusters that have dense positive links within, but which have dense negative links with other clusters. This is an indirect way to achieve balanced clusters. This study uses the terms “balance” and “imbalance” and develops algorithms that produce clustering results as few amount of imbalances as possible. In this paper, we propose a novel approach for exploring cluster structure of signed graphs that deals quickly with large-scale signed graphs. A new clustering algorithm is designed to deal with large-scale signed graphs, named fast clustering for signed graphs. This uses a random walk gap. A new random walk gap mechanism is constructed. The random walk gap with a gap is performed by making random walks on two graphs, one of which is only on positive edges and the other of which is on both positive and negative edges of signed graphs. The proposed random walk gap can extract cluster structure information from the gap between the two constructed random walk graphs. We then propose the fast clustering for signed graphs algorithm. Some experiments of synthetic and real data are used to demonstrate that the proposed fast clustering for signed graphs has good performance according to the criteria of imbalance and modularity. The next section details the proposed fast clustering for signed graphs algorithm.", "section": "Introduction", "doi": "10.1016/j.socnet.2018.08.008", "references": [131619556, 1411692141, 1502572743, 1937315027, 1967343101, 2033590892, 2069782692, 2087407617, 2091858563, 2094013877, 2097216034, 2102664282, 2144434012, 2232472329, 2469279958, 2605255209, 2914959486]}
{"paragraph": "With the rapid development of sensor networks, Web services, and radio frequency identification techniques, uncertain data has become ubiquitous. Uncertain data exist for a variety of reasons, which are mainly divided into the following: uncertainty from original data, summary data, privacy preservation, and automatic or semi-automatic information extraction. In many applications, such as the economic, military, logistics, finance, telecommunications, meteorological, and oceanic fields, uncertain data plays an increasingly critical role. Typical uncertain data applications include sensor networks, RFID applications, Web applications, and location-based service. In the last 30 years, great interest has been devoted to uncertain data management. In fact, uncertain data processing techniques concern two aspects of uncertainty: Attribute-level uncertainty. In an uncertain database, the database has attribute-level uncertainty if one or more attributes is uncertain in following cases: Attribute values are from a set of discrete values, each of which is associated with an existence probability; attribute values are consecutively distributed possible values, and they associate with a probability density function. When performing queries in such databases, each record extracts one possible value from its uncertain attributes or distribution to form an instance table. Many applications such as sensors, electronic labels, and GPS values have attribute-level uncertainty in data records. Record-level uncertainty. In an uncertain database, if records do not have uncertain attributes, but each record in this database exists with some probability, the database contains record-level uncertainty. More complex record-level uncertainty also includes a group of generation rules, each of which contains a group of records that provides the constraint condition of this record group. Usually, there are two types of generation rules: exclusive rules, which require that this group of records cannot appear at the same time, and coexistence rules, which require that this group of records must exist together. We can point out some of the steps that have marked the history of uncertain data management. One pioneering contribution is the proposition of the lineage approach, which represents uncertain data by a combination of classical database relations and propositional formulas. The study on probabilistic databases has been continued since then. In 2007, a probabilistic database was modeled as a large graphical network. In 2008, a remarkable survey on probabilistic databases was carried out with this approach. In probabilistic databases, the tuple-independent model has been widely applied owing to its mathematical simplicity. This model assumes tuples are independent and queries are transformed into Boolean expressions. In probabilistic databases with independent tuples, every result tuple is associated with a Boolean formula lineage. A query evaluation is computable in polynomial time if the Boolean formulas can be factorized into a form in which every variable appears at most once, called read-once functions. Read-once expressions are helpful because they are computable in polynomial time, in contrast to being computationally intractable for arbitrary Boolean functions. Since then, several other cases computable in polynomial time and their extensions have been studied. For probabilistic database systems, there has been extensive work under the relational database scheme, such as several systems that deal well with uncertainty of data in an extended relational model and support SQL-based query language. Moreover, a system developed by a university can handle attribute and tuple uncertainty with arbitrary correlations, as well as discrete and continuous probability density functions. In contrast with relational probabilistic database systems, full-fledged semi-structured probabilistic database systems are still missing. Two XML document query processing systems have been proposed. Another is an RFID data management system. For uncertain data streams, some systems support stream processing for uncertain data using continuous random variables. For semi-structured data and stream data, techniques on uncertain data have made great progress and researchers have achieved a wealth of research results. Some surveys have been proposed on uncertain data management. However, the existing surveys focus on uncertain relational data problems and involve only a few uncertain XML data problems. A survey on probabilistic XML data management was proposed in 2013. With the rapid progress made by research in this area, many novel techniques for uncertain data management have been proposed. We propose this survey to summarize the state-of-art research progress on both relational and semi-structured uncertain data, as well as to provide future research directions in the area of uncertain data management. In this survey, we will cover the area of uncertain databases in a broad sense and give an overall view. The remaining sections of this paper are organized as follows: In Section 2, we present relational uncertain data management issues, i.e., uncertain models, basic data operators, query processing techniques, indexing techniques, and uncertain data mining. In Section 2.1, we summarize relational uncertain data models, such as the possible world semantics and the probabilistic graphical model. Possible world is the most popular system of uncertain querying semantics, which provides the theoretical basis for the state-of-art uncertain database, the probabilistic database. In Section 2.2, we present the basic operators, i.e., selection, aggregation, and join operation, for query processing of uncertain data. In Section 2.3, various types of queries are defined and studied by database researchers, and various indexes are designed according to the problem properties. In Section 3 and Section 4, we describe the uncertainty management issues with semi-structured data, i.e., XML documents and graph data. Section 5 presents uncertain data stream management issues and techniques, and in Section 6, we introduce the quality issues of uncertain data.", "section": "Related Work", "doi": "10.1007/s11704-017-7063-z", "references": [37148511, 199916997, 1486776102, 1491547607, 1496151277, 1499195461, 1505129060, 1510148020, 1512485814, 1515568087, 1545445857, 1559068944, 1561514023, 1568607842, 1570856340, 1574922246, 1594296362, 1603304162, 1707644991, 1716383435, 1772931857, 1794915560, 1943213095, 1962705364, 1963853643, 1968461475, 1969642980, 1973266211, 1977245156, 1977848225, 1979578660, 1981339597, 1986019774, 1990391007, 1991249762, 1991785464, 1992609556, 1996544809, 1996923081, 1997141048, 2002298107, 2005044405, 2005852566, 2006718332, 2008974204, 2011130689, 2013333366, 2013885128, 2022501110, 2024400846, 2025051251, 2034518740, 2037314220, 2038822667, 2041358936, 2041763948, 2042502381, 2043354831, 2045630074, 2045928315, 2050290319, 2050868968, 2054693333, 2057272970, 2063191945, 2064379477, 2064926388, 2068447118, 2075422000, 2076530298, 2078686663, 2083613374, 2084273580, 2084591860, 2085532670, 2085608749, 2086741590, 2093038664, 2093584660, 2095897464, 2097584995, 2097995023, 2099413512, 2099700811, 2101366610, 2101742950, 2102943360, 2103269600, 2103895255, 2106582507, 2109402524, 2110037957, 2110553125, 2110796598, 2112070477, 2112948115, 2114258210, 2115826669, 2115857253, 2115986770, 2116440837, 2118178019, 2118291925, 2118462151, 2119081050, 2120207388, 2120825705, 2121043820, 2121161268, 2121722315, 2123017611, 2124647958, 2125434953, 2125791539, 2126969584, 2127186572, 2127947301, 2129035130, 2133457275, 2133695619, 2133839490, 2134206624, 2137092177, 2137442080, 2137536943, 2138271690, 2138414767, 2140237757, 2140757415, 2141700159, 2142909274, 2144342293, 2144602362, 2146407749, 2150061878, 2153404040, 2153508518, 2153610999, 2154709738, 2157813460, 2158501189, 2159948839, 2162400408, 2165211504, 2166214052, 2166916904, 2166994031, 2167956717, 2170896764, 2170902325, 2170936641, 2171688288, 2171776999, 2171943719, 2172146718, 2294071733, 2338322935, 2951002752, 2952121399, 2953227115]}
{"paragraph": "Recently, the multiagent systems have drawn much attention due to its wide application. A general multiagent system consists of many agents, where each agent can represent different individual with its own dynamics. Despite the different behaviors of each agent, agents can work together efficiently, which yields the consensus problem in numerous issues such as flocking, formation, cooperation, and so on. Moreover, the consensus problem of multiagent systems is one of the fundamental and significant topics. In multiagent systems, the consensus is achieved by a process that multiple agents share information with their own neighbors iteratively and gradually reach an agreement. Existing researches mainly focused on the consensus problem based on integrator, first-order and second-order dynamics, where the centralized and distributed consensus problems were consider in both undirected and directed network topologies. In addition to that, because nonlinear phenomena is everywhere in real-world, many studies were done with the nonlinear dynamics of multiagent systems. Moreover, impulsive control systems had been paid more attention in recent years in that an evolution of a system may suddenly change in real circumstances. To get closed to reality, impulsive control method is introduced because of its high efficiency with high-robustness and low-cost. Impulsive control had been widely applied to synchronization and consensus problems of kinds of complex networks over last decade. Consensus problem of multiagent systems with each agent described by impulsive dynamics in directed communication networks topology was investigated. By impulsive control method, several conditions were built for synchronization and consensus with switching topology. Consensus criteria of second-order multiagent systems were delivered based on so-called pulse-modulated intermittent control with nonlinear velocity dynamics, which can be reduced to the consensus problem under sampled control. Some papers also proposed the impulsive control method applied to discrete-time systems. A framework of discrete-time system under discrete-time impulsive control with time-delays was introduced. The definition of the jumping operator is different from continuous time system. The state impulsively changes at the next step of every impulsive instant. Moreover, recently such method had been applied to synchronization of complex networks and consensus of multiagent systems. Consider the constriction of communication among agents, the information required to be delivered cannot always be real-valued data that demands higher accuracy and more resources than quantized data. In such situation, communication of networks using quantized data seems advanced. The quantized consensus was proposed and asymptotic convergence of the multiagent systems can be achieved with integer-valued states. The consensus was guaranteed by the proposed static uniform quantizers with an infinite number of quantization levels. For more application to real circumstance, the quantized consensus of multiagent systems with nonlinear dynamics based on leader–follower case was also investigated. Furthermore, the impulsive control method was also applied to both static consensus and dynamical consensus problems, where the impulsive interval is the key to achieve quantized consensus. However, the bandwidth of the communication channel is sometimes limited, which may lead to much loss of important information. To avoid this, previous researches considered that the quantized data needs to be encoded to guarantee that no information will be distorted badly or totally lost. The consensus of multiagent system was achieved by a quantized-observer-based encoding–decoding with limited bandwidth communication as well as the case with fixed quantization level. The leaderless and leader–follower consensus of a special nth-order integrator systems with an exponential convergence rate were further discussed. A high efficient event-triggered scheme with limited communication bandwidth was considered for the first-order discrete-time multiagent system in directed digital networks, where the average-consensus can be achieved as same as general protocols. Seeing from previous researches, the quantized consensus problem of multiagent systems with limited bandwidth communication seems promising due to its wide applications. Many cases from different angels such as dynamics including linear and nonlinear cases, control methods including general protocol, event-triggered schemes, and impulsive protocol as well as the cases with unlimited and limited bandwidth communication were investigated, respectively. But to our best knowledge, there is no results on the consensus problem considered from all the above aspects simultaneously for now. So in this paper, we investigate such problem of nonlinear discrete-time dynamics with limited bandwidth communication via impulsive protocol based on encoding–decoding in directed networks topology. Furthermore, different from previous researches, the encoder and decoder only demand the information at impulsive instants. This paper aims at an investigation into the impulsive consensus of nonlinear multiagent systems with limited bandwidth communication by designing suitable quantizers, encoders, decoders, and impulsive protocol. An encoding–decoding scheme for such problem is proposed for dealing with quantized data. Based on nonlinear control system theory and impulsive control theory, several fundamental consensus conditions and criteria are obtained. Furthermore, the exponential convergence rate is also investigated. The organization of this paper is describe as follows. Fundamental preliminaries of graph theory are introduced in Section I. The design of encoders and decoders as well as the error systems and the according equivalent systems via impulsive protocol are presented in Section III. The impulsive consensus of multiagent systems with limited band width communication are analyzed and the sufficient conditions are obtained in Section IV. Furthermore, the exponential convergence rate is also discussed in Section IV. Numerical examples that strongly support the effectiveness of the theoretical results are presented in Section V. The conclusion of this paper is presented in Section VI.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2863108", "references": [1997505876, 2009220211, 2020274357, 2029274891, 2039993570, 2049119205, 2058766213, 2064991733, 2085172627, 2085213458, 2088919874, 2098127225, 2098760195, 2113584844, 2128928412, 2135529569, 2165744313, 2179502718, 2279241005, 2321207147, 2465203691, 2501046130, 2585867354, 2625546361, 2790395807, 2792338060, 2912291331]}
{"paragraph": "Due to the wide applications in engineering, distributed optimization for multiagent system is a hot topic in the optimization community. The objective is to minimize a sum of convex functions, which need to design an algorithm to makes each agent can find the optimal solution by exchanging local information with neighboring agents over the network. Recently, the algorithms of distributed optimization have been developed in discrete-time or continuous-time. In the following, we will briefly review the literature about these algorithms. In the past ten years, discrete-time multiagent optimization and their applications have been widely investigated. A discrete-time subgradient method was applied to multiagent optimization, where each agent minimize its own objective function while exchanging information locally with other agents in a time-varying topology. Based on saddle points of the Lagrangian and penalty functions, a distributed Lagrangian primal–dual subgradient algorithm was designed for solving objective function with a global inequality constraint. By removing a projection step in most discrete-time multiagent algorithm, a class of distributed stochastic gradient algorithms was developed to solve the problem by using only local computation and communication. Moreover, there have been various types of distributed algorithms proposed for solving different kinds of optimization problems. Recently, some researchers have started to focus on the distributed continuous-time optimization. An alternative distributed dynamical method was designed which converges to the optimal solution set of a sum of convex differentiable functions with globally Lipschitz gradients. Using a proportional–integral protocol with agents’ output information, a continuous-time multiagent system was developed for distributed constrained optimization. Nonuniform gradient gains and finite-time convergence in distributed continuous-time optimization were discussed. An extremum-seeking controller was designed for distributed optimization over sensor network. Moreover, some recent works on the application of distributed continuous-time optimization are discussed. A distributed Laplacian-gradient dynamics algorithm was developed that guarantees to find the optimal solution to the distributed economic dispatch with or without capacities constraints. Under any initial conditions, a novel distributed algorithm for optimal resource allocation with feasibility constraints and economic dispatch of power systems was proposed. Based on the theory of switched systems, the scaled consensus problem for switched multiagent systems composed of continuous-time and discrete-time subsystems was investigated. Based on multiple interconnected recurrent neural networks, a collective neurodynamic method for distributed optimization was designed. Considering the existence of time delay in signal processing and signal transmission process, some multiagent time delays consensus problems were investigated. Above these multiagent algorithms in distributed optimization, the continuous-time communications with neighboring agents are necessary. In reality, the frequent communications are not desirable for neighboring agents due to high energy cost. Therefore, how to design the distributed algorithm which can reduce communication cost is very meaningful. For example, some researchers proposed the event-trigger schemes which means that neighbors’ information is only available at discrete sampling time instants. In addition, if communications among agents occur only at some discrete-time instants, impulsive scheme can reduce the cost of communication. Up to now, many results about impulsive stability, synchronization, and consensus have been reported from the point of view of control theory. Using the switched Lyapunov function method, the exponential stability of nonlinear impulsive switched systems was discussed. The stability of nonlinear differential system with state-dependent delayed impulses was investigated. Considering the distributed impulsive controller, leader–follower synchronization in heterogeneous dynamic networks was studied. However, to the best of the authors’ knowledge, few results for distributed optimization algorithm using impulsive schemes have been reported in the literature. In this paper, based on continuous-time algorithms, we provide a framework of impulsive quasi-consensus for distributed constrained optimization. First, adopting the interior point method, the constrained optimization problem can be transformed into an unconstrained problem. Using logarithmic barrier penalty, the approximation error is mathematically analyzed. It shows that the approximation error is smaller when penalty parameter increases. Next, using the continuous-time gradient method and algebraic graph theory, the impulsive average quasi-consensus algorithm is designed, in which each agent can deal with the aforementioned local objective functions, and each agent can only communicate with neighboring agents at the impulsive instants. Under certain conditions, we can prove that the impulsive average quasi-consensus can be achieved. The main advantages of our algorithm are as follows. Compared with the continuous-time algorithm, the impulsive average quasi-consensus algorithms only regulate the state at the fixed-time instants, which can reduce the cost of communication with neighboring agents over the network. Especially, due to the interior point method, our algorithm has simple structure and the least number of state variables rather than extra state variables required in other algorithms. The remainder of this paper is organized as follows. In Section II, the preliminaries on algebraic graph theory and distributed optimization problem are introduced, respectively. The distributed impulsive quasi-consensus algorithm is proposed in Section III. Simulation results on two numerical examples and one sensor network localization problem are presented to verify the effectiveness and the performance of the proposed algorithm in Section IV. Finally, Section V concludes this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2869249", "references": [1923997169, 1968094122, 2022948311, 2027741263, 2040840525, 2063246107, 2092483181, 2120765325, 2130308005, 2151685478, 2164278908, 2164642931, 2191781618, 2215067230, 2273484141, 2341544006, 2342445519, 2342805660, 2375258435, 2406054937, 2409418461, 2421576504, 2464348263, 2517192452, 2521320587, 2526043704, 2530618030, 2539372614, 2553512835, 2584604366, 2593034620, 2611497108, 2612356425, 2744910090, 2761385453, 2766228189, 2767760906, 2791781861, 2796434525, 2806651853, 2963031548]}
{"paragraph": "E-mail becomes a necessary means of communication because of its convenience and high efficiency. But the number of spam is increasing since it can make big profits with a small spending by spreading advertisement or other disgust news to mail users. Some lawbreakers even send computer virus with an e-mail which results in a huge threat of computer. Spam, usually considered as unsolicited bulk e-mail or unsolicited commercial e-mail, has brought many troubles to our normal communication by e-mail. It was indicated that the number of spam was so large that a majority of network bandwidth and mailbox server’s storage are unable to be used in other important applications. The huge amount of spam also brought much interference to users and had very severe influences for people to work effectively. Moreover, the spam always had threats once it carrying malicious codes secretly which would affected the safety of computer and personal information. It can be seen that there are nearly 60% of e-mails are spam in 2014 and another report revealed a more serious statistical result with the spam rate more than 68% in the third quarter of 2014. In a word, spam detection is still a severe challenge. In order to reduce economical losses caused by spam and improve working efficiency, people from different fields had proposed many anti-spam methods in diversiform perspective, including changing the protocol of e-mail sending and simple keywords filtering, address protection, and so on. With the rapid improvement of artificial intelligence, more and more intelligent classification methods are adopted to cope with them, the most popular approach is with supervised learning methods. In addition, with its robustness and flexibility, automatic intelligent detection methods are widely used in spam e-mail filtering. Similar to other classification tasks, intelligent spam detection can be decomposed into three important research steps, commonly called feature selection, feature construction, and classifier design. The purpose of feature selection lies in selecting features which are much more important in the further processed steps and resulting in a lower dimensionality which is useful to save computation resource and improve accuracy of the classification model. Feature construction methods discover the inner relationship among all existing features and transform them into a new set first, then use this set of features to construct sample vectors. Supervised machine learning methods are very useful in pattern recognition and have been proved to be effective in spam detection domain. The prevalent and commonly used approaches and techniques are introduced in Section II. In this paper, an ensemble decision method for spam detection is proposed by utilizing both global features and local features of an e-mail in the process of decision making. Corresponding features are constructed by further exploiting and improving the term space partition-based feature construction approach. Sliding window technique with different length of windows is introduced to extract local features of the e-mail, as well as position correlated information. Global and local classifiers are constructed, respectively, based on corresponding feature vector sets and make ensemble decision with voting techniques. Five different benchmark corpora named PU1, PU2, PU3, PUA, and Enron-Spam are employed in our experiments to evaluate the performance of our novel spam detection method. As in standard classification performance analysis, we adopted accuracy and F1 measure as the main criteria to compare our results with others. The organization of other contents in this paper are as follows. Section II describes details of some widely used measurements of feature selection and some methods for constructing feature vectors. In addition, machine learning algorithms-based spam detection models are also introduced. The term space partition-based feature construction approach is described in Section III. The proposed ensemble decision method is presented in Section IV. Section V shows the results and comparison of experiments. Finally, the conclusion of this paper is presented in Section VI.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868794", "references": [327991062, 1482320265, 1504008138, 1535723536, 1552665700, 1563132617, 1754050374, 1964752234, 1965272602, 1970024574, 1970974276, 1972222614, 1985523146, 1986678144, 1990016010, 1996802155, 2003924941, 2014179354, 2023670764, 2025500887, 2028772504, 2033485922, 2040919914, 2061989622, 2067334282, 2069350935, 2084538762, 2089923329, 2095195675, 2103539910, 2109343507, 2112837588, 2114748216, 2119303463, 2119479037, 2120070835, 2120149881, 2128233170, 2133564696, 2133990480, 2135069544, 2141337627, 2145094598, 2153635508, 2155806188, 2160536005, 2169384781, 2251925957, 2271887370, 2346875348, 2398905593, 2409795150, 2435251607, 2548118777, 2565439473, 2577888896, 2579280245, 2592516822, 2765520544, 2793652052, 2963858765]}
{"paragraph": "A granule or an information granule is a group consisting of objects drawn together by indistinguishability, similarity and proximity of functionality. A granule may be interpreted as one of the numerous small particles forming a larger unit. The process of constructing granules or information granules is said to be granulation or information granulation. This process granulates a universe of discourse into a family of disjoint or overlapping information granules. To manage the construction, interpretation and representation of information granules is an important problem. Granular computing, which is a term coined jointly by Zadeh and Lin, takes a granule or an information granule as the basic unit of calculation and aims to build effective computing models for dealing with large-scale complex datasets and information. This approach is an important tool in data mining and knowledge representation. The purpose of granular computing is to explore an approximation scheme, which allows us to view a phenomenon with different levels of granularity and then effectively solve complex problems. The research issues of granular computing concentrate in three aspects, including information granulation, organization and causation. Zadeh noted that granulation involves decomposition of whole into parts, organization involves integration of parts into whole, and causation involves association of causes and effects. Lin and Yao explained the significance of granular computing, which has attracted the interest of many people. A granular structure is a mathematical structure of the family of information granules from a dataset, where the internal structure of each information granule is visible, and the coactions between information granules are tested by the visible structures. Naturally, the granular structure can be depicted as a vector consisting of information granules. Granular structures have been grouped in human granulation intelligence, and knowledge structures in a knowledge base have been studied. As an emerging field of research, granular computing has rapidly developed and has been applied in areas of data mining, data clustering, machine learning, artificial intelligence, approximate reasoning and knowledge discovery. Currently, the research on granular computing has four approaches, i.e., rough set theory, fuzzy set theory, quotient space theory and concept lattice theory. Rough set theory is a mathematical tool for disposing of the uncertainty of knowledge and has been successfully applied to machine learning, knowledge discovery, intelligent systems, expert systems, inductive reasoning, decision analysis, pattern recognition, mereology, and signal analysis. An information system based on rough sets was introduced by Pawlak. Many applications of rough sets, such as reasoning under uncertainty, feature selection, and classification and rule extraction are related to information systems. For granular computing in an information system, the information structure is a significant research topic. An equivalence relation is a special kind of similarity between objects from a dataset. Given an information system, each attribute subset determines an equivalence relation. The object set of this information system is divided into disjoint classes by this equivalence relation, and these classes are said to be equivalence classes. If two objects belong to the same equivalence class, then we may say that they cannot be distinguished under this equivalence relation. Thus, each equivalence class is seen as an information granule consisting of indistinguishable objects. The family of all these information granules constitutes a vector; this vector is said to be an information structure in the information system induced by this attribute subset. Equally, information structures in an information system are also granular structures in the meaning of granular computing. Information structures in an information system have been introduced and investigated, including their uncertainty measures in fully fuzzy information systems, and their invariant characterizations under homomorphisms based on data compression. Covering-based rough sets are the generalization of classical rough sets to deal with covering information systems. Covering-based rough sets are useful for successful applications in a variety of problems. A covering information system is made up of multiple coverings in the universe. Therefore, we can construct the information granule of each point in the universe from the perspective of a covering family and propose the concept of information structures in a covering information system. The aim of this paper is to investigate information structures in a covering information system under a framework of granular computing and granularity measure of uncertainty for this covering information system. The remainder of this paper is organized as follows. In Section 2, we review some concepts on binary relations, covering information systems and homomorphisms. In Section 3, we present the concept of information structures in a covering information system. In Section 4, we propose dependence and independence between information structures and define information distance between information structures. In Section 5, we explain the properties of information structures. In Sections 6, we present an application for information structures by investigating the granularity measure of uncertainty for covering information systems. In Section 7, we obtain invariant characterizations of covering information systems under homomorphisms. Section 8 summarizes this paper.", "section": "Methodology", "doi": "10.1016/j.ins.2018.09.048", "references": [1530320578, 1966564418, 1973689533, 1996844420, 2002680690, 2006675793, 2020614133, 2031978194, 2036999870, 2041926928, 2052608046, 2052719321, 2071765874, 2082266564, 2085029196, 2092044939, 2103514965, 2114383673, 2128771953, 2134866037, 2137576710, 2138080682, 2142864059, 2143040521, 2153676086, 2174369037, 2179045687, 2214361950, 2340020088, 2414329562, 2550850267, 2557776595, 2572412109, 2775094511, 2851233212, 2912565176, 2952248497, 2963129612]}
{"paragraph": "With the aid of vision sensors, the flexibility and intelligence of mobile robots can be highly enhanced, and tremendous efforts have been given to the vision-based robotic systems, including localization, environment perception, and control. For the visual servoing of mobile robots, accomplishing the trajectory tracking task with a monocular camera is one of the most active topics, and is performed through the teach-by-showing technique. Specifically, a set of images are recorded to express the desired trajectory during the teaching process. Then, the mobile robot is driven to track the desired trajectory with real-time visual feedback. Since the mobile robot is a nonholonomic system and the monocular camera cannot measure the depth information directly, both the nonholonomic constraint and the system uncertainties related to the unknown depth parameters should be carefully considered in the control development. To overcome the nonholonomic constraint, various approaches have been proposed, such as adaptive controller, slide mode controller, and neural network controller. However, these approaches do not introduce the sensor layer into the system modeling, and generally assume that the states of the mobile robot can be exactly obtained. Hence, they cannot be utilized to address the visual trajectory tracking problem which involves system uncertainties. After closing the feedback loop with vision sensors, different methods can be used to construct system errors, mainly including image-based, position-based, and multiple view geometry-based methods. The image-based methods design the error signals in the 2-D image space, while the position-based methods describe them in the Euclidean space. Although the image-based methods are robust to the system disturbance, it is difficult to regulate the orientation of the mobile robot to the desired value only using 2-D image errors. The position-based methods guarantee the error convergence in the Euclidean space. However, the classical position-based strategies require a priori geometric knowledge about the environment. An alternative is to construct the system errors based on the multiple view geometry. Generally, multiply view geometry-based methods can be divided into homography, epipolar geometry, and trifocal tensor. All of these methods describe the intrinsic geometric relationship among multiple images via transformation matrices. The corresponding feature points in different views can be extracted to calculate the transformation matrices. Then, the orientation and scaled translation of the mobile robot with respect to a reference coordinate frame can be obtained based on matrix decomposition. A homogrphy-based controller is designed to accomplish the trajectory tracking task with the assumption that the desired trajectory is persistently exciting. To relax the requirement on the desired trajectory and extend the work space, a visual trajectory tracking approach is proposed by combining the trifocal tensor with the key frame strategy. Moreover, the controller can compensate for both the unknown depth and the extrinsic parameters. Instead of defining the error signals with the scaled translation component, some works utilize the reconstructed orientation and the 2-D image information to design composite system errors, and thus are called 2.5-D visual servoing. A model-free unified tracking and regulation strategy of the mobile robot is developed, which integrates the 2.5-D visual information into the controller design. For the visual trajectory tracking, the existing works based on multiple view geometry require that the pose and velocity information related to the desired trajectory is available for ease of the controller design. After capturing a set of images to describe the desired trajectory during the teaching process, two offline steps are usually carried out to obtain the corresponding desired information. First, classical estimation or decomposition algorithms are used to extract the desired pose information from the prerecorded images. Then, the velocity information of the desired trajectory, i.e., the differentiation of the desired trajectory, is computed based on the obtained desired pose and numerical algorithms. Since image noises exist and the sampling rate of a vision sensor is slow in general, numerical algorithms may lead to big errors in the desired velocities, and thus affect the control performance. To deal with this problem, more robust numerical algorithms can be exploited, but that would lead to heavier offline calculation burden. Therefore, to enhance the practical applicability of a visual trajectory tracking approach, it is necessary to develop a controller without requiring the desired velocity information. Moreover, it is well known that the lack of depth information is an inherent issue when a monocular camera is applied for the visual servoing. To compensate for the unknown depth parameters, adaptive control methods are widely utilized. However, the convergence of the depth estimates to their actual values are not guaranteed by these methods. This implies that the Euclidean space cannot be reconstructed completely, which limits the further application of the vision-based robotic system. The unified tracking and regulation control of the mobile robot and the depth identification can be simultaneously addressed provided that a persistent excitation condition is satisfied. To relax the persistent excitation condition, concurrent learning techniques are exploited to estimate the unknown depth in the visual servo control procedure under a relaxed finite excitation condition. One of the core ideas of concurrent learning techniques is to construct the terms with the state-derivatives. Nevertheless, the state-derivatives are usually estimated by using numerical differentiation techniques, which could reduce the parameter estimation accuracy. Hence, achieving the depth convergence under relaxed persistent excitation condition without numerical differentiation is motivated. In this paper, a visual servoing approach is proposed to accomplish the trajectory tracking and the depth estimation tasks of the mobile robot. Specifically, by utilizing multiple view geometry techniques, the scaled translation and orientation information of the mobile robot is obtained to construct the system errors. Considering the nonholonomic constraint and the unknown depth parameters, an adaptive time-varying controller is developed to address the trajectory tracking problem. To facilitate the control development, the desired velocity information is estimated by a reduced order observer instead of being computed offline. Moreover, an augmented update law is designed to not only compensate for the unknown depth parameters but also identify the inverse depth constant. Lyapunov-based stability analysis is presented to show that the asymptotic trajectory tracking and desired velocity estimation can be achieved, and the depth convergence is ensured provided that a persistent excitation condition is satisfied. A robust data-driven algorithm is further proposed to estimate the inverse depth under a relaxed finite excitation condition. Finally, the performance of the proposed approach is validated by both simulation and experimental results. Compared to previous works, the main contributions of this paper include the following aspects. The adaptive tracking controller is developed based on the proposal of a reduced order desired velocity observer, which can avoid tedious offline calculation. Most of visual trajectory tracking methods use adaptive update laws to only compensate for the unknown depth information, while the augmented update law designed in this paper can also successfully identify the inverse depth constant under a persistent excitation condition. To relax the persistent excitation condition and reduce the impact of system disturbance, the robust data-driven algorithm is proposed for the depth estimation without relying on numerical differentiation. The remainder of this paper is organized as follows. Section II formulates the visual servoing problem and introduces the system model. The control and estimation scheme is proposed in Section III, and the robust data-driven algorithm for the inverse depth estimation is presented in Section IV. Simulation and experimental results are provided in Sections V and VI, respectively. Finally, the conclusion is given in Section VII.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2869623", "references": [1575596356, 1980765536, 1982795744, 1986107377, 2001865981, 2028044548, 2045975141, 2054797486, 2058843767, 2069612046, 2085261163, 2098087953, 2101714570, 2111073598, 2111358287, 2124037552, 2140985802, 2141429008, 2157577161, 2168199093, 2292922312, 2345110532, 2555485441, 2561721180, 2622287508, 2736053122]}
{"paragraph": "Graphs are among the most powerful and expressive data structures. They have been used to model complex systems of objects and their relationships in many real-world applications including the world wide web, social networks, transportation networks, biological networks and systems. Graph clustering is an important research topic in graph mining and analysis. It consists of partitioning the nodes of an input graph into several groups satisfying some predefined fitness criteria. Typical applications of graph clustering include the identification of functional complexes in protein-protein interaction networks and the detection of communities in social networks. The abundance of data generation in many real-world applications has led to many kinds of information becoming available. This information could be used to annotate graph nodes and edges. The annotation of multiple attributes over the graph elements makes it possible to consider multiple aspects during graph clustering, rendering this approach more robust. Biological networks are among the most important domains of application in which various types of data are available in massive amounts in online databases. Protein–Protein Interaction networks present an example of such biological networks for which huge amounts of heterogeneous data are freely available online, providing a wide range of information about the proteins making up the network nodes including their structures, the genes that encode them and the DNA sequences of these genes. This information may also relate to gene ontology, gene expression levels or genetic mutations. The consideration of all attributes together in a single fitness function results in all the attributes being given equal weights in the clustering which is not appropriate in many real-world applications. Furthermore, some attributes may be outweighed by others with a higher variance, biasing the clustering towards a particular aspect of the data, particularly in cases in which multiple attributes are correlated. This problem can be formalized more appropriately by defining multiple fitness functions separately over different subsets of attributes, such that only those describing the same aspects of the data are considered together in the same function. During the clustering, all the fitness functions are optimized, and the output solution is the one that best fits all the functions simultaneously. However, different fitness functions may conflict, making it impossible to improve one without degrading another. In this scenario, multiple optimal solutions are provided with respect to the set of considered fitness functions. Each solution is undominated and the set of all the solutions form the skyline solutions. Approaches for optimizing graph clustering with respect to predefined fitness functions are usually combinatorial and involve evaluating a very large number of candidate solutions. This task is, thus, computationally very expensive, and may even be infeasible for large graphs. Evolutionary computation methods have proved their efficiency in many applications in which it is hard to achieve an optimal solution. They can estimate near-optimal solutions rapidly even for large data, by restraining the combinatorial search space. An ideal graph clustering should generate clusters with a cohesive intra-cluster structure and nodes with homogeneous properties, by balancing structural and attribute similarities. In this paper, we propose a clustering approach for attributed graphs in which each subset of attributes is derived from a particular data source. Our approach looks for skyline graph clustering solutions based on the dominance relationship over a set of fitness functions. Each of these functions is defined over the graph topology or over a particular set of attributes. Our approach does not require the number of clusters in advance, and each skyline clustering solution is optimized with respect to all the fitness functions simultaneously. This makes the graph clustering more robust. As the skyline operator contains multiple optimal solutions, we defined a ranking procedure to provide an order over the undominated graph clustering solutions. We propose a genetic algorithm-based implementation to facilitate the use of our approach, even for large graphs. At a glance our approach can be summed up as follows: first, disjoint powerset systems are constructed from possible combinations of nodes to form potential candidate graph clusters. A set of chromosomes is then constructed with a personalized initialization algorithm that favors the density and connectivity of the initial candidate clustering solutions. Personalized genetic operators are then used on the first population to generate new optimized chromosomes iteratively. These new chromosomes are then evaluated, using multiple objective functions defined over the graph topology and the node and/or edge attributes. Encoding and decoding functions are also defined to map the contents of the chromosomes to their respective sets in the corresponding powerset system. After repeating this procedure for a predefined number of iterations, a set of approximate undominated graph clustering solutions based on the skyline operator is obtained. We show, through a real-world example of a protein-protein interaction network, that our graph clustering approach is efficient, and we demonstrate how the integration of attributes from multiple data sources makes it possible to obtain a more robust clustering than by considering the graph topology alone. The paper will be organized as follows. Section 2 presents the related works. Sections 3 and 4 define the preliminary concepts, the problem formalization and the proposed approach for evolutionary mining of skyline graph clusters of attributed graphs. In Section 5, we present the case study of the clustering of a large human protein-protein interaction network enriched with large sets of heterogeneous cancer associated attributes. In Section 6, we draw our conclusions.", "section": "Related Work", "doi": "10.1016/j.ins.2018.09.053", "references": [82500914, 1673310716, 1975680103, 1993530334, 2005180870, 2011587419, 2030006812, 2033590892, 2042851051, 2046491030, 2049153008, 2049864887, 2108814382, 2116798163, 2121443461, 2124738517, 2127048411, 2151554678, 2165891030, 2167115106, 2167159964, 2169706457, 2170188482, 2176433868, 2560757461, 2781583846, 2783806899]}
{"paragraph": "Multivalued logic (MVL) technology evolves from binary Boolean logic technology. In traditional Boolean logic systems a variable has only two values, 1 and 0, to represent two specific states, such as right and wrong in logic, on and off in circuits, and high and low in level. While in MVL systems, a variable can have many values to express many different states. For examples, in the Post algebra, it can express three symbols: 1) positive; 2) zero; and 3) negative; and in computer operating systems it can express five kinds of process state: 1) create; 2) ready; 3) execute; 4) block; and 5) exit. The MVL technology has the potential to increase the information capacity of each single line obviously and consequently it has a great advantage in the logic circuit design, reliability analysis, and optimization. With the expansion of circuit integration, the optimization of an MVL network can shrink the size of programmable logic arrays effectively, reduce the number of interconnections among chips in orders of magnitude, thereby improving circuit integration, speeding up hardware, and enhancing the ability of fault tolerance. Recently, the MVL network technology has also been applied in many other ways which are promising and attractive, such as pattern recognition, image processing, associative memories, and data mining. Especially it is also applied to multivalued neurons, which is based on the multivalued threshold logic and can match inputs and outputs arbitrarily by a partially defined multivalued function. It is necessary and important to optimize MVL networks. In order to optimize MVL networks, several kinds of methods, such as an algebraic method, circuit method, hyper-planes method, theorem-proving technique, and modular design approach have been used. Although these early methods can provide some optimized solutions, they are extremely time-consuming, even in orders of days and they can only solve small-size problems. As an alternative, direct cover (DC) can generate an efficient cover directly given an MVL network, omitting the intermediate procedure of creating prime implicants. DC requires much less computational time than those methods based on implicants. MVL networks optimized by DC contain no error just as the ones before optimizing. However, this technique aims at accuracy, regardless the number of optimized gate circuits. In other words, DC is poor in saving the cost of manufacturing and integration of circuits. Subsequently in recent years, DC is introduced into some intelligent algorithms, which can simplify MVL networks. Especially an ant colony optimization (ACO) algorithm combined with DC can improve the performance of MVL networks, which is called ACO-DC. ACO-DC is a state-of-the-art algorithm, which first uses some ants to decompose an MVL network, and then uses DC to mimic each subnetwork. It completes the cover of MVL networks successfully, and reduces the number of gate circuits effectively, yet not enough. Meanwhile a new learning MVL network is put forward to improve the performance of the networks based on algebra, and several kinds of technologies, such as error back-propagation and nonback-propagation, are introduced into the model. It can accumulate a priori knowledge about targets and processes, so as to evolve more effectively. In fact, a learning MVL network using error back-propagation requires some presuppositions: 1) the simulated function curve must be continuous and differentiable at each point and 2) the values of some parameters, e.g., weight and threshold are preassigned. It is obvious that these conditions involve some uncertainies. On the other hand, a learning MVL network based on nonback-propagation such as local search and simulated annealing needs no such prior knowledge, but it has its disadvantages: it is easy to fall into a local minimum and easy to converge prematurely. To jump out of a local minimum, a great deal of research has been carried out. A stochastic dynamic factor is introduced into a traditional local search method, which can permit the search to go a little farther along the error direction in order to escape from a local minimum when the search encounters a trap. A clonal selection algorithm incorporated with chaos is proposed to maintain the diversity of population and accelerate its evolution. The above-mentioned search methods have proved to be better than the traditional local search through a large number of experiments. They can deal with the local minimum issue, run smoothly in the global field, then reach some high-quality solutions in a reasonable time and show their robustness. Nevertheless, they have a common flaw: their measure of the solutions is unique and incomplete. In other words, they are all based on a single objective function which is used to guide their search in the solution landscape. Real-world optimization problems need usually be evaluated with several criteria, but not only one, and these criteria maybe in conflict with each other. Specifically, the optimization of MVL networks includes two objectives: 1) accuracy and 2) optimality. Accuracy is the most important measure of a system and optimality reflects the cost of a system. The methods mentioned above only take accuracy into consideration, but ignore optimality. They always try their best to find the best solutions with the least error, regardless of the cost of their solutions, which is clearly unreasonable. A modified error function combining accuracy and optimality is presented to evaluate the individuals of MVL networks by using a weighted sum. It must be emphasized that these two evaluation criteria used in the evaluation function are totally unequal: the accuracy plays a dominant role, while the optimality starts to work only when the accuracy of an individual is the same or almost the same as that of the other. This algorithm is strongly influenced by the weights that are tough to be determined objectively. Hence, it is a single-objective one in essence. In this paper, we for the first time propose a true multiobjective method to optimize MVL networks and try to find the optimal solutions. First and foremost, two independent objective evaluation functions are applied to the problems. They are similar to the previous definitions, yet we utilize them completely equally in our research, without any subjective parameters. Second, in single-objective problems the best solution appears when the objective function is the best while in multiobjective problems there are complex internal relations among various objectives. It would not be a subjective decision whether a solution is abandoned or not in solving MVL network problems. Sufficient optimal solutions should be collected in order to provide the needed decision support for practical applications. They include two kinds, the elite solutions with no error in accuracy and the Pareto optimal solutions whose accuracy and optimality are compromised with each other. The solution with both best accuracy and best optimality may not be turned into reality for the reason of cost or technical limitations. Thus, the best accurate solutions may have some different optimality for potentially different applications. Furthermore, some products hope to reduce the cost by allowing some noncritical errors. Therefore, Pareto optimal solutions should also have some different ranks. Altogether, two kinds of solutions should be uniformly distributed along the two evaluation functions. In MVL networks, the objective functions have simple phenotypes in which their variable values are discrete and limited, while the decision vectors corresponding to the objective functions have rich genotypes in which a decision vector has a number of variables. Because of these features, a novel algorithm based on traditional differential evolution (DE) is proposed to optimize MVL networks. A novel data structure and characteristic updating method for archive population are built to store elite solutions with different optimality and Pareto optimal solutions with different ranks. The simulation results indicate that our algorithm is significantly superior to the existing algorithms. The contributions of this paper are: Modeling MVL network learning task as a multiple-objective optimization problem and considering error and optimality of solutions simultaneously. Discriminating Pareto optimal solutions with several different ranks to provide fruitful solutions for practical applications. Designing a novel data structure and updating rule for the archive population to maintain the diversity and quality of solutions. Testing a set of MVL instances to evaluate the proposed approach and therefore giving more evidences about its superior performance over other existing algorithms including single-objective methods, several variants of multiple-objective methods, and DC-based methods. The remainder of this paper is organized as follows. In Section II, we give some basic knowledge in order to understand the algorithm to be proposed. In Section III, we illustrate a bi-objective algorithm based on DE and its characteristics in details. In Section IV, a great number of simulations are implemented and the results are analyzed. In Section V, we summarize this paper and put forward a further research plan.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868493", "references": [1544309315, 1561333824, 1563525949, 1584198977, 1595159159, 1956801889, 1975507646, 1984029699, 1991458509, 1994125175, 2002280630, 2025152094, 2026904884, 2026923346, 2026958463, 2034585384, 2051159673, 2054208673, 2056655435, 2061510426, 2084855067, 2092673054, 2092886720, 2095708000, 2105972436, 2106371135, 2112626954, 2120514998, 2126105956, 2129691920, 2137779163, 2141522269, 2148324414, 2150046657, 2151017751, 2154338144, 2156194072, 2156735594, 2156753779, 2158115054, 2163636073, 2193409108, 2342596926, 2576677224, 2611421169, 2751605210, 2764251381, 2805172798]}
{"paragraph": "Compilers are important infrastructure tools in software development, which provide syntax and semantics analysis for programs, as well as code optimization to accelerate software upgrades. For example, the security engineering group at Microsoft utilizes compilers to prioritize code review; the maintenance engineers at Hewlett-Packard improve the quality of code by removing compiler diagnostics in software systems. However, compilers may also contain bugs, and in fact quite many bugs are reported for widely-used compilers such as GCC and LLVM. Buggy compilers make a source program optimized or translated into a wrong executable module, which may behave differently from the expected behavior determined by the semantics of the source program. Once this happens, it can result in disastrous software failures especially in safety-critical domains. For instance, a bug in the compiler of HAL/S had even caused the failure of the NASA Shuttle software. Even worse, developers with little knowledge about compiler bugs customarily debug the software they are developing rather than the compilers they are using, which makes compiler bugs more difficult to be found. Therefore, guaranteeing the quality of compilers is a critical issue. Compiler testing is one of the most important ways to guarantee the quality of compilers. According to the previous studies, there are three issues to be addressed: how to generate adequate test cases to test compilers, how to find the test oracles to determine whether a test case triggers bugs, and how to reduce these test cases. Furthermore, two challenges are to be addressed. First, since the inputs of compilers are complex programs with furcated syntax structures and rigorous content constraints, undefined behaviors of language specification make the first issue and the third issue be a challenge. Second, since compiler testing lacks test oracles to determine whether the outputs of compilers are semantic equivalent with the programs before they are compiled, the test oracle problem makes the second issue be a challenge. During the past decades, a great number of researchers have proposed different approaches for addressing the above issues. Some successful random test case generators have been implemented to facilitate compiler testing, such as Orion, Csmith, Quest, randprog, and JTT. All of them can automatically generate abundant test programs for compilers without undefined behaviors. Simultaneously, various compiler testing techniques have been proposed to mitigate the test oracle problem, such as differential testing, random testing, Equivalence Modulo Inputs, mutation testing, and metamorphic testing. By employing the above testing techniques, a large number of compiler bugs can be detected. In addition, several reducers have been developed to minimize the test cases, such as Berkeley Delta, C-Reduce, and CL-Reduce. Thus, a set of small and valid test cases that trigger the same bugs as original ones can be reported to developers. However, as the number of related papers increases, there are few efforts to systematically identify, analyze, and classify the influential researchers, the state-of-the-art testing technologies, the collaborations among authors, and the co-occurrence of keywords, which results in an obstacle to characterize and understand compiler testing. In this study, we employ a systematic and comprehensive literature analysis framework to overcome the obstacle. First, we perform an extensive search to identify papers related to compiler testing, and extract the most important information from papers for the consequent analysis, such as the title, the keywords, and the author(s). Then, we conduct a bibliometric analysis to identify the most influential authors and papers, as well as the widely-used compiler testing technologies, so as to present an external overview of the compiler testing area. Last, we construct three collaboration networks to analyze the communities of authors and keywords, which can present internal evidence on the influential authors and hot topics in this area. The major contributions of this paper are summarized as follows: We conduct a bibliometric analysis for compiler testing literature. The results show that the USA is the most influential country with a large number of excellent researchers and institutions in the compiler testing area. In addition, various types of compilers are tested, ranging from C++, Java to Pascal, whereas C compilers draw much attention from academia. We combine association rule mining and collaboration analysis to construct three networks, including the co-authorship network, the author co-keyword network, and the keyword co-occurrence network. The results show that most researchers have broad interests in the compiler testing area. These researchers distribute in several scattered communities. The keywords “test case generation”, “automated testing”, and “random testing” frequently co-occur in compiler testing. The paper is structured as follows. Section 2 illustrates the challenges and the corresponding solutions in compiler testing. We demonstrate the components of literature analysis framework in Section 3. Then, Section 4 shows the findings from bibliographic and collaboration analyses. Section 5 provides an overview of related work. Section 6 concludes our paper and discusses the future direction.", "section": "Introduction", "doi": "10.1007/s11704-019-8231-0", "references": [109452506, 1486146156, 1499592573, 1525595230, 1575947743, 1596462940, 1963831852, 1966021031, 1973519892, 1979977091, 1988252511, 1996969457, 1999067077, 2009181019, 2014821466, 2017425457, 2023035194, 2030405312, 2032188103, 2032841931, 2034553498, 2036896594, 2041713059, 2071039852, 2071952624, 2086689864, 2094771270, 2095445208, 2096698236, 2098456636, 2109652142, 2124883940, 2125910575, 2130746431, 2132984320, 2140187381, 2141717815, 2153185479, 2155877593, 2169800777, 2170737051, 2173450663, 2296333508, 2313752879, 2370472429, 2390518826, 2461570336, 2506015293, 2532737545, 2560438921, 2594511738, 2616082100, 2617809069, 2793210894, 2799563612, 2884843021, 2952139678]}
{"paragraph": "Clustering plays an important role in machine learning, data mining, image segmentation, and pattern classification. The goal of clustering is to classify elements into clusters on the basis of their similarity. A large number of clustering methods have been brought forward. Classical methods include hierarchical clustering; K-means clustering; spectral clustering; support vector clustering; multiview clustering; genetic clustering; etc. Among these clustering methods, spectral clustering has become one of the popular methods because of its robustness and effectiveness. Generally, the performance of the spectral clustering is better than other methods. Spectral clustering is able to seek the optimal partitioning of data based on the spectral graph theory. Traditional clustering algorithms such as K-means can only perform clustering with convex distribution. If the sample spaces are nonconvex, K-means would fall into a local optimal solution. Compared with K-means, spectral clustering can perform clustering with nonconvex sphere of sample spaces and obtain the globally optimal solution in a relaxed continuous domain. Although many spectral clustering methods have been proposed, such as Min Cut, Ratio Cut, Normalized Cut, Min–Max Cut, Spectral Embedded Clustering, K-way Rcut, and K-way Ncut, all of these methods adopt a two-stage process. The first stage is to learn the relaxed continuous spectral vectors and the second stage is usually to employ K-means or spectral rotation to post-process the continuous spectral vectors in order to obtain the final binary cluster indicator matrix. In practice, the manner of separately performing the two stages is not able to jointly obtain the optimal solution. In this paper, in order to overcome the aforementioned drawback of spectral clustering, we propose a new spectral clustering framework that jointly performs spectral embedding and spectral rotation. That is, the real-valued cluster indicator matrix usually obtained by conducting spectral embedding in the intermediate stage and the binary cluster indicator matrix usually obtained by conducting spectral rotation in the last stage are iteratively computed in our method. Recently, a unified framework for discrete spectral clustering was proposed. The framework is able to obtain the final clustering results by one step and results in significant improvement of clustering performance. Nevertheless, the objective function of that framework has a term which employs an orthonormal matrix to approximate a nonorthonormal matrix. The approximation cannot be very precise in theory. As will be shown in a toy example, this method tends to generate incorrect clustering results for unbalanced data where the underlying numbers of clusters are far from uniform. By contrast, the proposed method is capable of overcoming the drawback because approximation is conducted in-between two orthonormal matrices. In summary, the novelty, contribution, and characteristic of the proposed method are as follows. A joint model is proposed to simultaneously and iteratively perform spectral embedding and spectral rotation with spectral embedding generating a real-valued cluster indicator matrix and spectral rotation generating a binary cluster indicator matrix. Compared to the classical spectral clustering methods, the proposed joint model is able to overcome the drawbacks of the information loss and the risk of the discrete clustering deviation. In the spectral rotation part of the proposed joint model, approximation is conducted in-between two orthonormal matrices: a matrix generated by spectral embedding followed by a rotation operation and a scaled cluster indicator matrix. Therefore, the proposed method is able to obtain an accurate clustering result. In addition, the proposed method is able to overcome the problem of the unbalance of previous frameworks. The physical meaning of the scaled cluster indicator matrix is interpreted. Moreover, the theoretical derivation of the scaled cluster indicator matrix is given. The insight in the scaled cluster indicator matrix is helpful to understand the proposed method and developing a new method. The proposed method cannot only achieve an accurate clustering result but also be implemented very efficiently. The optimization process of the proposed convergences is in about three iterations. The rest of this paper is organized as follows. In Section II, the related work is discussed. Classical spectral embedding and spectral rotation are described in Section III. The proposed method is presented in Section IV. The experimental results are presented in Section V. Finally, Section VI concludes this paper.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868742", "references": [62192748, 1673310716, 1943383135, 1997011019, 2006793117, 2023512014, 2083620785, 2103560185, 2106115875, 2108502868, 2108995755, 2117553576, 2121947440, 2123921160, 2125531986, 2127615881, 2129210885, 2131828344, 2132603077, 2134737843, 2135674549, 2152322845, 2153233077, 2155074104, 2155754954, 2160642098, 2549545452, 2569580858, 2569859441, 2576553562, 2586218630, 2748075849, 2790034647, 2803629456]}
{"paragraph": "Object tracking is a hot topic in computer vision. Due to its wide applications in motion analysis, video surveillance, human–computer interface, etc., it has attracted much attention recently. However, because of various complex factors, for example, occlusion, deformation, illumination variation, scale variation, fast motion, and background clutter, tracking is still a challenging task. A typical tracking system often has three modules, that is, the appearance model, the motion model, and the update model. The appearance model, which plays a crucial role, can be divided into two types. The first is the generative model, which only uses the information of the target. The second is the discriminative model, which often formulates tracking as a binary classification problem and is known as the tracking-by-detection method. Since the latter model adopts the information of both the background and the target, it seems to offer more accurate and robust representation. However, there are still some potential issues in existing tracking-by-detection methods. First, it is pointed out that the objectives of the tracker and the classifier are not consistent in many methods, where the objective of the tracker is to find the optimal location of the target precisely while the objective of the classifier is to predict the labels of the samples accurately. Second, the training samples are often undersampled randomly and assigned to equal weights. This sampling strategy does not make full use of the spatial constraints around the target and ignores the different importance of the samples. Besides the above issues, we also find that the high spatial correlations of adjacent image patches do not get enough attention, which increases the complexity of sampling, training, and detection. Furthermore, the size of the tracker is often fixed and the inner structure of the tracker is often neglected, so that the scale variation is not well addressed. These issues may be important factors affecting the tracking performance. Although some of these issues have been pointed out by some researchers, they have not been taken into account in a unified framework. Recently, many correlation filter-based tracking methods have been proposed and achieved great success. These methods formulate tracking as a ridge regression problem, which can make full use of the spatial information and solve the downsampling issue in the binary classification model. Besides, the correlation filter-based methods transfer the ridge regression to a correlation filter problem by the circulant structure assumption, which can be realized by fast Fourier transformation with high efficiency. Although these methods obtain both high tracking accuracy and fast tracking speed, there still exist some issues which degrade the performance of the trackers. For example, the correlation filter is obtained based on the circulant structure assumption, where the samples are represented by rectangle boxes larger than the true target and are implicitly generated by cyclic shift of the original target. However, it should be noted that these samples do not fit the real scene, as the sample with large horizontal translation displays. In addition, in order to make the samples satisfy the circulant structure assumption, a bounding box used to represent the sample should be much larger than the target and a Hanning window must be utilized to deal with the discontinuity of the image margin, which leads to two additional problems. The large bounding box contains much more background information which may not represent the target appearance accurately and brings more challenges to scale adaption and robustness to background changes. Besides, the Hanning window also changes the appearance of the training patch, adding the representation errors. In this paper, we propose a novel tracking method by learning a scale-adaptive tight correlation filter, attempting to address the above issues in tradition tracking-by-detection methods and in conventional correlation filter-based methods. First, we formulate tracking as a correlation filtering problem from the signal detection perspective and learn a tight correlation filter as the tracker, which does not need the prior assumption existing in other correlation filter-based methods. By taking the target as the signal of interest to detect, the objective of filtering is to find the optimal response corresponding to the target, which is consistent with the objective of tracking. Since the filter is set as the same size as the target, it is tight and can effectively reduce the effect of the background. The samples corresponding to the correlation filter are real rather than virtual, which improves the representation accuracy. In addition, we establish the relationship between the proposed correlation filtering formulation and the ridge regression model from the practical physics background, which enriches the tracking-by-detection framework. Different from the binary classification formulation which assigns discrete labels to samples, in our method, we would like to build a ridge regression model to assign continuous values to samples. Specifically, we point out that the correlation filter is a specific realization of the ridge regression, and the correlation filter corresponds to the appearance model of the tracking-by-detection framework. Second, we present a multiscale strategy to handle the scale estimation problem adaptively by using the spatial structure of the correlation filter. In our method, the learned correlation filter has the same size with the target. Since the filter retains the spatial structure in the 2-D plane, the multiscale filter banks can be approximately implemented based on the interpolation technique, which can be used for multiscale filtering. Then, the target scale can be estimated adaptively by exploring the optimal response on multiple scales heuristically. The experimental results in the benchmark datasets demonstrate that our method can achieve desirable tracking performance and outperform many state-of-the-art trackers, especially in the condition of scale variation. Third, we develop a novel location method in the motion model by making use of both the correlation filter and the distance importance weight of the candidate samples. In most traditional tracking-by-detection and correlation filter-based methods, the confidence score is obtained by only the appearance model, for example, the classifier or the correlation filter, and the candidate samples are assigned to the same importance. However, in this condition, the similar distractor, which is far away from the target, may cause the tracking drift. In this paper, we consider the impact of the distance corresponding to each candidate sample as well as the confidence obtained by the correlation filter. In other words, the final confidence map is the combination of the distance importance score and the filtering result, which can effectively reduce the impact of the similar distractors. The main contributions of this paper are three-fold. We formulate tracking as a tight correlation filtering problem instead of the circulant structural assumption-based correlation filtering. In this formulation, the size of the correlation filter is set the same as the target, which can effectively avoid the effect of the virtual samples and reduce the influence of the background information. Specifically, the zero padding and cropping strategy are introduced to efficiently realize the learning of the filter and tracking in the Fourier domain, which does not need the circulant assumption and can effectively address the boundary effect issue. We approximately construct the multiscale filter banks for scale adaption based on the interpolation strategy and dense feature extraction. Since the multiscale filters are directly built in the filter domain, our method may locate the target more precisely than many other scale adaptive methods which realize scale adaption by scaling the images. We present a novel location strategy to determine the tracking result which considers both the filtering result and the distance importance of the candidate samples. On one hand, the filtering results obtained by the multiscale filter banks reflect the basic location information from the appearance model. On the other hand, the distance importance of the candidate samples is taken into account, which can effectively alleviate the impact of the similar distractors. The remainder of this paper is organized as follows. In Section II, we review the tracking-by-detection methods, and talk about the correlation filter-based tracking methods. Section III formulates tracking as a correlation filtering problem and explains it from the perspective of ridge regression. The detailed realization of the proposed method is shown in Section IV and the experimental results are displayed in Section V. Section VI concludes this paper.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868782", "references": [29474918, 134197611, 161114242, 182940129, 791392043, 818325216, 1513768190, 1857884451, 1961006157, 1964846093, 1991156752, 1995903777, 1997121481, 2000326692, 2001785244, 2002406878, 2023640470, 2034938692, 2038943746, 2044986361, 2068094367, 2069351745, 2071551242, 2072528130, 2075742732, 2077931794, 2089961441, 2098854771, 2098941887, 2105403175, 2109579504, 2124211486, 2127575804, 2132103241, 2139047213, 2144992113, 2149829493, 2154889144, 2156137575, 2158592639, 2158827467, 2158917775, 2162383208, 2162919312, 2165037244, 2214352687, 2244252896, 2293873019, 2298605637, 2317058546, 2336929960, 2509359898, 2557641257, 2605145377, 2612225782, 2681067697, 2738318237, 2768634781, 2964334666]}
{"paragraph": "Electroencephalogram (EEG) is an electrophysiological monitoring method to record the brain electrical signals. In this typically noninvasive method, the electrodes are placed on the scalp to record the brain’s spontaneous electrical activity over a period of time. Arguably, analysis of the recorded EEG signal is a complicated task. The reason is that this type of analysis involves many decision variables making it a large-scale optimization problem. In order to deal with such large-scale optimization problems with real-time requirements, a fast optimization technique is required. In this context, modern intelligent optimization techniques and in particular evolutionary algorithms (EAs) are considered as viable tools and efficient alternatives to the traditional optimization techniques. However, it is well-known that the performance of the EAs decreases sharply for large-scale optimization problems with numerous decision variables. Many efforts have been made towards improving the current EAs or designing new EAs in order to successfully address the large-scale optimization problems. The complication degree of an optimization problem exponentially increases as the problem size increases. Up to now, few multi-objective evolutionary algorithms (MOEAs) have been successfully deployed to solve the large-scale optimization problems. An in-depth parametric study is a key to the success of MOEAs. On the other hand, for large-scale optimization problems, a fast process is also a critical factor for any algorithm. EAs have two important operators which heavily influence their convergent speed: mutation and crossover. Several studies have been carried out in the area of designing more efficient mutation and crossover operators for EAs. Recently, a reference-point-based many-objective evolutionary algorithm, called non-dominated sorting genetic algorithm - the third version (NSGA-III), has been proposed. In the present paper, this powerful algorithm is applied to solve complicated high-dimensional multi-objective optimization (MOO) problems originated from the EEG signal processing with thousands of decision variables. These large-scale optimization problems are proposed in IEEE CEC 2015. In the basic NSGA-III algorithm, only the SBX crossover operator is used. In this study, the effect of different crossover operators (SBX, SI, and UC) on the performance of the NSGA-III algorithm is fully investigated. In addition, improved crossover operators are proposed by combining SBX, SI, and UC with Stud. The new operators are then incorporated into NSGA-III in order to generate the offspring population for the next generation. Subsequently, ten variants of NSGA-III algorithms are developed and further verified on the large-scale optimization problems. The current paper is structured as follows: Section 2 reviews several most representative work on large-scale optimization problems and MOEAs. In Section 3, the basic NSGA-III algorithm is described. The proposed crossover operators are described in Section 4. The description of a large-scale optimization problem is given in Section 5. Section 6 provides the simulation results on large-scale optimization problems proposed in CEC 2015 big data competition. Finally, Section 7 provides a summary of the present work.", "section": "Introduction", "doi": "10.1016/j.ins.2018.10.005", "references": [1580723439, 1968219458, 1969803830, 1976442029, 1980048226, 2003961265, 2022485595, 2024008934, 2028885867, 2053498776, 2053900989, 2058361915, 2087812386, 2106334424, 2116661285, 2126105956, 2143381319, 2204362653, 2295576181, 2344916885, 2514388731, 2521918431, 2533800621, 2547860982, 2556421959, 2558150887, 2570235502, 2579361096, 2602908436, 2604129326, 2605049507, 2616346973, 2739608398, 2745261836, 2752606981, 2757662287, 2774381817, 2778143478, 2780124552, 2782424305, 2799163655, 2802576758, 2808993793]}
{"paragraph": "Many optimization problems in the real world contain multiple incommensurable and conflicting objectives. Such problems are named multi-objective optimization problems (MOPs), which have earned considerable attention in the practical applications such as recommendation system, shop scheduling. There are three main goals in a MOP: to improve the convergence property so that obtained solutions can get as close as possible to the true Pareto front (PF); to maintain the diversity of the generated solutions, in order that all solutions can spread evenly throughout the PF; to make use of the least computational complexity on condition that the performance of convergence and diversity is ensured. Over the last two decades, MOEAs have become increasingly popular for solving MOPs. Most MOEAs approximate the entire PF, however, the ultimate goal of a MOP is to help DMs to identify several or one final Pareto optimal solution that satisfy his/her most in practical application. In multiple criteria decision making (MCDM), DMs usually give preferences to identify the most preferred solution among multiple conflicting objectives. The preferred solution may be a single preferred solution or an approximation of the region of interest. The DM’s preference has received a great of attention on MOEAs, including the reference points, reference directions, utility function, fuzzy logic etc. The existing preference-based optimization methods can be classified into three categories according to the time when preferences are incorporated, i.e. before optimization (a priori), after optimization (a posteriori) and iteratively during optimization (interactive approach). Next, we give a simple review of these three categories. A priori methods: DMs need to input their preferences before the beginning of the optimization. Some typical priori methods are listed as follows: one method first combined a MOEA with the goal information as an additional standard for group membership allocation. Another proposed a guided MOEA, which modified the definition of dominance based on the DM’s preference information. A different approach proposed to combine the reference information of the reference point with the non-dominate sorting operator of the elitist non-dominated sorting genetic algorithm. Another proposed a preference based evolutionary approach which directly uses reference point information in fitness evaluation. Recently, a method incorporated the DM’s preference information into the multi-objective evolutionary algorithm based on decomposition, which modelled the DM’s preference information as an aspiration level vector. The disadvantages of these methods are that DMs may have limited knowledge about the problem and their preferences may be inaccurate or even misleading. A posteriori methods: DMs need to select a small number of solutions according to their preferences from the entire PF obtained by a MOEA. In fact, many well-known MOEAs belong to it. In addition, scholars have proposed some other novel algorithms to efficiently solve MOPs, such as Two-Archive, improved Two-Archive and the reference vector guided evolutionary algorithm. Compared with the a priori methods, DMs are able to better understand the trade-off relationships between the objectives. It should be noted, however, that it becomes increasingly hard to obtain a representative solution set as the number of objectives increases. Interactive methods: DMs need to articulate their preferences during evolution. They are allowed to modify their preferences, typically based on the domain knowledge acquired during the optimization. With a deeper understanding of the problem as the optimization proceeds, DMs are able to fine tune their preferences according to the obtained solutions in each iteration. Several interactive MOEAs have been proposed in the literature over the last 10 years. One proposed an interactive method that is able to convert different forms of preferences into one single format and then uses that to guide the search towards preferred solution(s). Another put forward a new preference based interactive simple indicator-based evolutionary algorithm, it extended the previous version by taking the preference information of the DM into account, a set of non-dominated solutions are iteratively shown to the DM and then DMs are required to provide his/her preferences by classifying this set into preferred and/or non-preferred solutions. A different approach presented an interactive preference-inspired co-evolutionary algorithm, similar to another, in which candidate solutions and goal vectors are co-evolved to focus on a region of interest, which is defined by user-provided reference point, weight and search range. It also allows the DM to brush the region in the objective space, alleviating the burden of setting parameters. One other proposed an interactive MOEA/D, named i-MOEA/D, during periodical interaction, a set of current solutions are offered to the DM, and the search is then guided by renewing the preferred weight region towards the neighborhood of the solution which is most preferred. Based on this, we can conclude that the key of the interactive algorithm is how the DM’s preference and the algorithm during evolution. In summary, eliciting the preference information in an interactive manner seems to be more interesting. This enables the DM to progressively learn and understand the characteristics of the MOP at hand and adjust their elicited preference information, consequently, solutions are effectively driven toward the region of interest. To the best of our knowledge, there does not exist an interactive MOEA that can make the region reduce gradually, until finding the closest solution to the DM’s preference information. Furthermore, there is no guarantee to find the preferred solutions when tackling many-objective problems. In this paper, we propose a new multi-layer interaction preference based multi-objective evolutionary algorithm through decomposition. MOEA/D is a recent MOEA framework with many successful applications. In the proposed algorithm, we use a new version of MOEA/D with a Maximin operator called MOEA/D-Maximin which can utilize the Maximin function to evaluate the fitness value of individual, it can get the non-dominated solutions easily and ensure the diversity of the population simultaneously. The developed multi-layer interaction strategy consists of two layer interaction, in the first-layer interaction, the DM will provide a reference vector and an initial radius to determine a rough preference range, and then update the solutions in this range. After the first-layer interaction, the DM will check whether he/she is satisfied with the result, the process will stop if the result is satisfactory, otherwise it will go on to the next interaction. In the second-layer interaction, the most preferred solution from the result of first-layer interaction will be chosen as the new preference direction, and the weight vector is redefined by an angle-based preference method. During the interactive process, the DM can control the search process and get more involved in the procedure. The main contributions of our work are summarized as follows: MOEA/D-Maximin is applied to find the DM’s preference, which is an effective approach. A required number of solutions can be gained by finding the same number of weight vectors around the reference direction and find the closest solution to each chosen weight vector. A novel multi-layer interaction strategy for searching the accurate solutions is proposed. During evolution, the preferred region is reduced gradually, making it easier for the DM to get the final preferred solution(s). A fast way to get preference region is established, during the selection process, the preference weight vector is redefined using the angle-based method. Besides ZDT problems with two objectives, the proposed algorithm shows its potentiality for solving many-objective problems, i.e., DTLZ problems with 3, 5, 8, 10 objectives. In the remainder of this paper, we first review some background knowledge and related works in Section 2. Section 3 introduces the main process of our proposed algorithm. Experimental designs and findings are presented in Section 4. Section 5 concludes this paper and discusses the future research.", "section": "Related Work", "doi": "10.1016/j.ins.2018.09.069", "references": [93031587, 114258432, 146421413, 240075219, 626466576, 1509866077, 1564638059, 1612556207, 1662894842, 1968219458, 2011048370, 2022485595, 2055092083, 2056654250, 2074921660, 2077433479, 2080858591, 2083457599, 2106884655, 2110828487, 2125899728, 2126105956, 2127950596, 2136386226, 2143381319, 2151879717, 2152551290, 2156194072, 2171996959, 2343601797, 2502932208, 2515223602, 2558359969, 2588157598]}
{"paragraph": "Clustering algorithms have become a widely used method due to their ability to provide new insights into unlabeled data sets. They consist in forming homogeneous groups of observations referred to as “clusters”. Clustering algorithms highlight the data’s inherent structure. However, the recent “big-data” phenomenon has greatly increased the number of features, leading to the emergence of high-dimensional data sets. Clustering techniques are consequently not always sufficient to discern the structure. The analysis of a cluster relies on a representative of the cluster (mean, mode etc.). However, the latter is itself described by a large number of features, which makes it more difficult to interpret and makes the summary of the data set less useful. From this consideration comes the need to also “summarize” the features, which can be done by gathering them into clusters, in parallel with the usual clustering of observations. Co-clustering methods seem to be a good option for performing this task because they perform joint clustering of rows and columns. The initially large data matrix can be summarized by a limited number of blocks that result from combining row-clusters and column-clusters. Among the most famous co-clustering techniques, the Non-negative Matrix Tri-Factorization consists in factorizing the data matrix into three matrices with the property that all three matrices have non-negative elements. More specifically, the approximation is achieved by minimizing the error function with constraints meaning that all elements of the matrices are greater than zero, and a matrix norm is chosen. The matrix represents the block matrix: an element of it summarizes the observations belonging to a row-cluster and a column-cluster. Despite the non-negative property of the matrices, it is not always easy to interpret the resulting matrices. For example, the matrices are not always normalized which makes it difficult to interpret them in terms of rows and columns belonging to corresponding clusters. Furthermore, this technique depends on the choice of the distance measure. Conversely, probabilistic approaches propose normalized membership matrices and do not require the user to choose a particular distance measure. In the Latent Block Model, referred to as “LBM”, the elements of a block are modeled by a parametric distribution. Therefore, the results give more information than a simple scalar, as mentioned in the previous methods. Each block is therefore interpretable via the parameters of the block-distribution. Moreover, model selection criteria such as the ICL criterion can be used for model selection purposes, including the choice of the number of co-clusters. This technique has proved its efficiency in co-clustering several types of data: continuous, nominal, binary, ordinal, and functional. For this reason, an extension of this model is used in the present work, although originally it was not able to take heterogeneous data as an input. Heterogeneous data sets are composed of features of different types. For example, in medicine, a patient’s file can be composed of images, text, continuous data, categorical data, and even functional data. Several clustering frameworks have been developed to address this particularity. The latent class model is frequently used. It assumes that the variables are conditionally independent upon the row-cluster membership. Consequently, the joint probability distribution function of the features of different types is obtained by the product of the PDFs of each individual feature. However, when the variables are inherently correlated in a row-cluster, this model is not suitable. To overcome this issue, some authors want to conserve standard marginal distributions but also try to loosen the conditional independence on the variables. For this purpose, they use copula, which allows definition of both the dependence model and the type of marginal distributions. The proposed model relies on the main assumption that each cluster follows a Gaussian copula. However, the authors note that model complexity increases with the number of variables, which is not suitable in a big-data context. Another way to address the issues of heterogeneous data is to see some variables as the manifestation of a latent vector. For example, one model considers continuous and categorical data and assumes that a categorical variable is the representation of an underlying latent continuous variable. Then, it is assumed that the continuous variables, observed and unobserved, follow a multivariate Gaussian mixture model. Until now, these methods have proposed models for basic data such as categorical and continuous data. In one approach, the authors allow the introduction of more complex data such as functional data or networks by projecting the data set into a reproducing kernel Hilbert space. Regarding the analysis of variables, multiblock methods, widely used in Chemistry and Biology, handle data sets that share the same observations but have variables measured differently. They aim at finding underlying relationships between these data sets. In particular, multiblock component models use latent variables to summarize the relevant information between and within the sets. However, none of these techniques were developed in a co-clustering framework. To the best of our knowledge, the only work to co-cluster heterogeneous data extends the LBM for data sets with continuous and binary data. The present work goes further by proposing an extension that can take into account four types of data: categorical, continuous, count and ordinal data. Furthermore, the inference algorithm can deal with missing values and proposes a way to impute them. Finally, the Integrated Completed Likelihood criterion is adapted to the proposed model in order to select the number of row-clusters and column-clusters. Co-clustering techniques can be seen as an efficient alternative method to the selection of variables thanks to its parsimony, especially in very high dimensions. In addition, it can produce interpretable sets of variables since it can group redundant variables or noisy variables. In this way, a first, naive answer is to manually select the informative blocks, but some alternatively define a model that automatically distinguishes the informative blocks for textual data sets. For mixed data, variable selection is more challenging. Some researchers perform clustering while incorporating variable selection and this method can produce homogeneous row-clusters. However, compared to co-clustering, it does not provide interpretable column-clusters, which may be essential for the summary of the data set, in particular with a high number of variables. The paper is organized as follows. Section 2 gives an overview of the LBM to help understanding of this paper. Then, it proposes an extension to a new LBM version that allows heterogeneous data sets. Section 3 proposes an algorithm for model inference, based on a Stochastic Expectation Maximization algorithm coupled with a Gibbs sampler. In Section 4, a description of the different types of data that can be taken into account with this method is given, and formulas for model inference are presented. Section 5 assesses the efficiency of the proposed method on simulated data while Section 6 shows how the method performs on real data sets. Section 7 provides a conclusion.", "section": "Methodology", "doi": "10.1016/j.csda.2019.106866", "references": [45199236, 583546247, 1982509351, 2067160380, 2109820980, 2144211451, 2224746910, 2460302605, 2472863357, 2552406463, 2562050914, 2597328883, 2599487788, 2727551782, 2771517242, 2794266768, 2800909518]}
{"paragraph": "Products of stochastic matrices is closely related to the ergodicity of Markov chains as well as the convergence properties of the averaging dynamics driven by them such as distributed optimization, distributed control of robotic networks, and study of opinion dynamics in social networks, etc. The notable works in the first domain can be traced back to those of Hajnal et al. and Wolfowitz more than half a century ago. Hajnal and Bartlett investigated the weak ergodicity of nonhomogeneous Markov chains and proved an inequality named after him which relates the convergence of the products to the scramblingness of the involved stochastic matrices. Wolfowitz investigated the products of stochastic, indecomposable, and aperiodic matrices and proved a sufficient condition for their convergence. The second domain has been started by earlier work and the seminal work of Tsitsiklis which can be formulated under the framework of consensus problem in networks of multiagent systems. Motivated by the broad applications of multiagent systems and the extensive investigation of consensus algorithms, products of stochastic matrices has been an active research area for many years. Various conditions for the convergence of the products have been proposed and proved. In particular, many important results on products of random stochastic matrices have been obtained in the past decade. A necessary and sufficient condition for almost sure convergence of products of independently and identically distributed stochastic matrices, namely, a directed spanning tree in the expectation of the communication graphs, has been derived. This result is generalized to stationary and ergodic sequences and further generalized to more general adapted sequences. From a different perspective, others characterized the convergence properties of independent random stochastic matrices in terms of infinite flow properties in a series of works. These works are further generalized based on a recent extension of the classical Kolmogorov–Doeblin decomposition-separation theorem to inhomogeneous Markov chains. Despite all these important progress, a fundamental limitation in the product theory of stochastic matrices is that it only admits non-negative entries, thus confining its applications in consensus analysis to networks with only non-negative coupling coefficients. Yet there are a variety of situations in which arbitrary coupling weights may arise. For example, when there are two opposite kinds of interactions between the agents, such as the friendly/hostile relationship between individuals in social networks and the activation/inhibition effect between genes in biological systems, negative weights can be used to model the disagreement tendency between the interacting agents. Besides, negative coupling weights may also occur in some auxiliary systems which arise in the process of theoretical analysis. However, due to the technicalities involved, to the best of our knowledge, only very few works have been done on networks with general coupling coefficients. Some work proved a sufficient condition for almost sure consensus among agents over independent identically distributed periodically switching communication graphs. Other work proved some sufficient conditions for both discrete-time and continuous-time consensus networks with general stochastic switching topologies and arbitrary coupling weights. Recently, convergence rates have been estimated for discrete-time consensus in multiagent systems with time-varying delays and general coupling weights. In these works, the coupling structures are described by matrices more general than stochastic ones which admit negative entries or even nonunit but equal row sums. Motivated by these more general matrix classes introduced in these works and the extensive applications of product theory of stochastic matrices, we consider developing new product theories for matrices that are general enough to admit negative entries. This will not only extend the existing results for the products of stochastic matrices but also facilitate the investigation of consensus in networks with arbitrary coupling weights. In this paper, we investigated the products of a class of matrices which can be seen as a generalization of that of stochastic matrices by admitting negative entries while keeping the unit row sum. We established some new results that generalize those from the product theory of stochastic matrices. Particularly, we obtained a generalized version of the classical Hajnal’s inequality. The main contribution of this paper can be summarized as follows: we investigated the products of generalized stochastic matrices and obtained an extended version of the classic Hajnal inequality; based on the general theoretical results, we established a new convergence result for a class of discrete-time consensus algorithms with general coupling coefficients and time-varying delays; and based on the results obtained, we analyzed the convergence properties of a class of continuous-time consensus algorithms with discrete-time communications and controller updates. The rest of this paper is organized as follows. Some basic definitions and results from matrix and graph theories are presented in Section II. Some results for the products of a class of generalized stochastic matrices are obtained in Section III. Based on these results, some convergence results are obtained for a class of discrete-time consensus algorithms with general coupling coefficients and time-varying delays in Section IV. These results are then applied to the analysis of a class of consensus algorithms in networks with continuous-time dynamics but discrete-time communications and controller updates in Section V. This paper is concluded in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868994", "references": [1547358136, 1964647857, 1984259283, 1985343525, 1998059521, 2050592771, 2054856671, 2068866314, 2075175322, 2076041173, 2092697398, 2103554135, 2107396783, 2114403255, 2125668987, 2144408773, 2148995214, 2160413708, 2165744313, 2166132612, 2167183308, 2171153629, 2215764026, 2417709453, 2525747818, 2763354547, 2781512642, 2783384406, 2792799644, 2800777482, 2804514960, 2963490876]}
{"paragraph": "Optimization problems involving multiple conflicting objectives are often known as the multi-objective optimization problems (MOPs), which can be formulated as follows. The decision space stands for the domain of feasible solutions, and the decision vector denotes a specific choice of decision variables. The symbols represent the number of objectives and decision variables, respectively. The objective functions map the decision space to the objective space. For any two solutions of an MOP, one is said to dominate the other if all the objectives of the former are no worse than those of the latter and at least one objective of the former is better than the corresponding objective of the latter. A solution is Pareto-optimal if there exists no solution dominating it. Generally speaking, all the Pareto-optimal solutions are called the Pareto set in the decision space and the Pareto front in the objective space. The problem with at least four objectives is known as a many-objective optimization problem. In addition, if an MOP has at least one hundred decision variables, it is known as a large-scale MOP. Similarly, a large-scale many-objective optimization problem refers to an MOP having at least four objectives and one hundred decision variables. To solve MOPs and MaOPs, a large number of multi-objective evolutionary algorithms and many-objective evolutionary algorithms have been developed over the past two decades. However, most existing algorithms mainly focus on the problems with small-scale decision variables, and their performance often deteriorates rapidly as the number of decision variables increases. As a result, solving MOPs or MaOPs with large-scale decision variables is an ongoing challenge in the evolutionary optimization community. Recently, some works for solving large-scale MOPs and large-scale MaOPs have been proposed on the basis of the cooperative co-evolutionary framework, where the large-scale problems are decomposed into a group of small-scale problems and solved in a cooperative and co-evolutionary manner. To solve MOPs or large-scale MOPs, the existing multi-objective evolutionary algorithms, as well as the large-scale variants, try to approximate the set of Pareto-optimal solutions using one population, which requires that the evolutionary algorithms simultaneously strike a good balance between convergence and diversity. However, balancing convergence and diversity becomes more challenging as the number of decision variables increases. Therefore, this work proposes to solve the large-scale MOPs by using a series of subpopulations, where each subpopulation is made to converge to only one solution. Specifically, we adopt the CMA-ES as the local optimizer for each subpopulation. Compared to other meta-heuristic algorithms such as the differential evolution, genetic algorithm, simulated annealing, or particle swarm optimization, the CMA-ES works well with a much smaller population, thus costing much fewer fitness evaluations when performing local search. To be more specific, the main contributions of this work are summarized as follows. We propose a scalable small subpopulations based covariance matrix adaptation evolution strategy, namely S3-CMA-ES, for solving large-scale MOPs. Different from the existing algorithms that approximate the Pareto fronts of MOPs or large-scale MOPs by one single population, each subpopulation in the proposed algorithm attempts to search one solution using a small population. A diversity improvement strategy is proposed to select new solutions, which are generated on the basis of the solutions from the converged subpopulations. On the basis of the selected solutions, this strategy further generates new subpopulations. The rest of the paper is organized as follows. Section 2 summarizes relevant studies on multi- and many-objective evolutionary algorithms. Then, the proposed algorithm is described in Section 3, followed by numerical experiments to demonstrate its superiority in Section 4. Finally, the paper is concluded in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.10.007", "references": [760913798, 1662894842, 1968219458, 2005650200, 2017299308, 2018344446, 2018712218, 2020320008, 2022485595, 2040622444, 2067544246, 2108968575, 2126105956, 2134154181, 2138537392, 2143381319, 2153654820, 2167145078, 2329749247, 2343601797, 2344916885, 2410777154, 2461861577, 2502932208, 2510493362, 2513211214, 2519112510, 2520973147, 2582096828, 2583230990, 2585632407, 2593887026, 2594344284, 2608705546, 2616257225, 2735649024, 2745599083, 2747918924, 2757662287, 2764251381, 2766133347, 2769746177, 2770118518, 2775348664, 2808165282]}
{"paragraph": "Classification is an important topic in machine learning. By removing irrelevant or redundant features, a feature selection (FS) algorithm can effectively reduce the dimension of data, shorten the learning time, and improve the classification performance. During the past decade, numerous FS algorithms have been proposed. Among them, meta-heuristic methods have shown a lot of advantages in dealing with feature selection problems due to their powerful exploration capabilities. Meta-heuristic feature selection methods include genetic algorithms, ant colony optimization, particle swarm optimization, firefly algorithm, memetic algorithm, artificial bee colony, grasshopper optimization algorithm, evolutionary gravitational search, etc. Generally, feature selection can be modeled as a multi-objective combinatory optimization problem, which mainly contains two objectives, the number of the selected features and the classification accuracy. In some cases, removing irrelevant and redundant features from the original datasets can improve the classification accuracy of classifiers. In other words, the feature size can be reduced while the classification accuracy is improved. However, when a dataset only contains the key features that must be used by classifiers, deleting any feature may increase the error rate of these classifiers. Here, the two objectives are conflicting with each other. Moreover, obtaining the report values of different features often needs different level costs, such as time, money, or other resources. A large number of features usually means a high cost. Therefore, reducing the number of the selected features is also an important indicator. Moreover, formulating a feature selection problem as a multi-objective optimization one is beneficial in obtaining a set of optimal feature subsets so as to meet various requirements of decision-makers. Multi-objective evolutionary algorithms can seek multiple solutions lying in the Pareto optimal front in a single run. The aforementioned algorithms have been widely applied in handling the feature selection problems. Because of its simplicity and efficiency, differential evolution (DE) is a very popular evolutionary algorithm used in the feature selection problems. However, most of the current study focuses on the single-objective case, maximizing the classification accuracy. There is little work on applying the DE to the multi-objective case. Multi-objective DE has been used in feature selection and applied to tasks such as multi-label feature selection, entity extraction in biomedical texts, and facial expression recognition systems. The results obtained show the effectiveness of the multi-objective DE in handling the feature selection problems. However, these approaches have the following disadvantages. Most of them adopt the DE/rand/1/bin strategy to generate candidate individuals, where the base vector in the mutation is randomly chosen from the population. Due to the randomness of the base vector, this strategy makes the population yield a good exploration performance but slows down the overall convergence. The elite individuals in the population only take responsibility for guiding the search of other individuals. Improving the self-learning ability of the elite individuals may enhance the exploration of the whole population. Therefore, how to boost the global exploration of the DE without sacrificing its convergence needs new effective strategies to be developed. To improve the capability of the original DE in dealing with the multi-objective feature selection, the present paper studies a new binary differential evolution with a self-learning strategy, namely MOFS-BDE. In our algorithm, the DE is responsible for exploring the search space and finding potential regions, while a self-learning strategy is utilized to effectively exploit these potential areas. The main contributions of this paper are as follows: a binary differential evolution algorithm with a self-learning strategy, MOFS-BDE, is proposed to attack the multi-objective feature selection problems; a new binary mutation operator based on the probability difference is designed to generate fresh solutions. Since the base vector is always the best one among the three randomly generated vectors, this operator can guide individuals to locate potentially optimal areas in a fast way; a new problem-specific self-learning strategy, namely one-bit purifying search, is proposed to refine the elite individuals in the population. Thus, the elite individuals not only take the responsibility of guiding the search of other individuals, but also have the self-learning capability; an efficient non-dominated sorting combined with crowding distance is employed to select appropriate parent individuals so as to reduce the time consumption of the selection operator in the regular differential evolution. The structure of our paper is as follows. Section 2 introduces the background of the feature selection problems, and Section 3 reviews some of the existing evolutionary feature selection approaches. The standard and modified DE are discussed in detail in Sections 4 and 5, respectively. Section 6 presents experimental results of the proposed MOFS-BDE. Finally, a few conclusions and remarks are given in Section 7.", "section": "Methodology", "doi": "10.1016/j.ins.2019.08.040", "references": [1418108976, 1607660464, 1833634424, 1964537798, 1970032027, 1970769764, 2013885787, 2019511760, 2027425155, 2058118608, 2069928051, 2070556583, 2083945868, 2093010752, 2105223996, 2106334424, 2126105956, 2131090729, 2143381319, 2156194072, 2162043657, 2165885026, 2210287443, 2254043540, 2343420905, 2397430692, 2411885377, 2561576489, 2588659412, 2593421093, 2604772753, 2616650140, 2724365251, 2736583283, 2739135567, 2742990887, 2745573163, 2754840697, 2757662287, 2765937321, 2776226778, 2790793279, 2791239441, 2801536506, 2932046728, 2945163040, 2953877793]}
{"paragraph": "Networked multirobot systems can be a potential aid in applications such as search and rescue, autonomous exploration, sensing and communication infrastructure, etc. Coordinating a group of robots involves repetitive tasks of rendezvous, formation control, and flocking of the distributed robots. Here, we focus on the rendezvous problem, in which the distributed robots need to gather at a common location either based on consensus or based on immediate goals. Significant progress has been made in the recent works addressing the rendezvous problem, namely by introducing distributed control laws for realizing a consensus to gather robots at a common location and by enabling fault tolerance in such controllers. Most of the studies rely on the assumption that the initial interaction topology is densely or completely connected or that a global coordinate frame of reference is available. However, such strong assumptions are difficult to satisfy in challenging GPS-denied large environments and in resource-limited distributed robots. Encouraged by the reconfigurable coordination mechanism, we show a motivating example scenario in which we assume only a few robots have advanced capabilities, namely to map and analyze the environment with 3-D sensors, and/or are different from other robots in terms of coverage and visibility. In such a scenario, it would be appropriate that these advanced robots lead the other member robots to their next target positions while avoiding obstacles in a cluttered environment. To realize this herding behavior, all the robots need to rendezvous at one of the group-leading robots. The interaction graph or topology between robots is sparsely connected, reflecting a realistic multirobot setting. Furthermore, we take inspiration from the autonomous recharging and coverage planning applications where gathering robots in multiple groups would be of significant interest. Therefore, we address the problem of distributed multipoint rendezvous without global localization. In this paper, we provide a solution to the multipoint rendezvous problem and contribute in the following ways. We propose a distributed and coordinate-free rendezvous control algorithm based on the hierarchical tracking of a connected graph. We show the theoretical guarantees of the algorithm such as convergence and link maintenance. The proposed approach is adaptive to changes in network topology and is capable of handling network and mobility faults. We extend the above algorithm to perform rendezvous at multiple points led by leader robots. We also propose to choose the leaders in each group by optimizing the distance traveled to realize the rendezvous task. We demonstrate the proposed algorithm in terms of scalability, efficiency, robustness to failures, and herding along with its dynamic nature through extensive simulation experiments in the Robotarium multirobot testbed. To validate the performance of our hierarchical tracking algorithm, we compare it against two algorithms available in the literature: the standard consensus algorithm and the circumcenter-based consensus algorithm. This paper is organized as follows. In Section II, we analyze the literature and show how we depart from others. Section III provides some background on graph theory, assumptions, and definitions. Specifically, the multipoint rendezvous problem is formulated in Section III-C. Then in Section IV, we introduce the proposed algorithm to realize a multipoint rendezvous of robots with limited speeds. We present the experiments and results in Section V and conclude this paper in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868870", "references": [1518154293, 1572260348, 1969573308, 1972603945, 1979569034, 1987474585, 1993693205, 2063801303, 2081475880, 2097569568, 2112141968, 2134100244, 2150973821, 2159465607, 2165744313, 2177443133, 2344219546, 2409707557, 2485926893, 2564284893, 2593733243, 2740693561, 2785694108, 2788957818, 2963673039, 2964138223]}
{"paragraph": "With the increasing demand for distributed systems and the growth in the scale of the applications, a traditional networked control system using wired communication networks to transfer the information between communicating units in the closed-loop control system introduces limitations. The use of wireless communication networks offers significant advantages over their wired counterparts, which results in low cost and flexible network architectures by decreasing the cost of the installation, modification, and upgrade of the system components compared to their wired equivalent. This control system architecture is referred to as a wireless networked control system (WNCS) in which the sensor and actuator communicate with the controller through a wireless network. WNCS has been attracting increasing research interest and has been finding various applications in smart grid, intelligent monitoring, and building automation. With its inherent advantages, WNCS facilitates prospects in the manufacturing industry. Although the WNCS provides many benefits compared with the traditional wired NCS, it is also predictable that the WNCS is more vulnerable than wired NCS due to its inherent openness nature from the wireless network. The communication system design for a WNCS requires guaranteeing the performance and stability of the control system, with the limited battery resources of sensor nodes, despite the unreliability of wireless transmissions and the shared wireless medium. As such, a deal of literature has been found on securing the WNCS in both the control and communication communities. Recently, a number of security incidents of NCS have been reported. For instance, the industrial control systems in Iran have been infected by the advanced computer worm named “Stuxnet”. The frequent occurrence and severe damage of such incidents motivate extensive investigations on the secure issues of WNCS. In the reported literature, two types of cyber attacks have been investigated, namely: the deception attack corrupting the data integrity and the denial-of-service (DoS) attack corrupting the data availability. To be more specific, the deception attack aims at modifying the information in the transmitted data packet and requires comprehensive information of systems a priori. In comparison, the DoS attack aiming to congest the information transmission is easier to launch because prior knowledge of the system is not required. Furthermore, it is worth mentioning that the information loss caused by the DoS attack could degrade the system performance or even render the whole systems unstable. Due to the potential threat associated with a DoS attack, it is significant to investigate the security issue of the WNCS subjected to a DoS attack. A literature survey shows that a number of methodologies have been reported to alleviate or cancel the DoS-attack-induced impact, such as packet dropout or time delays. The methodologies concerning the security of the control system under a DoS attack could be classified as an attack-tolerant method and attack-mitigated method. Specifically, the attack-tolerant method has been proposed such that the controlled system is capable of tolerating the undesired phenomena caused by DoS attackers while the attack-mitigated method has been employed in order to directly compensate the undesirable effect induced by DoS attacks. It is worth mentioning that for the attack-mitigated method, the cross-layer design methods have been employed which not only design the controller to adapt to the attack-induced phenomena, but they also develop security strategies to protect the communication network of the NCS. However, the investigation of the cross-layer design method for the control problem of WNCS has been really scattered, which formulates the main motivation of writing this paper. Summarizing the above discussions, although the cross-layer design approach has stirred some initial attention from the researchers, the co-design method to secure the WNCS has not been adequately addressed. In this paper, the WNCS is modeled hierarchically, where in the cyber-layer, a signal-to-interference-plus-noise ratio (SINR)-based security game is employed such that the players make a decision on their transmission power. In the physical layer, the player designs optimal control strategies subject to the network constraints imposed by the upper layer. Based on the obtained strategies, we propose a novel cross-layer design method, that is, the pricing mechanism design approach with which the studied WNCS can remain stable despite DoS attacks. To be specific, the contributions of this paper are highlighted as follows. A novel SINR-based game theoretic model is put forward to describe the interactions between the transmitter and DoS attacker. The sufficient condition is established for the existence and uniqueness of the Nash equilibrium, and the corresponding transmission power strategies are given for the transmitter and attacker, respectively. The impact from the attack and defense model on the controlled system is described in terms of packet dropout, which is assumed to be correlated according to the Markovian rule. In the presence of such packet loss, the optimal control scheme is developed in the delta-domain in order to better adapt to the fast-sampling protocol. Furthermore, a novel sufficient condition is established such that the control performance remains within the security region over the infinite horizon. An extended-state-observer-based dynamic feedback pricing mechanism is designed such that the studied WNCS remains within a desirable security region by strategically tuning the pricing parameters.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2863689", "references": [32864775, 1934866980, 1992456875, 1996056443, 2009257627, 2021959953, 2031578370, 2047399268, 2063688797, 2135216999, 2139197149, 2144114773, 2150113962, 2206930994, 2230652337, 2315448583, 2324627801, 2337446187, 2340384825, 2346026831, 2469287823, 2505598063, 2595324233, 2619461977, 2740518031]}
{"paragraph": "Fuzzy sets were proposed to describe an uncertain and vague concept by allowing graded membership values. In many practical applications, although the membership grade can discriminate elements on tiny details, a fuzzy membership function is difficult to interpret and understand based on its infinite number of values. In fact, there is no need to discriminate elements that are extremely similar to each other. The important reason is that an extremely accurate membership grade is unlikely to provide much valuable information. In addition, the research indicates that humans cannot deal with much information. Recent research indicates that the actual number of values that we are able to process is about four. Hence, the approximations of fuzzy sets by a few membership grades are considered to simplify complex problems. Because the approximations of fuzzy sets were viewed as a new research field, many scholars have devoted themselves to researching this issue. Initially, Zadeh briefly introduced a pair of thresholds to transform fuzzy membership grades in the unit interval into three regions, namely, complete belongingness, complete exclusion, and uncertainty, which are similar to the three disjoint regions of a rough set, meaning the positive, negative, and boundary regions, respectively. Similarly, the approximations of fuzzy sets have been extensively investigated. For example, a pair of thresholds was introduced to approximate a fuzzy set in formulating interval sets. Based on the conception of rough fuzzy sets, a pair of a lower approximation and an upper approximation of the rough fuzzy set was calculated to form a three-way approximation. However, there is no sufficient theory to interpret the two required thresholds. Then, the theory of shadowed sets, which provides an objective optimization function to calculate the two thresholds by means of searching for a balance point of uncertainty in given three disjoint parts, was proposed. Moreover, the composition of shadowed sets that can characterize the essence of fuzzy sets has two characteristics, which are as follows: effectively reducing the complexity of the calculation, and having a more concise explanation. Intuitively, the composition of shadowed sets obeys the following rules: when the membership grade of an element is near 1, it is elevated to 1 and placed into the positive region; when the membership grade of an element is near 0, it is reduced to 0 and placed into the negative region; when the membership grade of an element is neither near 1 nor near 0, it is placed into a shadowed region. In other words, it is placed into the boundary region. From these three rules, there are elevation and reduction operations that utilize the thresholds that can offer acceptable levels of the membership grade of an element that is near 1 and 0, respectively. Recently, because the idea of shadowed sets is novel and can simplify complex problems, it has attracted many scholars to research from different theoretical and applied fields. However, most applications of shadowed sets are limited by the condition that the sum of the thresholds equals one. To avoid the condition that the sum equals one, a decision-theoretic model that can calculate a pair of thresholds with different significance was proposed, which is mainly composed by the shadowed sets and three-way decisions. Recently, a general framework for studying three-way approximations of fuzzy sets was proposed. In this general framework, the existing methods for calculating optimal thresholds can be concluded according to three aspects: uncertainty invariance, minimum distance, and minimum cost. What is more significant is that the theory promotes the development of shadowed sets and three-way decisions. We shall see that the idea of three-way decisions plays a key role in the process of constructing the models. Because of the idea of three-way decisions, the thresholds can be calculated to divide a domain into three disjoint regions by a loss objective function. From this viewpoint, the following three issues must be considered in the three-way decision model: interpretation of a set of values for acceptance or rejection, construction of an evaluation function, and determination of acceptance and rejection. In the three-way decision model, a pair of thresholds is used to make decisions as follows: accepting the object if an object satisfies the criteria or is above a certain level; rejecting the object if an object does not satisfy the criteria or is below another level; deferring the object if an object cannot be determined to be accepted or rejected. Then, combining the shadowed sets with the three-way decisions, the thresholds can be obtained. Compared with the shadowed sets, this model offers several advantages as follows: it provides a meaningful interpretation of the objective function, which is the overall cost of three-way approximations; based on the value 0.5, a pair of thresholds can be immediately obtained in terms of various decision costs by minimizing the total cost; in addition, the acquired pair of thresholds need not satisfy the condition that their sum is one; the decision-theoretic three-way interpretation provides a new viewpoint to research on the shadowed sets. Although the condition that their sum equals one is not necessary in this model, it still requires that one threshold is greater than 0.5. This is because the shadowed region with the uncertain interval value is transformed into a single value 0.5. As a result, from the principle of the minimum decision cost, there is a difference between the obtained overall cost and the least cost in some special cases. In order to provide a more reasonable explanation, we suppose that both the unit cost of reducing the membership grade of an element to 0 and the unit cost of elevating the membership grade of an element to 1 are infinite. Furthermore, both the unit cost of reducing the membership grade of an element to 0.5 and the unit cost of elevating the membership grade of an element to 0.5 are constant and greater than 0. According to the formula for calculating the pair of thresholds in this model, the pair of thresholds are acquired approaching 1 and 0. Based on the above conditions, the problem is simplified to the nearest neighbor principle. That is, if one considers many points that are adjacent to a given point, the principle suggests that one should choose a point with the minimum distance to the given point. Based on the principle of the nearest neighbor, the optimal intermediate value varies with the data distributions. Therefore, the value 0.5 is not appropriately considered as the optimal point. In a sense, the meaningful value 0.5 lacks a certain adaptation for different data distributions. In order to extend the model, another variable value between 0 and 1, which can not only avoid the condition that one threshold is greater than 0.5 but also minimize the total cost, is proposed to replace the single value 0.5. Therefore, in this paper, a general three-way approximation of a fuzzy set with a variable value is defined. In some special cases, the variable will evolve into the value 0.5. That is, compared with the rough Bayesian model and parameterized rough set model, the proposed model can evolve into the original model and is applicable to different data distributions. Then, according to this new definition, a novel loss objective function is constructed. In addition, the relationship between the required thresholds and the variable is discussed in detail. Finally, the obtained thresholds are taken into the new loss objective function, and the question of searching the optimal membership grade is converted into an optimization problem. In order to address the optimization problem, particle swarm optimization is introduced to establish an effective heuristic algorithm to obtain an optimal value. Compared with other optimization algorithms, the basic idea of the PSO algorithm is inspired by social behavior of bird flocks to initialize a set of random solutions, and an iterative method is used to search for the optimal solution. Each possible solution is expressed as a particle in a group, and the position vector, velocity vector, and adaptation are three essential characteristics of each particle. All particles are flying at a certain speed in the search space, and there will be a global optimal solution by tracking the current optimal solution. Because of the simple concept, easy implementation, and quick convergence, the PSO algorithm has attracted much attention and been widely applied to fuzzy sets. In summary, our contributions in this paper are as follows. Based on the theory of shadowed sets and the decision-theoretic model, the concept of a general three-way approximation of a fuzzy set is proposed to replace the single value 0.5 with a variable value between 0 and 1. Then, by introducing parameters that can characterize the cost of the elevation and reduction operations in shadowed sets, a new loss objective function is established. From the perspective of the decision-theoretic rough set, the relationship between the required thresholds with different significance and the variable is obtained by introducing the idea of three-way decisions. The required pair of thresholds that is calculated by the proposed model in this paper need not satisfy the condition that their sum equals one and also need not satisfy the condition that one threshold is greater than 0.5. The main difference is that the proposed model is just a special model of the general framework. More importantly, the relative properties of the membership grade are discussed more deeply. The PSO as a heuristic algorithm is first introduced to search for the optimal value by minimizing the total cost. In addition, a series of simulation experiments validate that the proposed model can not only evolve into the earlier model but also provides richer insight into data analysis. The remainder of this paper is organized as follows. In Section 2, many preliminary definitions are reviewed briefly. In Section 3, the cost of the elevation and reduction operations in shadowed sets are presented. In Section 4, the relationship between the required thresholds and the variable value is discussed in detail. In Section 5, the related experimental results and analysis are provided. Finally, some conclusions regarding the advantages of the proposed model are drawn in Section 6.", "section": "Methodology", "doi": "10.1016/j.ins.2018.10.051", "references": [101345952, 1577668191, 1964228588, 1969535228, 1977880445, 1988695218, 1997362234, 1998965536, 2006873874, 2007929615, 2012313887, 2037053120, 2039742983, 2050237791, 2050515685, 2052722125, 2067145099, 2070542280, 2070813883, 2078757499, 2080797159, 2084718108, 2089923511, 2114832876, 2136191688, 2142185590, 2154437129, 2294062232, 2340020088, 2613659757, 2620114837, 2752073986, 2770194470, 2912565176, 2990482619]}
{"paragraph": "Tactile sensing is important for understanding human perception and motion, controlling robotic hands and fingers, navigating hand manipulations and tool operations, executing input on user interfaces, and developing haptic systems. For example, artificial skin applied to robots enables advanced haptic communication among humans, robots, and environments. In addition, tactile sensing during surgical operations is an important yet challenging topic for safe operation by human–machine collaboration. In sports science, mechanical interactions between the human body and various instruments are measured to analyze physical abilities. Hence, tactile sensing is a fundamental technology for capturing the actions of real-world objects. Force, pressure, stress, deformation, and contact area are considered as the primary variables for capturing the important features of mechanical interactions. In particular, the use of tactile imaging technology to visualize pressure or stress distribution on an object is valuable for indicating appropriate behavior in sports training, mobile biomonitoring in medical diagnostics and health care, designing optimal apparatus for rehabilitation, and stably grasping an object with a robotic hand. A common approach for tactile imaging is to use arrayed pressure-sensitive sensors. Although the method promises accurate pressure imaging, the fabrication process is complicated due to the complexity of the sensor structure and wiring. To enhance the design flexibility, a multilayer structure is used for resistive and capacitive tactile sensors. However, spatial resolution is limited due to the number of sensing elements. Camera-based sensing is also an effective method for detecting force-related information without any contact devices. However, the limitation of the workspace and the occlusion in the field of view pose inevitable issues. Recently, electrical impedance tomography has been used for developing soft and flexible tactile sensors. These methods utilize electrodes located on the border of a thin conductive sheet, which can respond to localized pressure via local changes in conductivity. Some studies investigated electrode positioning and drive patterns to improve the sensing performance. Others proposed a method for estimating multidirectional strain using anisotropic electrical impedance tomography. Although these methods do not need wires or sensing elements in the contact region, pressure-sensitive materials are required for creating sensors. Therefore, the fabrication process for these types of sensors is complicated, and adjusting their sensitivity and spatial resolution is challenging. Moreover, the characteristics of these sensors depend on the electrical properties of the materials, and it is difficult to use these sensors in the versatile geometries of a detector. One study proposed a touch-sensing method for the versatile geometries of a detector using electric-field tomography and machine learning. Although this method enables position-based touch recognition, performance differs among individuals and pressure distribution is not measurable. In this paper, we develop a universal tactile imaging method to obtain pressure distribution. From the potential at the electrodes, we estimate the electrical boundary conditions related to the contact state of electromechanically coupled conductive objects. Unlike previous electrical impedance tomography-based tactile imaging, the sensor comprises electromechanically coupled driving and probing layers. To estimate pressure distribution, the system solves an inverse problem to find the electrical boundary conditions related to contact pressure distribution. This paper expands upon earlier work in several important ways as it proposes a new imaging algorithm and an assessment for tactile imaging. The main contributions of this paper are as follows. To develop a tactile imaging sensor without using pressure-sensitive materials, a novel pressure imaging method using the electromechanical coupling of two conductors is proposed. With this strategy, a thin, flexible, and low-cost tactile sensor can be fabricated using arbitrary conductors. Thus, the sensor's sensitivity and pressure estimation accuracy can be greatly improved. To successfully establish the design flexibility of tactile imaging sensors, a tomographic approach is proposed for estimating the contentious pressure distribution of the electromechanically coupled conductors. As a potential application of the proposed method, we implement the sensor on a three-dimensional printed object using conductive paint.", "section": "Introduction", "doi": "10.1109/TIE.2018.2879296", "references": [1548071717, 1965890979, 2031763660, 2042081740, 2050609926, 2073060332, 2097381457, 2098253469, 2103017125, 2119056404, 2126358124, 2137462734, 2171130677, 2296120231, 2611427051, 2626149096, 2750736961, 2794327637, 2795198316]}
{"paragraph": "The standard definition of fuzzy sets identifies them with pairs, where X is a crisp domain and a membership function is defined on X and valued in a suitable structure L of membership degrees—often the real unit interval, which will be our choice throughout this paper. We then speak of a fuzzy subset of the crisp domain X or a fuzzy set on X. It can be noticed that even though the domain X can in general vary, and so fuzzy sets involved in a given construction can generally have different domains, the usual fuzzy set-theoretical operations (such as the unions or intersections of fuzzy sets) and relations (such as inclusion or equality) are traditionally defined only for fuzzy sets on the same domain X. One may then wonder if and how unions and intersections, or any other fuzzy set-theoretical relations and operations, can be defined for fuzzy sets with different domains. The common—and often implicit—method of circumventing the issue of different domains is to choose a sufficiently large reference set X so that the domains of all fuzzy sets under consideration are subsets of X, and extend each membership function to the whole domain X by setting its value to zero for all elements outside its original domain. Then all of the fuzzy sets become defined on the same domain, and the standard, common-domain definitions of fuzzy set-theoretical operations can be applied. We will call this method the unification of domains. It can be claimed that if supplemented with the unification of domains, the theory of fuzzy sets on a fixed domain is sufficiently general for all common purposes, and so the variability of domains need not be considered. In Section 2 of this paper we are, nevertheless, going to argue a case that the unification of domains is not always harmless; and that, therefore, the actual domains of fuzzy sets need sometimes be tracked and taken into account when applying fuzzy set-theoretic operations. The resulting problem of how to define suitable operations for fuzzy sets with different domains is examined in Section 3. In Section 4 we discuss several ways of representing fuzzy sets with variable domains and opt for a modified vertical representation as the most flexible option. The resulting technical apparatus of variable-domain fuzzy set theory will be described in the separate second part of this two-part paper. The capability of the variable-domain approach to resolve the issues raised in Section 2 is assessed consecutively in the closing sections of both parts of the paper.", "section": "Related Work", "doi": "10.1016/j.fss.2018.11.002", "references": [1898259180, 1966322836, 1980724971, 1989393769, 1991673847, 2009738227, 2015943153, 2025256895, 2054789666, 2066915024, 2083449452, 2087435110, 2118819262, 2912565176, 2943118972]}
{"paragraph": "Many real-world applications, such as engineering design, air traffic control, nurse rostering, and car controller optimization, require optimizing more than three objectives at the same time. These kinds of multi-objective optimization problems are usually denoted as many-objective optimization problems. Generally, MOPs, including MaOPs, can be mathematically represented where the decision variable vector belongs to the feasible region of the search space. It consists of m objective functions, and the objective space. An MOP is called an MaOP if m is greater than 3. Given two solutions and their corresponding objective vectors, one is said to dominate the other if and only if the objective values are less than or equal for every objective and not equal overall. If there does not exist any other solution that dominates a given one, then it is defined as a Pareto optimal solution. The set of all Pareto optimal solutions is called the Pareto set, and its corresponding objective vector set is called the Pareto front. In recent decades, a variety of multi-objective evolutionary algorithms have been proposed to deal with MOPs. Usually, an MOEA aims to find a tradeoff among different objectives and then provide a well-representative set of Pareto optimal solutions from which decision makers choose a final solution according to their preferences. However, the search ability of general MOEAs is considerably deteriorated when more than three objectives are considered at the same time. The main difficulties include: as the number of objectives increases, the Pareto dominance may lose selection pressure towards the Pareto front since the solutions obtained in each generation are more likely to be non-dominated with each other; aiming at finding a set of non-dominated solutions to approximate the entire Pareto front as much as possible, traditional MOEAs may require a large number of solutions to approximate the entire Pareto front, which becomes a hyper-surface in an MaOP; and the deterioration in visualization of non-dominated solutions makes it difficult for decision makers to choose a final solution. Facing these challenges, recently much effort has been applied to address MaOPs. The current work in evolutionary many-objective optimization can be roughly partitioned into two categories: algorithm enhancement and problem simplicity. Algorithm enhancement focuses on improving the ability of current general MOEAs to search for Pareto optimal solutions of an MaOP. In other words, they address MaOPs by directly designing new specialized algorithms to handle the difficulties that traditional MOEAs meet in MaOPs. In this category, one method is to modify the Pareto dominance, a direct strategy to decrease non-dominated solutions in each generation and increase selection pressure towards the Pareto front. However this method may decrease the diversity of population. Therefore, some works use different fitness evaluation mechanisms to replace Pareto dominance during selection. In these methods, the Pareto dominance relation is avoided so that the search ability of traditional MOEAs is not severely deteriorated when the number of objectives increases. A typical approach uses performance indicators, such as hypervolume, and multiple indicators, to evaluate and select solutions. However, a high computation cost for computing hypervolume casts a shadow on this method. Another approach uses the decision maker’s preference to select a specific region of the Pareto front and then deploys MOEAs to search for non-dominated solutions in this small region. The decomposition based approaches or reference-vector-guided approaches have also attracted much attention. Problem simplicity addresses MaOPs by simplifying the original optimization problems and then using existing MOEAs to deal with the simplified problems. Usually an MaOP is converted into an MOP or a set of MOPs with fewer objectives. In this category, the key issue is how to reduce the number of objectives. Principal component analysis has been used to identify the most essential objectives from the original objective set. Another idea has been proposed to find a minimum objective set that either preserves the Pareto dominance relation or just slightly changes it according to a predefined error. Some methods sort each objective based on its conflict with other objectives, and then select the top conflict objectives to optimize. Other approaches have used mutual information and a clustering method to remove redundant objectives iteratively. An unsupervised feature selection method has also been applied to identify the most conflicting objectives. Some researchers extracted the nonlinear correlation information entropy and used it to measure the linear and nonlinear correlation between different objectives. Then the most conflicting objectives were chosen as the objectives in the running process. Others constructed new objectives as linear combinations of the original objectives by maximizing the conflict between the reduced objectives. A clustering method has been proposed to reduce the objectives. This paper follows the idea of constructing new objectives as linear combinations of the original objectives. As in previous work, the Pareto optimal solutions of the simplified MOPs with reduced objectives may only cover parts of the Pareto front of the original MaOPs. Furthermore, to find the optimal weight vectors, some additional optimization problems have been introduced, which may make the problem more complicated. This paper investigates a novel method to compute the weight vectors, which works by constructing a reduced objective set that provides an approximation to the Pareto fronts of the original MaOPs. The basic idea is to use a fuzzy clustering method to partition the objectives based on the data collected in the previous search. The membership degrees denote the relationships between the objectives and they can be naturally used as the weight vectors. Extensive experiments have been conducted to demonstrate that our method has the most stable performance and is competitive with some well-known methods for dealing with ill-posed MaOPs. The rest of this paper is organized as follows. Section 2 introduces the proposed objective extraction method via fuzzy clustering, and then incorporates it into a traditional MOEA. Section 3 reviews the ill-posed benchmark function, then conducts extensive experiments to analyze the feasibility and necessity of computing valid weight vectors for constructing a reduced objective set, and finally verifies the algorithm’s performance and compares it with some state-of-the-art methods. Finally, the paper is concluded in Section 4 with some future work remarks.", "section": "Methodology", "doi": "10.1016/j.ins.2018.11.032", "references": [1493761729, 1497465174, 1525375343, 1591635931, 1595205399, 1605810911, 1607221389, 1733107040, 1842155679, 1868857639, 1969955637, 1980591224, 1999729620, 2018712218, 2020320008, 2025525377, 2038420231, 2040622444, 2074921660, 2110828487, 2116858902, 2120091775, 2126105956, 2136386226, 2140886193, 2143185749, 2143381319, 2166898117, 2312339748, 2343601797, 2344880386, 2430564265, 2521918431, 2546299924, 2548746833, 2735649024, 2755719112, 2775348664, 2781380637]}
{"paragraph": "It is undisputed that the fuzzy sets introduced by Zadeh play an important role in modeling imprecision. Fuzzy set theory has been analyzed and applied all over the world. The concept of a fuzzy number was introduced as a convex fuzzy subset of the real line; as a consequence, the arithmetic on fuzzy numbers called fuzzy arithmetic was formulated. A fuzzy set models vagueness by a membership function on X. The value is the degree of membership of an element to the concept. The fuzzy set is characterized by core and support, which are the set of elements with a degree of membership equal to 1 and the set of elements with a degree of membership greater than 0, respectively. Therefore, a fuzzy set can be approximated by a set with a three-valued degree of membership: full, greater than 0, and lacking membership grade. One of the approximations of the fuzzy set is a shadowed set. The shadowed set is a three-valued set modeling vagueness. The three degrees of membership of a shadowed set are: full membership, full exclusion, or an interval that is called a shadow. A shadowed set on X is a set-valued mapping. The approximation of a fuzzy set by a shadowed set is an elevation of the degree of membership at or above a determined threshold to full membership, a reduction of the degree of membership at or below a second threshold to full exclusion, and an assignment of the degree of membership between these thresholds to the interval. A method for the determination of the threshold values is based on optimization of an objective function. Another approach obtains the fuzziness of a fuzzy set by using a gradual number to calculate the considered threshold values. Some compute the thresholds for optimizing a decision cost. Others propose three-way approximations of a fuzzy set by three principles. There is also an approximation of a fuzzy number into the shadowed set by means of direct formulas for four points that characterize the shadowed set. Some use the shadowed set to simplify a fuzzy number. Dynamic fuzzy sets can be approximated by shadowed sets, and an algebraic approach to the shadowed set has also been proposed. Shadowed set theory has been used to optimize the partition thresholds for clusters in multigranulation rough-fuzzy clustering. The shadowed set that is obtained from the fuzzy number is called a shadowed number. It has been stated that Pedrycz’s shadowed set can be viewed as a specific case of three-way decisions. In the theory of three-way decisions, the universe is divided into three regions. In practice, the models of three-way decisions can be generalized by shadowed sets. So, in making multi-criteria and multi-objective decisions, a utility model as an overall evaluation function has to be constructed. Arithmetic operations on shadowed numbers are needed for more complicated utility models of three-way decisions. There are recent papers with multi-attribute or multi-criteria problems where calculations on uncertain variables are made. Shadowed numbers can be used for hybrid multi-attribute decision making and arithmetic operations can be defined for a limited set of shadowed numbers. Arithmetic operations on shadowed numbers can be applied in granular computing. Granules of information can be described with the use of shadowed numbers as they are with other sets such as intervals, fuzzy sets, or rough sets. There are many types of arithmetic for uncertain variables; a few examples of interval arithmetic are reviewed. In standard interval arithmetic, the calculations are made on the border values of the intervals, and results are in the form of intervals. This is similar in extended interval arithmetic, but here the operations on the border values of an interval are dependent on the functions that characterize the interval. These are the width function, the quotient of the endpoints of the interval, and a function which indicates whether an interval is positive or negative. In distributive interval arithmetic, in order for the distributive law to hold, the notation for an interval is modified. To ensure distributivity, the absolute value of an interval described by its center and radius is defined. The basic algebraic operations are defined on intervals with these indicators. In constrained or instantiation interval arithmetic, an interval is described by a non-decreasing linear function whose domain is the interval; the results of operations on intervals are also intervals. In affine interval arithmetic, the value of an interval is expressed by a polynomial called an affine expression. The polynomial consists of a central value, noise symbols, and floating-point coefficients. The multidimensional relative distance measure interval arithmetic describes an interval as a set of values with a variable in a given range. A direct result of operations in this method is a multidimensional granule of information which can be represented with such indicators as: a span, a distribution of cardinality measure, or the center of gravity. The lack of an arithmetic that defines basic operations on any shadowed numbers and can be used in both the theory of three-way decisions and in granular computing motivates the authors to propose a shadowed arithmetic. There is a need to make more precise calculations with shadowed numbers as with any other form of uncertain numbers. The aim of this study is to define a shadowed number and its shadowed arithmetic. The two kinds of shadowed arithmetic proposed in this paper are called standard shadowed arithmetic and multidimensional relative distance measure shadowed arithmetic; these are based on standard interval arithmetic and multidimensional relative distance measure interval arithmetic, respectively. These represent two different approaches to the obtained results. One results in a shadowed number, whereas the direct result obtained with the multidimensional method is a granule of information about the solution. A granule of information calculated with the multidimensional method has the form of a multidimensional formula from which a shadowed number can be obtained. These arithmetic operations on shadowed numbers can be used in the theory of three-way decisions and in granular computing. The multidimensional approach to the concept of uncertainty calculation results is free from the phenomenon of strongly increased entropy of solutions that is typical for the standard arithmetic of uncertainty. The innovation offered by this paper includes two concepts of arithmetic: standard shadowed arithmetic and multidimensional relative distance measure shadowed arithmetic, and the associated definitions of the shadowed number. This text also presents and proves properties of the basic operations in both methods, and the usage of both methods for solving algebraic problems. The rest of the paper is organized as follows: in the next section, the theoretical foundations of shadowed numbers and the two shadowed arithmetics are presented. Then, properties of the basic operations for both methods on shadowed numbers are discussed. Next, examples that show the differences between the two shadowed arithmetics are presented. Finally, some conclusions and directions for further research on the approaches introduced here are presented.", "section": "Introduction", "doi": "10.1016/j.ins.2018.11.047", "references": [101345952, 1461637024, 1964228588, 1979464351, 1985864380, 1998965536, 2006136802, 2006873874, 2036102558, 2038384345, 2040512820, 2052608046, 2052722125, 2058445758, 2070813883, 2078757499, 2089923511, 2092092154, 2114832876, 2131798279, 2153676086, 2297889545, 2413485267, 2488327324, 2591008318, 2620114837, 2756505297, 2806445482, 2912565176]}
{"paragraph": "Real-time systems are widespread nowadays and are quickly evolving. To achieve strong results on the correctness of system, it often requires, from the beginning of system design, modeling formalism to describe systems at an abstract level and corresponding verification techniques to reason about abstract system behaviour. In some sorts of real-time systems like mobile distributed systems or Cyber Physical Systems, agents’ behaviour often relate not only to real time, but also to physical or logical locations. For example, in Intelligent Railroad Crossing System, the smart train should inform the smart gate that it is coming when it arrives at the location at time t. Such constraint—that an event must be triggered at explicit time and location—is called spatio-temporal consistency. At high-level of system design, a formalism, where such constraints can be easily expressed, is required to help engineers design more reliable programs when it comes to refinements. Right specification languages are also needed to give an easy expression of some important properties that are of interest. Spatio-temporal consistency language is a modeling formalism proposed for describing spatio-temporal behaviour of real-time systems. It is a process algebra-like modeling language, and looks like an extension of CSP and CCS, with location taken as a primitive in its syntax. It aims at modeling system at high level. Not like other process algebras extended with location or time, like timed CSP, timed CCS, ambient calculus, or π-calculus, this language addresses a stronger constraint between time and location—spatio-temporal consistency—rather than only considering one of them alone. For example, in timed CCS an event can be triggered at an explicit time, while in ambient calculus an event can be triggered at a specific location. In the spatio-temporal consistency language, an event can only be triggered at some explicit time and location. Clock Constraint Specification Language is a specification language based on logical clocks. It was initially proposed as a companion language for the modeling language MARTE, and now has been fully developed as an independent language. Compared with traditional specification languages like LTL, CTL, TCTL and PSL, logical clock provides an intuitive way to express event sequences and its chronometric attributes. The logical and chronometric constraints between events can be easily expressed by the relationships between logical clocks. Comparing to traditional specification languages, this language alleviates the burden for specifying some crucial safety properties, since it provides a library of off-the-shelf often-used property patterns. A comparison of expressiveness between it and other languages has been analyzed. In order to give a verification support for the spatio-temporal consistency language, and inspired by the MARTE-based framework, a verification framework is proposed for modeling and verifying spatio-temporal systems—a special type of real-time systems where agents’ behaviour is related not only to real time, but also to locations. Compared to other frameworks for real-time systems, this framework focuses on capturing system behaviour at high level, with the advantage of addressing the spatio-temporal consistency of system behaviour at the syntax level. And it supports specifying and verifying logical and chronometrical timing constraints and possibly some carefully selected spatial constraints in the future in logical clock style, which could be complex or difficult to be expressed by other traditional specification languages. In this framework, in order to connect the spatio-temporal and logical clock components, a linking theory and a model checking framework are proposed to verify specifications. Some concepts are formalized and an observational mapping is built between clocks and events. To carry out model checking, theory and algorithms are proposed to translate both modeling languages into their equivalent Timed Automata. A general analysis for the computation complexity of the whole verification process is made, and it is shown that the framework runs nearly as fast as the traditional model checking process, despite the translation process. In order to show that the proposed verification framework is applicable, the translation into UPPAAL Timed Automata, a model checking tool widely used in academia and industry, is proposed. Technically, this work can be seen as a combination and extension of previous work, where the verification aspect of the modeling languages has been explored separately. In earlier work, Timed Automata semantics of the spatio-temporal language was introduced. The translation was given inductively based on the syntax structure. In this paper, a more general approach is taken by dividing configurations into regions. A translation strategy was previously proposed to encode logical clocks into Timed Automata and examples were given to illustrate the model checking scheme in UPPAAL. There the Timed Automata of logical clocks is based on untimed synchronous models, which is not suitable for this framework since the spatio-temporal language is asynchronous. Improvements are made by translating it into asynchronous Timed Automata models. A full translation from logical clocks into UPPAAL Timed Automata is given. The proposed model checking framework is partially based on the idea of the previous verification scheme, but is quite different as it considers the combination with the spatio-temporal modeling and the logical clock generators. This paper is organized as follows. Section 2 introduces some backgrounds about the modeling languages and Timed Automata. In Section 3, a spatio-temporal system—intelligent railroad crossing system—is introduced as an example and modeled using the proposed language. In Section 4, the main contribution is presented—the linking theory that connects the two modeling languages and the model checking framework. In Section 5, model checking is carried out in UPPAAL, based on the translation into Timed Automata. In Section 6, three safety properties of the railroad crossing system are verified in UPPAAL as an example. Section 7 concludes the work and discusses possible future works. Section 8 compares with some similar verification frameworks proposed in recent years.", "section": "Related Work", "doi": "10.1007/s11704-018-7054-8", "references": [105975728, 1491196069, 1590675544, 1962072139, 1968765218, 1973501242, 1982535211, 1998017916, 2017863278, 2021418494, 2044856809, 2053137492, 2078877482, 2081938726, 2090051150, 2101508170, 2110425399, 2115309705, 2126860147, 2128110618, 2128932399, 2132609679, 2137453611, 2137865376, 2152192540, 2160258739, 2912003593]}
{"paragraph": "With the rapid development of artificial intelligence technology and network technology, the internet of things has gradually become the mainstream of social development in the future. Under this background, the trade retail industry needs to establish its customer relationship network in combination with artificial intelligence technology. At the same time, it needs to conduct law mining in combination with customer selection behavior in network, and carry out personalized excavation of customers under the support of data mining technology to help customers make decisions. On this basis, it can effectively enhance the customer experience. The research on intelligent customer network has entered a climax since 2010, and related research also provides the basis for the creation of this article. The intelligent customer relationship network usually uses the customer's equipment's movement trajectory data, customer platform operating data, customer network base stations, and other content as customer behavior data. Using this data, researchers started relevant research. Some achieved a goal of recommending to the taxi driver the passenger sequence with the greatest revenue through in-depth analysis and excavation of GPS trajectory data in taxis. Others designed and implemented a time-awareness system that can be used to personalize the taxi drivers travel route with the greatest benefit per unit of time. Based on the different advertising platforms, an advertisement delivery system suitable for mobile web pages and mobile phone apps was proposed by analyzing customer location and related situational information and fully exploiting the mobility of customers in the mobile commerce system. A personalized travel package recommendation system based on tourist interest preferences was designed, which can recommend a set of personalized and best-suited attraction collections for tourists. Mobile customer's check-in data was studied and analyzed to obtain various features of the location social network; based on this, a location-based recommendation algorithm was designed and implemented. With the progress of research, many personalized recommendation systems for mobile clients have also been successfully launched, such as the Facebook mobile application of personalized push ads, the personalized Bizzy recommended by local shops, and the personalized reading system Zite. A detection method for mobile app ranking fraud was proposed by exploring personalized preferences mining method for mobile customers based on context awareness. Security privacy issues under personalized recommendation technology were discussed, and a mobile app recommendation algorithm was proposed to protect customer information security. Based on statistical analysis of a large number of microblog customer data, a method for personalizing popular micro topics was proposed by calculating similarities between microblog customers and micro topic. In addition, in terms of data sparsity and cold-start problems faced by collaborative filtering, the use of the K-nearest neighbor method was proposed to map attribute-feature and calculate the feature vectors of new customers and new projects. A combination of data migration and data clustering was proposed to solve the system cold start problem. To solve the problem of sparseness in collaborative filtering algorithms, a clustering method based on the attributes of the project and the use of the mean of the project categories to fill in the null values in the original scoring data was proposed. At present, major e-commerce platforms at home and abroad have developed their own mobile terminals. However, the research and application of personalized recommendation systems for mobile platforms is still in its infancy, and there is still room for improvement in its recommendation quality and operating efficiency. The idea of considering the trust between customers in the recommendation process was first proposed. The trust between customers is established through the displayed customer trust evaluation and debilitating spread. The trust is divided into reliability trust and decision trust. The reliability trust is the subjective probability that entity A acts according to entity B's expectations, and decision trust refers to the subjective degree of relative security feeling obtained by an individual trusting a certain entity in a certain environment. One approach uses the ratio of the number of customer recommendations to the total number of recommendations as the degree of trust between customers and applies this calculation method to the recommendation system, where the confidence value ranges from 0 to 1. A trust model based on fuzzy logic representation was also proposed based on the fuzzy nature of trust relationships. Benefiting from the development of Internet of Things technology and data mining technology, the spread of consumer trust has become multi-directional. It has been mentioned that all the characteristics of s-commerce (except for economic feasibility) had significant effects on trust and that trust had significant effects on purchase intentions. Hence, the characteristics of consumer trust communication and behavior decision-making under the Internet of Things are necessary to study. Based on the above analysis, we can see that the current decision model based on the Internet of Things to build a customer relationship network is less researched, and most of them are recommending unilateral information to customers based on personalized recommendations. Therefore, based on the Internet of Things technology, this study builds a more complete customer relationship network based on personalized recommendations, and adopts an improved collaborative filtering recommendation algorithm as a basis for decision models to extract contextual features that characterize customer trust. At the same time, this research uses the analytic hierarchy process to complete the model building process, helps customer relationship network service objects to provide decision support, completes product information recommendation, solves new customer cold start problems, and improves existing scoring prediction formulas. Therefore, this study fully considers the impact of customer trust relationships on the recommendation results in the scoring prediction process. The remainder of this article is organized as follows. Section 2 is the literature review and methodology. The experiment results are showed in Section 3. Section 4 presents discussion in the experiment results. The conclusion is presented in Section 5.", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2018.11.013", "references": [63326921, 165056234, 1727274110, 1874148276, 1943579973, 1994223401, 2015177282, 2042874218, 2043472477, 2056660873, 2061222800, 2103281268, 2108142795, 2128991589, 2228664849, 2310994956, 2773841654, 2788552370, 2789731659, 2790855116, 2893840913]}
{"paragraph": "In most real-world applications, uncertainties cannot be avoided. Many tools or methods have been proposed to deal with different kinds of uncertainties. For example, statistical methods are used to cope with randomness in many applications, whilst fuzzy sets are utilized to handle cognitive uncertainties. Since the appearance of the fuzzy set, it has been applied in many areas including the modelling, prediction and control domains. However, fuzzy membership functions are of a quantitative nature, which makes them inappropriate for applications that have a qualitative nature. It is practically important to approximate a fuzzy set by using several levels of grades to achieve an acceptable trade-off between accuracy and cost. Three-way approximations provide us with an important means to achieve this objective. Three-way approximations of fuzzy sets can be made through a pair of thresholds. The concept of shadowed set approximates a fuzzy set by mapping its membership grades to the truth values 0, 1 or to the unit interval. Decision-theoretic three-way approximations of fuzzy sets have been introduced in which the required thresholds are computed by minimizing the decision cost. An optimization-based framework for constructing three-way approximations of fuzzy sets has also been proposed. The three-way approximations can be interpreted in terms of the positive, negative and boundary regions within the theory of three-way decisions and have been studied by various researchers. As a special case of the three-way approximations of fuzzy sets, the shadowed set does not have precise numeric membership values. Its distribution is partitioned into three distinct zones, which are the core zone, the shadowed zone, and the exclusion zone. Core and exclusion zones have truth values, whilst the shadowed zone is an entire unit interval perceived as a zone of uncertainty. In recent years, various studies on shadowed sets have addressed the construction of shadowed set models. Generally, there exist two ways to generate shadowed set models. The first aims to induce shadowed sets from existing fuzzy sets. Extensive discussions have been made on the calculation of the optimized threshold for inducing shadowed sets from triangular, Gaussian and parabolic fuzzy sets. To remedy the deficiencies in existing methods and retain the total amount of fuzziness and the localized redistribution throughout a universe of discourse, a new algorithm for the induction of shadowed sets based on gradual grades of fuzziness was introduced along with analytical formulas to calculate the threshold values. Shadowed fuzzy sets have also been defined and their construction demonstrated. Shadowed set approximations of fuzzy numbers have been presented through a newly defined approximation measure, showing that the shadowed set approximation preserved the expected interval and width of a fuzzy number. Error-based three-way approximations of shadowed sets for dynamic fuzzy sets have been constructed and an alternative decision-theoretic formulation for calculating the thresholds has been presented. A principle of trade-off in games has also been applied to determine the thresholds of three-way approximations in the shadowed set context. The other way to generate shadowed set models is by shadowed clustering algorithms. The shadowed C-means clustering algorithm was developed which can automatically choose the threshold parameters and was applied to satellite image segmentation. A shadowed set-based rough-fuzzy clustering method was proposed, which automatically decides the threshold parameter and then determines the lower bound and boundary region of each cluster. A spatial shadowed C-means clustering algorithm was proposed by implanting local spatial information in the estimation procedure of membership values, and was successfully applied to image segmentation. A new technique of rough-fuzzy clustering based on multi-granulation approximation regions was developed to deal with the uncertainty associated with the fuzzifier parameter, and an active three-way clustering method via low-rank matrices was investigated to satisfactorily address the uncertain relationship between an object and a cluster. In the paradigm of Computing with Words, the first step is to construct models of the linguistic words. Different methods have been proposed to model linguistic words by fuzzy sets or membership functions. The fuzzy Delphi method has been used for membership function fitting and applied to a human resources assignment problem. The maximum entropy principle has been adopted to automatically determine the membership functions for linguistic words. A fuzzy measure method has been used for automatic membership function estimation. An interactive fuzzy multi-objective approach has been used to generate modified S-curve membership functions and applied to a transportation planning problem. These methods are all based on the collected crisp data. On the other hand, for a specific word, if one person is asked to provide a value for it, usually one interval will be given owing to the vagueness of the word and the uncertainty of the person surveyed. In addition, different people usually have different understandings of the same word. As a result, if different people are surveyed, different intervals will be obtained. Researchers have discussed how to construct interval type-2 fuzzy set models for linguistic words using this type of interval data. A type-2 fuzzistics method, which represents the interplay between fuzzy sets and statistics, has been proposed for constructing symmetric models based on the collected interval data. Conversion of interval data to Gaussian fuzzy sets via fuzzy statistics and membership function fitting has also been discussed, and cloud models for words have been generated. The shadowed set theory provides us with another tool to model the uncertainties in the collected interval data. However, shadowed sets are usually induced from the existing fuzzy sets or are established by a shadowed clustering algorithm. To the best of the authors’ knowledge, there is no research focusing on the interval data driven construction of shadowed set models. Therefore, how to utilize such interval data to construct the shadowed set models for linguistic words remains to be studied. On the other hand, how to construct the shadowed set models directly from the interval data and which types of shadowed sets are appropriate to be chosen as word models must be addressed before deciding on the data driven modelling algorithm. Another important point is how to evaluate the constructed shadowed set models. This paper attempts to solve these issues and provide an effective interval data driven modelling algorithm for constructing the shadowed sets for linguistic words. Our study, which directly utilizes the interval data to construct the shadowed set models, provides a new perspective in the research areas of shadowed sets. The main contributions of this study are listed in detail as follows. Corresponding to the popularly used fuzzy sets for linguistic word modelling, four kinds of shadowed sets are introduced according to their shapes. These shadowed sets may include the most widely used ones in real-world applications, especially in linguistic word modelling applications. The normal, left-shoulder and right-shoulder shadowed sets correspond to the most widely used trapezoidal or triangular shapes in modelling and control applications. An interval data driven approach is proposed to achieve the modelling of shadowed sets for linguistic words. The proposed approach includes three main steps, which are the interval data pre-processing, statistics computation of the remaining interval data, and the shape and parameter determinations of the shadowed sets. Especially, in the statistics computation step, two different methods – the tolerance limit method and the percentile method – are presented. To test the reasonability of the constructed shadowed set models, both the uncertainty-capture capability and accuracy are taken into account, and three novel indices are proposed. The first index computes the proportion of the left and/or right-end points being included in the shadowed zone, whilst the second index considers the ratio of the core zone to the shadowed zone. The third index is a hybrid index, which is comprised of the first two indices to balance the uncertainty-capture capability and the accuracy. To verify the proposed method, it is applied to two applications which are the 32-word codebook example and the thermal feeling words modelling problem. In addition, comprehensive comparisons with other algorithms and analyses of the results are provided. The organization of this paper is as follows. In Section 2, introductions and definitions of shadowed sets will be given. In Section 3, an interval data driven method for constructing shadowed set models will be proposed. In Section 4, the proposed method will be applied to the 32-word codebook example and the thermal feeling word modelling problem. In this section, detailed comparisons and comprehensive analysis will also be given. Finally, in Section 5, conclusions will be made.", "section": "Introduction", "doi": "10.1016/j.ins.2018.11.018", "references": [101345952, 1516346799, 1549841259, 1570082825, 1843415386, 1981231764, 1984472447, 1988695218, 1998965536, 2002278745, 2006873874, 2010360766, 2020694587, 2023843216, 2039742983, 2052722125, 2061430293, 2070813883, 2078757499, 2089923511, 2091336043, 2093400038, 2114832876, 2117501337, 2149017759, 2153406165, 2154437129, 2171527822, 2238283829, 2297889545, 2398851267, 2435864146, 2475810011, 2620114837, 2792234535, 2806445482, 2884189343, 2884615103, 2887732625, 2912565176]}
{"paragraph": "Modern systems are composed of several distributed components that work in real-time to satisfy a given specification. This makes them difficult to reason about manually and encourages the use of formal methods to analyze them automatically. This in turn requires the development of models that capture all the features of a system and still allow efficient algorithms for analysis. Further, to bring formal models closer to real world implementations, it is important to design robust models, i.e., models that preserve their behavior or at least some important properties under imprecise time measurement. In this paper, we consider Petri nets extended with time constraints for modeling real-time distributed systems. For timed variants of Petri nets, many basic problems are usually undecidable or algorithmically intractable. Our goal is to consider structural restrictions which allow us to model features such as unbounded resources as well as time-deadlines while remaining within the realm of decidability and satisfying some robustness properties. Time Petri nets are a classical extension of Petri nets in which time intervals are attached to transitions and constrain the time that can elapse between the enabling of a transition and its firing date. In such models, the basic verification problems considered include: reachability, whether a particular marking can be reached in the net; termination, whether there exists an infinite run in the net; boundedness, whether there is a bound on the number of tokens in the reachable markings; and firability, whether a given transition is fireable in some execution of the net. It turns out that all these basic problems are in general undecidable for time Petri nets, though they are decidable for the untimed version of Petri nets. The main reason is that time Petri nets are usually equipped with an urgent semantics: when the time elapsed since enabling of a transition reaches the maximal value of its interval, a transition of the net has to fire. This semantics breaks monotony. Indeed, with a time Petri net, one can easily encode a two-counter machine, yielding undecidability of most of the verification problems. Decidability can be obtained by restricting to the subclass of bounded time Petri nets, for which the number of tokens in all places of reachable markings is bounded by some constant. Other ways to obtain decidability are by weakening the semantics or restricting the use of urgency. Another important problem in this setting is the question of robustness. Robustness can be defined as the preservation of properties of systems that are subject to imprecision of time measurement. The main motivation for considering this is that formal models usually have an idealized representation of time, and assume an unrealizable precision in measurement of time which cannot be guaranteed by real implementations. Robustness has been studied extensively for timed automata and more recently for time Petri nets, but decidability results are only known in a bounded-resource setting. The definition of the semantics plays an important role both to define the expressive power of a model, and to obtain decidability results. When considering unbounded nets, where multiple and a possibly unbounded number of tokens may be present at every place, one has to decide whether transitions should be considered as multiply enabled, and if so, fix a policy to handle multiple instances of enabling. This becomes even more complicated when real-time constraints are considered. Indeed, several possible variants for the multiple enabling semantics have been considered. In this paper, we fix one of the variants and consider time Petri nets equipped with a multi-enabling urgent semantics, which allows to start measuring elapsed time from every occurrence of a transition enabling. This feature is particularly interesting: combined with urgency, it allows for instance to model maximal latency in communication channels. We adopt a semantics where time is measured at each transition's enabling, and with urgency, i.e. a discrete transition firing has to occur if a transition has been enabled for a duration that equals the upper bound in the time interval attached to it. Obviously, with this semantics, counter machines can still be encoded, and undecidability follows in general. We focus on a structural restriction on time Petri nets, which restricts the underlying net to be free-choice, and call such nets Free-choice time Petri nets. Free-choice Petri nets have been extensively studied in the untimed setting and have several nice properties from a decidability and a complexity-theoretic point of view. In this class of nets, all occurrences of transitions that have a common place in their presets are enabled at the same instant. Such transitions are said to belong to a cluster of transitions. Thus, with this restriction, a transition can only prevent transitions from the same cluster to fire, and hence only constrain firing times of transitions in its cluster. Further, we disallow forcing of instantaneous occurrence of infinite behaviors, that we call 0-delay firing sequences. This can easily be ensured by another structural restriction forbidding transitions or even loops in time Petri nets labeled with such constraints. Our main results are the following: we show that for a free-choice time Petri net under the multi-enabling urgent semantics, and in the absence of 0-delay firing sequences, the problem of checking firability of a transition and of termination are both decidable. The main idea is to show that, after some pre-processing, we can reduce these problems to corresponding problems on the underlying untimed free-choice Petri net. More precisely, we are able to show that every partially-ordered execution of the underlying untimed net can be simulated by the time Petri net, i.e., it is the untimed prefix of a timed execution of the time Petri net. To formalize this argument we introduce definitions of untimed and timed causal processes for unbounded time Petri nets, which is another contribution of the paper. Finally, we address several robustness questions. The problem of robustness for time Petri nets has previously been considered, but shown decidable only for bounded classes of nets. We show that the problem of robustness of firability with respect to guard enlargement, i.e., whether there exists a value such that enlarging all guards of a time Petri net by that value preserves the set of fireable transitions, is decidable for free-choice time Petri nets without 0-delay firing sequences. We also consider the same question for guard shrinking, i.e., existence of a value such that shortening the guards of a time Petri net by that value preserves the set of fireable transitions. We show that this problem is also decidable in our model. Finally, we consider robustness of termination with respect to guard enlargement or shrinking, and show that this question is decidable as well. To the best of our knowledge, these are the first decidability results on robustness for a class of unbounded time Petri nets. Related work. Verification, unfolding, and extensions of Petri nets with time have been considered in many works. Other than time Petri nets, a different yet common approach to integrate time to Petri nets is to attach time to tokens, constraints to arcs, and allow firing of a transition if all constraints attached to transitions are satisfied by at least one token in each place of its preset. This variant, called Timed-arc Petri nets, enjoys decidability of coverability, but cannot impose urgent firing, which is a key issue in real-time systems. A variant of Timed-arc Petri nets with place invariants is proposed in the TAPAAL tool. This allows for the modeling of urgency, but at the cost of decidability of coverability in unbounded nets. In time Petri nets, weak semantics have been proposed, where clocks may continue to evolve even if a transition does not fire urgently. With this semantics, time Petri nets have the same expressive power as untimed Petri nets, again due to lack of urgency, which is not the case in our model. Recently, variants of time and timed-arc Petri nets with urgency have been considered, where decidability of reachability and coverability is obtained by restricting urgency to transitions consuming tokens only from bounded places. This way, encoding of counter machines is not straightforward, and some problems that are undecidable in general for time or timed-arc Petri nets become decidable. The free-choice assumption in this paper is orthogonal to this approach and it would be interesting to see how it affects decidability for timed Petri nets with urgency. Finally, partial-order semantics have been considered in the timed setting. A notion of timed process for timed-arc Petri nets has been defined and a semantics to timed-arc nets with an algebra of concatenable weighted pomsets has been given. However, processes and unfoldings for time Petri nets have received less attention. An initial proposal was used to define equivalences among time Petri nets. Unfoldings and processes were refined to obtain symbolic unfoldings for safe Petri nets. The closest work to ours defines processes to reason about the class of free choice safe time Petri nets. However, this work does not consider unbounded nets, and focuses more on semantic variants with respect to time-progress than on decidability or robustness issues. This paper is an extended version of results presented previously. With respect to this first contribution, it contains all the proofs in full details, and an extended comparison between the single and multi-enabling of time Petri nets. It also establishes new results on robustness of firability and termination, that are now considered with respect to guard enlargement as well as shrinking. The paper is organized as follows: Section 2 introduces notations and defines a class of time Petri nets with multi-enabling semantics. Section 3 defines processes for these nets. Section 4 introduces the subclass of Free-choice time Petri nets and relates properties of untimed and timed processes. In Section 5, this relation is used to prove decidability of firability and termination for free-choice time Petri nets and, in Section 6 to address robustness of firability and termination to guard enlargement and shrinking. Section 7 discusses the assumptions needed to obtain decidability, and issues related to decidability of other problems in free-choice nets, before conclusion.", "section": "Related Work", "doi": "10.1016/j.jlamp.2018.11.006", "references": [63786410, 101037006, 1495551332, 1509494512, 1546958647, 1562433349, 1576317809, 1823152540, 1878074184, 1937565299, 1965237160, 1987557885, 2018780789, 2040633651, 2061288205, 2083567135, 2101508170, 2129140418, 2133170941, 2136315358, 2144938038, 2145580394, 2151612633, 2151720555, 2174804035, 2289622999, 2402880788, 2506850889, 2952981239]}
{"paragraph": "Route planning is important not only for our daily life, but also for business map engines like Google and Bing Maps. It is a key component in urban computing. Although the shortest or fastest paths are used commonly in road networks, they may be insufficient in some situations, while the popular route that refers to a path being travelled frequently is sometimes important. For example, drivers who travel in an unfamiliar city may prefer a popular path which may be safer with better traffic condition and road quality, and a taxi passenger may want to travel along a popular path in case of a roundabout trip. Moreover, since travel cost dynamically depends on the traffic conditions and other factors, such as traffic lights, people care more about the travel cost, i.e., how long it takes or how much it costs at the time they are leaving, so that accurate travel cost estimation will improve people’s satisfaction. With the development of intelligent transportation in cities, more and more GPS-equipped vehicles are running on the road networks. As a result, a large number of time-stamped GPS trajectories are consecutively generated. Based on the historical citywide trajectories, popular paths can be constructed by finding out how people usually travel between locations, and with the temporal information in the trajectories, it is possible to estimate the travel cost of paths. Route planning or driving direction planning has been studied in recent years and some influential works have been published. Some propose a framework to find out the practically fastest route at a given departure time based on a landmark graph learned from a large number of historical taxi trajectories. However, the fastest route is not always popular, some shortcuts may reduce the travel time but increase the risk and uncertainty of a trip. The work performs a two-stage routing algorithm based on the graph to find the fastest route. The first stage is to find the rough route represented by a sequence of landmarks whose travel time can be estimated by their model and the second stage is to find a practically detailed fastest route in the road network based on speed constraints. But the travel time of the detailed fastest route may be different from the estimated travel time at the first stage. Since there may exist several different paths between two landmarks and the estimated travel time is just the mean travel time of all possible paths. In most cases, the travel time of the detailed fastest route is less than the estimated travel time, which will cause inaccurate travel time estimation of the route. Another model estimates the travel time of a given path by using the optimal route concatenation which considers dependencies between road segments. However, it cannot be applied to route planning directly. To find the popular route with travel cost estimation, three challenges must be addressed. Data sparseness and coverage: due to the complexity of road network, we cannot guarantee that all roads have been covered by trajectories. Moreover, for each road, it is practically impossible that there are sufficient trajectories during all time intervals. We must thus contend with data sparseness to respond to users’ path queries between any locations at any time intervals. On the other hand, road network is not always available, it is unpractical to find the popular routes or roads by counting the support on each road. Thus, how to get the paths or roads from trajectories is meaningful. Travel cost modeling: different roads have different travel cost, because road quality and road condition are not the same. For example, the highway with less traffic lights travels faster than the roads with more traffic lights. Meanwhile, the travel cost of the same road also varies at different time. For example, the travel time at rush hour is usually longer than that at the non-rush hour due to traffic jam. Hence, an intelligent modeling for the travel cost on each road at different time that can benefit accurate travel cost estimation is a challenge, especially under the fact of data sparseness. Path cost estimation: the cost of a path can be estimated by summing up, however, the results are only accurate if the travel costs of roads are independent. In practice, the travel costs may be dependent due to intersection or turning. To derive accurate travel cost of path, the dependencies must be considered. Moreover, estimating the travel cost for a given path is not enough to answer a path query, so it is challenging to estimate the travel cost of paths while routing. In this paper, we aim at finding the popular route with minimal travel cost from source to destination and estimating the travel cost for this route. To deal with the above challenges, we devise a system to achieve this goal. Firstly, we construct a popular traverse graph based on the historical trajectories, where each node is a popular location, and each edge is a popular route between two locations. Subsequently, for each popular route in this graph, we use the minimum description length principle to model the travel cost for each popular route at different time intervals, so that each time interval has a stable travel cost. Finally, based on the graph, given a source-destination pair and a leaving time, for accurate travel cost estimation, we find the fastest popular route in consideration of the optimal route concatenation which considers the dependencies between road segments. The contributions of this paper are summarized below. We propose a novel structure, called popular traverse graph, from trajectories without road network information, which contains the popular routes between popular locations. We present a self-adaptive method using the minimum description length principle to model the travel cost on each popular route in the graph. We devise an efficient routing algorithm which combines optimal route concatenation with route planning on the popular traverse graph. We conduct extensive experiments upon a real dataset of millions of trajectories generated by more than ten thousand taxis over a month in Beijing. The results show that our method is both effective and efficient. Moreover, we implement and visualize our system through a mobile app. The remainder of the paper is organized as follows. Section 2 reviews the related work. Section 3 describes some preliminary knowledge. Section 4 describes the overview of our framework. Section 5 introduces the popular traverse graph, and the method to model the travel cost for each popular route. Section 6 details the routing algorithm. Section 7 reports the evaluation and a brief conclusion is given in Section 8. Compared with our earlier proceeding paper, this extended paper mainly claims following contributions. First, we define our problem more formally and devise and implement a cloud-mobile based popular route planning system for solving it, where the mobile app is designed. The architecture of our system is introduced in Section 4, and the detailed interactions of the mobile app is presented in Section 7.4. Second, in Section 6, we improve the efficiency of our routing algorithm by more efficient optimal concatenation computation, which enables faster response to user query. In addition, we discuss the time complexity of our routing algorithm. Third, we conduct more comprehensive experiments to validate our system. For example, we evaluate the popular traverse graph with different parameter settings and test the performance of our method on different popular traverse graph. In addition, we evaluate the efficiency of our improved routing algorithm compared to the method in our previous work. Moreover, a case study on the mobile app is illustrated.", "section": "Methodology", "doi": "10.1007/s11704-018-7249-z", "references": [1519770485, 1567097384, 1673310716, 1936915774, 1969483458, 1981398125, 2005854848, 2008003976, 2023279748, 2031674781, 2075364600, 2084224084, 2097268493, 2111160151, 2124484773, 2141596757, 2144475703, 2149742236, 2155044259, 2162203086, 2169528473, 2172041433, 2268120789, 2477483659, 2556302735, 2561969513, 2573698050, 2579398585, 2583466634, 2744444739, 2750332094, 2770729327, 2788086856, 2795273206]}
{"paragraph": "The rapid development of information technology promotes the data to be generated at unprecedented rates in recent years. The total amounts of data in the world has increased nine times within five years, and this figure is expected to double at least every two years in the future. The advent of big data era provides great opportunities for enterprises to improve competitive advantages and makes significant impacts on value creation in the process of production, R&D, operational management and service. But the enterprise in big data environment has to suffer more challenges and risks than before due to severe competition, especially for the high-tech enterprise. How to improve high-tech enterprise’s innovation performance and core competence in big data environment has been a key issue and attracts much attention. Most of literatures discussing enterprise innovation generally focused on innovation capability, organizational learning and the use of advanced technology. Corporate governance, however, is also an important factor that influences high-tech enterprise innovation significantly. Corporate governance aims at decreasing manager’s opportunistic behavior, enhancing accuracy and effectiveness of innovation decision-making, and improving firm’s ability to cope with external uncertainties. The company with strong corporate governance typically performs better and gains higher returns especially in a more complex environment. In this article, we concentrate on the impact of the corporate governance on high-tech enterprise innovation in big data environment. Specifically, we explore how the managerial power (internal governance) and enterprise’s network centrality (external governance) affects enterprise innovation, and systematically analyze their influence mechanism. Managerial power is one of the corporate governance structures and makes important influence on firm’s strategic choice and performance. Similar governance structure in high-tech enterprises often leads to different innovation performance, which lies in the difference of managerial power. For example, the team headed by Jack Ma in Alibaba Group has great control over the corporate decision-making and resources configuration, and helps the corporation make great achievements in technologic innovation. In the development of big data environment, it’s necessary to examine the role of managerial power in enterprise innovation. On the other hand, the enterprises close to central network position are more likely to accumulate valuable resources and gain the competitive advantages. The managers with higher managerial power are often inspired to make connections with other companies, which helps to enhance enterprise’s network centrality, and promote the enterprise innovation by acquiring more resources from others. But previous researches paid less attention to the role of corporate networks which formed by interlocking executives in innovation practices. In this research, we mainly address following questions: how does managerial power (internal governance) and network centrality (external governance) influence high-tech enterprise innovation in big data environment; how does network centrality mediating the relationship of managerial power and innovation performance; considering about the regional big data environment, how will above relationships make differences. Overall, this article makes several contributions. First, different from prior researches, we pay attention to the role of corporate governance in the high-tech enterprise under big data environment, which provides a new insight into the research in this field. Second, this paper fills in the extant literatures which focused on either internal-based governance or network-based governance, and combines with both governance perspectives to explore the high-tech enterprise innovation in big data environment for the first time. Third, this paper expands the research of interlocking directors and focuses on the interlocking executives including board of directors, CEO and other TMT members. Because it’s a common phenomenon in Chinese capital market that most of executives hold management positions in at least two companies. Fourth, we examine the differences under distinct regional big data environment, which enriches the research of enterprise innovation on the regional-level. The following research are structured as: the second part reviews the literatures related to the research; the third part theoretically analyzes the influence mechanism among the relationships of managerial power, network centrality and enterprise’s innovation performance, and proposes the hypotheses; the fourth part describes the methodology; the fifth part reports the results of empirical research; the sixth part makes the discussions of key findings and the implications for both research and practice, and the final part makes conclusion and discusses our study’s limitations and future research directions.", "section": "Introduction", "doi": "10.1016/j.ijinfomgt.2018.11.009", "references": [1973532288, 2008420823, 2042822990, 2052709366, 2065198144, 2068301507, 2081234480, 2105239853, 2113680315, 2121318675, 2159128662, 2159708909, 2222723904, 2323667237, 2407961458, 2470527509, 2515574205, 2519706319, 2596910185, 2741053108, 2763315368, 2763473974, 2765656061, 2776789918]}
{"paragraph": "It is well-known that modal logic is a fragment of first-order logic with very appealing model-theoretic and computational properties, namely the small model property, decidability of satisfiability, and the tree model property. The latter is a consequence of bisimulation invariance of any property expressible in modal logic; in fact, a first-order property of nodes in a Kripke structure is definable in modal logic if and only if it is bisimulation-invariant. These positive features come at a price: the expressive power of modal logic is very limited. Many attempts have been made at finding logics with greater expressive power that retain decidability and other desirable properties. Prominent examples are temporal logics like CTL or more expressive branching logics which retain decidability, bisimulation invariance, tree and small model properties. Another closely related example is the modal μ-calculus which extends modal logic with special second-order fixpoint quantifiers. It also possesses these properties. The term hybrid logic refers to a family of logics which enrich modal logics with certain first-order constructs. This takes their expressiveness beyond the limitations of bisimulation invariance. Typically, hybrid logics extend modal logic with features like nominals—names for particular states in a Kripke structure or atomic propositions that hold true in a unique node—as well as restricted existential quantification in the form of the binder and the jump modality. The requirement on a nominal to be true at a single state is a very strong one. This is what destroys bisimulation invariance: clearly a formula can distinguish two copies of a state in the tree unfolding of a Kripke structure when only one of them can carry a particular name. Another intuitive interpretation of nominals and binders is given as reference points for temporal properties, and this is the name under which those logics were originally invented. However, despite increasing the expressive power vastly, one can typically easily define model checking procedures or other reasoning methods for these hybrid logics. In some cases, this can even be done without increasing the computational complexity. This raises the question of how hybrid extensions of more expressive logics like temporal logics behave. In this paper we consider a combination of two of these extensions: adding hybrid features and going from modal to temporal logics. We consider the hybridisation of the full computation tree logic and its prominent fragments CTL, fair fragments, with nominals, binders and jumps and study the complexity of their model checking problems. This logic allows arbitrary linear-time formulas underneath a path quantifier as opposed to CTL, which couples every linear-time temporal operator directly with a path quantifier. As a consequence, the syntax of this logic naturally features path and state formulas such that every state formula is also a path formula. They are correspondingly interpreted over paths and states of a Kripke structure. The syntax of CTL only features state formulas, and this is why the following considerations do not arise when studying only hybrid extensions of CTL. The presence of these two kinds of formulas gives some choice for the introduction of the hybrid operators: the unary binding and jump operators could be allowed in front of path formulas or could be required to only precede state formulas. Not every combination is meaningful. First, consider a binding formula, which intuitively reads as “name the current state of evaluation x and then proceed to checking whether φ holds under this assumption.” Clearly, φ can naturally be a state formula, but this statement also makes sense when it is a path formula because paths naturally have a first state to which x would get bound. The situation for the jump modality is different, read as “continue evaluating φ at the node x.” Note that x is supposed to be bound to a node. Hence, this is meaningful if φ is a state formula. It would only be meaningful for a genuine path formula if x was bound to a state on a particular path beforehand. Then the evaluation of φ can continue at the state x on that path. Syntactically, this is for example guaranteed when no path quantifier occurs between the binder and the jump. These considerations lead to three different hybrid extensions, depending on what kind of formulas are preceded by the binders and jumps. The reason for the additional syntactic restriction is purely the need for a well-defined semantics. For the same reason, we cannot consider the case in which a binder precedes a state formula and the jump precedes a path formula. We study the model checking problems for these logics over finite Kripke structures. Recall that model checking the full logic is PSpace-complete while model checking CTL can be done in P and related fragments are also tractable. We show that the complexity of the full logic extends to its hybridisation. On the other hand, model checking HCTL already becomes PSpace-hard. Allowing genuine path formulas under binders increases the complexity: model checking this logic is ExpSpace-complete. Finally, the complexity of the hybrid logic depends on the number of variables that are being used: with a single variable it is also ExpSpace-complete, with at least two variables it is only non-elementarily decidable. We also refine the analysis of the combined complexity of model checking these hybrid variants and give upper and lower complexity bounds for the data and expression complexity, i.e., the complexity of model checking a fixed formula, respectively a fixed Kripke structure. The model checking problems for various hybrid logics have been considered before. Some examined hybrid logics with an LTL-like syntax which are, nevertheless, interpreted over Kripke structures like simple branching-time logics. Past-time operators are featured in this logic as well, so the logic is in fact a hybrid extension of a variant of Propositional Dynamic Logic with Converse. The complexities obtained for various fragments range from P to PSpace. Improvements over those techniques have been made. Earlier, polynomial-time decidability of model checking of the hybrid extension of modal logic with a single variable has been noted. Hybrid extensions of genuine temporal and similar logics have been considered before but the focus has typically been on the complexity and decidability of their satisfiability problems. Some studies have explored hybrid extensions of simpler branching-time logics and obtained some decidability results in the area of satisfiability checking. As a rule of thumb, two variables make satisfiability undecidable, and the logics with a single variable tend to stay decidable but the complexity of satisfiability checking is one exponential worse than that of the underlying temporal logics. There has also been an attempt at defining and studying the hybrid extension of this logic but no considerations on the interplay between path formulas, binders and jumps have been made. Claims about the non-elementary complexity of satisfiability for the one-variable fragment have to be questioned because the lower bound proof uses formulas that are syntactically not well-formed and therefore cannot be given a well-defined semantics. Some have studied hybrid versions of the modal μ-calculus but these extensions feature nominals only; binders and jumps are not included. Furthermore, they only look at satisfiability. The full logic can be translated effectively into the modal μ-calculus at an exponential blow-up. Thus, a formula can in principle be model checked by translating it into the modal μ-calculus first and then using some model checking algorithm for this logic. This does not necessarily yield an optimal procedure, though. The model checking problem for the fully hybrid μ-calculus, the extension of the modal μ-calculus with nominals, binders and jumps, has recently been studied and its model checking problem has been identified to be ExpTime-complete. This, however, does not yield an upper bound on the model checking problem for the full hybrid temporal logic and therefore also not for its fragments, not even a non-optimal one, as it was also shown that this logic can express certain properties that the fully hybrid μ-calculus cannot. Model checking for μ-calculus with nominals is very briefly discussed with the result that it is basically the same as model checking the modal μ-calculus since nominals without jumps can simply be treated like atomic propositions. The aim of this article is to provide a comprehensive study of the complexity of the model checking problem for hybrid extensions of temporal branching-time logics. It extends a preliminary version with full proofs and the aforementioned study of data and expression complexity. The rest is organised as follows. Section 2 introduces the full logic and its fragments CTL and related extensions as well as their hybridisations. In Section 3 we then investigate their model checking problems. We give optimal upper and lower bounds not only for their combined complexities but also for data and expression complexities. Section 4 then identifies a family of fragments with better model checking complexities while still extending the branching time logics with hybrid operators.", "section": "Related Work", "doi": "10.1016/j.jlamp.2018.11.007", "references": [142499984, 1503647529, 1568275823, 1596995530, 1608094044, 1824565677, 1975657455, 1976513602, 1982129592, 2003227046, 2004306067, 2012935476, 2015640848, 2017460625, 2017759899, 2021711161, 2048355938, 2067441543, 2090612177, 2102548669, 2107444004, 2112476716, 2112535338, 2144726625, 2186597297, 2560471064, 2569214543, 2592897984, 2758859489]}
{"paragraph": "In the past several decades, digital visual information and its quality measurement were widely used in various fields of our daily life which have fundamental importance and crucial role to numerous image processing applications. Due to the subjective nature of human visual perception, image quality assessment becomes a more complex and difficult problem in image processing applications. Unfortunately, images are inevitably subjected to a wide range of distortions during acquisition, compression, storage or transmission, which degrade the visual quality of images. These distortions may influence the perceptive comfort of human eyes and sometimes even disturb the accuracy of our observation. It is necessary for the imaging devices to automatically quantify the quality loss due to distortions. This gives rise to the demand for efficient objective image quality assessment algorithms in accordance with the human visual system. There are mainly two approaches of image quality measures. First is subjective image quality assessment approach according to which opinion scores are estimated with the help of human observers. This is the most natural way; it is hence reliable but complex, expensive, time consuming and impossible to be incorporated in automatic imaging systems. Second is objective image quality measurement approach based on mathematical expressions provided to estimate automatically the quality of image. These measures take into consideration the image itself and assume that all users look at the image in the same conditions and the fact that these conditions vary from one user to another is ignored, that is why another approach of image quality measures has emerged considering viewing conditions such as image resolution and viewing distance. From several years, a long panel of image quality assessment models has been proposed, claiming to have made headway in their respective domains. All classification schemes available in the literature agree that, according to the availability of information on the reference image, the objective image quality metrics can be classified into three different categories: full reference metrics for which both the original and the distorted images are required, reduced reference metrics for which a description of the original image into some attributes and the distorted image are both required, and no reference metrics where only the distorted image is required. Most of the traditional image quality assessment metrics are based on the full reference method such as mean squared error and peak signal-to-noise ratio. However, they often correlate poorly with subjective visual quality. In the literature, some other full reference algorithms have been established to achieve higher performance such as structural similarity index, multi scale structural similarity index, the visual information fidelity, the visual signal-to-noise ratio, perceptual similarity, and structural variation quality index. As a compromise, the reduced reference method provides a flexible solution for the condition when the reference image is not fully available, minimum amount of information about the reference along with the distorted image is useful in quality computation. Very recently, several popular reduced reference image quality assessment metrics have been developed based on image distortion modeling, human visual system modeling, natural scene statistics modeling, and finally machine learning oriented metrics. In the presented work, we are interested in no reference metrics; the existing ones can be categorized into: distortion-specific methods which assume that the image quality is affected by one or several particular kinds of distortions and extract distortion-specific features for quality prediction. The disadvantage of this approach is that it is distortion specific and hence also application specific, while the number of distortions introduced to images is large. Opinion aware methods train a model to predict the image quality score based on a number of features extracted from the distorted image. The first opinion aware method called blind image quality indices is based on natural scene statistics. It mainly consists of two steps: estimation of the presence of a set of distortions in the image and evaluation of the quality of the image according to those distortions. Later this method is modified and named distortion identification-based image verity and integrity evaluation which is a recent wavelet-based algorithm; it consists of a two-stage framework: the distorted images are classified into a distortion class using support vector machine, then support vector regression is applied to predict quality score. Then a new opinion aware model is proposed that works in the discrete cosine transform domain and is named blind image integrity notator using DCT statistics, where efficient features are calculated from a model of block DCT coefficients and used to map the quality to human rating via support vector regression model. Later this model is extended using more sophisticated DCT features. Another model that operated in the spatial domain is proposed namely blind reference image spatial quality evaluator. The image features are calculated using a model of mean subtracted contrast normalized, then these features are fit to a predictive score using a support vector regression model. Besides this model, another opinion aware method namely free energy based robust metric is developed using three groups of descriptors, including features of the free energy and structural degradation information, features inspired by the human visual system, and features measuring the deviation in naturalness in the distorted image. These features were transformed in a single value using support vector regression which will be compared with the subjective scores. Opinion unaware methods do not need training samples of distortions nor of human subjective scores. The first opinion unaware method called natural image quality evaluator extracts local features from an image in the spatial domain and fits the feature vectors to a single global multivariate Gaussian model. The image quality is calculated using a distance between the model of image in question and that of natural image. Another opinion unaware method called integrated local version of this method extracts five types of features from each patch of distorted image and used them to train a model, then a quality score is calculated over each patch and the final quality score is obtained by average pooling. A new type of solution for building opinion unaware methods has emerged to solve the problem where the subjective evaluation is not available or difficult to do especially for the big image database. A method was proposed for screen content images where four types of descriptors describing image complexity, statistics of screen content image, brightness and sharpness are extracted from each image, then a regression module is trained using the training samples labeled with a full reference image quality assessment model which achieved good performance when using with screen content images to replace the subjective judgment. Using the same previous procedure, another metric for image enhancement is built, a set of 17 features are extracted from each enhanced image by considering five factors: contrast, sharpness, brightness, colorfulness, and naturalness, these images are labeled using the most performing full reference method for enhanced images, then the support vector regression is used to transform these features to a global quality score. In this paper we propose a fast and efficient no reference image quality assessment in spatial domain for color images, a simple and few number of descriptors are extracted from Lab color space using two image databases, these descriptors were used as input data to relevance vector machine algorithm to get objective scores. A comparison of our metrics with seven state of the art no reference metrics show that these features achieve good results in terms of correlation and monotonicity. The remainder of this paper is organized as follows: related works in Section 2, description and analysis of the proposed method in Section 3, in Section 4 a description of the used regression module, validation of the algorithm in Section 5 followed by a conclusion in Section 6.", "section": "Methodology", "doi": "10.1016/j.matcom.2018.11.005", "references": [1526068416, 1545192369, 1964357740, 1964728608, 1977725648, 1982471090, 1988649016, 2001896471, 2004871019, 2009272644, 2025157389, 2046119925, 2057224480, 2063360098, 2068482139, 2102166818, 2107476778, 2124562516, 2129644086, 2133665775, 2137512539, 2140094223, 2141983208, 2153582625, 2161907179, 2162692770, 2163370434, 2340480757, 2430408220, 2579658395, 2594112328, 2621054742, 2741739307, 2757064044, 2767446501, 2767660127]}
{"paragraph": "Multi-agent systems are collections of autonomous agents that operate according to some local rules and a limited mutual awareness. They are a convenient formalism for representing several classes of complex systems and can support formal reasoning about them. An issue that arises when considering a multi-agent system is how to determine whether a global property of interest emerges from the combination of the local behaviors of the different individual agents. The availability of a formal description of a multi-agent system allows one to apply automated verification techniques and can be instrumental for obtaining strong guarantees about its global behavior. Simulation-based approaches, on the other hand, may be more effective when dealing with larger multi-agent systems, due to the considerably large state spaces resulting from their distributed and asynchronous nature. Therefore, the two approaches should be considered complementary to each other. In this paper, we introduce a language for describing multi-agent systems that lends itself to an intuitive design of local specifications and that can be used as the basis for automated analysis. The language, which we call LAbS for Language with Attribute-based Stigmergies, is simple yet versatile enough to model several interesting classes of systems. It combines stigmergic interaction with attribute-based communication. A key concept of the language is that of virtual stigmergy, a distributed data structure that can model global knowledge. Each agent operates only on his local copy of the stigmergy, that stores his own partial knowledge of the system. Individual knowledge is then asynchronously propagated across other local stigmergies. Thus, changes by an agent may indirectly affect the behavior of another. In the originally proposed version of the virtual stigmergy, agents are concrete entities, each at a specific position in space, that can communicate across the stigmergy only if they are within a given distance from each other. To increase expressiveness, we generalise stigmergic interaction to arbitrary predicates over exposed features, referred to as attributes, of the agents. In fact, our language has no explicit concept of position for agents, and thus of neighborhood. An agent can have instead local attributes, and predicates over these attributes can express the conditions for two agents to be allowed to exchange knowledge. Movement is no longer seen as a specific action; an agent may update the attribute that encodes its position by performing a standard update action. The generalisation of stigmergic interaction outlined above increases the flexibility of the language and allows to model a wider class of systems. However, it is still not sufficiently expressive to naturally model those classes of multi-agent systems where the global environment plays a crucial role. To address this shortcoming, we extend our language with tailored primitives to explicitly model actions on the environment. This work extends our original presentation of the language in several ways. The new syntax and semantics support multiple stigmergies and improve the specification of situated systems. Multiple stigmergies allow us to naturally describe further interesting classes of systems, where agents can communicate in different ways. For instance, they can be used to directly model multi-robot systems where robots have multiple sensors and communication devices, and decide the equipment to use depending on specific environmental conditions. As for situated systems, environment variables may now directly occur in expressions and guards. This makes it easier to describe systems where agents also interact via the environment. New case studies have been added to those related to birds flocking, robots foraging and opinion formation. To vindicate the flexibility of our language and its ability of expressing different interesting classes of systems, in this paper we also model Boids, and population protocols. The former is a widely-used model of flocking behavior as observed in different classes of natural systems. It extends the flocking case study by allowing additional interaction strategies, for getting closer and avoiding collisions, and not only for moving in the same direction. The latter are a type of gossip protocols that rely on a distributed communication paradigm inspired by the spreading of epidemics and by the gossip phenomenon observed in social networks. For both classes of systems, we do provide experimental results about their automated analysis. Our modeling of Boids systems shows the benefits of adding multiple stigmergies to the language and, to the best of our knowledge, our work reports the first results about formal verification of such systems; previous investigations only exploited simulation-based techniques. The rest of the paper is organized as follows. In Section 2 we present a revised version of the formal semantics of the core language, allowing us to define systems where agents interact indirectly through multiple stigmergies. In Section 3 we demonstrate the features of the language by modeling the Boids system, and include preliminary results about its automated analysis together with a discussion about the impact on verification of the different parameters of the specified system and of the used verification tools. In Section 4 we further enrich the language with environment-oriented primitives and show how LAbS can naturally model other classes of systems dealing with robot foraging, opinion formation and gossiping. In Section 5 we summarise our main achievements, compare our work with others, and suggest directions for future research.", "section": "Introduction", "doi": "10.1007/978-3-030-04771-9_26", "references": [24701999, 59714529, 64119988, 121297392, 191696817, 233122475, 292160745, 1060861436, 1482847502, 1501097462, 1507006488, 1508927144, 1544985403, 1549166962, 1566132409, 1571641412, 1613817315, 1631091010, 1729887387, 1750856813, 1864649459, 1951714113, 1973501242, 1976092684, 1991609163, 1995124372, 2017150251, 2021089599, 2031522732, 2044484214, 2059505795, 2082511574, 2095156325, 2098579316, 2098914525, 2108943074, 2118331730, 2122875695, 2129538349, 2132182116, 2142219372, 2150312211, 2161253570, 2167340365, 2177603835, 2254102208, 2260243663, 2281788661, 2284655150, 2395613050, 2404253444, 2460389227, 2567377301, 2775821128, 2799135505, 2883472767, 2884484164, 2888321432, 2894761520, 2950142800]}
{"paragraph": "Large-scale systems have been found in many applications, such as autonomous vehicles; electric power systems; satellite formations; and robotics. In large-scale systems, subsystems usually exchange information through a communication network. The communication network used in the large-scale systems offers numerous advantages like simple installation, easy maintenance, and low cost. However, the information transmitted through the network may suffer from communication delay. As a result, the information available to each subsystem is incomplete at each time step. To achieve the best system performance with the incomplete information, decentralized control has been proved to be a useful control technique. However, the design of the optimal decentralized control strategy is a challenging task, because it is computationally intractable in general. For example, the optimal decentralized control policies may be nonlinear even for the linear system. Decentralized control with communication delay has attracted a lot of research attentions since 1970s. The optimal decentralized control with one step delay sharing pattern was studied by the matrix minimum principle and by the second-guessing technique. For the multiple step delays sharing pattern, two structural results to the optimal decentralized control design were established. The decentralized stochastic control with symmetric delay and asymmetric delay has been studied by a common information approach. A common feature of the results is that each subsystem estimates not only its own subsystem state but also the others. This implies that the subsystem state is estimated more than one time. For the case that each subsystem only estimates its own subsystem state, Wang et al. studied the decentralized output feedback control for a two-player system with one step communication delay. For delay patterns arising from a communication graph, the optimal decentralized LQG control problems were investigated by the information hierarchy graph and by the independence decomposition technique. The results are for the state-feedback controller design. The output-feedback case was considered, but the result is only suitable for a three-player system with a chain structure. The sufficient statistics of linear control strategies was studied for large-scale systems with delay pattern defined over a communication graph. In those studies, the communication delay is assumed to be determinate. In practice, the network environment is affected by random factors, and the communication delay is naturally random. The LQG problem with varying communication delay has been studied. The results proposed are sound, but are only for two-player system with state feedback case. The framework of the randomized information pattern can be used to model state-feedback LQG problem with random communication delay, but the realization of the optimal controller design is not derived. Thus, the optimal output feedback LQG control with random communication delay is not fully studied. This paper focuses on the optimal decentralized output-feedback control of a large-scale system. The information is transmitted from one subsystem to other subsystems through a network with random delays. The random delay satisfies Bernoulli distributions. Under this setup, we derive a linear matrix equation used to design the optimal controller for global estimation case. Also, the optimal value of the cost function is obtained, and is shown to strictly increase as the delay probability increases. The optimal LQG problem under local estimation case is also studied. An iterative algorithm is exploited to design the gains of the optimal controller for local estimation case. It is shown that the algorithm converges to person-by-person optimum. The optimal value of the cost function has the same monotonicity as the one of global estimation case. Finally, two numerical examples are given to illustrate the effectiveness of the theoretical results. The contribution of this work is summarized as follows. Compared to prior work, the local estimation case is also studied in this paper, while others only studied the global estimation case. The local estimation is important for large-scale systems. One advantage of local estimation is that it consumes less computational resources. Compared to the global estimation case, the main challenge of the local estimation case is that the available estimated state is incomplete, thus, is not a sufficient statistic for optimal decision. Then, the corresponding optimization problem is nonconvex. Our contribution is to establish the framework of the optimal decentralized controller under local estimation, and exploit an iterative algorithm to compute the gains of the controller in the sense of person-by-person optimum. We study the random delay case instead of the determinate delay case. The main challenge of the random delay case is to deal with the random sparse structure constraints induced by the random communication delay. Our contribution is to propose the method of Hadamard product derivative to design the optimal decentralized controller under the random sparse structure constraints. We investigate the property of the cost function with respect to the delay probability. Our contribution is to prove that the optimal value of the cost function is monotonically increasing with the increase of the delay probability. Based on this result, we can find the critical delay probability effectively by the binary search method for a given value of the cost function, such that the optimal value of the cost function is smaller than the given one when the delay probability is below the critical delay probability. To the best of our knowledge, the similar results do not appear in the related literature. Our contribution is to design the optimal decentralized output-feedback controller for more general large-scale systems. However, the results established earlier are only for two-player systems under state feedback and uncorrelated subsystem noises. In other words, the methods proposed previously are unsuitable for our problem.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868968", "references": [1932883063, 1979094997, 1979177727, 1991888757, 1997144258, 2047691814, 2052705342, 2056313725, 2092691181, 2105876604, 2110240006, 2113789941, 2115679890, 2119099274, 2126230455, 2146890818, 2151756741, 2237353654, 2326020541, 2344639242, 2551093915, 2578362634, 2587622940, 2618231873, 2759084312, 2784185058, 2963254670, 2964182829]}
{"paragraph": "Financial crisis prediction (FCP) is highly essential for financial firms, which aims to reduce the upcoming losses by calculating possible risks and avoids new credit proposals when the default risk is higher than a predefined acceptance level. This process is also entitled as a credit default classification process, which refers a customer as “non-default” when he pays back the loan, or the customer is referred as “default”. The accuracy of the FCP plays an important role to determine the financial firm’s productivity and profitability. For instance, a small positive adjustment in the accuracy level of a potential user with default credit will minimize a huge future loss of an organization. FCP can be considered as a data classification problem, which refers a customer as “non-default” when he pays back the loan, or the customer is referred as “default”. Numerous researches have been done on the classification of FCP, started from the year of 1960′s. At the earlier days, traditional methods employed mathematical functions to predict financial crisis, which differentiates the financial institution between stronger and weaker ones. In the year 1990′s, the concentration has moved towards artificial intelligence (AI) and machine learning (ML) based expert models like neural network (NN) and support vector machine (SVM). Recently, AI techniques are adopted to refine the conventional classification models. However, the presence of many features in the high-dimensional financial data leads to several issues like overfitting, high computational complexity and low interoperability. This is because of the curse of dimensionality, which occurs based on the ratio of number of features and number of instances. The easiest way to solve this issue is reducing the number of available features using feature selection methodologies. Feature selection process intends to identify appropriate subset of features and has significant implications for issues like reduced noise by removing noisy features, save cost and computation time required to implement an appropriate model, simplifying resultant models and facilitating easy use and updation of models. The chosen subset of features are useful to represent a classification function, which affects several dimensions of classification like learning time, accuracy of the classification algorithm and cost integrated with the features. The feature selection methodology is used in various applications like data mining, ML and pattern recognition, to reduce the dimensionality of a feature space and to enhance the predictive accuracy of a classification algorithm. On the basis of evaluation criteria, feature selection techniques are divided into wrapper, embedded and filter based methods. Wrapper method uses a learning algorithm as a part of evaluation, to assess the goodness of the selected feature subset. Though wrapper methods are widely used, it has some limitations like high computational complexity, finding user-specified parameter of the learner and inherent learner limitations. Embedded methods are computationally less complex than wrapper methods, but the chosen feature subset is not independent on the learning algorithm. Because of these limitations, filter method is employed in this study. Filter approaches evaluate feature subset using predefined metrics rather than the learning models and chosen features. The process of feature selection is considered as an optimization problem, with a performance measure for every subset of features, which denotes expected classification performance of the resultant model. The problem is to search the feature subset space to find the optimal or near-optimal one on the basis of performance measure. Various methods have been presented to determine the suboptimal solutions in comparably smaller amount of time. Stochastic algorithms like simulated annealing, scatter search, ant colony optimization (ACO) and genetic algorithms are popular because they produce high precision results at faster rate. ACO is one of the popular swarm intelligence technique, widely applicable for approximate optimizations. It is a popular metaheuristic algorithm capable of achieving well enough solutions to solve NP hard problems in a reasonable time period. It has been adopted to hold several real world problems like scheduling, vehicle routing, industrial problem and so on. ACO is used to solve tedious combinatorial problem in 1990′s. It is based on the foraging nature of real ant colonies. The ACO algorithm has been chosen for FCP because of the following reasons: ACO leads to better exploration which makes it suitable for appropriate feature selection and the selected features resulted to efficient classification performance. Moreover, the increased number of features in the financial dataset makes the ACO algorithm suitable for the feature selection. The mapping of any optimization problem to ACO algorithm requires a sequence of steps include graph representation, heuristic desirability, positive feedback process and constraint satisfaction method. The contribution of the paper is summarized as follows. This paper designs an effective ACO-FCP model by the incorporation of ACO based feature selection (ACO-FS) and ACO based data classification (ACO-DC) algorithms. The inclusion of ACO-FS method identifies the optimal feature subset and helps to improve the classification performance of ACO-DC method, which in turn improves the overall performance of the ACO-FCP model. The proposed method is applied to a set of five benchmark dataset which includes qualitative bankruptcy dataset, Analcat data bankruptcy dataset, Australian Credit dataset, German Credit dataset and Polish dataset. For comparative analysis of feature selection results, namely genetic algorithm (GA), Particle Swarm Optimization (PSO) algorithm and Grey Wolf Optimization (GWO) algorithm based feature selection methodologies are used. To test the classifier results, ACO-DC method is compared with well-known nine classifiers. The experimental results reported that the proposed ACO-FCP model shows better performance when compared to the state of art methods. The remaining part of this paper is structured as follows: Section 2 summarizes the state of art methods of feature selection and data classification approaches on FCP. Section 3 explains the proposed ACO-FCP model in a clear and classified manner. Section 4 presents the performance evaluation of proposed method against different dataset and the paper is concluded in Section 5.", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2018.12.001", "references": [1862312035, 1969054362, 1985716252, 1996413276, 2004473119, 2014915963, 2017337590, 2029765676, 2031239249, 2039355518, 2071193822, 2072461903, 2080614264, 2089939190, 2093195672, 2093980846, 2109508799, 2116045745, 2118573797, 2122684851, 2125965138, 2130759652, 2148633389, 2149237332, 2154929945, 2162523902, 2167101736, 2167435760, 2172238468, 2180466864, 2307376191, 2336505047, 2341338744, 2343420905, 2508160469, 2744516260, 2887289205]}
{"paragraph": "Since the introduction of the term “Granular Computing (GrC)” by Zadeh and T.Y. Lin in 1997, we have witnessed a rapid development and fast growing interest in this topic. It is well accepted that the theories of fuzzy sets and rough sets are two primary contributions that have occurred from the emergence of GrC. In recent years, shadow set and three-way decisions are regarded as special models of Granular Computing to solve the problem of information granularity selection. GrC can be used in many fields, such as variable granulation, system granulation, concept granulation, cluster analysis, intelligent systems, and data processing. In set theory, for rough sets or a cluster analysis, a granule may be interpreted as a subset of a universal set. In programming, a granule can be a program module. Many GrC models and methods have been proposed and studied. GrC may be studied based on two related issues: granulation and computation. The former is related to the construction, representation, and measurement of the granules. The latter is related to the computing and reasoning with granules and granular structures. In general, two types of granulation processes exist: functional granulation and relational granulation. If the process is based on the attributes of the objects, it is known as functional granulation because the attributes are mathematical functions from the set of objects to the set of values. The granulation process is relational if it is based on the relationship between objects. Unlike a GrC-based framework for mining relational data found in the literature, where information granules derived from an information system are defined based on the notion of related sets, it is based on an information system defined for relational data. Herein, relational data implies the relationship between objects. For example, in a network, an edge between two nodes represents a relational tie. Relational granulation is particularly useful for the analysis of network data because a network is a relation between nodes. Liau formalized social networks into a fifth GrC model, known as the relational GrC model. In their work, they obtained logical characterizations of different positional equivalence relations and subsequently transformed them into a functional granulation process. The maximum flow problem was first presented by Fulkerson and Dantzig in 1955, and was solved by Ford and Fulkerson in 1956. Over the past few decades, researchers have proposed many efficient algorithms for solving the maximum flow problem. However, the size of real-world network has grown far larger than the amount of available memory in conventional machines. Therefore, more efficient algorithms are required to solve this problem. Two primary types of methods are used to handle the increasing scale of networks. The first solution is parallel computing or cloud computing. However, this naturally requires solving distributed programming issues, e.g., data distribution, load balancing, scheduling, fault tolerance, and communication, among other factors. The other solution is to simplify the flow network. In the maximum flow problem, Lee and Rieger studied the relation between the maximum flow and the substructure in complex networks. They showed that the flow efficiency is related to particular topological properties of the network. Liers and Pardella proposed the shrink max edge (SME) method by compressing the maximum capacity edge to reduce the network size. However, this method cannot significantly reduce the mass of the network, and is not applicable to a large number of networks. Scheuermann and Rosenhahn presented a method for graph simplification, which constructs a slim graph by merging nodes that are connected by simple edges. It also demonstrates that the maximum flow of the much smaller graph remains identical. In our work, we introduce a method for simplifying a complex problem. This method reduces the complexity of the problem and effectively solves the maximum flow problem. However, developing a method for granulating the network, aiming at finding the approximate optimal solution of the maximum flow on the original network, remains a challenge. Based on the discussion above, a novel method, called the maximum flow based on Quotient Space Theory (MF-QST), for solving the maximum flow problem based on Quotient Space Theory is proposed herein. The primary idea of the quotient space is forming a suitable granular space to describe and solve the problem, as well as constructing the relation of different granular spaces to simplify the problem. The motivation of Quotient Space Theory, proposed by Zhang & Zhang, originates primarily from Hobbs’ idea that the world can be conceptualized at different granularities and easily translated from one level of abstraction to another, i.e., they can be handled hierarchically. In the proposed method MF-QST, a community as a specific substructure is first given. Subsequently, the relational granulation criterion is discussed in detail to filter and retain the appropriate substructure. Next, the construction of the quotient network based on Quotient Space Theory is formed. Finally, the maximum flow algorithm is used to compute the maximum flow on the quotient network as the approximated maximum flow on the original network. The remainder of this paper is organized as follows. Section 2 introduces some concepts to formalize the maximum flow problem. Related studies on Quotient Space Theory are also described. The proposed method, MF-QST, is discussed in Section 3. Section 4 describes the experimental results of the proposed method, and discusses some qualitative and quantitative characterizations. Finally, some concluding remarks are provided in Section 5.", "section": "Introduction", "doi": "10.1016/j.ins.2018.12.009", "references": [1512189463, 1984627379, 2018804864, 2036841467, 2040753112, 2082272625, 2113137767, 2152216760, 2153676086, 2154500141, 2170755382, 2235896546, 2340020088, 2537382886, 2652860524, 2751469631, 2792234535, 2806445482, 2883737195, 2898660825, 2912565176]}
{"paragraph": "As a novel decision-making approach, three-way decisions are to divide a universal set into three pair-wise disjoint regions by developing an appropriate strategy. During the application, these regions can be interpreted by the corresponding strategies (or semantics), e.g., three-way decisions with shadowed sets, three-way decisions with granular computing, three-way decisions with cognitive computing and three-way decisions with decision-theoretic rough sets (DTRSs). From the general form, it has an acceptance decision region, a deferment decision region and a rejection decision region, which exactly maps the positive region, the boundary region and the negative region of rough sets, respectively. Among the researches of three-way decisions, three-way decisions with DTRSs proposed by Yao are a major topic of research. It has attracted the attention of many researchers. For instance, by using the game theory, Yao and Azam developed a new approach to determine the probabilistic thresholds of rough sets and further discussed the applications of game-theoretic rough sets. Hu deeply studied the measurement on decision conclusion in three-way decision spaces. In allusion to the risk attitudes of the decision maker, Li and Zhou designed three models of three-way decisions. Liang and Liu provided us some different perspectives for the determination of the loss functions of DTRSs. In multiset-valued information tables, Zhao and Hu investigated two generalized decision-theoretic rough set models for three-way decisions. Huang et al. developed multi-granulation DTRS method for acquiring knowledge from multi-scale intuitionistic fuzzy information tables. In the framework of Bayesian decision procedure, DTRSs can successfully explain three corresponding regions by considering the decision risk factors. Nowadays, it has been applied in many fields, such as the risk decision-making, the government decision-making, etc. From the existing research works of three-way decisions with DTRSs, the determination of its two basic ingredients is the essential question, i.e., the loss function and the conditional probability. In particular, the loss function is evaluated by the decision maker, which is related to the risk factors. Fuzzy multisets, as an extension of the multisets, were initially studied by Yager and Miyamoto. Then, Paul and John investigated type-2 fuzzy multisets. Sharma et al. discussed fuzzy multiset regular languages. Riesgo et al. proposed some basic operations of fuzzy multisets. In recent years, as an extension of fuzzy multisets, dual hesitant fuzzy sets (DHFSs) proposed by Zhu et al. provide us a new evaluation format. Compared with hesitant fuzzy sets (HFSs), DHFSs have generalized the concepts of HFSs and fuzzy sets (FSs), which mainly assess the value of the membership degree. As with intuitionistic fuzzy sets (IFSs), DHFSs are made up of the membership degree and the non-membership degree, but its two parts may possess several possible values, i.e., HFSs. It has been applied in many fields, e.g., the blood transshipment judgement, the medical diagnosis problem, the teacher evaluation, the optimal assignment of projects to teams. In the aspect of decision-making procedure, Farhadinia proposed a method for computing the correlation coefficient of DHFSs. Ren and Wei developed a prioritized multi-attribute decision-making method to solve dual hesitant fuzzy decision-making problems. Based on a new similarity measure, Singh designed a method for solving dual hesitant fuzzy assignment problems with restrictions. In a word, DHFSs can be suitable to a more complicated and uncertain environment. Fortunately, Liang et al. introduced the new hesitant format of DHFSs into DTRSs and deeply studied the corresponding three-way decision model. For that work, it mainly discussed the loss functions of DTRSs with dual hesitant fuzzy elements (DHFEs). Most of the existing research works of three-way decisions ignore the risk appetite of the decision maker. However, in our real life, the decision result can be impacted by the risk appetite of the decision maker, e.g., an optimistic decision, a pessimistic decision, and an equable decision. In fact, the decision maker is always of bounded rationality under risk and uncertainty. We introduce two questions to highlight the motivation that leads us to consider the risk appetite, and explain the importance of the risk appetite in different scenarios. Question 1: Imagine that you are richer by 20000 than you are today, and that you face a choice between options: (A) receive 5000; (B) a 50% chance to win 10000 and a 50% chance to win nothing. Which one do you choose? Question 2: Now imagine that you are richer by 30000 than you are today, and that you are compelled to choose one of two options: (C) lose 5000; (D) a 50% chance to lose 10000 and a 50% chance to lose nothing. Which one do you choose? In fact, a fully rational decision-maker would treat the two decision-making problems as identical, because they are identical when formulated in terms of states of wealth. According to the statistical results, you probably choose the gamble in Question 2 and the sure thing in Question 1. When the decision maker faces the certainty and uncertainty, he or she tends to the risk aversion. When the decision maker merely encounters the uncertainty, he or she tends to the risk preference. As a valuable tool to cope with the risk appetite character, TODIM (an acronym in Portuguese for Interactive Multi-Criteria Decision Making) has been widely used in decision making under risk and uncertainty. In accordance with the risk appetite of the decision maker, it can help the decision makers adjust the corresponding parameters and then makes the decision results to conform more to the decision makers’ preferences. For the TODIM method, the reference point is pivotal. Since the reference point is determined by pairwise comparison, we can judge the gain or the loss. Then, the dominance or the perceptive value can be further computed based on the gain and the loss. In this paper, according to the basic model of dual hesitant fuzzy three-way decisions, we introduce the TODIM method into three-way decisions and propose risk appetite dual hesitant fuzzy three-way decisions. Our research work starts from the loss function of DTRSs. With regards to the basic ingredient of three-way decisions, we study dual hesitant fuzzy entropy and cross-entropy measures for determining the conditional probability. In this case, the conditional probability information is objectively deduced from the loss function matrix. Then, considering the different comparison methods of DHFEs, we further design two types of strategies to deduce three-way decisions by utilizing the pairwise comparison of loss functions. One is the score function of DHFEs. The other is the likelihood of DHFEs. Finally, these results are used to support the project investment evaluation of online peer-to-peer (P2P). This study involves the risk appetite of decision maker and designs a series of decision analysis methodologies for three-way decisions. Hence, it can vastly enrich the range of applications. The remainder of this paper is organized as follows: In Section 2, we list some symbols used in this paper in advance. Section 3 provides basic concepts of DHFSs and TODIM. The determination of the basic ingredients of dual hesitant fuzzy three-way decisions model is deeply analyzed in Section 4. By introducing TODIM, the approaches for deriving three-way decisions are further developed in Section 5. Section 6 uses the example of the project investment evaluation of P2P in Internet finance to elaborate and compare the two approaches of Section 5. Section 7 concludes our research work and indicates future studies.", "section": "Related Work", "doi": "10.1016/j.ins.2018.12.017", "references": [95749022, 1606022329, 1747894202, 1883715000, 1979092190, 1979454583, 1990116832, 2008010437, 2014417436, 2037003318, 2041404167, 2068843436, 2070737895, 2070813883, 2071162171, 2080404663, 2083199031, 2084718108, 2086869454, 2092666229, 2114832876, 2116365436, 2157687268, 2297889545, 2340020088, 2504734778, 2588487610, 2591943698, 2594843209, 2594848528, 2605263372, 2831624444, 2887732625, 2889210423, 2898660825]}
{"paragraph": "Cyber-physical systems (CPSs), whose most notable feature is tight integration and cooperation between cyber and physical components, have attracted attention from many researchers in the past decades. Typically, CPSs consist of processing units monitor, control physical processes by means of sensors and actuators networks, such as transportation networks, future power systems and smart grids, and high speed train systems. Thus, they are prone to failures especially cyber attacks on the data and communication channels, thus causing damages or breakdown, for example, the Stuxnet storm which damaged the Iran’s nuclear program, breakout accident in nuclear plant, and power blackouts in Brazil. Many works devoting to the attack detection, secure estimation, and control of CPSs have recently appeared in the literature, which can be classified in three classes. The first class addresses attacks. Teixeira et al. presented a novel attack space based on the adversary’s model knowledge, disclosure, and disruption resources, and it illustrates the attack effects by implementing experiments on a wireless network control system, whereas the attack detection and secure control is not discussed. In second class, detection of attacks to CPSs is investigated. For example, the detectability of an attack to CPSs is explored with structured system theory and a Luenberger like observer is proposed to detect the attack. Whereas, if only an upper bound on the cardinality of the attacked sensors is available, the number of needed monitors is combinatorial in the size of the attacked sensors. A sliding mode observer-based method is presented to estimate the attack to system dynamics and sensors separately, in addition to the state estimation. Nevertheless, the results are only applicable to the systems where the so called observer matched conditions are valid. A sufficient and necessary detectability condition of an attack is proposed in terms of the system dynamics eigenvectors by exploring the strong observability of the system. A sufficient and necessary condition of undetectable attacks in the presence of side initial state information is presented, and a detector is proposed to detect attacks in a finite steps. It is noted that, secure state estimation is not involved. Even though the strong observability helps to describe the undetectable attacks, as many CPS are distributed, we feel that, a more explicit characterization is needed and distributed finite time attack detection is needed. The third class focuses on secure state estimation or secure control design in the presence of attacks. A specific computationally feasible decoding algorithm is proposed to estimate states of CPS when some sensors are corrupted and it also gives a characterization on the maximum number of attacked sensors allowed for this decoder to correctly estimate the states. By showing how to design a secure local control loop to improve the resilience of the system, and presenting L1/Lr decoder for secure state estimation, these results are extended. By utilizing the sparse observability of a system, it shows that the state can be estimated securely under an s-sparse attack if and only if the system is 2s-sparse observable, and an event triggered observer is proposed to estimate the state securely. These results are extended with a novel multimodal Luenberger observer based on efficient satisfiability modulo theory to reduce the complexity of the estimation problem. Attack detection and secure estimation are also investigated for linear systems under sensor attacks in the presence of noise, which shows that detectable attacks can be detected in finite, yet sufficiently large number of steps. The complexity of the estimation problem can be efficiently reduced by satisfiability modulo theory. However, the estimation algorithms are in a centralized form. Mo et al. investigated the resilient detection when there exist s attacks to p sensors. A minimax optimization method is proposed to minimize the worst-case probability of error against all possible manipulations by the attackers. These results are extended. Again, it only shows that, asymptotical convergence is obtained and when the number of attacks is more than or equal to half the number of sensors, attackers can render the information provided by the manipulated measurement useless, thus an optimal worst-case detector is proposed by solely using the a-priori information, without utilizing all measurements. Also note that, the results are in centralized form and only admit exponential convergence of estimation errors or a finite, yet sufficiently large steps to obtain the secure state estimation, which implies that it may need quite long time. This calls for a prescribed finite time distributed secure estimation and secure control. Therefore, motivated by these above discussions, we address distributed secure state estimation and control of the CPSs with some sensors being corrupted by malicious attackers, with the block diagram of the considered CPS and the proposed schemes given in Fig. 1. The presented schemes consist of distributed secure preselectors, distributed finite time observers, and a virtual fractional dynamic surface-based distributed secure controllers. Our contributions and methodologies can be outlined as follows. By exploring the distinct properties of unidentifiable attacks for linear CPS, sufficient conditions that secure state estimation can be solvable is established. Also based on the above conditions, we propose distributed observers with secure preselectors to solve the secure state estimation problem. It is shown that under the sufficient conditions, states can be exactly obtained in a given finite time. Then with the obtained secure state estimation, distributed secure controllers based on a virtual fractional dynamic surface are designed, which guarantee that, the state of the CPS can be made track the desired trajectories with finite time containments, and all the signals are continuous and bounded. Also some guidelines on the choice of the design parameters are presented. The remainder of this paper is organized as follows. In Section II, system models are given and some definitions are presented. Also our objectives are formulated as secure state estimation and secure control problems. In Section III, a sufficient condition ensuring that the secure state estimation can be solvable is proposed and established. Then finite time observers with secure preselectors are designed to solve the secure state estimation. Section IV proposes a fractional dynamic surface-based distributed secure controller to solve the secure control problem. Section V presents a numerical simulation of a islanded micro-grid system under sensor attacks to verify our theoretical findings. Finally, Section VI gives some concluding remarks.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868781", "references": [66053595, 1486477872, 1972523481, 1974729410, 2003530067, 2010255981, 2039427951, 2044762091, 2062132646, 2074851925, 2083730444, 2112784437, 2221032419, 2273673096, 2324357600, 2395221178, 2556022294, 2963209376, 2963639842]}
{"paragraph": "Recent years have witnessed a considerable increase in the diffusion of, and interest in, new forms of peer production based on decentralized interaction within communities of independent participants. One of the main questions motivating the current interest in peer production concerns the emergence of order in the almost complete absence of hierarchical organizational structures and centralized coordination mechanisms. This issue is at the heart of what Padgett and Powell identify as the problem of “emergence” – or how organizational and social structures arise out of vortexes in the flow of social life rather than being buildings of stone. Addressing questions about the emergence of organized order becomes particularly important – and difficult – when the absence of hierarchical conflict resolution mechanisms makes controversies among participants difficult to settle and potentially detrimental to the peer production process and its outcomes. How can order be achieved and maintained – and how can the production of anything collectively valuable be possible – under conditions of extreme decentralization and latent conflict that characterize peer production? Because conflict increases the private cost of producing public goods, these questions are at the heart of the current debate about the sustainability of peer production, and the survival of the open source production movement. Conflict in peer production organizations cannot just be settled by fiat, or by relying on hierarchical authority, but only by building collective consensus. In the near-absence of formal organizational structure, peer production projects are mostly regulated through informal networks arising from task-oriented interaction within communities of participants. Our study is guided by two non mutually exclusive hypotheses on the emergence of organizational order from decentralized text production and editing activities in Wikipedia – the free encyclopedia that anyone can edit, and the peer production organization of interest in this paper. Wikipedia may be considered as broadly representative of open peer-production projects where voluntary participants contribute and edit content that is made collectively and freely available. The hypotheses we formulate allow us to express fundamental theoretical principles of social organization in terms of hypotheses on the evolutionary dynamics of signed event networks. According to the first hypothesis, reputation hypothesis, positive and negative interaction (i.e., agreement and disagreement) are explained by the reputation of the target actor. The reputation hypothesis predicts that more reputable actors are more likely to receive agreement, while disagreement flows toward less reputable actors. According to the second hypothesis, balance hypothesis, expressions of agreement and disagreement are organized according to membership in latent communities of friends and enemies. The balance hypothesis predicts that agreement is expressed mainly towards friends and disagreement mainly towards enemies – regardless of their reputation. Balance theory explains the formation of signed networks, but empirical evidence for it has been mixed. Some suggest status theory as an alternative which can explain the structure and evolution of signed networks. Status theory predicts that negative relations tend to point away from actors with high status and toward actors with low status, while positive relations tend to flow from lower to higher status actors. The predictions derived from the reputation hypothesis are a subset of the predictions that can be derived from status theory. More precisely, the reputation hypothesis makes only predictions related to in-coming relational events, such that actors that received many positive events in the past are more likely to receive positive events and less likely to receive negative events in the future, and actors that received many negative events in the past are less likely to receive positive events and more likely to receive negative events in the future. The reputation hypothesis reflects an alter-centric conception of status as a social position conferred to ego by alters through acts of deference. This view of status is considered alter-centric because deference cannot be seized by an actor but rather is something that is awarded by others. The predictions of the reputation hypothesis are consistent with results produced by studies of dominance hierarchies in animal societies. These alternative perspectives on the micro-mechanisms of network formation imply different network macro-structures. According to the reputation hypothesis, actors are assigned reputation values from a one-dimensional scale that influences probabilities to receive agreement or disagreement, regardless of the sending actor. The ratio of incoming positive ties over incoming negative ties would increase with higher reputation. In contrast, the balance hypothesis posits the emergence and progressive crystallization of a polarized network in which two groups, factions, mutually fight each other. Membership in these groups explains the probabilities to receive agreement or disagreement – but only if we take into account the group membership of the sending actor. According to balance theory, actors would be more likely to agree with members of their own group but more likely to disagree with members of the other group. Thus, members from opposing factions would assess contributions of the same third user differently. We test these hypotheses in an analysis of networks of signed relational events among the contributing users of the 1206 Wikipedia articles that are labeled as controversial. Controversial articles are those which regularly become biased and are likely to suffer future disputes, as defined by Wikipedia. We extend currently available relational event models and analyze patterns of agreement, positive relations, and disagreement, negative relations, among contributing users of Wikipedia given the full history of their previous interaction. The models we specify and estimate include explanatory mechanisms encoding effects consistent with both balance and reputation hypotheses. We focus on the subset of controversial Wikipedia articles because we expect this context to be uniquely useful for identifying and illuminating the coordination mechanisms underlying the hypotheses that we have outlined and because controversial articles involve a high level of interaction among contributors almost by definition. Because our study covers the complete lifetime of Wikipedia, this feature of controversial articles gives rise to a very large sample of relational events. The bipartite structure directly connecting contributing users to text in Wikipedia, dually connects contributors. For this reason, observable expressions of agreement or disagreement connecting users to text through acts of editing may be interpreted as agreement or disagreement between users. Thus, contributors interact through their joint involvement in the production of text – the raw input of Wikipedia articles. We are interested in identifying and interpreting patterns in the emergent social order in Wikipedia resulting from this signed event network. Signed network data have been collected and analyzed throughout the history of social network analysis, and the analysis of signed network relations is currently experiencing a surge of renewed interest. Linking the dynamics of production relations in Wikipedia to explicit models for signed networks allows us to illuminate fundamental general issues in the analysis of peer production. To foreshadow the results of our analysis, we find strong support for balance theory: the micro-dynamics of positive and negative events seem to support a balanced, and hence polarized, macro-structural social order. An additional contribution of our study is to show that the alter-centric status implications of the reputation hypothesis, namely the predictions related to in-coming relations, receive strong empirical support; but, on the other hand, the predictions of status theory related to out-going relations are not supported. Specifically, we show that actors initiating many negative events do not necessarily have high status, and actors initiating many positive events do not necessarily have low status. Likewise, we find no empirical evidence for the anti-reciprocity of positive and negative relations predicted by status theory. Thus the empirical predictions of status theory, which received strong support in the context of signed networks of relations such as like/dislike or high/low esteem, have to be restricted in our setting to the alter-centric components of status that are captured by our reputation hypothesis. Above and beyond these substantive insights, our paper also makes a clear methodological contribution by extending current relational event models for signed, weighted, and directed social interaction data. Building on previous models, we make several methodological improvements that are necessary to deal with large networks of relational events and propose an indicator of user reputation that proves to be one of the strongest and most reliable predictors for future positive or negative events. After presenting the theoretical background in more detail in Section 2, we describe the empirical setting in Section 3. The construction of edit networks and relational event models for these is detailed in Section 4. Results are presented and discussed in Section 5. We close by discussing implications of our work and outline possibility for future work in Section 6.", "section": "Introduction", "doi": "10.1016/j.socnet.2018.12.003", "references": [575879543, 1595890742, 1930424165, 1967343101, 1968058338, 1991430386, 2003959894, 2012701861, 2024962845, 2043026073, 2043253351, 2054703469, 2067509321, 2081085399, 2086038483, 2089973895, 2093753056, 2102064986, 2102271161, 2102664282, 2111122424, 2120476697, 2125815607, 2132531582, 2132781340, 2136387422, 2137818934, 2142517301, 2148540666, 2148785382, 2158139315, 2164181247, 2170340032, 2232472329, 2285889764, 2741466057, 2950048827]}
{"paragraph": "The impact of information technology (IT) security vulnerabilities can be substantial: In an industry study, IBM estimates that reputation-related costs resulting from software security vulnerabilities which lead to a disruption of business operations range in the millions of dollars per disruption. The economic consequences of breaches have been examined by FireEye, a network security company. Specifically, their data breach cost report for 2016 revealed that 76% of respondents would take their business away from a vendor that had demonstrated negligent data handling practices. Similarly, the 2016 Cost of Data Breach report showed that the average total cost of a breach is US$4 million, an increase of 29% since 2013, with disruptions in daily operations being the most severe category of impact. In the aftermath of a breach, firms are challenged to mitigate the long-term financial impact by restoring customer trust. In essence, these reports indicate that vulnerabilities pose permanent risks for firms for which they need to be prepared. These risks are as diverse as they are plentiful, e.g., network attacks, loss or theft of personal data, loss or theft of commercially sensitive information, inoperable IT systems, intellectual property infringement, and extortion, which can lead to serious financial damage. Predictions of the numbers of post-release vulnerabilities are an important input for several managerial decisions in which avoiding aforementioned damages is a critical objective. Especially, those that are designed without assuming access to proprietary information, such as code structure or software development practices, are needed in a range of situations. First, from the perspective of organizations developing software, established techniques for predicting and detecting bugs are complemented by techniques specifically designed for forecasting post-release vulnerabilities. In this specific context, vulnerability forecasting methodologies which do not require analyses of software systems are convenient for developers to avoid degrading service quality and to assess vulnerabilities when software systems are not available, e.g., due to maintenance. Significant managerial decisions include proactively prioritizing and directing resources for security inspection, testing, and patching accordingly. Predicted numbers of vulnerabilities can also serve as critical input for strategic decisions on when to release a software product. Second, from the perspective of organizations managing their software portfolio, numbers of vulnerabilities expected in external software products inform decisions to acquire, and discontinue software. In this case, forecasting techniques that do not require access to the code or other non-public information are the only viable option to forecast vulnerabilities of proprietary software whose code is not publicly accessible. Such assessments of vulnerability offer measures of trustworthiness and security of software products, which are necessary to evaluate the functional characteristics of software products in software portfolio management decisions, including selection and discontinuance decisions. Third, organizations developing apps and extensions must react to vulnerabilities and corresponding security updates of their underlying platform software, such as browsers and operating systems, extending the relevance of anticipating vulnerability occurrence to resource planning and platform-homing decisions of third-party developers. Our study focuses on the research challenge of forecasting the number of post-release security vulnerabilities in subsequent periods of time. Time-series analyses can be expected to provide a viable option for vulnerability predictions for two reasons. First, the rolling-release model adopted by many software projects, such as the Linux kernel, results in the regular release of revised software that can be subjected to scrutiny and attacked by hackers. Second, annual hacker meetings create regular spikes in vulnerability searches. Although substantial research on pre-release vulnerability detection has been published, our sample does not provide evidence for declining post-release vulnerabilities detection rates for software products still under active development. This indicates that despite evolving techniques for pre-release vulnerability detection, the importance of post-release vulnerability forecasting remains intact. To reliably forecast the number of vulnerabilities for a particular system or software package, we contend that forecasting methodologies must account for four fundamental properties of security vulnerabilities. First, vulnerabilities are rare events; to be specific, it is not uncommon that no vulnerabilities are reported throughout several months. Second, with respect to those months where vulnerabilities are observed, there are a few periods in which a comparatively high numbers of vulnerabilities are reported. For instance, 19 vulnerabilities were reported for the Firefox browser in April 2012, while there were none in May and June, 2014. Third, time series of vulnerabilities are not necessarily stationary, meaning that they do not have the same expected value and variance at each point in time. One reason for this is the development of software within the version history. While some versions represent minor changes, others include substantial changes in the software. For example, the completely overhauled Firefox implemented in the new Quantum version represented major changes in performance and security. These include a stricter and more confined framework for extensions and additional sandboxing. In our study, we therefore take different versions of each package into account and examine them separately. Finally, the discovery of vulnerabilities may follow seasonal patterns, which is explained by the increasing implementation of time-based software release cycles, and which are becoming the dominant development model in open-source and proprietary projects. For instance, the Linux project releases new kernels on a regular basis, while Microsoft follows a time-based model for releasing updates for Windows. The academic literature dealt with the study of IT security vulnerabilities using regression techniques for prediction, machine learning techniques, statistical analyses with the help of reliability growth models and vulnerability discovery models, and time series analysis. While an evaluation of these methodologies shows sound performance values, we observe that none of these approaches consider methodologies which account for the unique rareness of occurrence and high volatility of vulnerabilities. Furthermore, only two recent studies focus on vulnerability forecasting from a time series perspective. While one implemented ARIMA and exponential smoothing, the other implemented both regression models and machine learning techniques to forecast vulnerabilities of browsers, operating systems, and video players. Both studies show an acceptable fit and can be helpful to forecast security vulnerabilities. However, the techniques applied in these studies do not explicitly address the specific properties of security vulnerabilities. Since prediction accuracy depends on the characteristics of the forecasting methodology, we implement methodologies that are particularly suitable for the properties of security vulnerability time-series, such as Croston’s methodology. Furthermore, the particular system or software package under consideration needs attention, as different packages have different release cycles and numbers of vulnerabilities that are not taken into account when not grouped together. It is necessary to differentiate between different versions due to changes within the development history. We therefore argue that the prediction accuracy depends on the system or software packages. For instance, the number of vulnerabilities is related to the market share and the maturity stage of the product: for example, some researchers point out that if a system or software starts to attract attention and users start switching to it, the number of vulnerabilities will increase. Another example is the degree of maturity. A system or software is likely to have more vulnerabilities in its early stages rather than a mature one which has been used and tested for years. Finally, the usage of suitable accuracy metrics is also a crucial point when examining the forecast quality. The academic literature provides a lot of accuracy metrics, but not all are suitable when the time series are zero-inflated. For example, prediction accuracy metrics which compute the percentage error of the forecast and actual vulnerabilities are not adaptable by definition. These metrics produce infinite or undefined values when there are no actual vulnerabilities reported for time t. The aforementioned arguments concerning the methodology, object and metrics of vulnerability prediction result in the research question, how accurately can different forecasting methodologies predict IT security vulnerabilities, for which we analyse the accuracy with regard to its robustness along the dimensions of examined system and software packages and applied metrics. To the best of our knowledge, this study is the first that analyses the effect of forecasting methodologies which take into account the uniqueness and rareness of vulnerability time series and applies forecasting metrics that are suitable in this context. The remainder of the paper is structured as follows: Next, we provide an overview of related work. In Section 3, we explain our methodology and the data set. In Section 4, we present and discuss the results of our empirical study. The paper closes with a summary.", "section": "Related Work", "doi": "10.1016/j.cose.2019.101610", "references": [110007310, 147550463, 172316423, 1497444954, 1501506223, 1588821330, 1605921502, 1964593071, 1965520378, 1989989024, 1997236144, 2004758929, 2021348304, 2043837581, 2048872030, 2056646884, 2067148378, 2069910799, 2079753286, 2089055951, 2093973026, 2094586065, 2096274199, 2106578314, 2107548653, 2111965688, 2113693268, 2120197657, 2123258673, 2131937798, 2137789775, 2140243388, 2142141201, 2163593802, 2166336492, 2191886897, 2278080352, 2337585601, 2516920790]}
{"paragraph": "In the area of geometry there are now a large number of computational tools that can be used to perform many different tasks, dynamic geometry systems, computer algebra systems, geometry automatic theorem provers, among others. All these tools are clients of geometric information—information that can be found on repositories of geometric knowledge such as Intergeo, TGTP, GeoGebra Materials, among others. It can be claimed that the usefulness of such servers of geometric information is directly related with the possibility of an easy retrieval of the information a given user is looking for. Therefore the information should be organised in such a way that it will be possible the design of filters adjusted to the user's preferences. The organisation of information through the taxonomy concept allows to allocate, retrieve and communicate information within a system in a logical way, that is, in classes, subclasses, sub-subclasses, and so on. Each of these levels aggregate information about the existing documents in the repository. An advantage of this form of access is the user's guarantee of best selection of searched term, since the classes contain mutually exclusive topics. Different taxonomies would answer to different users' needs. The problems in the servers must be classified in such a way that, in response to a client query, only the problems in the user's level and/or interest and/or language are returned. If the organisation of a large field of knowledge like mathematics are to be considered, taxonomies like Mathematics Subject Classification can be found. In this paper the much narrower scope of constructive geometry is addressed, i.e. geometric constructions made by dynamic geometry systems and geometric problems, eventually with an associated construction, manipulated by geometric automated theorem provers. The term “geometric problem” is here used in a general way, it is used in relation to the case study, i.e. taxonomies for TGTP. There was and there is an important debate on the distinction between problems and theorems, some scholars consider that they should be distinguished, others consider that problems can be reduced to theorems, others consider that theorems can be reduced to problems. This debate is very old and continues today. However, this paper is a case study about taxonomies for TGTP, therefore theorems, their proofs, construction problems and their solutions are considered from a single and uniform taxonomic point of view. For researchers in geometric automated theorem proving it will be interesting to look for: conjectures not yet proved by GATPs; theorems with readable proofs; theorems proved efficiently; etc. If a taxonomy based on those criteria would please the researchers in automatic geometric reasoning, it might not be completely suitable for the educational community. When designing filters for educational purposes, education levels, levels of geometry reasoning and also personal preferences must be considered. Adding to other approaches, a new approach to geometrography is considered, taking into account a very interesting point of view on geometrical constructions classification. Applying geometrography's principles to the dynamic geometry systems, it is possible to redefine the concepts of coefficient of simplicity and a new coefficient of freedom to measure the complexity and dynamics of a DGS construction. With reference to the paper's case study, Thousands of Geometric problems for geometric Theorem Provers (TGTP) is a web-based repository of geometric problems with integrated GATPs. It is being developed to support the testing and evaluation of geometric automated theorem proving systems. The list of problems in TGTP can be explored with some powerful textual and geometric search mechanisms, but, if adapting the search to each user's needs is pursued, it is necessary to introduce a classification for each TGTP problem, stating their characteristics, in face of one or more intended users' expectations. Originally TGTP was aimed to the geometric automatic theorem provers community, as said above to support the testing and evaluation of geometric automated theorem proving systems, so its expected audience is mostly researchers whose background is mathematics and/or computer science and whose research focus is automatic reasoning, formalisation of mathematics, artificial intelligence, among others. The interest in proofs and proving in mathematics education and the application of GATPs in mathematics education opens a new community of potential users of TGTP. The classification of each TGTP problem at a given educational level should be possible and not very difficult. The classification accordingly to a level of geometry reasoning would be more difficult. Nevertheless such classifications are useful in any educational environment, e.g. when linking with educational platforms like the Web Geometry Laboratory. The current search mechanisms in TGTP allow its users to search for a given specific problem, or set of problems, e.g. look for Ceva's theorem, look for all problems with the word “circumcircle” in its description, look for problems containing some given geometric configuration. The introduction of taxonomies in TGTP can add a filtering step that, together with the text and geometric search mechanisms, will allow to tailor TGTP's usefulness to each user's needs. For example, a secondary school teacher preparing a class about circumcenter centre would choose filters: CCS classes, CO.A.1 and C.A.3; construction complexity, simple; proofs in education, verification: good, filtering the TGTP database, or any other geometry knowledge repository, in such a way that a good set of examples could be browsed and choose as teaching materials. An interesting development of this research is the application of the taxonomies presented in this paper to proof assistant area. We do not consider this application here, however we give some suggestions about it. Overview of the paper: The paper is organised as follows: first, taxonomies for GATP research and taxonomies for education will be discussed. Then, the application of taxonomies to TGTP will be analysed providing some examples, also a new approach to geometrography will be presented. Finally, conclusions are drawn and future work will be discussed.", "section": "Methodology", "doi": "10.1016/j.jsc.2018.12.004", "references": [144472580, 158485348, 161392887, 1268136390, 1409110429, 1515021858, 1559406907, 1839505014, 2031433674, 2032405865, 2064030682, 2088823840, 2120243289, 2125565018, 2131408699, 2136058053, 2282070325, 2567325662, 2609408480, 2613478558, 2793599632, 2803506036, 2887162633]}
{"paragraph": "The first geometry software tools, such as the Geometric Supposer, appeared in early 1980s, followed by Cabri, the Geometer's Sketchpad and a number of others, like Cinderella, DrGeo, Eukleides, GCLC, KSEG, to name just a few. Geometry tools have found their way to classrooms worldwide and became an irreplaceable component of mathematical education. The most popular geometry, and not only geometry, educational tool GeoGebra has tens of millions of users. Over the years, geometry tools got additional features, including intelligent ones, such as automated proving based on a range of methods of properties of constructed geometric figures. Recently, functionalities of increasingly popular touch-based devices, such as mobile phones and tablets, got supported by some geometry tools, for instance, Sketchometry and QuickDraw. The key problem here is recognition of geometric figures drawn on a screen by fingers or pens; this feature is related to but still significantly different from recognition of geometric objects and their relationships from scanned images. Most of geometry software tools are dynamic geometry tools, built from the very beginning and still around one central scenario: the user chooses several free points and, using them, constructs some other points and other geometric objects. Then, the user can move, drag, a chosen free point and explore how the constructed points and other objects change. This feature is very simple but very appealing and effective in mathematical education: instead of exploring static geometric figures, students can explore closely related figures obtained by dragging some free points and guess some properties about them. We are not aware of any DGS tool that allow dragging the constructed points. That is certainly not trivial, unlike the standard scenario where it is easy to recompute all constructed objects based on new locations of the free points. Namely, in order to drag one constructed point while keeping some other two points fixed, one has to solve a possibly complex, and sometimes even unsolvable using straightedge and compass, construction problem. Let us consider a simple example: the user chooses three free points A, B, and C and then, using angle bisectors and intersections, constructs the incenter I of the triangle ABC. If the user drags, for instance, the point A, the construction of the point I is simply replayed with the new points A, B, and C, and the new point I is obtained. Let us suppose now that the user wants to drag the point I while keeping B and C fixed, and allowing A to move: for that, the following non-trivial construction problem has to be solved: given a point B, a point C, and a point I, construct the point A, such that I is the incenter of the triangle ABC. This functionality that we are aimed at is illustrated in a figure. In this paper we describe our prototype dynamic geometry tool Touch&Drag that enables dragging constructed points. It uses solutions of hundreds of location construction problems generated automatically by our solver ArgoTriCS. Compared to most of other popular geometry tools, the second distinctive feature of Touch&Drag is that it uses functionalities of touch based devices, but the functionality for dragging constructed points can be integrated also into other sorts of dynamic geometry tools.", "section": "Introduction", "doi": "10.1016/j.jsc.2018.12.002", "references": [1534758548, 1976053863, 1984350501, 2006962766, 2010315317, 2032405865, 2053860863, 2064030682, 2093871241, 2099684934, 2131408699, 2294949920, 2400850545, 2795900304, 2950855305]}
