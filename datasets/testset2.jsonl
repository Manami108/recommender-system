{"paragraph": "Symbolic methods are used to reason about concurrent systems specified by rewrite theories in many ways, including cryptographic protocol verification, logical LTL model checking, rewriting modulo SMT and related approaches, inductive theorem proving and program verification, and reachability logic theorem proving. One key issue is that the rewrite theories used in several of these approaches go beyond the standard notion of rewrite theory and also beyond the executability requirements. For example, conditions in rules are not just conjunctions of equations, but quantifier-free formulas in an often decidable background theory, and the rewrite rules may model open systems interacting with an environment, so that they may have extra variables in their righthand sides. Furthermore, each of the approaches just mentioned makes different assumptions about the rewrite theories they handle; no general notion has yet been proposed. There are also unsolved issues about symbolic executability; even though symbolic execution methods in some ways relax executability requirements, in other ways symbolic execution imposes strong restrictions on the rewrite rules to be executed. For example, unless both the lefthand and righthand sides of a rewrite rule are terms in an equational theory having a finitary unification algorithm, symbolic reachability analysis becomes extremely difficult and is usually outside the scope of current methods. There is also plenty of terra incognita. For example, we all assume and require that the rewrite theories we are going to symbolically execute are coherent. But no theory of coherence, or methods for guaranteeing it, have yet been developed for these new kinds of theories. The upshot of all this is that, as usual, the new wine of symbolic reasoning requires new wineskins. This work is all about such new wineskins. It begins by asking, and providing answers for, two main questions: How can the notion of rewrite theory be generalized to support symbolic reasoning? and What are the appropriate symbolic executability requirements needed for such rewrite theories, and how can they be ensured for, and made available to, as wide as possible class of theories? Questions are answered as follows. The first question is answered by motivating and presenting a notion of generalized rewrite theory suitable for symbolic reasoning and subsuming the standard notion as a special case. It also defines an initial model semantics for such theories in an associated category of algebraic transition systems. The second question is then answered by using such a semantics to identify symbolic executability requirements, including a generalized notion of coherence and an easier to check characterization of it. Solutions are provided for two related problems: how can ground coherence be ensured automatically under reasonable requirements, and how can the class of generalized rewrite theories that can be symbolically executed be made as wide as possible by means of adequate theory transformations. The answer to the first problem is new even for standard rewrite theories and can be quite useful to semi-automate equational abstractions. The answer to the second problem is very general; under mild conditions symbolic executability can be ensured for a very wide class of generalized rewrite theories by two theory transformations. Once answers to the above foundational questions have been given, one can ask, and provide answers to, the following high-level question: What suitable symbolic methods can be developed to reason about generalized rewrite theories, including properties satisfied by the initial models of such theories? Since different methods are possible, the following answers are given. Since the symbolic reasoning involved is about the behavior of the concurrent system specified by a generalized rewrite theory, the first order of business is to find a language of state predicates amenable to symbolic reasoning with the rewrite theory. The language proposed further develops the language of pattern predicates, whose atomic formulas are constrained terms, where u is a constructor term and φ is a quantifier-free formula, so that it describes the ground instances of u that satisfy the constraint φ. Rules in a generalized rewrite theory are conditional rules where φ is a QF formula. Rewriting with such rules is shown to be sound and complete to answer universal reachability questions. Due to the presence of extra variables in r and φ, execution of such rewrite rules may be hard to mechanize; therefore, sufficient conditions making this kind of generalized rewriting decidable are also given. For symbolic model checking applications the crucial question is how to solve existential reachability problems, where A and B are pattern predicates and we want to know if there is a concrete state satisfying predicate A from which a concrete state satisfying B can be reached. For example, a theory may specify a cryptographic protocol, A may specify its initial states, and B may specify a class of attack states. It is shown that for the wide class of rewrite theories characterized, rewriting defines a predicate transformer on the power set of the set of states, which can be effectively computed when such sets of states are definable by pattern predicates. Such effective description coincides with the notion of constrained narrowing, which therefore provides a sound and complete symbolic method for existential reachability analysis. Since the most common symbolic model checking problems involve the verification of invariants, it is explained in detail how invariants and their complements, including the case of inductive invariants and coinvariants, can be analyzed and sometimes fully verified by the constrained narrowing method. Rewriting modulo SMT can be naturally understood as a special, restricted case of the more general constrained narrowing method, which implicitly subsumes it. This does not decrease the usefulness of rewriting modulo SMT, since its implementation can be substantially more efficient than that of constrained narrowing. Section 2 gathers preliminaries. Section 3 defines generalized rewrite theories, their categories of models, including initial ones, and studies in detail the coherence problem for such theories. Section 4 then defines several theory transformations that can automatically ensure coherence. The language of pattern predicates and its computability properties are presented in Section 5. The last three symbolic methods described are then presented in Section 6. Related work and conclusions are discussed in Section 7. Proofs are relegated to Appendix A. This paper substantially extends the conference paper in the following ways. Sections 5–6 on pattern predicates and on symbolic methods are entirely new and contain many new concepts and results. Sections 1, 3 and 7 and the list of References have been substantially expanded. Many fully developed examples are presented. Proofs of all previous and new results are included.", "section": "Introduction", "doi": "10.1016/j.jlamp.2019.100483", "references": [8727156, 9489112, 26731332, 111391299, 176106274, 213397635, 967948971, 1140833825, 1439634531, 1480537711, 1484737241, 1496435538, 1510965759, 1511903507, 1516900991, 1536580911, 1547751563, 1550776561, 1551437553, 1568059522, 1570621944, 1598154376, 1601294428, 1603788080, 1678943029, 1751962834, 1760894569, 1813383841, 1850275321, 1860618800, 1861590051, 1878255659, 1964783683, 1966865798, 1987280813, 1995691455, 2007008828, 2016262003, 2022877924, 2029176884, 2029227380, 2042924775, 2042987983, 2047406870, 2047719075, 2056016391, 2063727779, 2066904164, 2068188619, 2084330491, 2101508170, 2101989701, 2106892818, 2121096888, 2131863441, 2131886132, 2146005157, 2163482209, 2170798886, 2171347031, 2172982628, 2223467641, 2259551671, 2282814833, 2291243181, 2293504102, 2294307095, 2296667264, 2316113835, 2531460317, 2535818195, 2586953020, 2609361663, 2755558640, 2760967884, 2771728256, 2791470433, 2791762271, 2798818732, 2810367355, 2913689616, 2949280820]}
{"paragraph": "The impact of information technology (IT) security vulnerabilities can be substantial: In an industry study, IBM estimates that reputation-related costs resulting from software security vulnerabilities which lead to a disruption of business operations range in the millions of dollars per disruption. The economic consequences of breaches have been examined by FireEye, a network security company. Specifically, their data breach cost report for 2016 revealed that 76% of respondents would take their business away from a vendor that had demonstrated negligent data handling practices. Similarly, the 2016 Cost of Data Breach report showed that the average total cost of a breach is US$4 million, an increase of 29% since 2013, with disruptions in daily operations being the most severe category of impact. In the aftermath of a breach, firms are challenged to mitigate the long-term financial impact by restoring customer trust. In essence, these reports indicate that vulnerabilities pose permanent risks for firms for which they need to be prepared. These risks are as diverse as they are plentiful, e.g., network attacks, loss or theft of personal data, loss or theft of commercially sensitive information, inoperable IT systems, intellectual property infringement, and extortion, which can lead to serious financial damage. Predictions of the numbers of post-release vulnerabilities are an important input for several managerial decisions in which avoiding aforementioned damages is a critical objective. Especially, those that are designed without assuming access to proprietary information, such as code structure or software development practices, are needed in a range of situations. First, from the perspective of organizations developing software, established techniques for predicting and detecting bugs are complemented by techniques specifically designed for forecasting post-release vulnerabilities. In this specific context, vulnerability forecasting methodologies which do not require analyses of software systems are convenient for developers to avoid degrading service quality and to assess vulnerabilities when software systems are not available, e.g., due to maintenance. Significant managerial decisions include proactively prioritizing and directing resources for security inspection, testing, and patching accordingly. Predicted numbers of vulnerabilities can also serve as critical input for strategic decisions on when to release a software product. Second, from the perspective of organizations managing their software portfolio, numbers of vulnerabilities expected in external software products inform decisions to acquire, and discontinue software. In this case, forecasting techniques that do not require access to the code or other non-public information are the only viable option to forecast vulnerabilities of proprietary software whose code is not publicly accessible. Such assessments of vulnerability offer measures of trustworthiness and security of software products, which are necessary to evaluate the functional characteristics of software products in software portfolio management decisions, including selection and discontinuance decisions. Third, organizations developing apps and extensions must react to vulnerabilities and corresponding security updates of their underlying platform software, such as browsers and operating systems, extending the relevance of anticipating vulnerability occurrence to resource planning and platform-homing decisions of third-party developers. Our study focuses on the research challenge of forecasting the number of post-release security vulnerabilities in subsequent periods of time. Time-series analyses can be expected to provide a viable option for vulnerability predictions for two reasons. First, the rolling-release model adopted by many software projects, such as the Linux kernel, results in the regular release of revised software that can be subjected to scrutiny and attacked by hackers. Second, annual hacker meetings create regular spikes in vulnerability searches. Although substantial research on pre-release vulnerability detection has been published, our sample does not provide evidence for declining post-release vulnerabilities detection rates for software products still under active development. This indicates that despite evolving techniques for pre-release vulnerability detection, the importance of post-release vulnerability forecasting remains intact. To reliably forecast the number of vulnerabilities for a particular system or software package, we contend that forecasting methodologies must account for four fundamental properties of security vulnerabilities. First, vulnerabilities are rare events; to be specific, it is not uncommon that no vulnerabilities are reported throughout several months. Second, with respect to those months where vulnerabilities are observed, there are a few periods in which a comparatively high numbers of vulnerabilities are reported. For instance, 19 vulnerabilities were reported for the Firefox browser in April 2012, while there were none in May and June, 2014. Third, time series of vulnerabilities are not necessarily stationary, meaning that they do not have the same expected value and variance at each point in time. One reason for this is the development of software within the version history. While some versions represent minor changes, others include substantial changes in the software. For example, the completely overhauled Firefox implemented in the new Quantum version represented major changes in performance and security. These include a stricter and more confined framework for extensions and additional sandboxing. In our study, we therefore take different versions of each package into account and examine them separately. Finally, the discovery of vulnerabilities may follow seasonal patterns, which is explained by the increasing implementation of time-based software release cycles, and which are becoming the dominant development model in open-source and proprietary projects. For instance, the Linux project releases new kernels on a regular basis, while Microsoft follows a time-based model for releasing updates for Windows. The academic literature dealt with the study of IT security vulnerabilities using regression techniques for prediction, machine learning techniques, statistical analyses with the help of reliability growth models and vulnerability discovery models, and time series analysis. While an evaluation of these methodologies shows sound performance values, we observe that none of these approaches consider methodologies which account for the unique rareness of occurrence and high volatility of vulnerabilities. Furthermore, only two recent studies focus on vulnerability forecasting from a time series perspective. While one implemented ARIMA and exponential smoothing, the other implemented both regression models and machine learning techniques to forecast vulnerabilities of browsers, operating systems, and video players. Both studies show an acceptable fit and can be helpful to forecast security vulnerabilities. However, the techniques applied in these studies do not explicitly address the specific properties of security vulnerabilities. Since prediction accuracy depends on the characteristics of the forecasting methodology, we implement methodologies that are particularly suitable for the properties of security vulnerability time-series, such as Croston’s methodology. Furthermore, the particular system or software package under consideration needs attention, as different packages have different release cycles and numbers of vulnerabilities that are not taken into account when not grouped together. It is necessary to differentiate between different versions due to changes within the development history. We therefore argue that the prediction accuracy depends on the system or software packages. For instance, the number of vulnerabilities is related to the market share and the maturity stage of the product: for example, some researchers point out that if a system or software starts to attract attention and users start switching to it, the number of vulnerabilities will increase. Another example is the degree of maturity. A system or software is likely to have more vulnerabilities in its early stages rather than a mature one which has been used and tested for years. Finally, the usage of suitable accuracy metrics is also a crucial point when examining the forecast quality. The academic literature provides a lot of accuracy metrics, but not all are suitable when the time series are zero-inflated. For example, prediction accuracy metrics which compute the percentage error of the forecast and actual vulnerabilities are not adaptable by definition. These metrics produce infinite or undefined values when there are no actual vulnerabilities reported for time t. The aforementioned arguments concerning the methodology, object and metrics of vulnerability prediction result in the research question, how accurately can different forecasting methodologies predict IT security vulnerabilities, for which we analyse the accuracy with regard to its robustness along the dimensions of examined system and software packages and applied metrics. To the best of our knowledge, this study is the first that analyses the effect of forecasting methodologies which take into account the uniqueness and rareness of vulnerability time series and applies forecasting metrics that are suitable in this context. The remainder of the paper is structured as follows: Next, we provide an overview of related work. In Section 3, we explain our methodology and the data set. In Section 4, we present and discuss the results of our empirical study. The paper closes with a summary.", "section": "Related Work", "doi": "10.1016/j.cose.2019.101610", "references": [110007310, 147550463, 172316423, 1497444954, 1501506223, 1588821330, 1605921502, 1964593071, 1965520378, 1989989024, 1997236144, 2004758929, 2021348304, 2043837581, 2048872030, 2056646884, 2067148378, 2069910799, 2079753286, 2089055951, 2093973026, 2094586065, 2096274199, 2106578314, 2107548653, 2111965688, 2113693268, 2120197657, 2123258673, 2131937798, 2137789775, 2140243388, 2142141201, 2163593802, 2166336492, 2191886897, 2278080352, 2337585601, 2516920790]}
{"paragraph": "Aggregation functions carrying out the process of merging a given number of data into a representative value are essential tools in many applications, from mathematics and computer science to economics and social sciences. There are many different classes of aggregation functions. T-norms and t-conorms are particularly important in the theory of fuzzy sets and its applications. Uninorms with more applications like fuzzy logic, expert systems, neural networks and fuzzy system modeling are important aggregation operators generalizing the notions of t-norms and t-conorms. Similarly, nullnorms are also a generalization of t-norms and t-conorms and they are extremely related to uninorms. Recently, the order generating problem from logical operators has been considered by many researchers. In this sense, T-partial order, induced by t-norms, has been defined on a bounded lattice L. S-partial order induced by t-conorms has been given in a similar way. The set admitting some incomparability with respect to the T-partial order has been defined and deeply investigated. It is well known that the structures of uninorms and nullnorms are special combinations of t-norms and t-conorms. As a natural result of this, V and U-partial orders, respectively induced by nullnorms and uninorms and extending the T and S-partial orders to more general forms, have been introduced and some properties have been investigated. An equivalence on the class of nullnorms on a bounded lattice based on the equality of the sets admitting some incomparability with respect to the order induced by nullnorms has been investigated and the relations between the equivalence classes of nullnorms and the equivalence classes of their underlying t-norms and t-conorms have been studied. Also, some conditions under which the U and V-partial orders coincide have been determined. In the present paper, we study the relationships between the orders induced by nullnorms and uninorms and investigate the relations between the sets admitting some incomparability with respect to those orders. Also, we present some relationships between the algebraic structures related to those orders. The paper is organized as follows. We shortly recall some basic notions and results in Section 2. In Section 3, we deal with the relationships between the orders induced by uninorms and nullnorms, we investigate the sets and their relationships. Also, we present a relationship between the sets for any distributive nullnorms. In Section 4, we study the relationships between the algebraic structures obtained from the U and V-partial orders. We end the paper with a section of conclusions.", "section": "Related Work", "doi": "10.1016/j.fss.2018.12.020", "references": [58344607, 860336327, 950851027, 1775786317, 1979524006, 1986302899, 2002138403, 2024487576, 2043951079, 2048176910, 2048288497, 2060878354, 2062715728, 2064292138, 2087945213, 2093915645, 2114065574, 2270423577, 2334464549, 2398354956, 2560315083, 2612350507, 2738479764, 2753477481, 2778530798, 2911316030]}
{"paragraph": "Space manipulators are playing important roles with the increasing demand of space operations, such as on-orbit assembly, maintenance, and capturing space debris. The problems associated with the space manipulator capturing unknown objects are the model uncertainties and the external disturbances. It is difficult to design model-based controller for precise and fast motion control of the space manipulator. To deal with the model uncertainties and the external disturbances, many control strategies have been proposed, such as adaptive control, robust control, sliding mode control (SMC), fuzzy control, and neural network (NN) control. SMC is widely used in nonlinear systems since it can ensure good performance for both certain and uncertain models. Early results of a sliding mode-based controller for robotic manipulators can be seen, where a PID sliding surface was designed for the variable structure controllers. An adaptive sliding mode controller is designed for trajectory tracking of a space manipulator. The proposed controller is verified using numerical simulations for the nominal model and the uncertain model. A new adaptive sliding mode control scheme is proposed for a robotic manipulator with good tracking performance and small chattering effects. An adaptive integral sliding mode controller combined with time-delay estimation is designed for an industrial robot manipulator. Experimental results verified the accuracy and robustness of the adaptive robust controller. However, most SMC-based control methods can only guarantee the asymptotic stability of trajectory-tracking errors of robot manipulators. For the finite-time control of manipulators, terminal sliding mode control (TSMC) is an effective approach for uncertain systems. Feng and Yu proposed a global nonsingular terminal sliding mode controller for rigid manipulators, which can eliminate the singularity problem related to the conventional TSMC and guarantee the finite time convergence of system-tracking error from any initial state to the equilibrium. A continuous finite-time control scheme was proposed for trajectory tracking of rigid manipulators using a new form of terminal sliding modes. Faster and high-precision tracking performance can be obtained compared to the conventional SMC. TSMC-based control can also be applied to trajectory tracking of autonomous underwater vehicles, spacecraft, and robotic airships. However, for large uncertainties, a large switching control gain is needed, which may lead to reduced control accuracy. In practice, actuator saturation may exist in the finite-time attitude control and the joint control of space manipulators since the finite-time control need a fast-transient response with a large control torque. Many studies have been done on the attitude control of spacecraft under actuator saturation, or the joint control of robotic manipulators with input saturation. When the space manipulators move fast, the spacecraft's body may be affected by the motion of the manipulators. Few studies combine the rapid attitude control of the spacecraft and the joint tracking control of robotic manipulators with model uncertainties, actuator saturation, and external disturbances. A NN is an effective method in dealing with complex dynamic systems since it can learn and approximate any nonlinear function. NN has already drawn much attention in the aerospace field. Different control strategies have been combined with a NN for controlling complex systems. An adaptive NN controller was proposed for robotic manipulators. A NN-based integral sliding mode controller was designed for biped robot. A NN-based terminal SMC scheme was proposed for robotic manipulators including actuator dynamics. This paper focuses on the trajectory tracking of a multilink space manipulator under actuator saturation. The multilink space manipulator can be viewed as a multibody system. Kane's method is the formulation of highly specialized computer-based methodology for the modeling and simulation of multibody systems. Compared to the Lagrange method and Newton–Euler approach, Kane's method was found to be more efficient for the calculations of multijoint manipulators. Kane's equations are both intuitive and easy to develop. Therefore, the dynamics of the space manipulator is derived using Kane's method. A radial basis function (RBF) NN is used to approximate the uncertain model of the space manipulator since it can approximate any given continuous nonlinear function with advantages, such as a simpler structure, fast learning, and good approximation ability. An auxiliary system is designed to compensate for the actuator saturation. Based on the NN approximation and the proposed auxiliary system, a novel finite-time nonsingular terminal sliding mode controller is designed for attitude tracking and joint control of the space manipulator. The main contributions of this paper are summarized as follows: Because the dynamics model of multilink space manipulator is very complex, an efficient formulation of the dynamics model is derived using Kane's method. The dynamics model is not only suitable for the given multilink space manipulator but also for any space manipulator with multiarms and multilinks. A finite-time auxiliary system is proposed for the compensation of actuator saturation. The influence of the actuator saturation on the auxiliary system can be adjusted by the parameter b3, which can avoid any possible overcompensation for the control of space manipulator when the actuator saturation is very large. A novel finite-time controller is proposed combining the NN approximation, the auxiliary system, and the adaptive terminal sliding strategy. The proposed controller can not only achieve finite-time attitude tracking of the spacecraft but also the joint-tracking control of the robotic arm with a good performance. The rest of this paper is organized as follows. In Section II, the dynamics model of the space manipulator is established using Kane's method. An auxiliary system and the NN-based adaptive terminal sliding mode controller are designed for the space manipulator considering model uncertainties, actuator saturation and disturbances in Section III. Lyapunov theory is used to prove the finite-time convergence of the trajectory-tracking errors of spacecraft attitude and manipulator joints. Numerical simulations are presented in Section IV to verify the effectiveness of the proposed controller. Section V concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2902789", "references": [2001218902, 2012890691, 2020349619, 2054307697, 2055930596, 2129275525, 2140814132, 2253274025, 2343693428, 2402675186, 2407115150, 2546451873, 2546578181, 2608224837, 2768993855, 2779392991, 2807402986, 2811291079, 2952877854]}
{"paragraph": "Internet of Things has quietly entered a multiple intelligence industry. Today, in advocating energy savings and environmental protection, the configuration optimization of IoT resources has become an urgent problem to be solved. There are many artificial intelligence algorithms for resource scheduling, which, however, were developed for specific applications and not well suited for solving IoT service problems studied in this paper. As a service, we will regard the entire layout of IoT as a service system. The solution to the resources optimization allocation problem of service-oriented networked collaborative equipment is a very complex issue, which belongs to a typical NP-hard combinatorial optimization problem. The question is: How to minimize the consumption of resources, and shorten the service time? In other words, how can multiple optimal services in enormous candidate sets be selected to meet the above objectives? So it is going to be a challenging multiobjective optimization problem. Many researchers have attempted to solve the problems of multiobjective service selection in web services. Some presented the first approximation scheme for multiobjective quality-driven service selection. Others launched research on multiobjective optimization of quality of service. They introduced Pareto set model for QoS-aware service composition. One approach supported decision makers in finding robust, QoS optimized service compositions using clustering. In our previous research work, we implemented adaptive web service composition inspired by the neuroendocrine-immune system. However, the above work focused on web service composition based on QoS. IoT services, whose features are large-scale, heterogeneity, unreliability, and dynamic in nature, are different from web services. An important challenge to address in the domain of IoT services composition is the development of efficient services selection algorithms for optimal management of both energy and QoS. This issue becomes crucial in the case of large-scale IoT environments composed of thousands of distributed entities. Some stated that IoT is a paradigm in which real-world physical things can be connected to the Internet and provide services through the computing devices attached. A three-layer QoS scheduling model for service-oriented IoT was proposed. The sensing as a service model is expected to be built on top of the IoT infrastructure and services. Then, services were assigned to interfaces with heterogeneous resources and produced optimal solutions for this computationally hard problem. From the analysis of the above literature, other approaches dealing with services selection were mostly unaware of energy issues, or they noted the minimum energy consumption but only as a unilateral goal. There are very few studies on multiobjective optimization in the context of IoT service, especially considering the equipment energy consumption and service time. It is necessary to develop a multiobjective optimization algorithm for IoT services, which can offer more practical value, for example, intelligentized facility agriculture and industrial manufacture. Evolutionary multiobjective optimization has become one of the mainstream research directions in the field of evolutionary computation. Some made a comprehensive review of the modern multiobjective evolutionary algorithms. Typical multiobjective optimization algorithms based on artificial immune systems include multiobjective immune algorithm, constrained multiobjective immune algorithm, an artificial immune network for multiobjective optimization called vector immune system, nondominated neighbor immune algorithm, and so on. Besides, in terms of vaccine, immune genetic algorithm and strategies of selecting vaccines and constructing an immune operator were proposed. Others emulated a biological notion in vaccines to promote exploration in the search space. In recent years, a novel immune clonal algorithm for multiobjective optimization was proposed. A degeneration recognizing clonal selection algorithm for multimodal optimization was designed. A new multiclass clustering method based on maximum margin clustering algorithm and immune evolutionary algorithm was proposed. In addition, some focused on local search strategies. An auxiliary local improvement operator convergence acceleration operator was introduced, and hill climber with sidestep was designed for the local search. A new multiobjective optimization framework based on nondominated sorting and local search was introduced. A novel ranking strategy called global margin ranking was adopted which deployed the position information of individuals in objective space to gain the margin of dominance throughout the population. Furthermore, in order to consider the coordination between the population and environment, and population and population in the evolutionary process, coevolutionary mechanism has been introduced into the immune optimization algorithm, and good results have been obtained for solving combinatorial optimization problems. The competition model and cooperative model are two important models in the coevolutionary multiobjective optimization algorithm. The coevolutionary algorithm based on cooperative model has achieved great success in solving single objective optimization problem. A cooperative coevolutionary algorithm for multiobjective optimization was presented, which was capable of maintaining archive diversity by dynamic sharing and extending operator. A competitive-cooperation coevolutionary algorithm was also proposed. Multiple subpopulations, respectively, optimized the part of decision variables. The difference is that the mapping relationship between each subpopulation and the decision variables are not fixed but determined by competitive results. Some other works also employed coevolutionary technique and multiple populations for multiobjective optimization. Based on immune system model, several subpopulations evolved using different evolutionary strategies. An immune coevolutionary algorithm with two stages was designed to search the optimal balanced partitions. A coevolutionary immune algorithm for garment matching problem was proposed, introducing dominance affinity and distance affinity. Cooperative coevolutionary algorithms for multiobjective capacitated arc routing problem were proposed. Two subpopulations that were cooperatively coevolved using the coevolutionary algorithm were employed to achieve a better global optimality for the estimated radial basis function neural network. A novel coevolutionary mechanism based on elite strategy was proposed, where elite individuals were used to guide the search. A framework named hyper multiobjective evolutionary algorithm was proposed. The size of subpopulation was adjusted according to the corresponding algorithm’s performance. Multiple subpopulations were adopted, and clustering and statistical methods were used to guide the generation of new population and the local search. Moreover, some researchers decomposed a multiobjective optimization problem in a collaborative manner. In addition, inspired by the mammalian endocrine system, an artificial endocrine controller for power management was designed in robotic systems. The endocrine mechanism was introduced to regulate cooperative coevolution among the particles. The aforementioned research work usually adopts multipopulation to implement coevolution; nevertheless it is rare to organize the population in a hierarchical way. Meanwhile, little has been done to embed the endocrine regulation mechanism into the evolution of subpopulations in multiobjective immune algorithm, while there is a natural synergy between the immune system and the endocrine system. Thus, the main motivation of this paper is to simulate the functions of immune-endocrine system and map these mechanisms to the coevolution of multiple populations, so as to more efficiently solve the multiobjective optimization of IoT service problems. Inspired by the existing achievements and the human immune-endocrine mechanism, we propose an immune-endocrine system inspired hierarchical coevolutionary multiobjective optimization algorithm in this paper. This algorithm employs the hierarchical structure, i.e., foundation layer and top layer, which evolves and learns from the ideas similar to previous elite strategy but not the same. It can provide optimal nondominated decision-making for service-oriented resource optimization allocation problem in IoT systems. Experimental results demonstrate the proposed algorithm is efficient to minimize the consumption of resources and shorten the service time. The main contributions of this paper are as follows. Inspired by the endocrine regulation mechanism, an endocrine-based strategy is designed and embedded in the subpopulation evolution process, which can guide efficient cooperative interactions among subpopulations and assist the top population toward global optimal solutions. The human forgetting memory mechanism is introduced into the evolution of the top population, which successfully solves the choice problem of nondominated solutions. Using clustering and statistical method during the evolution process, different components operated on the x-axis and y-axis are proposed, which can make the operations more directionally and purposefully. The rest of this paper is organized as follows. Section defines the multiobjective optimization model of IoT service. Section proposes the algorithm and details search mechanisms and strategies. The performances of the proposed approaches are evaluated and discussed in Section. Section concludes this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2866527", "references": [1150848239, 1511346087, 1965040190, 1967981290, 1968164389, 1970356115, 1980186244, 1988242323, 1994969405, 1999218527, 2011587419, 2017299308, 2020320008, 2021135079, 2034392538, 2035292227, 2035960005, 2039298419, 2040331308, 2046491030, 2053855431, 2055142708, 2077098488, 2095771532, 2098907614, 2102785645, 2106334424, 2121429049, 2122340689, 2126105956, 2137165245, 2147573707, 2156262512, 2169574584, 2193409108, 2219491737, 2271389764, 2272065637, 2275596639, 2323358965, 2336681276, 2338772919, 2344331011, 2401044109, 2517051814, 2517908245, 2586316225, 2911992235]}
{"paragraph": "Organizations need to innovate in response to changing customer demands and opportunities offered by technology and changing marketplaces, structures and dynamics. Joshi, Chi, Datta, and Han examine the relationship between IT and firm innovation focusing on knowledge capabilities that are enhanced through the use of IT, and demonstrate that IT plays a significant role in enhancing firm innovation. The combination of Big Data and Business Analytics represents one of the latest opportunities for organizations to change their practices by the use of IT. It is argued that organizations need to act swiftly to benefit from Big Data and Business Analytics by using them to create innovation and competitive advantage. The concept of Business Analytics is not new, but has recently re-emerged as an important area of study owing to its developing capabilities to handle Big Data. New IT processing technologies such as Hadoop and cloud services enable Business Analytics to deal with Big Data to provide descriptive, predictive and prescriptive analysis. Business Analytics thus clearly has commonalities with Operational Research. Ranyard, Fildes, and Hu refer to Business Analytics as apparently extending the scope of Operational Research practice, but the precise relationship between Business Analytics and Operational Research remains a matter of debate. Although Business Analytics is increasingly being used in organizations, there is a lack of theory linking analytics to innovation, and hence also a lack of practical guidance for managers. In particular, models of the innovation process do not usually include any explicit form of data acquisition, analysis or use. For example, Choi, Narasimhan, and Kim include only generating rates of product and process knowledge, the process of generation being unspecified, and Pan and Li similarly use only learning rate parameters. An exception is the work of Vidgen, Shaw, and Grant. One of the research questions they considered was how do organizations extract or create value from data. Their analysis - a Delphi study and three case studies - led to 21 recommendations, though there was no attempt to structure these into a causal model. Despite strong claims that Business Analytics can enhance innovation through product or service differentiation using Big Data, there remains a need for theory and empirical evidence to link Business Analytics and innovation. Many businesses are still struggling to figure out how, where and when to use Business Analytics to achieve a worthwhile return. Until the mechanisms underlying Business Analytics and its contribution to improved business performance are better understood, realizing desired outcomes, such as innovation, remains uncertain. It is notable that the research agenda for Operational Research in the analytics age set out by Mortenson et al. concentrates on research into Business Analytics itself rather than on links between Business Analytics and outcomes. Therefore, it is imperative to investigate and confirm if, how and to what extent Business Analytics contributes to innovation. This paper seeks to fill this research gap by proposing and validating a new model to explain the relationships between Business Analytics and product or service innovation. In so doing, it is important not to regard Business Analytics as just a technical development, but also one related to organizational culture. Like any technique, Business Analytics will always yield findings of some sort, but only if organizations choose to act on those findings can any innovation occur. Achieving competitive advantage as a result would be clear evidence that organizations have acted on the Business Analytics findings. An appropriate cultural focus when examining Business Analytics is the concept of data-driven culture. The term data-driven culture has been in use for many years, but with the emergence of Big Data, it has attracted much more attention from practitioners and researchers because they argue that to maximize the potential Business Analytics business value, a relevant organizational culture must be in place. Most Operational Research writers on Business Analytics acknowledge the importance of organizational culture, but few consider the acquisition of the data being analyzed, Hindle and Vidgen and Pape being notable exceptions. Yet the acquisition of data needs to be a purposeful activity – part of environmental scanning, which is a basic process of any organization to acquire and use data from the external environment to assist management in problem definition and decision making. As Big Data technologies enable organizations to acquire a vast array of data about their environments, the role of environmental scanning Big Data must be considered when studying Business Analytics' impact on innovation. To link analytics, data and culture, absorptive capacity theory thus appears highly relevant, because this theory relates to an organization's ability to recognize the value of new, external information, assimilate it and apply it to commercial ends. This is a crucial element of the path from Business Analytics to innovation. Yet as far as we are aware, ours is the first study to use it to help understand how Business Analytics affects innovation, and how managers might change their organizations to reap the benefits from Business Analytics. Therefore, this research aims to examine specifically the relationships between Business Analytics, data-driven culture, environmental scanning, new product or service innovation, and competitive advantage. To achieve this research aim, this study employs a deductive approach. A number of hypotheses are proposed from an information processing and use perspective, drawing on absorptive capacity theory. These hypotheses are integrated into a research model to explain how Business Analytics, working through environmental scanning and data-driven culture, contributes to new product or service innovation, and subsequently competitive advantage. To test the research model, a survey questionnaire is designed to collect quantitative data from UK commercial organizations. Survey data collected from 218 UK companies are used to test the research model. The remainder of this paper is structured as follows. Section 2 provides a literature review on the key concepts and theoretical considerations. Section 3 discusses the development of the research model. Section 4 explains the research method including research constructs, the associated measurements, and data collection process. Section 5 presents the data analysis and results. It is followed by discussion in section 6 and conclusion in Section 7.", "section": "Introduction", "doi": "10.1016/j.ejor.2018.06.021", "references": [92988379, 1419724723, 1542083448, 1555809017, 1721421031, 1920432947, 1969359483, 2013261784, 2013593749, 2034957137, 2038195563, 2041579435, 2070564502, 2081084322, 2096460314, 2107023540, 2121991049, 2122141202, 2126071066, 2132747867, 2136252976, 2141975087, 2164921310, 2264142365, 2281782812, 2284916459, 2304517173, 2498362352, 2590255024, 2626664234, 2769967834, 2803453471]}
{"paragraph": "In recent years, Deep Neural Networks DNNs have achieved state-of-the-art results on many tasks. As a typical DNN model, an autoencoder network can extract impressive and representative features from the inputs. However, the features are usually used to obtain low errors with the implicit assumption that different mistakes cause the same cost. In reality, the losses of some errors may be ten times or hundred times as much as some others. For example, it is annoying if an innocent person is misclassified as a potential terrorist, but it may be far more disastrous if a terrorist is misclassified as a normal person and escapes surveillance. A high-precision model may still lead to high costs because of the errors in high-cost areas. Moreover, the autoencoder network requires a long training time to acquire the optimal feature description functions. The finer features with more training time result in lower misclassification cost. However, as an important kind of test cost, time cost also affects the total cost. The Sequential Three-Way Decision S3WD model, which was firstly proposed by Yao, presents a solution to these problems. Aimed at the minimum total cost, it balances the misclassification cost and test cost in the decision procedure. In S3WD, the Three-Way Decision 3WD mechanism is utilized to select the optimal decisions for samples at each step. When the available information at the current step is insufficient to make a certain decision for an ambiguous sample, a temporary non-commitment is an apt choice, as the wrong decisions usually result in higher cost. Therefore, boundary decision non-commitment is regarded as a feasible choice in 3WD. Assuming that different decisions lead to different losses, 3WD adopts a Bayesian theory-based cost-sensitive strategy. In reality, non-commitment is usually a temporary decision. Decision makers will collect more information to get a clearer idea about the ambiguous samples in the former decision steps. With the increase of information, more samples will be given definite decisions and the misclassification cost decreases. The decisions are sequentially made based on the information at the current granularity, which forms a sequential decision-making process. In the decision procedure, time cost keeps increasing with decision steps, which also needs to be considered in many applications. To obtain the lowest total cost, the decision procedure will stop at an appropriate granule level. As the basis of sequential decision procedure, multi-granular features are of significant importance for S3WD. Traditional S3WD models were applied in the information table or decision table, where the attribute reduction methods were utilized to construct the multi-granular feature set, as the attributes in the table can be directly regarded as features. For datasets without attributes, feature extraction methods were widely adopted to extract features from the inputs. Therefore, based on a linear feature extraction method, Li et al. proposed a formal definition of sequential granulation method for an image dataset. Unfortunately, the proposed definition is not suitable for nonlinear feature extraction methods such as autoencoder networks, which are more powerful and widely used than linear methods. Motivated by previous works, we intend to propose a multi-granular feature set definition based on an autoencoder network. An autoencoder network can be divided into two parts: feature extraction and reconstruction. As the training steps increase, the reconstructed outputs become more similar to the inputs. The reason is that the granule description function becomes more powerful and the extracted features are at a finer granulation level. Although the dimensions of the extracted multi-granular features remain unchanged, the attached information increases with the training steps, which is in line with the multi-granular computing of the multilevel features. The feature extraction function in different training epoches can be viewed as the granular feature description function. Therefore, an autoencoder network-based multi-granular feature set definition is proposed. In the existing dynamic 3WD models, different granularities of features have different dimensions, which are not suitable for describing the defined feature granular structure. To address this issue, a modified cost-sensitive sequential 3WD model is provided. In the proposed 3WD model, there are three different sets, namely, the training dataset, validation dataset and test dataset. When a new granular feature description function is computed from the training set, the new granular features for validation samples will be computed. Specifically, when the parameters in the autoencoder network are updated from one epoch of the training dataset, the features for validation samples will be computed using the newly updated network. With the increase of training time, the associated discriminative information in features increases, which will lead to less misclassification cost. In most cases, the decreasing velocity of misclassification cost gradually slows down, while the time cost keeps increasing in the decision-making process. Therefore, the decision-making process will cease at the minimum point of the total cost, where the description function and the granular level of features are optimal for the current dataset. Then the granular features of test samples can be obtained at the optimal granular level. The decision makers can also stop the decision process when the total cost is under a given satisfactory threshold. The proposed approaches enable dynamic 3WD models to leverage insights from the attractive autoencoder networks and provide an applicable method to combine these two kinds of models. The remainder of the paper is organized as follows: In Section 2, a review of the dynamic 3WD models is presented. In Section 3, we formulate a definition on the autoencoder-based multi-granular features. In Section 4, a modified sequential 3WD model is presented based on the extracted features. Section 5 provides an experimental analysis. Finally, the conclusion is given in Section 6.", "section": "Related Work", "doi": "10.1016/j.ins.2019.03.061", "references": [95749022, 984339528, 1041128124, 1843766148, 1861960977, 2006793117, 2013217210, 2021423479, 2064630666, 2067145099, 2071198963, 2092491543, 2107091340, 2107566963, 2162755671, 2214725774, 2217596628, 2310435784, 2330216853, 2345465422, 2512066509, 2537382886, 2549529522, 2551396410, 2555871690, 2561208659, 2565881538, 2588572297, 2591767974, 2600072788, 2600420962, 2619471640, 2620114837, 2707976390, 2727405836, 2730386428, 2737801118, 2764035296, 2770194470, 2784166218, 2791661525, 2799813871, 2888019619, 2889210423, 2892294644, 2896017664, 2904406045, 2912670664]}
{"paragraph": "A clear recent trend in information technology is the rent by many users and enterprises of the storage/computation services from other parties. With cloud technology, what was in the past managed autonomously now sees the involvement of servers, often in an unknown location, immediately reachable wherever an Internet connection is present. Today the use of these Internet services typically assumes the presence of a Cloud Service Provider (CSP) managing the service. There are a number of factors that explain the current status. In general, the procurement and management of IT resources exhibit significant scale economies, and large-scale CSPs can provide services at costs that are less than those incurred by smaller players. Still, many users have an excess of computational, storage, and network capacity in the systems they own and they would be interested in offering these resources to other users in exchange of a rent payment. In the classical behavior of markets, the existence of an infrastructure that supports the meeting of supply and demand for IT services would lead to a significant opportunity for the creation of economic value from the use of otherwise under-utilized resources. This change of landscape is witnessed by the increasing attention of the research and development community toward the realization of Decentralized Cloud Storage (DCS) services, characterized by the availability of multiple nodes that can be used to store resources in a decentralized manner. In such services, individual resources are fragmented in shards allocated (with replication to provide availability guarantees) to different nodes. Access to a resource requires retrieving all its shards. The main characteristics of a DCS is the cooperative and dynamic structure formed by independent nodes (providing a multi-authority storage network) that can join the service and offer storage space, typically in exchange of some reward. This evolution has been facilitated by blockchain-based technologies providing an effective low-friction electronic payment system supporting the remuneration for the use of the service. On platforms such as Storj, SAFE Network Vault, IPFS, and Sia, users can rent out their unused storage and bandwidth to offer a service to other users of the network, who pay for this service with a network crypto-currency. However, if security concerns and perception of (or actual) loss of control have been an issue and slowing factor for centralized clouds, they are even more so for a decentralized cloud storage, where the dynamic and independent nature of the network may hint to a further decrease of control of the owners on where and how their resources are managed. Indeed, in centralized cloud systems, the CSP is generally assumed to be honest-but-curious and is then trusted to perform all the operations requested by authorized users (e.g., delete a file when requested by the owner). The CSP is discouraged to behave maliciously, since this would clearly impact its reputation. On the contrary, the nodes of a decentralized system may behave maliciously when their misbehavior can provide economic benefits without impacting reputation (e.g., sell the content of deleted files). Client-side encryption typically assumed in DCSs provides a first crucial layer of protection, but it leaves resources exposed to threats, especially in the long term. For instance, resources are still vulnerable in case the encryption key is exposed, or in case of malicious nodes not deleting their shards upon the owner’s request to try reconstructing the resource in its entirety. Protection of the encryption key is therefore not sufficient in DCS scenarios, as it remains exposed to the threats above. A general security principle is to rely on more than one layer of defense. In this paper, we propose an additional and orthogonal layer of protection, which is able to mitigate these risks. On the positive side, however, we note that the decentralized nature of DCS systems also increases the reliability of the service, as the involvement of a collection of independent parties reduces the risk that a single malfunction can limit the accessibility to the stored resources. In addition to this, the independent structure characterizing DCS systems - if coupled with effective resource protection and careful allocation to nodes in the network - makes them promising for actually strengthening security guarantees for owners relying on the decentralized network for storing their data. In this paper, we present a solution to enable resource owners to securely store their resources in DCS services, to share them with other users, while still being able to securely delete them. Our contribution is threefold. First, leveraging the protection guarantees offered by All-Or-Nothing-Transform (AONT), we devise an approach to carefully control resource slicing and allocation to nodes in the network, with the goal of ensuring both availability (i.e., retrieval of all slices to reconstruct the resource) and security (i.e., protection against malicious parties jointly collecting all the slices composing a resource). The proposed solution also enables the resource owners to securely delete their resources when needed, even when some of the nodes in the DCS misbehave. Second, we investigate different strategies for slicing and distributing resources across the decentralized network, and analyze their characteristics in terms of availability and security guarantees. Third, we provide a modeling of the problem enabling owners to control the granularity of slicing and the diversification of allocation to ensure the aimed availability and security guarantees. We demonstrate the effectiveness of the proposed model by conducting several experiments on an implementation based on an available DCS system. Our solution provides an effective approach for protecting data in decentralized cloud storage and ensures both availability and protection responding to currently open problems of emerging DCS scenarios, including secure deletion. In fact, common secret sharing solutions (e.g., Shamir), while considering apparently similar requirements are not applicable in scenarios where the whole resource content (and not simply the encryption key) needs protection, because of their storage and network costs (e.g., each share in Shamir’s method has the same size as the whole data that has to be protected). Outline: The remainder of the paper is organized as follows. Section II introduces the basic concepts. Section III defines the properties of a decentralized allocation function with respect to replication and protection. Section IV discusses slicing and allocation strategies. Section V illustrates availability and security guarantees and discusses the setting of parameters guiding slicing and allocation. Section VI illustrates the implementation of our approach on a real DCS service and presents experimental results. Section VII discusses related work. Finally, Section VIII concludes the paper. The proofs of theorems are provided in Appendix.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2916673", "references": [1482576848, 1844512392, 1966924625, 2043508455, 2058231031, 2067410255, 2093375206, 2099516661, 2102710358, 2147504831, 2175033270, 2407115164, 2536293744, 2570363240, 2624307925]}
{"paragraph": "Computing the Gröbner basis of an ideal with respect to a given term ordering is an essential step in solving systems of polynomials. Certain term orderings, such as the degree reverse lexicographic ordering, tend to make the computation of the Gröbner basis faster. This has been observed empirically since the 1980s and is now supported by theoretical results, at least for some nice families of inputs, such as complete intersections or certain determinantal systems. On the other hand, other orderings, such as the lexicographic ordering, make it easier to find the coordinates of the solutions, or to perform arithmetic operations in the corresponding residue class ring. For instance, for a zero-dimensional radical ideal in generic coordinates, the Gröbner basis of the ideal for the lexicographic ordering has a specific triangular form with all polynomials of degree less than a certain bound and the leading polynomial squarefree; this is known as the shape lemma. The points in the variety are then directly obtained from these polynomials. As a result, the standard approach to solve a zero-dimensional system by means of Gröbner basis algorithms is to first compute a Gröbner basis for a degree ordering and then convert it to a more exploitable output, such as a lexicographic basis. The latter step, while of polynomial complexity, can now be a bottleneck in practice. This paper focuses on this step; in order to describe our contributions, we first discuss previous work on the question. Let the input ideal be zero-dimensional. We assume that we know a monomial basis of the quotient ring together with the multiplication matrices of the variables in this basis. We denote by D the degree of the ideal, which is the vector space dimension of the quotient ring. Starting from a degree Gröbner basis, computing the multiplication matrices efficiently is not a straightforward task. Previous work showed how to do it efficiently, and more recent algorithms achieve faster bounds, at least for some favorable families of inputs. The FGLM algorithm computes the lexicographic Gröbner basis of the ideal using linear algebra operations. While the algorithm has an obvious relation to linear algebra, lowering the runtime was only recently achieved. Polynomials with the triangular shape mentioned earlier form a very useful data structure, but there is no guarantee that the lexicographic Gröbner basis has such a shape. When it does, we say the ideal is in shape position. As an alternative, one may use the Rational Univariate Representation algorithm to describe the zero-set by means of univariate rational functions, where the multiplicity of a root coincides with that of the ideal at the corresponding point. Using rational functions allows one to control precisely the bit-size of their coefficients when working over fields like the rationals. These algorithms rely on duality, which will also be at the core of our algorithms. They compute sequences of values involving trace forms and generic linear combinations of variables. From these values, one may then recover the output using structured linear algebra calculations. A drawback is that we need to know the trace of all elements of the basis, which is polynomial-time computable but not trivial. Randomization can alleviate this issue. It has been shown that computing values involving traces of random linear forms allows one to deduce a description of the variety by a set of rational functions with univariate polynomial denominators. The tuple computed by such algorithms is called a zero-dimensional parametrization of the variety. It generally differs from a Rational Univariate Representation, since the latter keeps track of multiplicities. Starting from such a parametrization, we can reconstruct the local structure of the ideal at its roots using additional algorithms. The most costly part of the randomized algorithm is computing the trace values; the rest involves standard univariate techniques. It was later pointed out that the multiplication matrices can often be expected to be sparse, and estimates on their sparsity have been given assuming the validity of certain conjectures. Based on this, sparse variants of FGLM have been designed. For example, if the ideal is in shape position, one can recover its lexicographic basis using trace values involving a linear form. For less favorable inputs, these algorithms fall back on more general methods. The techniques used are based on Krylov subspace methods and Berlekamp-Massey techniques, similar to those used for solving sparse linear systems and problems in integer factorization or discrete logarithm computations. Block versions of these methods allow for parallelization and were pioneered in that context. It is natural to adapt this strategy to Gröbner basis computations. One prior work showed how to compute the main univariate polynomial using such block techniques, assuming the base field is finite. The algorithm computes the roots of this polynomial and substitutes them into the system before computing a Gröbner basis for each root. Our first contribution is to give a block version of the randomized algorithm that extends the previous approach to compute all polynomials in the parametrization for essentially the same cost as computing the univariate polynomial. More precisely, the bottleneck of the algorithm is the computation of a block-Krylov sequence; once this sequence is computed, all polynomials in the zero-dimensional parametrization can be obtained. Unlike some previous algorithms, ours deals with any zero-dimensional ideal, not just those in shape position, though it assumes the base field has sufficiently large characteristic. The output is somewhat weaker than a Gröbner basis, as multiplicities are not computed. While we focus on sparse multiplication matrices, we also analyze the dense case. Our second contribution refines the first by trying to avoid computations with a generic linear form, motivated by the fact that its multiplication matrix is often denser. The refined algorithm first computes a zero-dimensional parametrization of a subset of the variety for which we can take a simple variable, and then applies the general algorithm to the rest. If the initial subset is large, this is expected to provide a speed-up. For experiments, our algorithms have been implemented in C++ using standard linear algebra and symbolic computation libraries. The paper is organized as follows. The next section reviews results on scalar and matrix recurrent sequences and gives an algorithm to compute a scalar numerator. Section 3 describes sequences arising in FGLM-like algorithms and proves some refined versions of earlier results. The main algorithm is given in Section 4, and the refinement is in Section 5. An appendix provides proofs of technical results on recurrent sequences. Complexity model: we count basic operations at unit cost. Most algorithms are randomized and involve selecting a vector of field elements; success is guaranteed if the vector avoids a certain hypersurface in parameter space. Suppose the input ideal is generated by a set of polynomials. Given a zero-dimensional parametrization found by our algorithms, one can always evaluate those polynomials at the parametrization, modulo the univariate polynomial. This allows us to check whether the output describes a subset of the variety, but not whether we have found all solutions. If the parametrization has the same dimension as the quotient ring, we can infer that we have all solutions and that the ideal is radical, so the output is correct. In what follows, we assume that the number of variables equals the dimension of the quotient ring, to simplify cost estimates. We use standard bounds for polynomial multiplication, and assume super-linearity of the time function. Then two matrices over the field with polynomials of degree less than a bound can be multiplied efficiently. Acknowledgments: We thank several researchers for helpful discussions and a reviewer for useful remarks. This research was partially supported by grants and institutional programs.", "section": "Related Work", "doi": "10.1016/j.jsc.2019.07.010", "references": [149384130, 1502078890, 1562183207, 1590894046, 1968561983, 1976677460, 1979462002, 1985924702, 1986421748, 1986905837, 1995653528, 2001224529, 2003767038, 2011693299, 2015497178, 2029891988, 2057341276, 2059454546, 2059522106, 2071483197, 2081523647, 2088713140, 2153077274, 2161330625, 2238531909, 2729305841]}
{"paragraph": "Real-time systems are widespread nowadays and are quickly evolving. To achieve strong results on the correctness of system, it often requires, from the beginning of system design, modeling formalism to describe systems at an abstract level and corresponding verification techniques to reason about abstract system behaviour. In some sorts of real-time systems like mobile distributed systems or Cyber Physical Systems, agents’ behaviour often relate not only to real time, but also to physical or logical locations. For example, in Intelligent Railroad Crossing System, the smart train should inform the smart gate that it is coming when it arrives at the location at time t. Such constraint—that an event must be triggered at explicit time and location—is called spatio-temporal consistency. At high-level of system design, a formalism, where such constraints can be easily expressed, is required to help engineers design more reliable programs when it comes to refinements. Right specification languages are also needed to give an easy expression of some important properties that are of interest. Spatio-temporal consistency language is a modeling formalism proposed for describing spatio-temporal behaviour of real-time systems. It is a process algebra-like modeling language, and looks like an extension of CSP and CCS, with location taken as a primitive in its syntax. It aims at modeling system at high level. Not like other process algebras extended with location or time, like timed CSP, timed CCS, ambient calculus, or π-calculus, this language addresses a stronger constraint between time and location—spatio-temporal consistency—rather than only considering one of them alone. For example, in timed CCS an event can be triggered at an explicit time, while in ambient calculus an event can be triggered at a specific location. In the spatio-temporal consistency language, an event can only be triggered at some explicit time and location. Clock Constraint Specification Language is a specification language based on logical clocks. It was initially proposed as a companion language for the modeling language MARTE, and now has been fully developed as an independent language. Compared with traditional specification languages like LTL, CTL, TCTL and PSL, logical clock provides an intuitive way to express event sequences and its chronometric attributes. The logical and chronometric constraints between events can be easily expressed by the relationships between logical clocks. Comparing to traditional specification languages, this language alleviates the burden for specifying some crucial safety properties, since it provides a library of off-the-shelf often-used property patterns. A comparison of expressiveness between it and other languages has been analyzed. In order to give a verification support for the spatio-temporal consistency language, and inspired by the MARTE-based framework, a verification framework is proposed for modeling and verifying spatio-temporal systems—a special type of real-time systems where agents’ behaviour is related not only to real time, but also to locations. Compared to other frameworks for real-time systems, this framework focuses on capturing system behaviour at high level, with the advantage of addressing the spatio-temporal consistency of system behaviour at the syntax level. And it supports specifying and verifying logical and chronometrical timing constraints and possibly some carefully selected spatial constraints in the future in logical clock style, which could be complex or difficult to be expressed by other traditional specification languages. In this framework, in order to connect the spatio-temporal and logical clock components, a linking theory and a model checking framework are proposed to verify specifications. Some concepts are formalized and an observational mapping is built between clocks and events. To carry out model checking, theory and algorithms are proposed to translate both modeling languages into their equivalent Timed Automata. A general analysis for the computation complexity of the whole verification process is made, and it is shown that the framework runs nearly as fast as the traditional model checking process, despite the translation process. In order to show that the proposed verification framework is applicable, the translation into UPPAAL Timed Automata, a model checking tool widely used in academia and industry, is proposed. Technically, this work can be seen as a combination and extension of previous work, where the verification aspect of the modeling languages has been explored separately. In earlier work, Timed Automata semantics of the spatio-temporal language was introduced. The translation was given inductively based on the syntax structure. In this paper, a more general approach is taken by dividing configurations into regions. A translation strategy was previously proposed to encode logical clocks into Timed Automata and examples were given to illustrate the model checking scheme in UPPAAL. There the Timed Automata of logical clocks is based on untimed synchronous models, which is not suitable for this framework since the spatio-temporal language is asynchronous. Improvements are made by translating it into asynchronous Timed Automata models. A full translation from logical clocks into UPPAAL Timed Automata is given. The proposed model checking framework is partially based on the idea of the previous verification scheme, but is quite different as it considers the combination with the spatio-temporal modeling and the logical clock generators. This paper is organized as follows. Section 2 introduces some backgrounds about the modeling languages and Timed Automata. In Section 3, a spatio-temporal system—intelligent railroad crossing system—is introduced as an example and modeled using the proposed language. In Section 4, the main contribution is presented—the linking theory that connects the two modeling languages and the model checking framework. In Section 5, model checking is carried out in UPPAAL, based on the translation into Timed Automata. In Section 6, three safety properties of the railroad crossing system are verified in UPPAAL as an example. Section 7 concludes the work and discusses possible future works. Section 8 compares with some similar verification frameworks proposed in recent years.", "section": "Related Work", "doi": "10.1007/s11704-018-7054-8", "references": [105975728, 1491196069, 1590675544, 1962072139, 1968765218, 1973501242, 1982535211, 1998017916, 2017863278, 2021418494, 2044856809, 2053137492, 2078877482, 2081938726, 2090051150, 2101508170, 2110425399, 2115309705, 2126860147, 2128110618, 2128932399, 2132609679, 2137453611, 2137865376, 2152192540, 2160258739, 2912003593]}
{"paragraph": "In the past two decades, fractional differential equations have been applied in many fields of science, in which space fractional diffusion equations are used to model the anomalous transport of solute in groundwater hydrology. For space fractional diffusion equations with constant coefficients, analytical solutions can be obtained by utilising the Fourier transform methods. However, many practical problems involve variable coefficients, in which the diffusion velocity can vary over the solution domain. The work involving space fractional diffusion equations with variable coefficients is numerous. Meerschaert et al. considered the finite difference method for the one-dimensional one-sided and two-sided space fractional diffusion equations with variable coefficients, respectively. Zhang et al. explored the homogeneous space-fractional advection–dispersion equation with space-dependent coefficients. Ding et al. presented the weighted finite difference methods for a class of space fractional partial differential equations with variable coefficients. Moroney and Yang proposed some fast preconditioners for the numerical solution of a class of two-sided nonlinear space-fractional diffusion equations with variable coefficients. Chen and Deng discussed the alternating direction implicit method to solve a two-dimensional, two-sided space fractional convection–diffusion equation on a finite domain. Wang and Zhang developed a high-accuracy preserving spectral Galerkin method for the Dirichlet boundary-value problem of a one-sided variable-coefficient conservative fractional diffusion equation. Liu et al. developed a new fractional finite volume method for solving the fractional diffusion equation with a space–time dependent variable coefficient. Li et al. developed novel finite volume methods for Riesz space distributed-order diffusion equation and the Riesz space distributed-order advection–diffusion equation. Feng et al. proposed the finite volume method for a two-sided space-fractional diffusion equation with variable coefficients. Chen et al. considered an inverse problem for identifying the fractional derivative indices in a two-dimensional space-fractional nonlocal model with variable diffusivity coefficients. Jia and Wang presented a fast finite volume method for conservative space-fractional diffusion equations with variable coefficients. Feng et al. presented a new second order finite difference scheme for a two-sided space-fractional diffusion equation with variable coefficients. Chen et al. presented numerical methods and analysis for a multi-term time-space variable-order fractional advection–diffusion equations and applications. Liu et al. proposed numerical methods for solving the multi-term time fractional wave equations. In fact, many mathematical models and problems from science and engineering must be computed on irregular domains and therefore seeking effective numerical methods to solve these problems on such domains is important. Although existing numerical methods for fractional diffusion equations are numerous, most of them are limited to regular domains and uniform meshes. Research involving unstructured meshes and irregular domains is sparse. Liu et al. presented unstructured-mesh Galerkin finite element method for the two-dimensional multi-term time space fractional Bloch–Torrey equations on irregular convex domains. Fan et al. presented unstructured mesh finite element method for the two-dimensional multi-term time-space fractional diffusion-wave equation on an irregular convex domain. Yang et al. proposed the finite volume scheme for a two-dimensional space-fractional reaction–diffusion equation based on the fractional Laplacian operator, which was computed using unstructured triangular meshes on a unit disk. Burrage et al. developed some techniques for solving fractional-in-space reaction–diffusion equations using the finite element method on both structured and unstructured grids. Qiu et al. developed the nodal discontinuous Galerkin method for fractional diffusion equations on a two-dimensional domain with triangular meshes. Liu et al. presented the semi-alternating direction method for a two-dimensional fractional FitzHugh–Nagumo monodomain model on an approximate irregular domain. Qin et al. also used the implicit alternating direction method to solve a two-dimensional fractional Bloch–Torrey equation using an approximate irregular domain. Karaa et al. proposed a finite volume element method implemented on an unstructured mesh for approximating the anomalous subdiffusion equations with a temporal fractional derivative. Yang et al. established the unstructured mesh finite element method for the nonlinear Riesz space fractional diffusion equations on irregular convex domains. Fan et al. extended the unstructured mesh finite element method developed by Yang et al. to the time-space fractional wave equation. Feng et al. investigated the unstructured mesh finite element method for a two-dimensional time-space Riesz fractional diffusion equation on irregular arbitrarily shaped convex domains and a multiply-connected domain. Le et al. studied the finite element approximation for a time-fractional diffusion problem on a domain with a re-entrant corner. To the best of our knowledge, the control volume finite element method has not been generalised to allow the solution of space fractional diffusion equations with variable coefficients. In this paper, we will consider the unstructured mesh control volume method for the following two-dimensional space fractional diffusion equation with variable coefficients on an arbitrarily shaped convex domain subject to the initial condition and boundary conditions where all are assumed to be two known smooth functions. When the boundary of the solution domain is nonconstant or curved, for example a convex domain with left boundary, right boundary, lower boundary and upper boundary, we define the Riemman–Liouville fractional derivative accordingly. One important application of this is in the study of cardiac arrhythmias. In two dimensions, the fractional FitzHugh–Nagumo monodomain model can be rewritten as a two-dimensional Riesz space fractional reaction–diffusion model, which can be used to describe the propagation of the electrical potential in heterogeneous cardiac tissue. This electrophysiological model of the heart can describe how electrical currents flow through the heart controlling its contraction and can be used to ascertain the effects of certain drugs designed to treat heart problems. The major contribution of this paper is as follows. Different from previous work, we consider the control volume method for the two-dimensional space fractional diffusion equation with variable coefficients, in which the space fractional operator is either the Riemman–Liouville fractional derivative or Riesz space fractional derivative. To the best of our knowledge, this is a new contribution to the literature. We propose a novel technique utilising the control volume method implemented with an unstructured triangular mesh to deal with the space fractional derivative on an irregular convex domain, which we believe provides a very flexible solution strategy because our considered solution domain can be arbitrarily convex. Compared to the finite difference method, our method requires fewer grid nodes to generate the meshes in the solution domain partition. For the methods considered in this paper, we construct the control volumes using triangular meshes and transform the problem from the solution domain to a single control volume. Then we integrate the problem over an arbitrary control volume and change the control volume integral to a line integral over the control volume faces, which is approximated by the midpoint approximation. Moreover, we utilise the linear basis function to approximate the fractional derivatives at the midpoints of the control volume faces, in which some numerical techniques are used to handle the non-locality of the fractional derivative of the basis function. We explore the property of the stiffness matrix generated by the integral of the space fractional derivative. We find that the stiffness matrix is sparse and not regular. Especially, the smaller the maximum edge of the triangulation is, the more sparse the stiffness matrix becomes. Therefore, we choose a suitable sparse storage format for the stiffness matrix and utilise the bi-conjugate gradient stabilised method iterative method to solve the linear system, which is more efficient than using the Gaussian elimination method. We present several examples to verify our method, in which we make a comparison of our method with the finite element method proposed for solving the Riesz space fractional diffusion equation on a circular domain. The bilinear form involves eight fractional derivative terms and the approximation of two-fold multiple integrals, which are approximated by Gauss quadrature. While for the control volume method, we use a form to generate the stiffness matrix form, in which we only need to calculate four fractional derivative terms and the approximation of line integrals. The numerical results demonstrate that our method can reduce CPU time significantly while retaining the same accuracy and approximation property as the finite element method. The numerical results also illustrate that our method is effective and reliable and can be applied to problems on arbitrarily convex domains. The outline of this paper is as follows. In Section 2, the unstructured mesh control volume method is proposed and the full implementation details are provided. Then the property of the stiffness matrix is explored and a fast iterative solver is developed for the linear system. In Section 3, several numerical examples are presented to verify the effectiveness of the method and comparisons are made with existing methods to highlight its computational performance. Finally, some conclusions of the work are drawn.", "section": "Introduction", "doi": "10.1016/j.cam.2019.06.035", "references": [374977727, 1972505528, 1996761814, 2001744739, 2009456713, 2032744100, 2063052480, 2065839215, 2083046996, 2093685032, 2100104735, 2112900014, 2124718863, 2142031974, 2155216327, 2236929209, 2294672943, 2302272192, 2325660078, 2460690371, 2545450475, 2592797361, 2620738344, 2732661672, 2752814607, 2766334764, 2772727350, 2793924997, 2905890484, 2914535700, 2915417020]}
{"paragraph": "Over the past decades, exoskeleton robots have been developed for human power augmentation and rehabilitation training. One of the most critical issues in controlling a robotic exoskeleton is to enable the robots to learn the human’s experiences and skills so that the robots could actively cooperate with the human subject. In this paper, a hierarchical control scheme is proposed, which includes a high-level learning model for interaction that can learn human experience and skills from demonstration, and low-level robot motion control that enables the robot to be back drivable. One way to achieve successful and safe human–robot cooperative manipulation is to make the robot possess the ability of estimating human intention. There are multiple types of information that a robot could use to predict human intention, including motion, partner posture, and interaction force between the human and environment among others. For example, electromyography signals, generated by motor neuron impulses that activate the muscle fibers, can be correlated with the torque at the joint level produced by muscles, which contains the human intention information. The electromyography-based intention estimation methods are related to complex calibrations that are necessary for accurate and reliable modeling. The relation between electromyography signals and torques involves several nonlinearities and strongly depends on the placement of electrodes. Compared to model-based methods, electromyography-based techniques do not require a dynamic model of the limb interaction with the environment. An alternative to estimating human intention is to enable the robot to learn from demonstrations and transfer the human experience and skills from demonstrations to the robot. Until now, there are many different approaches investigated for learning from demonstration. For example, learning algorithms such as hidden Markov models and Gaussian mixture models have been used. A prediction framework based on hidden Markov models was proposed to estimate the human’s intended trajectory, which employs a position-based impedance controller to map the haptic observations to the estimated trajectory. Online incremental learning of full body motion primitives was proposed, which partitioned human motion trajectory into motion primitives and then used a hidden Markov model to encode each motion primitive. Several sets of trajectories were modeled by a library of Gaussian mixture models, and the human intention inference algorithm was developed using unsupervised Gaussian mixture models. The application of a statistical framework was presented, which endows a robot with the ability to perform a cooperative manipulation task with a human subject. A comprehensive survey of robot learning from demonstration, a technique that develops policies from example state to action mappings, has been presented. Dynamical movement primitives for modeling attractor behaviors of autonomous nonlinear dynamical systems have also been presented. However, the above works seldom consider the variability of human–robot cooperative manipulation among multiple demonstrations or exploit it in the reproduction phase. When a human subject interacts with a robot, one goal is to model the human behavior which is recorded during demonstration, and to determine the underlying features and constraints of the interaction. During demonstration, a demonstrator teaches a robot how to execute the task successfully and allows it to analyze the behavior recorded by using statistical approaches. In this paper, a strategy for learning human skills from demonstration that utilizes Gaussian mixture models for high-level interaction is presented. In human–robot cooperative manipulation, the human subject should offer a minimum impedance required to complete a cooperative task. But due to variation in the interaction force caused by environmental disturbances, the motion of the robot end-effector would deviate far from the desired, which may lead to the failure of the task. The proposed strategy for learning human skills from demonstration compares the impedance behavior of a demonstrator in those successful trials with that of the subjects who want to perform the same task. First during demonstration, the demonstrator performs the tasks individually and successfully. The underlying features and constraints of the interaction are statistically analyzed and learned using Gaussian mixture models based on logged human–robot interaction data. Then, during reproduction, the robot predicts the interaction force similar to those in demonstrations by using Gaussian mixture regression, to assist the subjects with external assistance to be sure that the task can be completed successfully. To encourage the subjects to actively participate into the cooperative task, provision of assistance to the subject is adjusted according to the variability of the demonstrator’s behavior observed in the demonstration phase. Therefore, the presented learning-based strategy transfers the underlying characteristics and constraints of the given impedance-based task to the exoskeleton robot, which leads to cooperative interaction between the human subject and the robot, where the robot provides assistance only when needed. Furthermore, we extended the learning model to task-parameterized Gaussian mixture models in order to reproduce motions with different targets. On the other hand, low-level motion control of the robot needs to be designed to achieve comfortable and safe interaction with the robot. In order to design an admittance control when the model of the exoskeleton robot is unavailable, a model-free PID-type admittance control has been proposed. In order to achieve the goal of the operator feeling as if a wearable robot is a natural extension of the body, a novel admittance control for a wearable robot has been developed. However, these works do not consider the physical constraints of an upper limb exoskeleton. Various physical constraints such as safety specifications and physical limits normally exist in many robotic systems. Violation of constraints may degrade the performance of the system. For example, the maximum value of a robotic manipulator’s joint position is constrained by its physical configuration and it needs to avoid violating these physical limits when it operates. Without taking constraints into account, the control design would lead to the failure of control. Therefore, it is a challenge to deal with the constraint as one of the control objectives. Many methods have been proposed to handle the constraints problem for various mechanical systems. Contractive model predictive control was proposed for constrained nonlinear systems. To handle the constraints in robotic systems, a barrier Lyapunov function was developed. A systematic control design for single-input–single-output nonlinear systems with output constraints was presented. Barrier Lyapunov function is used for the single-input–single-output output-constrained nonlinear system, and it is used for the nonlinear system with time-varying constraints. A novel integral barrier Lyapunov function is designed for the control of a single-input–single-output nonlinear system with state constraints. As we can see from the above works, the barrier Lyapunov function method has been proven to be an effective method in the control of robotic system with constraints. However, none of these works considers human–robot interaction for a robotic exoskeleton, nor the use of asymmetric barrier Lyapunov function. For low-level robot motion control, this paper proposed an admittance control scheme with an inner position controller. Admittance control provides an established mechanism to specify a dynamic relationship between position and force in a system, and provides definable degrees of compliance. The admittance control uses the interaction force as input and outputs the position of the robot end-effector in task space, and it requires an inner position controller to track this position. In order to avoid singularities in task space so that the robot can operate without violating asymmetric physical constraints, and to improve the tracking performance, we designed an asymmetric barrier Lyapunov function-based adaptive neural network controller, which allows the tracking errors to remain within the constraints. Asymmetric barrier Lyapunov function is used to cope with asymmetric physical constraints while adaptive neural network is used to approximate the unknown model parameters of the robotic system. In this paper, we present a learning-based hierarchical control scheme which consists of a high-level learning model that is capable of characterizing cooperative impedance-based manipulation tasks, and a low-level admittance controller with an inner position controller that is able to deal with asymmetric constraints. The robot first observes and learns the impedance-based behavior of the demonstrator in the demonstration phase, and then predicts the movements the subject should have based on recorded demonstrated motion data so that it can assist the subject by compensating for the differences between predicted movements and the subject’s actual movements during reproduction. The contributions of this paper are threefold: a newly designed asymmetric barrier Lyapunov function-based adaptive neural network controller that achieves asymptotic tracking without violating asymmetric output constraints and its application in an exoskeleton robot; a learning-based control scheme which guides exoskeleton robot to aid human with assistance only when needed; extension of task-parameterized Gaussian mixture model into human–robot cooperative manipulation for exoskeleton robot.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864784", "references": [1682895868, 1977013839, 1986014385, 1986024064, 1992171281, 1997610562, 2001709653, 2021756757, 2023172481, 2032277247, 2034214945, 2056996905, 2062373349, 2074809255, 2076024315, 2079025554, 2091914309, 2102393714, 2113698995, 2115123877, 2122447324, 2130301822, 2136719407, 2139595765, 2140546948, 2142572589, 2165575781, 2200632918, 2210365848, 2322447463, 2549092026, 2561515991, 2565780995, 2588114174, 2609675074, 2738231311, 2962848549]}
{"paragraph": "Electricity production and consumption are one of the major challenges of today’s society. Economical, ecological and political concerns are all at stake. In order to optimize the efficiency of the electricity distribution system, many approaches arose. One of them is demand-side management (DSM). The idea behind DSM is that instead of fitting the production to the demand, the demand can be adapted to the production. DSM has been an important topic for the last thirty years and became more relevant with the introduction of the smart grid paradigm. The ever growing communication among the actors of the grid indeed allows for a better management of the energy consumption. Works on DSM include for example households possess appliances that need to receive energy with time and quantity constraints. The usage of the appliance is scheduled so as to minimize the load peak. A distributed DSM model is considered in a case study of the UK shows that efficient DSM could lead to smaller load peaks and less carbon gas emissions. In another study, the use of thermal energy storage for DSM is reviewed, whereas the integration of wind power generation is the object of another. DSM can be achieved through many means, but the most used one consists in pricing policies. From a general point of view, one study aims to quantify the effects of price regulation policies. In another, the reaction of a household to time-based pricing is considered. Recent works include a study where a mixed complementarity problem is used to model electricity consumers offering load shedding on the market. In another, a three-stage stochastic flexibility problem is studied: in a market framework, a company wishes to purchase reserve capacities. From the point of view of the market, one study is concerned with the DSO-TSO interaction (respectively distribution and transmission system operator). Finally, one proposes a new pricing system, which is not based on the marginal cost, but aims to cover ancillary costs due to fluctuations in the demand. In this work, we focus on load shifting, a technique of DSM that features a fixed overall demand. To achieve it, price incentives are determined. We thus model and solve the problem of an electricity supplier competing against other suppliers on the market to sell energy to various actors, in a context of demand-side management. More precisely, the energy supplier defines time-dependent prices for his energy, knowing that his clients will adapt by shifting their loads, either directly (local agents) or indirectly (aggregators). This sequential and hierarchical decision-making process can be adequately modeled as a trilevel problem, that is an extension of a bilevel problem. Bilevel problems formalize the concept of Stackelberg games, and have received a great interest in the past thirty years. Let us recall that a bilevel problem is an optimization problem (called the leader’s problem) in which at least one of the variables is constrained to be solution of another optimization problem (called the follower’s problem). Applications of bilevel programming are numerous: toll pricing, network design problems, flight tickets pricing. Bilevel programming is in general NP-hard, even in its simplest form where both objectives and all constraints are linear. Multilevel optimization problems correspond to the more general case where more than two levels of optimization problems interlock. When there is more than one optimization problem at the leader level, the terminology used in the literature is Multi-Leader–Follower-Game (MLFG) if two additional conditions are met. First, the various leaders play in a noncooperation competition context, and second, each of the leaders’ problems is at least a multilevel problem. The model considered in this work is a MLFG in which the leaders’ problems are trilevel. Note that this structure of model has been rarely considered in the literature, due to its complexity. Multilevel and MLFG are increasingly used in the energy domain, more precisely on pricing matters. For example, one study presents a bilevel energy pricing model in order to decrease the amplitude of the peak loads, whereas another considers a longer time horizon and aims to make the energy system sustainable in a situation where the consumers optimize their investments in distributed generation. In another, generation companies aiming to find the cheapest possible production schedule make contracts with micro-grids that can generate or store energy, and can thus help the generation companies to cope with production surplus or lack of generation. In another, multilevel problems are used to define price zones within the grid. In another, the role of aggregators in the energy market is studied, and a trilevel model is proposed. In this model, a system operator aims to optimize his operating costs thanks to load shifts executed by end customers. Aggregators play an intermediary role between the system operator and the end customers. More precisely, the system operator offers a percentage of his benefit to each of the aggregators, and the aggregators offer pecuniary rewards to the end customers they are in contract with to induce a load shift. A heuristic to solve the trilevel model is defined. One limitation of the proposed model is that the cost function of the leader appears in the objective function of the aggregator, a situation that is not realistic for many trading situations since no aggregator is supposed to know the cost function of the supplier. In this paper, we consider a similar interaction model, but first the cost function of the supplier only appears in his own problem, and second the actors of the intermediary level (ILAs) have the possibility to trade energy among themselves. Moreover we do not use heuristic techniques and our reformulation and resolution of the problem are theoretically thoroughly justified. An important contribution of the paper is the definition of two new solution concepts that are adapted to this context of price management. Indeed, breaking with the classical optimistic approach usually made in case of nonuniqueness of the followers equilibrium solution, we define the so-called revisited optimistic approach and semi-optimistic approach that are based on some smart selection among the followers’ equilibriums, sharply taking into account the structure of the problem. The paper is structured as follows: first, we present a Trilevel Demand-Side Management model (denoted by (TDSM)) and define the actors involved in the model in Section 2. Second, in Section 3, we formulate (TDSM) as a bilevel problem, thanks to an explicit resolution of the lowest level problems. In Section 4, we propose three different approaches to solve the bilevel version of (TDSM). First a classical method, consisting in replacing the lower level problems by their KKT conditions in the leader’s problem leads in Section 4.1 to the so-called Classical optimistic approach. Second, in Section 4.2, we simplified the bilevel reformulation of the game by selecting a specific class of Generalized Nash Equilibriums (GNE) among the followers, providing thus the revisited optimistic approach. Third, in Section 4.3, we define a new class of GNE, called semi-optimistic, that simplifies a lot the formulation of the game and therefore its numerical treatment. Finally, in Section 5, we compare and comment the solutions computed through the three approaches.", "section": "Methodology", "doi": "10.1016/j.ejor.2019.03.005", "references": [1934866980, 1968859249, 1975884865, 1997949764, 2008668719, 2072230340, 2078794010, 2095818975, 2105308124, 2156790188, 2560939558, 2587875810, 2804352134, 2894264899, 2903400337, 2909677344]}
{"paragraph": "The traditional business model is changed by the expansion of big data and the evolution of Internet of Things technologies are changing gradually. E-commerce is the combination of traditional business model and network technology as well as information technology in the information era, so it is facing with both important opportunities and challenges. Internet of Things uses sensor technologies to acquire information, such as the status, location and properties of physical world. Increasing number of traditional management model has made breakthrough based on the combination of new technologies such as IoT, big data and cloud storage. A sensor can be used to detect physical phenomenons such as humidity, temperature, and even human behaviors. Multiple sensors can be attached to one object or device. For example, speed sensors could be attached to vehicles to detect the speed. It is difficult to obtain such information through traditional sources, but is capable in the Internet of Things. User behavior and preferences could be known more accurately based on the information, which is of great importance in e-business. Today, new sensor technologies, such as the eye-tracking sensor technology, which was emerged and developed in recent years makes it possible for us to know the behavior characteristics of consumers during the online shopping process better. Further, sensor data could be sent to the cloud, then processed by the big data platform, and ultimately serve for webpage design, e-marketing and also e-advertising. Therefore, the application of IoT can promote the improvement of consumer performance in electronic market, and it can be used to realize the business intelligence in electronic market. The development of IoT technology can not be separated from the progress of wireless sensor networks. Cognitive radio technology is used in wireless sensor networks, so the sensor network node can perceive the surrounding spectrum information in real-time, and acquire usable spectrum resources, and complete the communication process by selecting the spectrum resources that haven’t been used yet. Therefore, it can not only be used to relieve the crowdedness of public frequency, but also improve the utilization rate of network in using idle spectrum, and increase the optional working bandwidth of nodes. Nodes can choose idles spectrum for working dynamically, it will reduce the waiting time and conflicts of nodes to get channels. Therefore, it can improve the handling capacity of network and shorten the communication delay of network. With the continuous development of IoT technologies and the emergence of new algorithms and sensors, IoT technologies plays an increasingly important role in our future lives. In this paper, grip force and eye-tracking sensors are used to analyze consumers’ online reviews search behavior. In terms of e-commerce, big data technology has always been used to analyze the consumers’ loyalty, purchasing power, and demand degrees. Different pricing strategies will be designed based on the findings of this analysis, so as to realize business intelligence, which is the development tendency in the future. Items produced with recycled water are among the recycled products. Recycled water reuse contributes to eliminating water pollution and improving water supply, which is of great significance to address the problem of worldwide water scarcity. Therefore, it is essential to research how to promote recycled water reuse and guide consumers to buy items produced with recycled water. In the past 100 years, human water demand was 8 times higher more than before. Human activity has been the dominant factor that affects natural water circulation system. Its implications are so severe that the environmental carrying capacity has been broken, which has triggered irrevocable destruction to the ecosystem. In urban areas where accommodate dense industries and people, the gap of demand and supply for water resources has been acute. Therefore, it is of necessity to combat water scarcity in socioeconomic development by replacing freshwater with alternative water sources. Recycled water reuse in modern society can be traced back to the mid-19th century. However, before 1990, due to the backwardness of sewage treatment technology, the recycled water can only be used for agricultural irrigation in backward areas. However, in the following 10 years, with the rapid development of science and technology, the recycled water with advanced treatment has been directly or indirectly used as drinking water source in some developed countries. Nowadays, sewage treatment technology has been developed to produce recycled water that can meet the water quality standard for any needs. However, in reality, the reuse of recycled water is still limited to several uses as it was decades or even 100 years ago without any progress. The reason why the reuse of recycled water can not be transformed into real productivity despite of the fact that it already broken through the technical difficulties is the resistance of the reuse technology of recycled water. As early as the end of last century, scholars have keenly realized that backwardness of technology is not the biggest obstacle for the publicity of recycled water reuse project, but the public's psychological unacceptability. Since then, many research and engineering examples have repeatedly confirmed that public exclusion of recycled water reuse projects is a key factor affecting its publicity. Pollutants in water can be removed by technological method, but it is difficult to remove the public perception of sewage dirty. The public often reject to use recycled water reuse because of the disgust that recycled water is Toilet to Tap. This phenomenon was discovered by scholars as early as the 1970s and described as the public’s aversion to the impurity of recycled water. In subsequent studies, nausea was shown to have a strong predictive effect on public exclusion for recycled water reuse. Even if authoritative scientists guarantee the quality of recycled water, but it does not change the public's opinion about recycled water reuse. As a result, the reuse of recycled water is more likely to be rejected by the public than other alternative water resources such as rainwater reuse and desalination. In recent studies, scholars have defined this phenomenon once in context always in context as spiritual contagion, which means recycled water was sewage water, so it is and will be sewage water all the time. As a result, even though there are no pollutants in the recycled water, the public still can not change their mind about recycled water. Callaghan et al supported this conclusion in a study that they conducted in Australia, it was found that the public tend to associates recycled water with uncleanness and pollution instinctively. Some scholars attributed this phenomenon to the public’s pursuit of pure natural products, and found that the public always prefer to get pure natural products, which make it very difficult for them to accept recycled water. In Wester's research, we further found that the potential threat to human health caused by the residues of harmful microorganisms and chemical constituents, and which is a key factor why the public’s affection towards recycled water reuse is not positive. Despite water treat technologies have been great developed in recent years, a growing number of disease-causing agents have also been discovered, leading to a hanging suspicion among people in terms of the safety of recycled water reuse. When Dolnicar et al. reviewed the documents of recycled water reuse, they found that public acceptance of recycled water reuse has been proved to be associated with the concerns among public on health risk induced by recycled water reuse. West et al. further revealed that due to health risk arising from the harmful virus and chemical residuals, people reject recycled water reuse. Moreover, lower public acceptance has been found in the recycled water with higher human contact. Nevertheless, the problem that whether the consumers’ health risk perception on various usage of recycled water changes over human contact degrees hangs. For the above issues, this research aims at exploring how human contact degrees of recycled water reuses affect Chinese consumers on-line shopping of products produced by recycled water. This research endeavors to respond to the following questions: How does the degree of human contact change with different recycled water reuse? Does the public cognition on the safety of recycled water products change over the public cognition of human contact degrees of recycled water reuses? How public cognition on human contact degrees of recycled water reuses affect people in terms of prices of the recycled water products?", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2019.03.010", "references": [1979626518, 2032106001, 2129368140, 2325803613, 2341474478, 2403237691, 2501873384, 2508998601, 2735656498, 2763473974, 2792329197, 2800114294, 2885468577, 2887238322, 2896356445]}
{"paragraph": "Haptic sensations provided by commercially available wearable devices are often limited to vibrations, reducing the possibility of rendering complex contact interactions. Toward more realistic touch sensations, researchers started to study how to provide other types of cutaneous stimuli in a wearable and unobtrusive way. One of the first wearables able to provide rich cutaneous sensations has been presented by Minamizawa et al. Two motors, placed on the nail side of the fingertip, move a belt in contact with the user's finger pulp. The belt applies a normal force to the user's fingertip when the motors rotate in opposite directions, whereas it applies a shear force when the motors rotate in the same direction. The device's motor inputs are calculated by implementing a simple proportional law between belt displacement and target stimuli. More recently, Prattichizzo et al. presented a similar cable-driven 3-degree-of-freedom cutaneous device for the fingertip. A static platform, housing three dc motors, is located on the nail, whereas a mobile platform, acting as the end effector, is placed in contact with the finger pulp. The mobile platform can press into the user's fingertip and reangle to simulate contacts with slanted surfaces. This device has been used for applications in robotic teleoperation and virtual reality, mostly employing position-based control approaches. Although this type of cutaneous devices have been successfully used in various scenarios, their end effectors always contact the finger skin. As a result, these devices are not capable of conveying the sensation of making and breaking contact with virtual and remote surfaces, which are known to be important for haptic interaction. To overcome this limitation, Kuchenbecker et al. presented a passive contact location display to be attached to a grounded haptic interface. The kinesthetic feedback provided by the grounded interface bends the internal springs of the display and brings a shell in contact with the user's finger, providing the sensation of making and breaking contact with the rendered surface. Frisoli et al. achieved a similar effect by creating a finger-mounted thimble that moves a 5-DoF flat contact plate around the fingertip. More recently, Girard et al. developed a 2-DoF wearable haptic device able to render shear forces at the fingertip. It is composed of a parallelogram structure actuated by two dc motors that move a peg responsible for the shear feedback. A recent review on wearable haptic interfaces can be found in literature. To provide well-rounded sensations, researchers have also worked on designing wearable interfaces able to provide both kinesthetic and cutaneous stimuli. For example, Cempini et al. developed an underactuated fingertip exoskeleton with a custom self-alignment mechanism to absorb human robot joint axes misplacement. It has been designed for close human–robot interaction applications and it is driven using a hierarchical two-layer position controller. Sarac et al. recently presented an underactuated hand exoskeleton able to adapt its shape and size to objects during grasping. It has been designed for applications in virtual and augmented reality. Indeed, providing both cutaneous and kinesthetic feedback has often showed better performance than providing either cutaneous or kinesthetic feedback alone. For example, when kinesthetic feedback was enriched with cutaneous cues, Frisoli et al. found a significantly lower threshold for curvature discrimination for stimuli constituted of spheres with curvatures ranging in the interval 4–6. Pacchierotti et al. showed that providing both cutaneous and kinesthetic feedback improved the performance of a teleoperated pick and place task with respect to conveying, separately, either kinesthetic or cutaneous feedback. Similarly, Meli et al. found that providing cutaneous stimuli alone performed worse than providing cutaneous and kinesthetic stimuli in a hole-in-peg task. More recently, Quek et al. used a teleoperation system to perform two manipulation tasks using kinesthetic feedback, skin deformation feedback, and the combination of both. The combined kinesthetic and skin deformation feedback achieved better performance and higher participant ratings compared to kinesthetic or skin deformation feedback alone. This paper presents the control and evaluation of a novel modular interface for haptic interaction. The device is composed of a 3-DoF fingertip cutaneous device and a 1-DoF finger kinesthetic exoskeleton, which can be either used together as a single device or separately as two different devices. The 3-DoF fingertip module is composed of a static upper body and a mobile platform: the static body is located on the nail; supporting three small servo motors; and the mobile platform is placed in front of the finger pulp. The two parts are connected by three articulated legs, according to a revolute-revolute-spherical kinematic chain. The legs are actuated by the servo motors and can move the platform away and toward the user's finger skin as well as rotate it to mimic contacts with arbitrarily oriented surfaces. With respect to the cable-driven fingertip devices presented previously, our 3-DoF fingertip module solves the indeterminacy due to the underactuation of the platform. Moreover, it is one of the most compact and lightweight devices ever presented. The 3-DoF fingertip module alone weighs 24 g for 35×50×43 mm dimensions, and its design is inspired by Chinello et al. The finger exoskeleton is a 3-DoF planar mechanism: two DoFs allow the adaptability to different finger sizes, whereas the third one provides the actuation. Once the exoskeleton is worn, a static part is fixed on the proximal phalanx, whereas a mobile part is fixed close to the distal interphalangeal joint axis. In this way, the finger kinematic structure, composed of the proximal and intermediate phalanges connected through the proximal interphalangeal joint, constraints the exoskeleton kinematics, reducing the overall system mobility to 1 DoF. Consequently, only one motor is needed to actuate the finger exoskeleton system and provide kinesthetic stimuli to the PIP DIP articulation. With respect to similar interfaces, the proposed 1-DoF finger exoskeleton is extremely compact and lightweight: it weighs only 18 g for 117×30×42 mm dimensions. The complete kinesthetic and cutaneous devices weighs 42 g for 117×50×43 mm dimensions. Moreover, it can be easily adjusted to fingers of different sizes. Finally, to the best of our knowledge, it is the first time two wearable devices are designed in such a modular way.", "section": "Introduction", "doi": "10.1109/TIE.2019.2899551", "references": [2004961613, 2030898082, 2035102408, 2095370056, 2099082346, 2099914424, 2111121152, 2112418710, 2115394969, 2116822311, 2125068540, 2160250688, 2237068932, 2241992751, 2337591881, 2574832239, 2612110125, 2758511764, 2784796184, 2800266831, 2885564406, 2895422053, 2900630341]}
{"paragraph": "Many real-world applications, such as engineering design, air traffic control, nurse rostering, and car controller optimization, require optimizing more than three objectives at the same time. These kinds of multi-objective optimization problems are usually denoted as many-objective optimization problems. Generally, MOPs, including MaOPs, can be mathematically represented where the decision variable vector belongs to the feasible region of the search space. It consists of m objective functions, and the objective space. An MOP is called an MaOP if m is greater than 3. Given two solutions and their corresponding objective vectors, one is said to dominate the other if and only if the objective values are less than or equal for every objective and not equal overall. If there does not exist any other solution that dominates a given one, then it is defined as a Pareto optimal solution. The set of all Pareto optimal solutions is called the Pareto set, and its corresponding objective vector set is called the Pareto front. In recent decades, a variety of multi-objective evolutionary algorithms have been proposed to deal with MOPs. Usually, an MOEA aims to find a tradeoff among different objectives and then provide a well-representative set of Pareto optimal solutions from which decision makers choose a final solution according to their preferences. However, the search ability of general MOEAs is considerably deteriorated when more than three objectives are considered at the same time. The main difficulties include: as the number of objectives increases, the Pareto dominance may lose selection pressure towards the Pareto front since the solutions obtained in each generation are more likely to be non-dominated with each other; aiming at finding a set of non-dominated solutions to approximate the entire Pareto front as much as possible, traditional MOEAs may require a large number of solutions to approximate the entire Pareto front, which becomes a hyper-surface in an MaOP; and the deterioration in visualization of non-dominated solutions makes it difficult for decision makers to choose a final solution. Facing these challenges, recently much effort has been applied to address MaOPs. The current work in evolutionary many-objective optimization can be roughly partitioned into two categories: algorithm enhancement and problem simplicity. Algorithm enhancement focuses on improving the ability of current general MOEAs to search for Pareto optimal solutions of an MaOP. In other words, they address MaOPs by directly designing new specialized algorithms to handle the difficulties that traditional MOEAs meet in MaOPs. In this category, one method is to modify the Pareto dominance, a direct strategy to decrease non-dominated solutions in each generation and increase selection pressure towards the Pareto front. However this method may decrease the diversity of population. Therefore, some works use different fitness evaluation mechanisms to replace Pareto dominance during selection. In these methods, the Pareto dominance relation is avoided so that the search ability of traditional MOEAs is not severely deteriorated when the number of objectives increases. A typical approach uses performance indicators, such as hypervolume, and multiple indicators, to evaluate and select solutions. However, a high computation cost for computing hypervolume casts a shadow on this method. Another approach uses the decision maker’s preference to select a specific region of the Pareto front and then deploys MOEAs to search for non-dominated solutions in this small region. The decomposition based approaches or reference-vector-guided approaches have also attracted much attention. Problem simplicity addresses MaOPs by simplifying the original optimization problems and then using existing MOEAs to deal with the simplified problems. Usually an MaOP is converted into an MOP or a set of MOPs with fewer objectives. In this category, the key issue is how to reduce the number of objectives. Principal component analysis has been used to identify the most essential objectives from the original objective set. Another idea has been proposed to find a minimum objective set that either preserves the Pareto dominance relation or just slightly changes it according to a predefined error. Some methods sort each objective based on its conflict with other objectives, and then select the top conflict objectives to optimize. Other approaches have used mutual information and a clustering method to remove redundant objectives iteratively. An unsupervised feature selection method has also been applied to identify the most conflicting objectives. Some researchers extracted the nonlinear correlation information entropy and used it to measure the linear and nonlinear correlation between different objectives. Then the most conflicting objectives were chosen as the objectives in the running process. Others constructed new objectives as linear combinations of the original objectives by maximizing the conflict between the reduced objectives. A clustering method has been proposed to reduce the objectives. This paper follows the idea of constructing new objectives as linear combinations of the original objectives. As in previous work, the Pareto optimal solutions of the simplified MOPs with reduced objectives may only cover parts of the Pareto front of the original MaOPs. Furthermore, to find the optimal weight vectors, some additional optimization problems have been introduced, which may make the problem more complicated. This paper investigates a novel method to compute the weight vectors, which works by constructing a reduced objective set that provides an approximation to the Pareto fronts of the original MaOPs. The basic idea is to use a fuzzy clustering method to partition the objectives based on the data collected in the previous search. The membership degrees denote the relationships between the objectives and they can be naturally used as the weight vectors. Extensive experiments have been conducted to demonstrate that our method has the most stable performance and is competitive with some well-known methods for dealing with ill-posed MaOPs. The rest of this paper is organized as follows. Section 2 introduces the proposed objective extraction method via fuzzy clustering, and then incorporates it into a traditional MOEA. Section 3 reviews the ill-posed benchmark function, then conducts extensive experiments to analyze the feasibility and necessity of computing valid weight vectors for constructing a reduced objective set, and finally verifies the algorithm’s performance and compares it with some state-of-the-art methods. Finally, the paper is concluded in Section 4 with some future work remarks.", "section": "Methodology", "doi": "10.1016/j.ins.2018.11.032", "references": [1493761729, 1497465174, 1525375343, 1591635931, 1595205399, 1605810911, 1607221389, 1733107040, 1842155679, 1868857639, 1969955637, 1980591224, 1999729620, 2018712218, 2020320008, 2025525377, 2038420231, 2040622444, 2074921660, 2110828487, 2116858902, 2120091775, 2126105956, 2136386226, 2140886193, 2143185749, 2143381319, 2166898117, 2312339748, 2343601797, 2344880386, 2430564265, 2521918431, 2546299924, 2548746833, 2735649024, 2755719112, 2775348664, 2781380637]}
{"paragraph": "This paper proposes a comprehensive method for adaptive tuning of complex contour curve using omnidirection touch probe (ODTP), optimal interpolation and motion planning, and hybrid multiaxis robot (HMAR) with dual drive gantry-type machine (DDGM). These solutions mainly focus on the higher order complex contour maneuvering for automated machining tasks, such as laser cutting, welding, gluing, polishing, deburring, etc., with dual functions, namely machining and verification all in one platform. An ODTP is orthogonally attached to the tool station, so it can perform reverse engineering tasks through adaptive tuning of the curve with optimal interpolation and motion planning solutions. The key features of this paper are to develop multifunctional capabilities integrated into one platform including machining and verification. For the complex contouring tasks, normally, a custom-made controller is required for this application to implement the difficult interpolation issues. In this paper, a general controller is used to present an algorithm that generates a smooth and efficient three-dimensional (3-D) reverse higher order complex contour trajectory composed of piecewise small segments under certain constraints, for example, maximum acceleration and a defined feedrate. The theoretical description of the algorithm is developed and the entire hybrid machine system is designed and implemented in our lab. Experimental proof of concept is successfully demonstrated with a video clip attached in the supplementary material. High accuracy 3-D laser scanners can make the contour curve similar to a real object, but it might have blind spots, not able to detect and it is rather expensive. Real contact verification of the object's actual dimension using the ODTP instead of the laser scanner is shown and implemented in this paper. Since we need to obtain the normal direction of real object contour curve, the six-degree of freedom ODTP with tool center point (TCP) mode is essential for 3-D reverse engineering applications. Moreover, the qualitative comparisons of ODTP, coordinate measuring machine (CMM), and 3-D laser scanner are shown in Table I. In Step 1, this process examines how/which feature locations of the objects are detected by the ODTP, and maintain the ODTP in TCP mode normal to the object contour curve. This step addresses/tackles the disadvantages of nondirectional touch probes. Moreover, this step is detailed in Sections II-A to C. In Step 2, the main contribution in this paper is provided, adjusting one of the adaptive parameters of the curve easily allows for the contour curve to approach the object. A conventional nondirectional touch probe needs to detect the object on many contact points that cover almost the entire object. Furthermore, Section II-D describes the solution in more detail. In Step 3, the optimal interpolation is implemented, and replace the conventional interpolation with optimal interpolation assignment, this step can increase the accuracy of reverse contouring in Section III-A. In Step 4, we merge both the adaptive tuning curve and the optimal interpolation developments to create the basis curve line from Step 2 and Step 3. The basis curve line is used to generate the 3-D high order complex contour surface/object in the following step. This step is described in Section III-B. Our proposed process theorem eliminates the bottleneck found in conventional methods. In Step 5, this step utilizes the symmetry, extension, and synthesis properties of the object to simplify and efficiently generate the 3-D complicated complex contour surface/object, for example a spherical bowl. This step can decrease the time/frequency of the ODTP to be used. For the conventional nondirectional touch probe, it is difficult to complete the reverse engineering of complicated complex contour surface/object. Moreover, this step is described in Section III-C. In Step 6, the obtained 3-D reverse contour curve becomes the real object through the HMAR with DDGM. In this process, we use a general controller to complete the task instead of a custom-made controller. We assign the acceleration time (TA) within the HMAR with DDGMs system constraints and obtain the constant moving speed, the efficient motion is implemented in this step. Moreover, this step is described in Section IV. The constant moving speed is very sensitive and important for some applications, such as gluing, welding, and laser cutting to preserve good quality processing, and this will be further discussed in experimental results, which are shown in Section V. Based on the abovementioned steps, the 3-D reverse surface/object can be successfully obtained, and the conclusion is provided in Section VI.", "section": "Methodology", "doi": "10.1109/TIE.2019.2898593", "references": [1968642526, 1998898046, 2003327206, 2010075357, 2086332056, 2094627939, 2097786609, 2105437924, 2106835785, 2114860731, 2117571644, 2134429612, 2157087522, 2162824042, 2524627468, 2598353255, 2748932079, 2776089143]}
{"paragraph": "In everyday life, we are surrounded by textual information: academic papers, technical reports, patents, business contracts, application forms for business procedures, presentation slides, emails, etc. Working with documents is ubiquitous in our work life, as well as our personal life. When working with documents, people frequently print them, not only for the ease of reading, but also for comparing information from different documents. New digital environments, such as tabletop systems, allow documents to be displayed in a similar way to a sheet of paper on a table. However, we need to understand how to optimize the reading and writing experience in tabletop systems for creating efficient and comfortable work environments. This work focuses on investigating preferred orientation of paper documents in relation to readers during reading, so readers can maintain a comfortable body posture and can easily handle documents while performing the reading as efficiently as possible. Informal observations of people reading physical paper documents show that right-handed people often rotate documents counter-clockwise and/or place documents to the right side of the body. When actively working with a document, such as writing, annotating or drawing, it is often rotated to a steeper angle. People rarely read documents placed at the central or median line of their body without any rotation. Scientific studies have reported on document positions, for instance, it was noted that readers rotated their working documents 0–20° when reading and 30–45° when writing. It was also observed that artists continuously rotated and adjusted the drawing bit-by-bit to keep the active drawing area in the same position relative to the hand. Another study reported that participants felt uncomfortable when reading on horizontal displays because the document was presented at an unnatural reading angle. It was observed that the orientation of documents or other objects, such as photos, could signal users’ intention to share them during collaborative work. For example, users indicated that they were not ready to share an object by placing it upside-down or at an odd angle. The document orientation can also serve as a barrier around a personal workspace, or as a prompt to others to view it. A few tabletop systems have specifically been developed to support reading or small group discussions around text documents. In these situations, being able to adjust a document rotation and location is important for creating a comfortable work environment. However, most of these tabletop systems assume that people prefer reading a document aligned to their median line. For instance, many systems provide a feature to automatically rotate documents to be perpendicular to the table’s closest edge. Some tabletop systems allow users to manually orient the documents. However, if the optimal document orientation is known, systems can automatically position documents according to users’ location and task. If reading and writing performance varies with document orientations, automatic positioning of documents can be an important feature for task efficiency. In this paper, we establish the optimal document orientation and investigate reasons for users’ preferences. We conducted five experiments to uncover both subjectively preferred and objectively optimal document orientation for left-handed and right-handed people. Although we do not use a tabletop device, our results can be applicable to tabletop applications as people’s activity with the document and their position relative to it remains the same. Therefore, we finally discuss how these results impact these systems.", "section": "Methodology", "doi": "10.1080/07370024.2018.1427584", "references": [1269801495, 1992306856, 1998408655, 2005855683, 2007580360, 2009220433, 2021346535, 2030533114, 2037001467, 2079876841, 2082591969, 2094567668, 2094982166, 2101082575, 2101532415, 2102773632, 2106779688, 2121172310, 2122624458, 2130334143, 2135718368, 2137092817, 2140706899, 2148969329, 2151942546, 2155811000, 2156317066, 2158165714, 2158707444, 2169463011]}
{"paragraph": "In 2004 Web 2.0 was launched, marking the beginning of a new era in the history of the internet. This era is characterized by a shift from passive to more active internet consumption, allowing users to develop and disseminate their own content and to communicate. Social media constitutes the most popular platforms for these actions. These are internet-based applications that allow the creation and exchange of user generated content. These applications include blogs, social networking sites such as Facebook, content-sharing sites such as YouTube and more. Since the mid-2000s, social media joined other sources of health information by becoming a platform for posting, sharing or commenting on health-related content and for joining health-related groups. Yet research on health-related internet use has concentrated mostly on searching for online health information and much less on using the internet for other health-related purposes, such as online health services and health participation, which is the focus of this study. The notion of health participation is still poorly defined. Usually, active health-related participation refers to communication regarding health issues. On social media, however, it may include more actions. Three main types of health participation activities are mentioned in the literature: sharing personal experiences regarding chronic health conditions, discussing the work of health institutions, usually by posting of reviews on doctors, and posting or commenting on health-related content. The current study uses the lifestyle/exposure theoretical framework. According to this theory, the probability individuals will behave in a specific way is the result of their lifestyle and habits. Hence, differences in probabilities of engaging in a behavior reflect differences in lifestyle. Since lifestyle behaviors, like any other behavior, are a consequence of an individual's position in the social stratification system, differences in behaviors can be attributed to socioeconomic background. This theory has been tested in the context of offline and online victimization. Nevertheless, it has never been used to explain activities unrelated to victimization, especially those related to health participation via social media. Furthermore, gender differences in health participation have never been examined using this theoretical framework. The issue of gender in health-related use of the internet is well documented in studies in the fields of internet sociology and public health. The most stable finding across studies is that female users search for health information more than male users. Yet, due to the small body of knowledge on gender differences in health participation, the following research question emerges: What is the gender structure of health participation on social media? This question can be broken down into three alternatives: the known gender differences in searching for OHI are replicated in health participation behaviors, the absolute monopoly hypothesis; male users are more active health participants than female users, the areas of control hypothesis; there are no gender differences at all in health participation, the democracy hypothesis. According to the absolute monopoly hypothesis, women are expected to be dominant not only in searching for online health information, but also in the domain of active health participation. The absolute monopoly hypothesis reflects traditional gender role models, according to which women tend to take part in online health-related activities due to their family role of health caregivers and health managers. In contrast, according to the areas of control hypothesis, women and men will have separate domains of dominance in health-related internet use. Female users will be dominant in searching for health information, while male users are expected to be dominant in the health participation domain. This hypothesis is based on the finding that male users tend to be frequent contributors to online discussions, reflecting differences in socialization such that women, as compared to men, are socialized to more passive gender roles. Finally, the democracy hypothesis contends that there are no gender differences in health participation on social media. Some studies found no such differences with respect to communicating on health issues via the internet, thereby supporting this hypothesis. This study makes several contributions to the field. From the theoretical perspective, it extends the lifestyle/exposure theory to cover gender differences in online activities, which are unrelated to the field usually studied in the context of this theory. From the empirical perspective, this study employs a toolkit approach, according to which health participation on social media is considered multidimensional and therefore investigated using several outcomes. The article is organized as follows. First, we provide a general literature review, followed by a discussion of the research hypotheses and other factors that can affect health communication via social media. After that, we describe the methodology used in the study. Then we provide the descriptive, bivariate and multivariate results of the study. Finally, we discuss the results and outline the main conclusions.", "section": "Methodology", "doi": "10.1016/j.chb.2019.08.016", "references": [1961406311, 1969052573, 1973434331, 1981531558, 2026229375, 2028182940, 2029430065, 2029522813, 2029833201, 2054865483, 2071997882, 2092129161, 2110016748, 2131567188, 2169205257, 2399742221, 2559738839, 2765897036, 2775442000]}
{"paragraph": "Let be a commutative ring with unity, and let be a multivariate polynomial. The multi-point evaluation problem consists in evaluating at several given points in . Let be polynomials in of degrees and let be a monic polynomial in of degree . The modular composition problem consists in computing modulo . This is equivalent to the computation of the remainder of the Euclidean division of by . It turns out that these two problems are related and that they form important building blocks in computer algebra. Theoretically speaking, Kedlaya and Umans have given efficient solutions to both problems when is a finite ring of the form where is a monic polynomial. The design of practically efficient algorithms remains an important challenge. The purpose of this paper is to revisit the algorithms by Kedlaya and Umans in detail, to sharpen their theoretical complexity bounds, and get some insight into the required data size for which this approach outperforms asymptotically slower algorithms. Let denote a complexity function that bounds the number of operations in required to multiply two polynomials of degree in . We will often use the soft-Oh notation: means that ; see for technical details. The least integer larger or equal to is written . The largest integer smaller or equal to is written . The -module of polynomials of degree is denoted by ≔. In the univariate case when , the evaluation of at points in can be achieved with operations in . We refer the reader to for the description of the well known algorithm based on remainder trees. Algorithms with the smallest constant hidden in the “” may be found in . By allowing precomputations that only depend on the set of points, this evaluation complexity even drops to as shown in . For specific sets of points, such as geometric progressions or TFT points, multi-point evaluation requires only operations in ; see . The univariate situation does not extend to several variables, unless the set of evaluation points has good properties. For instance if has the form with , then fast univariate evaluations may be applied coordinate by coordinate. Fast algorithms also exist for suitable initial segments of such Cartesian products. Other specific families of sets of points are used for fast evaluation and interpolation of multivariate polynomials in sparse representation; see for some recent results. In the bivariate case when , a smart use of the univariate case leads to a cost , where bounds the partial degrees of . In 2004, Nüsken and Ziegler improved this bound to —here the constant is such that a matrix over may be multiplied with another rectangular matrix with operations in . When is a field the best currently known bound is due to Huang and Pan. In 2008, Kedlaya and Umans achieved a major breakthrough for the general case. In they showed the following statement (simplified here for conciseness): let be a fixed rational value, given in with partial degrees in any at most , and evaluation points in , then can be computed with bit operations, provided that and where is a constant independent of . This result was stated for random access memory machines. In fact, some of the underlying arguments (such as the use of lookup tables) need to be adapted to make them work properly on Turing machines. This is one of our contributions in this paper. In a nutshell the Kedlaya–Umans algorithm proceeds as follows: If is “sufficiently small”, then we exhaustively evaluate at all points in , using fast univariate multi-point evaluation with respect to each coordinate. Otherwise, the evaluation of at is reduced to the evaluation of the preimage of in at the preimages of in . Through the Chinese remaindering theorem, the latter evaluation over further reduces to several independent multi-point evaluation problems modulo “many” prime numbers that are “much smaller” than . These evaluations of mod at are handled recursively, for .", "section": "Introduction", "doi": "10.1016/j.jco.2019.04.001", "references": [1562183207, 1581260235, 1583935850, 1625225258, 1754110195, 1764552993, 1941702124, 1974551963, 1989233798, 1990568802, 2010953531, 2029631600, 2030141403, 2033519499, 2033961328, 2042513324, 2052024558, 2055580199, 2060682338, 2060801388, 2084801848, 2090127958, 2090840450, 2096280747, 2104070031, 2130918732, 2166717942, 2168527104, 2170156299, 2207819601, 2550792289, 2625628003, 2738987789, 2773766672, 2785230309, 2788641555]}
{"paragraph": "Plant-wide optimization and control can significantly improve overall performance of an industrial plant. Usually, the industrial plant consists of multiple complicated subprocesses, each of which may have different objective and performance requirements. The processes need to be operated in a collaborative way to fulfill the desired overall performance for the plant. The main tasks of the plant-wide optimization and control are twofold: first, determining optimal production index for each subprocess from the viewpoint of the overall performance of the plant; second, calculating control inputs for the individual units to achieve the optimal production index. In industrial plants, the management department formulates production plans and performance requirements for every process. Ideally, each subprocess should be operated and controlled to not only guarantee its own performance but also achieve the overall performance of the plant. In practice, every industrial process is complicated, and the operational status of one process will influence the optimization and control of the next process in series. Furthermore, frequent changes in feed conditions challenge a plant-wide optimization strategy. In most industrial plants, the plant-wide optimization and control generally rely on the experience and knowledge of human experts. Based on the overall performance requirements for a plant, experts determine a set-point of the production index yspm for the subprocess m. For example, a production index may include product quality, energy or raw material cost, and requirements of other technical indicators. An engineer of a process calculates optimal control um based on a specific production index. The optimal control inputs are delivered to a low-level controller, which controls actuators. Because of the limited experience and knowledge, many times the experts cannot make the decisions in time. Also, manual setting of the production indices cannot ensure overall performance of the plant. Due to a lack of coordinating mechanism in setting the production indices, the manual strategy cannot effectively deal with the effects of the varying feed conditions on the plant-wide production indices. Therefore, under the manual strategy, the M systems in the plant are independent of each other with each system caring for itself. If the feed conditions are constant and the processes operate in steady-state with no disturbances, the manual strategy may work satisfactorily. However, in most industrial processes, the output of one subprocess influences the subprocess next to it. Hence, the production index for each unit should not be determined by its own required performance, but rather by the overall performance of the plant. In addition, the feed conditions of the process are time-varying, sometimes in a large fluctuation. Furthermore, presence of uncertainties and stochastic disturbances are often inevitable in the real processes. Hence, the manual strategy is often not adequate. A computerized coordination strategy is needed and important. The plant-wide optimization and control methodology has attracted a great deal of attention in recent years. A knowledge-based global operation approach is proposed for a mineral processing plant. The operational indices for processes are generated based on the case-based reasoning. A plant-wide control structure is designed based on steady-state indices and multiobjective optimization. A distributed model predictive control (MPC) is proposed for plant-wide hot-rolled strip laminar cooling process. Each subsystem in this process is controlled by local MPC, and these local MPCs cooperate with their neighbors to achieve the overall performance. In another work, eco-efficiency factor was considered in the plant-wide control to determine the control loop configuration. An economic objective function was used to calculate the optimal output set-points. The optimal set-points of the production indices influence the plant-wide control performance significantly. When determining the optimal set-points of the production indices, coordinating optimization plays a vital role. The coordination strategy contributes to dynamically adjusting the production indices of processes to achieve the overall performance when the feed conditions change, including large fluctuations. To address this weakness, supervisory MPC is proposed for the plant-wide optimization and control. The supervisory MPC coordinates the local MPCs to attain plant-wide optimum. In these schemes, steady-state optimization or real-time optimization was employed to obtain the optimal operating points. The fluctuation in the feed conditions is a common presence in industrial processes. To our knowledge, the set-point optimization for industrial processes with fluctuating feed conditions and complicated dynamics of processes is still a challenge for the plant-wide control. After the optimal production indices for the subprocesses being determined, the set-point tracking control is another challenge. In the control community, MPC is a powerful technique for industrial processes. There are many successful applications of the MPC in the industrial processes, for example, in the polymerization process, cooling process, metallurgical process, and wastewater treatment process. One of the important issues in the MPC is having a sufficiently accurate system model. For a real-world industrial process, however, mathematical structure of a system is usually difficult to accurately identify, especially for a large-scale plant-wide process. Therefore, we use a data-driven modeling method to establish the plant-wide process model. Among data-driven modeling techniques, fuzzy neural network (FNN) has been shown to have good modeling performance in many applications. For example, a generalized ellipsoidal-basis-function-based FNN was developed to identify the nonlinear steering model, and a recurrent FNN was constructed for the predictive control of industrial processes. The parameter tuning method is important in designing the FNN. To improve the ability of FNN, a growing-and-pruning approach was proposed to optimize the structure of the FNN. Then, a supervised gradient decent method was used for parameter learning. An improved genetic algorithm was designed to optimize the structure and parameters of the FNN. The present paper explores the learning rate in the gradient decent algorithm and the convergence of the designed FNN. To address the above-mentioned problems, we propose a hierarchical coordinating optimization and control strategy to adjust the production indices coordinately and achieve the plant-wide optimal control. The strategy consists of the following three layers: a plant-wide coordinator, a local optimizer, and a local controller. In the coordinating layer, a plant-wide optimization problem is constructed to achieve the overall optimal performance. The local optimizer minimizes the cost objective for each subprocess and delivers its production index to the coordinating layer. Then, the coordinator adjusts the production indices according to the overall performance, and the final optimal set-points of the production indices are obtained. MPC technique is adopted to calculate the control inputs for each unit in the local controller. A FNN with gradient-based learning algorithm is employed to approximate the dynamic behavior of the process. The convergence of the FNN is discussed. The algorithm of numerically solving the optimization problem in coordinating layer is given. A model correction approach is proposed to correct the FNN online according to the feedback error information. Experiments in the largest zinc hydrometallurgy plant in China verify the effectiveness of our strategy. The rest of this paper is organized as follows. Section II describes the proposed hierarchical coordinating optimization and control strategy. The FNN and its convergence analysis are presented in Section III. Section IV presents our experiments in the largest zinc hydrometallurgy plant in China. Section V concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2902790", "references": [1976084970, 1977230515, 2012921684, 2021863362, 2026104641, 2029661056, 2054446390, 2066872418, 2073787051, 2106276452, 2170262723, 2299857028, 2323630342, 2531749924, 2769115058, 2790103029, 2801671143]}
{"paragraph": "Due to the massive expansion of smart phones, which have dramatically revolutionized many fields, exquisite screen content (SC) pictures have been extensively employed as the medium for webpage browsing, online gaming, and advertising. Compared with the ever-popular natural scene (NS) pictures, SC pictures simultaneously contain pictorial, textual, and graphic contents. Such types of pictures can be broadly applied in industrial safety monitor-control and early-warning systems, as shown in Fig. 1. During the last few years, picture quality assessment (QA) has long been one attractive research topic, on account of its crucial applications to compression, enhancement, restoration, transmission, etc. Generally speaking, the quality of a picture can be assessed by two types of methods, including subjective assessment and objective assessment. The former is decisive and generally regarded as the benchmark because it accurately judges the overall picture quality through human viewers. Nonetheless, subjective assessment inevitably causes time-consuming, expensive, and labor-intensive issues and, therefore, is unable to be widely applied in the real-time application scenarios. To assess the picture quality efficiently and effectively, many objective picture QA models were proposed based on low-level vision, brain theory, statistics, etc., for better mimicking the process of human visual perception. Nevertheless, the methods mentioned above are primarily applied for NS pictures. As compared with NS pictures, SC pictures have the features of thin lines, limited colors, and diversified shapes, which lead to a series of challenging issues. In previous studies, it is illustrated that those QA models for NS pictures fail to evaluate SC picture quality. Hence, a growing amount of attention has been focused on this emerging research field. More recently, objective assessment models using a few reference information were proposed. These models, e.g., reduced-reference wavelet-domain quality measure of screen content pictures (RWQMS), reduced-reference quality measure of screen content pictures (RQMSC), and prediction backed model (PBM), can feasibly evaluate the quality of SC pictures even compared with the QA models with complete reference information. As compared with NS pictures, current research of quality evaluation of SC pictures still lies in the infancy stage. On one hand, abundant reference information is still required (e.g., screen content perceptual quality assessment (SPQA) and gradient direction-based index (GDI)). On the other hand, the prediction monotonicity is far from satisfactory (e.g., RWQMS, RQMSC, and PBM). In order to circumvent the deficiencies, we devise a novel method for accurately estimating the quality of SC pictures using little information of the original picture. Motivated by many research studies, which indicate that incorporating the gradient magnitude information in picture QA performs well particularly in extracting structures, in this paper, the gradient magnitude is fused, respectively, with the anisotropy measurement and the uncertainty information to detect the macroscopic and microscopic structures, such that the integral and detailed structures of a picture can be captured. Subsequently, we compare and combine the differences of macroscopic and microscopic structures between a corrupted SC picture and its corresponding lossless version to infer the overall quality score. Finally, we apply the histogram establishment method to reduce the dimensionality of extracted features. As such, minimum additional information is required to be transmitted to infer the visual quality, which is very practical for real application scenarios. The main contributions of this paper are summarized as follows. First, to the best of our knowledge, we propose a new effective QA framework of SC pictures, which appropriately incorporates the measurements of macroscopic and microscopic structural variations. Based on this framework, this paper deploys the proper measurements to capture the variations of macroscopic and microscopic structures and then fuses these two measurements to infer the overall quality prediction of the input SC picture. Second, our QA approach has achieved superior performance when compared with the current state-of-the-art QA metrics for quality evaluation of SC pictures. Third, the proposed QA model merely adopts very sparse reference information, i.e., two features, which can be exactly conveyed in the header file with very few bits. The rest of this paper is organized as follows. In Section II, the current QA metrics and models are introduced. Section III presents the design philosophy and methodology of the quality model. In Section IV, the experimental setup, results, analysis, and application are described and discussed. Finally, Section V concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2905831", "references": [1509918867, 1561538450, 1964743554, 1964859077, 1966528181, 1971014006, 1972006393, 1981259164, 1982471090, 1988649016, 2000594943, 2006784329, 2015196405, 2019002170, 2063360098, 2076136231, 2089190525, 2103997799, 2127393810, 2133665775, 2141983208, 2142731874, 2153582625, 2158546915, 2161907179, 2171125155, 2323509952, 2507677080, 2507774658, 2516858621, 2579658395, 2594112328, 2596372226, 2621054742, 2743390484, 2746214548, 2757064044, 2777280533, 2782579108, 2794680924, 2894938704]}
{"paragraph": "Clustering plays an important role in machine learning, data mining, image segmentation, and pattern classification. The goal of clustering is to classify elements into clusters on the basis of their similarity. A large number of clustering methods have been brought forward. Classical methods include hierarchical clustering; K-means clustering; spectral clustering; support vector clustering; multiview clustering; genetic clustering; etc. Among these clustering methods, spectral clustering has become one of the popular methods because of its robustness and effectiveness. Generally, the performance of the spectral clustering is better than other methods. Spectral clustering is able to seek the optimal partitioning of data based on the spectral graph theory. Traditional clustering algorithms such as K-means can only perform clustering with convex distribution. If the sample spaces are nonconvex, K-means would fall into a local optimal solution. Compared with K-means, spectral clustering can perform clustering with nonconvex sphere of sample spaces and obtain the globally optimal solution in a relaxed continuous domain. Although many spectral clustering methods have been proposed, such as Min Cut, Ratio Cut, Normalized Cut, Min–Max Cut, Spectral Embedded Clustering, K-way Rcut, and K-way Ncut, all of these methods adopt a two-stage process. The first stage is to learn the relaxed continuous spectral vectors and the second stage is usually to employ K-means or spectral rotation to post-process the continuous spectral vectors in order to obtain the final binary cluster indicator matrix. In practice, the manner of separately performing the two stages is not able to jointly obtain the optimal solution. In this paper, in order to overcome the aforementioned drawback of spectral clustering, we propose a new spectral clustering framework that jointly performs spectral embedding and spectral rotation. That is, the real-valued cluster indicator matrix usually obtained by conducting spectral embedding in the intermediate stage and the binary cluster indicator matrix usually obtained by conducting spectral rotation in the last stage are iteratively computed in our method. Recently, a unified framework for discrete spectral clustering was proposed. The framework is able to obtain the final clustering results by one step and results in significant improvement of clustering performance. Nevertheless, the objective function of that framework has a term which employs an orthonormal matrix to approximate a nonorthonormal matrix. The approximation cannot be very precise in theory. As will be shown in a toy example, this method tends to generate incorrect clustering results for unbalanced data where the underlying numbers of clusters are far from uniform. By contrast, the proposed method is capable of overcoming the drawback because approximation is conducted in-between two orthonormal matrices. In summary, the novelty, contribution, and characteristic of the proposed method are as follows. A joint model is proposed to simultaneously and iteratively perform spectral embedding and spectral rotation with spectral embedding generating a real-valued cluster indicator matrix and spectral rotation generating a binary cluster indicator matrix. Compared to the classical spectral clustering methods, the proposed joint model is able to overcome the drawbacks of the information loss and the risk of the discrete clustering deviation. In the spectral rotation part of the proposed joint model, approximation is conducted in-between two orthonormal matrices: a matrix generated by spectral embedding followed by a rotation operation and a scaled cluster indicator matrix. Therefore, the proposed method is able to obtain an accurate clustering result. In addition, the proposed method is able to overcome the problem of the unbalance of previous frameworks. The physical meaning of the scaled cluster indicator matrix is interpreted. Moreover, the theoretical derivation of the scaled cluster indicator matrix is given. The insight in the scaled cluster indicator matrix is helpful to understand the proposed method and developing a new method. The proposed method cannot only achieve an accurate clustering result but also be implemented very efficiently. The optimization process of the proposed convergences is in about three iterations. The rest of this paper is organized as follows. In Section II, the related work is discussed. Classical spectral embedding and spectral rotation are described in Section III. The proposed method is presented in Section IV. The experimental results are presented in Section V. Finally, Section VI concludes this paper.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868742", "references": [62192748, 1673310716, 1943383135, 1997011019, 2006793117, 2023512014, 2083620785, 2103560185, 2106115875, 2108502868, 2108995755, 2117553576, 2121947440, 2123921160, 2125531986, 2127615881, 2129210885, 2131828344, 2132603077, 2134737843, 2135674549, 2152322845, 2153233077, 2155074104, 2155754954, 2160642098, 2549545452, 2569580858, 2569859441, 2576553562, 2586218630, 2748075849, 2790034647, 2803629456]}
{"paragraph": "Route planning is important not only for our daily life, but also for business map engines like Google and Bing Maps. It is a key component in urban computing. Although the shortest or fastest paths are used commonly in road networks, they may be insufficient in some situations, while the popular route that refers to a path being travelled frequently is sometimes important. For example, drivers who travel in an unfamiliar city may prefer a popular path which may be safer with better traffic condition and road quality, and a taxi passenger may want to travel along a popular path in case of a roundabout trip. Moreover, since travel cost dynamically depends on the traffic conditions and other factors, such as traffic lights, people care more about the travel cost, i.e., how long it takes or how much it costs at the time they are leaving, so that accurate travel cost estimation will improve people’s satisfaction. With the development of intelligent transportation in cities, more and more GPS-equipped vehicles are running on the road networks. As a result, a large number of time-stamped GPS trajectories are consecutively generated. Based on the historical citywide trajectories, popular paths can be constructed by finding out how people usually travel between locations, and with the temporal information in the trajectories, it is possible to estimate the travel cost of paths. Route planning or driving direction planning has been studied in recent years and some influential works have been published. Some propose a framework to find out the practically fastest route at a given departure time based on a landmark graph learned from a large number of historical taxi trajectories. However, the fastest route is not always popular, some shortcuts may reduce the travel time but increase the risk and uncertainty of a trip. The work performs a two-stage routing algorithm based on the graph to find the fastest route. The first stage is to find the rough route represented by a sequence of landmarks whose travel time can be estimated by their model and the second stage is to find a practically detailed fastest route in the road network based on speed constraints. But the travel time of the detailed fastest route may be different from the estimated travel time at the first stage. Since there may exist several different paths between two landmarks and the estimated travel time is just the mean travel time of all possible paths. In most cases, the travel time of the detailed fastest route is less than the estimated travel time, which will cause inaccurate travel time estimation of the route. Another model estimates the travel time of a given path by using the optimal route concatenation which considers dependencies between road segments. However, it cannot be applied to route planning directly. To find the popular route with travel cost estimation, three challenges must be addressed. Data sparseness and coverage: due to the complexity of road network, we cannot guarantee that all roads have been covered by trajectories. Moreover, for each road, it is practically impossible that there are sufficient trajectories during all time intervals. We must thus contend with data sparseness to respond to users’ path queries between any locations at any time intervals. On the other hand, road network is not always available, it is unpractical to find the popular routes or roads by counting the support on each road. Thus, how to get the paths or roads from trajectories is meaningful. Travel cost modeling: different roads have different travel cost, because road quality and road condition are not the same. For example, the highway with less traffic lights travels faster than the roads with more traffic lights. Meanwhile, the travel cost of the same road also varies at different time. For example, the travel time at rush hour is usually longer than that at the non-rush hour due to traffic jam. Hence, an intelligent modeling for the travel cost on each road at different time that can benefit accurate travel cost estimation is a challenge, especially under the fact of data sparseness. Path cost estimation: the cost of a path can be estimated by summing up, however, the results are only accurate if the travel costs of roads are independent. In practice, the travel costs may be dependent due to intersection or turning. To derive accurate travel cost of path, the dependencies must be considered. Moreover, estimating the travel cost for a given path is not enough to answer a path query, so it is challenging to estimate the travel cost of paths while routing. In this paper, we aim at finding the popular route with minimal travel cost from source to destination and estimating the travel cost for this route. To deal with the above challenges, we devise a system to achieve this goal. Firstly, we construct a popular traverse graph based on the historical trajectories, where each node is a popular location, and each edge is a popular route between two locations. Subsequently, for each popular route in this graph, we use the minimum description length principle to model the travel cost for each popular route at different time intervals, so that each time interval has a stable travel cost. Finally, based on the graph, given a source-destination pair and a leaving time, for accurate travel cost estimation, we find the fastest popular route in consideration of the optimal route concatenation which considers the dependencies between road segments. The contributions of this paper are summarized below. We propose a novel structure, called popular traverse graph, from trajectories without road network information, which contains the popular routes between popular locations. We present a self-adaptive method using the minimum description length principle to model the travel cost on each popular route in the graph. We devise an efficient routing algorithm which combines optimal route concatenation with route planning on the popular traverse graph. We conduct extensive experiments upon a real dataset of millions of trajectories generated by more than ten thousand taxis over a month in Beijing. The results show that our method is both effective and efficient. Moreover, we implement and visualize our system through a mobile app. The remainder of the paper is organized as follows. Section 2 reviews the related work. Section 3 describes some preliminary knowledge. Section 4 describes the overview of our framework. Section 5 introduces the popular traverse graph, and the method to model the travel cost for each popular route. Section 6 details the routing algorithm. Section 7 reports the evaluation and a brief conclusion is given in Section 8. Compared with our earlier proceeding paper, this extended paper mainly claims following contributions. First, we define our problem more formally and devise and implement a cloud-mobile based popular route planning system for solving it, where the mobile app is designed. The architecture of our system is introduced in Section 4, and the detailed interactions of the mobile app is presented in Section 7.4. Second, in Section 6, we improve the efficiency of our routing algorithm by more efficient optimal concatenation computation, which enables faster response to user query. In addition, we discuss the time complexity of our routing algorithm. Third, we conduct more comprehensive experiments to validate our system. For example, we evaluate the popular traverse graph with different parameter settings and test the performance of our method on different popular traverse graph. In addition, we evaluate the efficiency of our improved routing algorithm compared to the method in our previous work. Moreover, a case study on the mobile app is illustrated.", "section": "Methodology", "doi": "10.1007/s11704-018-7249-z", "references": [1519770485, 1567097384, 1673310716, 1936915774, 1969483458, 1981398125, 2005854848, 2008003976, 2023279748, 2031674781, 2075364600, 2084224084, 2097268493, 2111160151, 2124484773, 2141596757, 2144475703, 2149742236, 2155044259, 2162203086, 2169528473, 2172041433, 2268120789, 2477483659, 2556302735, 2561969513, 2573698050, 2579398585, 2583466634, 2744444739, 2750332094, 2770729327, 2788086856, 2795273206]}
{"paragraph": "Many location-based tasks are rather hard to answer by computer algorithms, e.g., taking photos in scenic spots and checking the traffic in a specific location. Recently spatial crowdsourcing is proposed to address this problem by harnessing the crowd to process these tasks, which crowdsources location-based tasks to the crowd and asks the crowd to answer the tasks. In spatial crowdsourcing, there are three entities. The requester has a location-based problem and posts some location-based tasks to the spatial crowdsourcing platform or server. The worker browses the tasks on the spatial crowdsourcing platform and answers her/his tasks of interest and gets monetary awards by answering the tasks from the requester. The server assigns each task to appropriate workers, e.g., the workers who are close to the task based on Euclidean distance. Existing techniques require the server to know the exact locations of workers and tasks to achieve effective task assignment, which may incur the following privacy issues. Location Privacy of Both Workers and Tasks. Firstly, the server can obtain the locations of workers and tasks, as it utilizes the locations to assign tasks to workers. Secondly, the requester can deduce the locations of workers who answer her/his tasks, because only nearby workers can answer the tasks. Thirdly, all workers can browse the tasks and get the locations of tasks. Task Privacy. The tasks are public on the server. Both the server and workers can get the task information, which will result in privacy leakage on task content. Several approaches have been developed to address the location privacy issues. To applied differential privacy to protect the worker location privacy. However, they require an online trusted-third-party TTP to assign the tasks, which results in unnecessary task-assignment delays. When workers want to update their locations, the online TTP needs to publish new statistical location data, which incurs high communication overheads. They try to support dynamic update of worker locations, but the update is still constrained by limited number of publishing timestamps because of the privacy budgets. Wong proposed a distance recoverable encryption scheme that can compute the distance of the encrypted data; however, it needs to compare all pairs of tasks and workers, thus the computation overhead is prohibitively expensive. Thus it calls for new privacy-preserving techniques in spatial crowdsourcing. In this paper, we study the privacy protection problem in spatial crowdsourcing. Our goal is to protect the following information in spatial crowdsourcing. The locations of workers are protected against the server and the requester. Workers’ locations need to be protected, because workers need to upload their real-time locations to get tasks from the server and they will suffer from real-time location leakage if the server sells the data to malicious attackers. The locations and content of tasks are protected against the server and workers except the workers that the tasks are assigned to, but these workers do not know the information of other tasks. The privacy of task location and task content needs to be protected for two reasons. Firstly, if a task is assigned to some workers, the workers’ locations can be deduced, because only nearby workers can obtain the tasks. It is necessary to keep task location privacy to protect worker location privacy. Secondly, a task may be in a sensitive place, e.g., hospital or nightclub. The requester does not want the server and workers to know the location and content of tasks. We aim to build a privacy-preserving system with three salient features. It can be easily integrated into spatial crowdsourcing platforms. It does not require an online TTP, which means that it does not require any TTP involved in crowdsourcing protocols, including location submission, request submission, and request matching and distribution. It can easily support dynamic workers and requesters. There are several challenges in designing such a system. How to protect the locations of workers and tasks while capturing the distance between workers and tasks? How to efficiently assign tasks to workers and enable instant task assignment on protected data? How to protect the content of tasks such that only assigned workers can access the information? To address these challenges, we propose an effective location privacy protection framework. We first split the whole geo-space into grids, map the locations of workers and tasks to grids, and encrypt the grids as codes. The worker only needs to submit the code of her/his locations to the server and the requester submits a set of codes for each task. Then the server utilizes the code to compute the distance between tasks and workers and efficiently assigns tasks to the close workers with theoretical guarantee. We leverage both attribute-based encryption and symmetric-key encryption to encrypt the tasks such that only the workers in specific locations can decrypt the tasks and only the requester who publishes the tasks can obtain the answers from the workers. To summarize, we make the following contributions. We propose a privacy-preserving framework for protecting location privacy of workers and tasks and the content privacy of tasks in spatial crowdsourcing without online TTPs. It does not require any TTP involved in the crowdsourcing protocols, including location submission, request submission, and request matching and distribution. We devise a grid-based location protection method, which can protect the locations of workers and tasks while keeping the distance-aware information on the encrypted locations such that we can quantify the distance between tasks and workers with an error bound. We formalize the task assignment problem as an optimization problem with a constraint on an error bound and propose an efficient approximate algorithm to solve the problem. The proposed approximate algorithm efficiently assigns a large number of tasks, which can instantly assigns tasks to nearby workers on protected data. We develop a hybrid encryption algorithm to encrypt the tasks, which ensures encrypted connections between requesters and workers and allows workers to decrypt the tasks, while not requiring them retrieving decryption keys. It leverages attribute-based encryption and symmetric-key encryption algorithms to build secure channels and ensure that the task is delivered securely and accurately by untrusted servers. Particularly, our algorithm ensures authorized workers can always have valid attributes to match desirable attributes specified by requesters. We analyze the security properties of our method that can defend against different attacks constructed by servers. We have conducted real experiments on real datasets. Experimental results on real datasets show that our method outperforms existing approaches.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2913232", "references": [1503499126, 1513551553, 1532761024, 1569083856, 1946208861, 1966517804, 1973141741, 1976613415, 1985243104, 1992322278, 1993141132, 2001347093, 2003541063, 2022758663, 2031535482, 2036698797, 2041480327, 2047967489, 2049049427, 2059618043, 2063695313, 2074356711, 2077524676, 2124038145, 2129157759, 2138001464, 2146697949, 2148797863, 2151254335, 2157861192, 2158112071, 2164947865, 2169046918, 2196824873, 2210284195, 2263154529, 2270455647, 2401447850, 2467748160, 2487561892, 2516624821, 2517028219, 2580334333, 2791990314, 2793546227, 2950674756]}
{"paragraph": "Trices are structures with three binary operations that are a generalization of lattices in triangular situations. Trices were firstly defined and developed in subsequent works. In this paper, we investigate their role as the membership values structure for fuzzy sets. Historically, lattice-valued fuzzy sets following the usage of the unit interval were introduced by Goguen. Then there were generalizations, mostly by using lattices with additional operations as a co-domain, the most important is a residuated lattice. Later, there were other generalizations, like semilattice and bi-semilattice valued fuzzy sets. Trice-valued fuzzy sets were firstly introduced as an attempt to generalize intuitionistic fuzzy sets. An additional reason for introducing trice-valued fuzzy sets, connected to applications, appears in dealing with complex orders or movements of objects in the space. Namely, if an object in the two or three dimensions is influenced by several forces, then its movements could not be modeled by a partial order, hence neither by a lattice. For such situations, multi-semilattices are suitable algebraic structures. Moreover, even residuated lattices, which are known as the main membership values structure for fuzzy sets, are a special case of multi-adjoint lattices, in which the underlying poset has a complete lattice structure. A particular case of these structures, suitable for applications in two or three-dimensional spaces are trices, mentioned above. Still, the main motivation for our research is to provide a suitable mathematical model for the theory of three-way decisions. This approach to decision making has been developed recently in the rough set theory by Yao. The main reason was to deal scientifically, using rough sets, with problem solving and information processing. The focus was on dividing a whole into three main parts and then developing strategies to act on each of these parts. A framework for investigating three-way decisions may further be a concept analysis, granular or cognitive computing. For more approaches to three-way decisions mostly in the framework of rough sets, Liang et al. provided relevant studies. Investigating three-way decisions in an axiomatic framework, Hu introduced decision measurement, decision condition and decision evaluation function and established three-way decision spaces. He also investigated several approaches to three-way decisions based on fuzzy sets, interval-valued fuzzy sets, interval sets, shadowed sets, rough sets, and introduced three-way decision spaces based on posets. A particular semi-three-way spaces approach was presented also by Hu. In the mentioned papers there is an extensive literature concerning several aspects of three-way and other multi-criteria decisions. Within the fuzzy set theory multicriteria decision making has also been investigated, see the extensive presentation by Abdullah and others including Hong and Choi. Finally, a contribution to three-way decisions are shadowed sets, introduced by Pedrycz and applied to decision procedures. Thresholds appearing in applications of shadowed sets have recently been investigated by Zhang and Yao. In one hand, this paper should establish a theoretical background for trice-valued fuzzy sets, and in the other it should provide arguments for applications in multi criteria decisions, in particular in three-way decisions, using also ideas of shadowed sets. Concerning the theoretical aspect, we investigate trice-valued fuzzy sets from the point of view of cut sets. These fuzzy sets possess three families of cuts, connected by particular equivalence relations. We determine set-theoretic properties of cuts, proving that they form a centralized and also a semi-closure system. At the end of this part, we present theorems of synthesis of trice-valued fuzzy sets by families of subsets which should be their cuts. In Section 5, we explain an idea of using trice-valued fuzzy sets in three-way decisions, providing also an appropriate example. Our approach to three-way decisions has two levels. The first level is based on the fact that in many decisions there are three criteria A, B, and C, where C is some middle option between A or B, or it is the only remaining solution etc., not necessarily excluding each other. The items attributes, properties, characteristics upon which decisions are made are evaluated under an order for example from good to bad with respect to each of these three criteria. Thus we get three not necessarily linear, neither generally independent orderings, and an ordering structure dealing with such a situation is a trice, presented in this paper. In this way constructed trice T is the membership values structure for a fuzzy set on the collection U of individuals or objects to which the decisions are related. In the second level we use the known procedure of three-way decisions by fuzzy sets, applied on each of the three orders in a trice. Then we combine this approach with the techniques of shadowed sets.", "section": "Introduction", "doi": "10.1016/j.ins.2018.09.007", "references": [95749022, 101345952, 1511669739, 1660981353, 1883715000, 1969921454, 1979092190, 1998965536, 2048472139, 2064232703, 2114832876, 2140146517, 2297889545, 2475810011, 2519715111, 2560804083, 2588487610, 2620114837, 2884615103]}
{"paragraph": "An aggregation operator of dimension n is usually defined as a real function such that for n items yields an aggregation value also in the same set, fulfilling the following conditions: if all inputs are equal, then the output equals the input. Since the number of items to be aggregated is quite often unknown or not predefined, this definition was extended by considering families of those aggregation operators allowing the aggregation no matter the number of items to be aggregated. A family of aggregation operators, nowadays more frequently known as aggregation function, provides aggregation operators for each possible dimension of data. But since this paper represents an improvement of a previous work, we keep the denomination FAO to stress the need of assuring that aggregation operators within an aggregation function should be a family, by imposing appropriate consistency conditions like stability for example. A FAO was defined as a set of aggregation operators, each being in charge of the aggregation when the number of items is precisely n. In this way, a FAO is able to aggregate any collection of items, no matter its length n. FAOs have been also called Extended Aggregation Functions by other authors. Many properties related to aggregation operators have been studied, but few authors have devoted their efforts to study the relationships between those operators in order to assure their consistency within the same FAO. As noted in previous works, most of the aggregation operators properties usually imposed to a FAO are properties that apply to each isolated operator, without implying any kind of consistency that links the family of operators defining each particular FAO. In other words, no relationship among the aggregation operators in a FAO is being assured. In this context, authors studied how to assure consistency when the input cardinality changes from n data to m data. Some proposals for consistency of aggregation operators within the same FAO were defined, so a robust process of aggregation under cardinality changes can be guaranteed. Changes in cardinality of data can be produced due to missing data or unexpected data, but also simply because we are receiving data without having a priori fixed the size of the experiment, for example. The main proposal for FAO consistency in previous work was Strict Stability. In this paper, the stability of some of the most commonly used FAO was studied, including minimum, maximum, median, arithmetic mean, geometric mean, harmonic mean, OWA, weighted mean and product. The structure of the input was also considered, widening the scope of the notion of stability for cases of unstructured data, lineally structured and hierarchically structured data. In addition, for each different kind of input data a new definition of stability was introduced, allowing several levels of stability, from strict stability to asymptotically strict stability, almost sure strict stability and, finally, instability. Stability of a FAO is based on the concept of continuity, as small input changes should imply small output changes. In particular, for a collection of n items, when a new item is added, such that it is close to the aggregation of the previous n items, then the new aggregation should be close to the previous aggregation. Symmetry in a FAO seems a natural assumption in some contexts, whenever the position of the data is not relevant. Notice that from this point of view, the notion of self-identity can be regarded as a particular case of the strict stability property within a symmetric FAO. Strict stability or self identity property under symmetry produces a natural kind of robustness within a FAO, since the aggregation operators of a strict stable FAO are forced to hold some transversal continuity between aggregation operators of different cardinality, assuring a specific consistency no matter if the cardinality of the aggregation process changes. This is why such stability has been considered in different contexts, as weights determination for weighted average mean, missing data problems or the development of contextual indexes, among others. The problem of assuring some kind of consistency for the operators within a FAO is in our opinion a key issue in aggregation. If no transversal consistency condition is imposed, the whole aggregation process can be perceived as arbitrary, and the subsequent analysis and decision might be considered not reliable. Nevertheless, the previous definition of strict stability can be viewed as a local property since the relations between the different members of the family are only imposed in a local way. Such stability property is satisfied for a family of aggregation operators when the aggregation of n+1 values coincides with the aggregation of the first n values, if the last one coincides with such aggregation. But what should be the relation between different dimensions in other cases? Strict stability was justified as a sufficient condition to guarantee certain robustness in the aggregation process, since most of the aggregation operators are continuous, idempotent or symmetric. But it is not clear that such a property will be enough for other families of aggregation operators that quite often appear in real applications. In order to extend the notion of stability, we will use penalty functions to measure the degree to which the condition that the new item is close to the aggregation of previous items holds, in a similar way as some families of aggregation operators were defined. Based on these penalty functions we present a global idea of stability describing the situations in which both concepts are equivalent. We will also show that in some situations a strictly stable FAO might present bad performance, and that some not strictly stable FAO might be intuitively considered as stable. This paper is organized as follows: The next section is dedicated to remind the classical notions of stability, self identity and penalty functions. In Section 3 we present some problems such a definition of stability presents. Section 4 goes deeper on the concept of general stability based on penalty functions, the relation between both concepts is analyzed and some relevant results are presented. We will conclude the paper with some final remarks.", "section": "Methodology", "doi": "10.1016/j.fss.2019.01.004", "references": [33843073, 144479953, 162720555, 174527817, 1966665286, 1970650251, 1972136707, 1983091208, 1983659965, 1985946666, 1995107680, 2008553971, 2070051880, 2073829155, 2090808518, 2226225018, 2523416820, 2588324678, 2734394287]}
{"paragraph": "It is undisputed that the fuzzy sets introduced by Zadeh play an important role in modeling imprecision. Fuzzy set theory has been analyzed and applied all over the world. The concept of a fuzzy number was introduced as a convex fuzzy subset of the real line; as a consequence, the arithmetic on fuzzy numbers called fuzzy arithmetic was formulated. A fuzzy set models vagueness by a membership function on X. The value is the degree of membership of an element to the concept. The fuzzy set is characterized by core and support, which are the set of elements with a degree of membership equal to 1 and the set of elements with a degree of membership greater than 0, respectively. Therefore, a fuzzy set can be approximated by a set with a three-valued degree of membership: full, greater than 0, and lacking membership grade. One of the approximations of the fuzzy set is a shadowed set. The shadowed set is a three-valued set modeling vagueness. The three degrees of membership of a shadowed set are: full membership, full exclusion, or an interval that is called a shadow. A shadowed set on X is a set-valued mapping. The approximation of a fuzzy set by a shadowed set is an elevation of the degree of membership at or above a determined threshold to full membership, a reduction of the degree of membership at or below a second threshold to full exclusion, and an assignment of the degree of membership between these thresholds to the interval. A method for the determination of the threshold values is based on optimization of an objective function. Another approach obtains the fuzziness of a fuzzy set by using a gradual number to calculate the considered threshold values. Some compute the thresholds for optimizing a decision cost. Others propose three-way approximations of a fuzzy set by three principles. There is also an approximation of a fuzzy number into the shadowed set by means of direct formulas for four points that characterize the shadowed set. Some use the shadowed set to simplify a fuzzy number. Dynamic fuzzy sets can be approximated by shadowed sets, and an algebraic approach to the shadowed set has also been proposed. Shadowed set theory has been used to optimize the partition thresholds for clusters in multigranulation rough-fuzzy clustering. The shadowed set that is obtained from the fuzzy number is called a shadowed number. It has been stated that Pedrycz’s shadowed set can be viewed as a specific case of three-way decisions. In the theory of three-way decisions, the universe is divided into three regions. In practice, the models of three-way decisions can be generalized by shadowed sets. So, in making multi-criteria and multi-objective decisions, a utility model as an overall evaluation function has to be constructed. Arithmetic operations on shadowed numbers are needed for more complicated utility models of three-way decisions. There are recent papers with multi-attribute or multi-criteria problems where calculations on uncertain variables are made. Shadowed numbers can be used for hybrid multi-attribute decision making and arithmetic operations can be defined for a limited set of shadowed numbers. Arithmetic operations on shadowed numbers can be applied in granular computing. Granules of information can be described with the use of shadowed numbers as they are with other sets such as intervals, fuzzy sets, or rough sets. There are many types of arithmetic for uncertain variables; a few examples of interval arithmetic are reviewed. In standard interval arithmetic, the calculations are made on the border values of the intervals, and results are in the form of intervals. This is similar in extended interval arithmetic, but here the operations on the border values of an interval are dependent on the functions that characterize the interval. These are the width function, the quotient of the endpoints of the interval, and a function which indicates whether an interval is positive or negative. In distributive interval arithmetic, in order for the distributive law to hold, the notation for an interval is modified. To ensure distributivity, the absolute value of an interval described by its center and radius is defined. The basic algebraic operations are defined on intervals with these indicators. In constrained or instantiation interval arithmetic, an interval is described by a non-decreasing linear function whose domain is the interval; the results of operations on intervals are also intervals. In affine interval arithmetic, the value of an interval is expressed by a polynomial called an affine expression. The polynomial consists of a central value, noise symbols, and floating-point coefficients. The multidimensional relative distance measure interval arithmetic describes an interval as a set of values with a variable in a given range. A direct result of operations in this method is a multidimensional granule of information which can be represented with such indicators as: a span, a distribution of cardinality measure, or the center of gravity. The lack of an arithmetic that defines basic operations on any shadowed numbers and can be used in both the theory of three-way decisions and in granular computing motivates the authors to propose a shadowed arithmetic. There is a need to make more precise calculations with shadowed numbers as with any other form of uncertain numbers. The aim of this study is to define a shadowed number and its shadowed arithmetic. The two kinds of shadowed arithmetic proposed in this paper are called standard shadowed arithmetic and multidimensional relative distance measure shadowed arithmetic; these are based on standard interval arithmetic and multidimensional relative distance measure interval arithmetic, respectively. These represent two different approaches to the obtained results. One results in a shadowed number, whereas the direct result obtained with the multidimensional method is a granule of information about the solution. A granule of information calculated with the multidimensional method has the form of a multidimensional formula from which a shadowed number can be obtained. These arithmetic operations on shadowed numbers can be used in the theory of three-way decisions and in granular computing. The multidimensional approach to the concept of uncertainty calculation results is free from the phenomenon of strongly increased entropy of solutions that is typical for the standard arithmetic of uncertainty. The innovation offered by this paper includes two concepts of arithmetic: standard shadowed arithmetic and multidimensional relative distance measure shadowed arithmetic, and the associated definitions of the shadowed number. This text also presents and proves properties of the basic operations in both methods, and the usage of both methods for solving algebraic problems. The rest of the paper is organized as follows: in the next section, the theoretical foundations of shadowed numbers and the two shadowed arithmetics are presented. Then, properties of the basic operations for both methods on shadowed numbers are discussed. Next, examples that show the differences between the two shadowed arithmetics are presented. Finally, some conclusions and directions for further research on the approaches introduced here are presented.", "section": "Introduction", "doi": "10.1016/j.ins.2018.11.047", "references": [101345952, 1461637024, 1964228588, 1979464351, 1985864380, 1998965536, 2006136802, 2006873874, 2036102558, 2038384345, 2040512820, 2052608046, 2052722125, 2058445758, 2070813883, 2078757499, 2089923511, 2092092154, 2114832876, 2131798279, 2153676086, 2297889545, 2413485267, 2488327324, 2591008318, 2620114837, 2756505297, 2806445482, 2912565176]}
{"paragraph": "Leader–follower multiagent systems that coordinate and cooperate over an information exchange network have been increasingly applied in science and engineering. Typical applications of leader–follower systems include distributed coordination in robotic networks, formation and propagation of opinions in social networks, and analysis of biochemical reaction in biological networks. In such applications, agents are classified as either leaders or followers, where leaders are a small subset of the agents tasked to direct the overall network behavior, while the remaining agents, i.e., followers, are influenced by the leaders via the underlying connectivity of the network to perform desired tasks. The success of these applications relies on the capability of driving the network to a desired state by external controls via selected leaders, i.e., network controllability. Consequently, network controllability must be ensured in network design to enable leader–follower control. However, network controllability is deeply coupled with agent dynamics and the underlying network topology whose interactions are still largely unexplored. Based on the interactions among agents, multiagent networks can be classified as either cooperative or competitive networks. Cooperative networks are commonly modeled as unsigned graphs containing only positive edge weights, where positive weights indicate cooperative relationships between agents. Average consensus is a typical example of cooperative networks, where agents positively value information collected from neighboring agents and achieve group consensus via collaboration. If a graph allows to admit negative edge weights, it is called signed graph. Signed graphs are widely used to represent networks with antagonistic interactions. For instance, a positive or negative weight in signed graphs can be used to model friend or adversary relationship in social networks and collaborative or competitive relationship in multiagent systems. Controllability on cooperative networks has been extensively studied in the literature. Leader–follower controllability was considered for the first time, where the network controllability was characterized based on the spectral analysis of the system matrix. Graph theoretic approaches were then explored to provide characterizations of network controllability. For instance, it was established that symmetry with respect to a single leader can potentially lead to uncontrollability. Graphical and topological characterizations of network controllability were investigated. Graph-distance-based lower bounds on the rank of the controllability matrix were developed. Sufficient and necessary conditions for network controllability were developed for tree topologies, grid graphs, and path and cycle graphs. Other than graphical characterizations of network controllability, structural properties of cooperative networks were also exploited from matrix-theoretical perspectives to reveal the connections between network controllability and underlying graphs. Other representative works that investigate network controllability include additional results. Besides characterizing network controllability, various methods, e.g., combinatorial and heuristic selection methods, were developed to select leaders to ensure controllability of given networks. Leader selection in complex networks was investigated, where, in addition to ensure network controllability, control energy was also taken into account when selecting leaders. Leaders were selected with either minimum number or minimum energy cost to ensure the controllability of dynamic networks. Other representative works regarding leader selection for network controllability were presented. However, all of the aforementioned results focus on the characterization of network controllability and leader selection on cooperative networks, without considering networks with potential antagonistic interactions. In addition, due to the existence of negative weights in signed networks, most existing analysis tools and leader selection approaches are no longer applicable to signed networks. Recent emergence of the control and analysis of signed graphs with applications in social networks, brain networks, and complex networks motivates the research on controllability problems, where such networks often need to be driven to desirable states by external inputs via selected control nodes within the network. For instance, the controllability of signed graphs were partially studied via structural balance in several works. As a variant of controllability, herdability over signed and directed graphs was considered, which investigated the reachability of a specific set, rather than the whole state space as in controllability. A submodular optimization-based leader selection approach was developed to ensure leader–follower consensus in signed networks. However, network design in terms of leader group selection to ensure the controllability of the signed network remains largely unattended in the literature. Therefore, this paper is particularly motivated to study the leader group selection to ensure controllability of signed networks. In this paper, leader–follower controllability on signed networks is investigated. Specifically, we consider signed noncooperative networks, which admit both positive and negative edges. The signed network is capable of representing a variety of practical network applications, such as social network, fault tolerant networks, and secure networks, where the network may have both friendly and adversarial interactions. Motivated by the broad range of potential applications, it is of particular interest in this paper to identify a small subset of nodes in the signed network, such that the selected nodes are able to drive the network to a desired behavior, even in the presence of antagonistic interactions. In other words, this paper focuses on leader selection to ensure the controllability of signed networks. In particular, based on the classic controllability notations, graph-inspired topological characterizations of the leader–follower controllability of signed networks are first developed. Such characterizations investigate the interaction between the underlying network topology and agent dynamics and pave the way for leader selection on signed networks. As the signed path and cycle graphs are basic building blocks for a variety of networks, the revealed topological characterizations are then exploited to develop leader selection methods for signed path and cycle graphs, where topological properties are explored to extend existing controllability analysis from unsigned to signed networks. Along with illustrative examples, heuristic algorithms are developed showing how leader selection methods developed for path and cycle graphs can be potentially extended for more general signed networks. The contributions of this paper are multifold. First, controllability ensured leader group selection on signed networks is considered. Despite extensive study of controllability of unsigned networks, relatively few research effort focuses on signed networks. To the best of our knowledge, this paper is one of the first attempts to consider leader group selection on signed networks from graphical perspective. Specifically, we develop leader selection rules for signed path and cycle networks, which provides constructive approaches to select leaders for network controllability. Since most networks can be considered as a combination of path and cycle networks, the developed leader selection rules on path and cycle graphs can be potentially extended to more complex and sophisticated networks. Second, the developed leader group selection approaches are generic, in the sense that they not only hold for signed graphs but also for unsigned graphs, since unsigned graphs are a particular case of signed graphs that only consider positive edges. For instance, the leader selection approaches developed in this paper are consistent with the results developed for unsigned graphs, while the leader selection approaches developed for unsigned graphs are not generally applicable to signed graphs. Third, in contrast to most existing matrix-theoretical approaches to characterize network controllability, graph-inspired understandings of network controllability are realized in this paper. Specifically, we investigate the relationship between the network controllability and the underlying topology and characterize how leader-to-leader and leader-to-follower connections affect the controllability of a signed network with Laplacian dynamics. Such graphical characterizations of network controllability are able to provide more intuitive and effective means in selecting leaders for network controllability. As a result, the leader group selection methods developed for signed path and cycle graphs in this paper can be conveniently employed without requiring any matrix-theoretical analysis.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868470", "references": [1963646571, 1963649370, 1966437309, 1967509640, 1977594883, 2003949298, 2014706340, 2020809274, 2021807663, 2043052736, 2055340995, 2056735670, 2060176024, 2077076780, 2080131446, 2081632029, 2082273344, 2088760612, 2100145912, 2111409074, 2111725629, 2120791740, 2128928412, 2140623975, 2159924869, 2160643434, 2167516879, 2168109446, 2231268124, 2257788506, 2315383458, 2404649140, 2472069939, 2511127296, 2518887685, 2587622940, 2608101173, 2734772652, 2789372517, 2796759283, 2801653436, 2962951410, 2963398368]}
{"paragraph": "Current cloud computing mainly exploits hypervisor-based virtualization, in which the cloud provider offers virtual machine VM images with different size of resources such as CPU, memory, and network bandwidth to its clients. Each VM runs a guest operating system which may or may not have the same as host operating system on the physical machine PM, and the translation between the guest operating system and host operating system is provided by a hypervisor. As the scale of modern cloud datacenters gradually increase, more and more complex applications are processed by hundreds of thousands of storage and computing devices. Further, there exists an intense competition among cloud providers in all cloud service models. Meanwhile, it is generally known that service level agreement SLA is an explicit statement of expectations and obligations between cloud providers and cloud clients. Therefore, SLA and energy efficiency are greatly paid attention by the cloud providers. Especially, when there exists SLA violations, it not only results in penalties applied to the cloud providers, but also loss of potential clients on account of the risk of the news of bad user experience spread through social media. For example, since the availability of Rackspace’s services is less than 99% in a month, it refunds 30% of a consumer bill; and in 2013, U.S. cloud datacenters guzzled an estimated 91 billion kilowatt-hours of electricity, The electricity consumption is expected to touch 140 billion kilowatt-hours by 2020. Therefore, reducing SLA violation rates while increasing energy efficiency and utilization of cloud resources for clients requires effective management of resource provisioning. In addition to the traditional cloud services including software as a service SaaS, platform as a service PaaS, and infrastructure as a service IaaS, a new type of cloud service called containers as a service CaaS has been introduced by Amazon Web Services and Google. When IaaS and PaaS provide respectively virtualized computing resources and application specific runtime services, CaaS is the missing layer between IaaS and PaaS, and glues these two layers together. As we all known that containers are exploited to deploy and manage applications by developers. Compared to hypervisor-based virtualization, the container-based virtualization is one recent trend in cloud computing, it is moving away from the VM-based cloud and has several advantages, such as low resource consumption, portability, and lightweight. Thus, containers can be considered to be a new revolution in the cloud era. Docker is a good example of a container management system. Nowadays, with the emergence of Docker, containers are increasingly gaining in popularity and adopted by cloud providers such as Amazon, Azure, Google, and Digital Ocean. Although a Docker container is like a VM, it is much more efficient than VM. This is because that it eliminates the guest operating system and the hypervisor, while keeping the containers portable across Linux operating system kernels. Therefore, Docker image size is typically in the MB range, while VM images are typically in the GB range. Recently, Russell et al. introduced that reboot and boot time of Docker container are only 2% and 60% of a corresponding KVM-based VMs, respectively. Moreover, comparing to the corresponding VM, it consumes only 16% of memory and 6% of CPU. Besides, workload tests show that processing capacity on PMs has almost the same as running inside Docker containers. Moreover, the significant improvement of the container-based virtualization over the hypervisor-based virtualization is not only in performance, but also in deployment density. Typically, due to the lightweight nature, the deployment density of containers leads to 6–12 times as great as deployment density of VMs. In the case of high deployment density, fewer PMs are allocated to support the same amount of clients, this means that the cloud provider pays less capital such as purchase and operation costs such as energy. In view of these reasons, many open source communities and companies, such as OpenStack, Microsoft, IBM, Google, and VMware, are stepping up investing in container-based virtualization technologies. It is also worth noting that VMs and Docker containers are truly better together. That is, VM-Docker configurations clearly achieve close to, even slightly better than native performance. Therefore, CaaS services are usually arranged at the top of VMs. Cloud providers argue that VMs offer another layer of security for untrusted workloads. Meanwhile, containers might provide appropriate environment for semi-trusted workloads. Although CaaS is increasingly gaining popularity and will become one of the major cloud resource models, no scheme has been proposed to deal with container consolidation problem with usage prediction for improving the SLA and energy efficiency in CaaS datacenters. Therefore, in this context, we present a container consolidation scheme with usage prediction called CNCUP to reduce the power consumption while complying with the SLA. Since the load level of a PM is mainly decided by one key factor CPU, CNCUP only considers CPU resource utilization. However, it can be extended to cover other resource dimensions, such as memory, disk, and network bandwidth. The key contributions of this paper are described as follows. We formulate a proactive container consolidation problem based on current and predicted CPU utilization using local history of the considered PMs as a bin-packing problem with objectives to reduce the power consumption while complying with the SLA of CaaS datacenters. To solve the above problem, we introduce an efficient container consolidation scheme using linear regression based model. Like any consolidation solution, our scheme exploits three stages to tackle the container consolidation problem. Firstly, the results of container consolidation scheme decide whether container migration is triggered. Secondly, some containers are selected as migration objects. Finally, migration destinations PM/VM should be identified for hosting these selected containers. We build a CaaS environment and conduct comprehensive simulation using ContainerCloudSim toolkits on the real workloads. Experimental results show that our scheme can achieve better performance gains. The remainder of this paper is organized as follows. In Section 2, we introduce related work. In Section 3, we present the system architecture and container consolidation problem. In Section 4, we propose the container consolidation scheme with usage prediction. In Section 5, we evaluate performance of our scheme using existing PM selection algorithms. Finally, we conclude this paper in Section 6.", "section": "Related Work", "doi": "10.1007/s11704-018-7172-3", "references": [854771698, 986683926, 1490129032, 1533797123, 1560425030, 1582571683, 1601065559, 1966715578, 1977110635, 1999832827, 2001065285, 2019609318, 2034609836, 2075168211, 2099197225, 2110374615, 2117372312, 2125262761, 2129538983, 2151025124, 2160756722, 2279902707, 2343635147, 2388510220, 2410776937, 2573769111, 2574415905, 2604186514, 2781146421]}
{"paragraph": "The rapid development of information technology promotes the data to be generated at unprecedented rates in recent years. The total amounts of data in the world has increased nine times within five years, and this figure is expected to double at least every two years in the future. The advent of big data era provides great opportunities for enterprises to improve competitive advantages and makes significant impacts on value creation in the process of production, R&D, operational management and service. But the enterprise in big data environment has to suffer more challenges and risks than before due to severe competition, especially for the high-tech enterprise. How to improve high-tech enterprise’s innovation performance and core competence in big data environment has been a key issue and attracts much attention. Most of literatures discussing enterprise innovation generally focused on innovation capability, organizational learning and the use of advanced technology. Corporate governance, however, is also an important factor that influences high-tech enterprise innovation significantly. Corporate governance aims at decreasing manager’s opportunistic behavior, enhancing accuracy and effectiveness of innovation decision-making, and improving firm’s ability to cope with external uncertainties. The company with strong corporate governance typically performs better and gains higher returns especially in a more complex environment. In this article, we concentrate on the impact of the corporate governance on high-tech enterprise innovation in big data environment. Specifically, we explore how the managerial power (internal governance) and enterprise’s network centrality (external governance) affects enterprise innovation, and systematically analyze their influence mechanism. Managerial power is one of the corporate governance structures and makes important influence on firm’s strategic choice and performance. Similar governance structure in high-tech enterprises often leads to different innovation performance, which lies in the difference of managerial power. For example, the team headed by Jack Ma in Alibaba Group has great control over the corporate decision-making and resources configuration, and helps the corporation make great achievements in technologic innovation. In the development of big data environment, it’s necessary to examine the role of managerial power in enterprise innovation. On the other hand, the enterprises close to central network position are more likely to accumulate valuable resources and gain the competitive advantages. The managers with higher managerial power are often inspired to make connections with other companies, which helps to enhance enterprise’s network centrality, and promote the enterprise innovation by acquiring more resources from others. But previous researches paid less attention to the role of corporate networks which formed by interlocking executives in innovation practices. In this research, we mainly address following questions: how does managerial power (internal governance) and network centrality (external governance) influence high-tech enterprise innovation in big data environment; how does network centrality mediating the relationship of managerial power and innovation performance; considering about the regional big data environment, how will above relationships make differences. Overall, this article makes several contributions. First, different from prior researches, we pay attention to the role of corporate governance in the high-tech enterprise under big data environment, which provides a new insight into the research in this field. Second, this paper fills in the extant literatures which focused on either internal-based governance or network-based governance, and combines with both governance perspectives to explore the high-tech enterprise innovation in big data environment for the first time. Third, this paper expands the research of interlocking directors and focuses on the interlocking executives including board of directors, CEO and other TMT members. Because it’s a common phenomenon in Chinese capital market that most of executives hold management positions in at least two companies. Fourth, we examine the differences under distinct regional big data environment, which enriches the research of enterprise innovation on the regional-level. The following research are structured as: the second part reviews the literatures related to the research; the third part theoretically analyzes the influence mechanism among the relationships of managerial power, network centrality and enterprise’s innovation performance, and proposes the hypotheses; the fourth part describes the methodology; the fifth part reports the results of empirical research; the sixth part makes the discussions of key findings and the implications for both research and practice, and the final part makes conclusion and discusses our study’s limitations and future research directions.", "section": "Introduction", "doi": "10.1016/j.ijinfomgt.2018.11.009", "references": [1973532288, 2008420823, 2042822990, 2052709366, 2065198144, 2068301507, 2081234480, 2105239853, 2113680315, 2121318675, 2159128662, 2159708909, 2222723904, 2323667237, 2407961458, 2470527509, 2515574205, 2519706319, 2596910185, 2741053108, 2763315368, 2763473974, 2765656061, 2776789918]}
{"paragraph": "Let be a finite referential set of n elements and let us denote by the subsets of X. The set of subsets of X is denoted by and denotes the cardinality of A. Depending on the context, elements of X have different meanings (criteria in Multicriteria Decision Making, players in Game Theory, and so on). A fuzzy measure over X is a set function satisfying boundary conditions and monotonicity. We will denote by the set of all fuzzy measures over X. The Choquet integral of a function with respect to a fuzzy measure μ on X is defined by where is a permutation of the set satisfying and Fuzzy measures over a finite referential set, together with Choquet integral, constitute a powerful tool appearing in many different fields, such as Decision Making, Game Theory, Evidence Theory, and many others. For example, in Multicriteria Decision Making fuzzy measures are able to model interactions among criteria, as well as situations of veto or favor. For this reason, the study of the different aspects of these measures (or any of their subfamilies) has attracted the attention of many researchers. One of these problems is determining the geometrical properties of the set of all fuzzy measures over a referential set of fixed cardinality; in this sense, it is easy to see that this set is a convex bounded polyhedron, and this also happens to many other subfamilies, such as k-additive measures, k-symmetric measures or belief functions, among others. Related to the geometrical structure of or any subfamily, we have the problem of randomly generating a measure with respect to the uniform distribution. In other words, we want to select a set of fuzzy measures in a family such that no region takes advantage of a larger probability than other region with the same volume. This is an interesting problem for several reasons. First, and most evident, it is an appealing problem from a mathematical point of view. Next, being able to generate points in a polytope allows to approximate several characteristics, as the center of gravity, the volume, and so on; these problems could be solved numerically by using Monte–Carlo methods if we know how to generate uniformly inside these polytopes. We state now a more practical reason, related to the problem of identification of fuzzy measures. This problem arises when we have some sample data (perhaps in a non-numerical scale) and we look for the fuzzy measure that best fits these data. A method to deal with the problem of identification of some convex families of fuzzy measures based on genetic algorithms has been proposed; in that paper, the cross-over operator considered is the convex combination of points, taking advantage of the fact that can be seen as a polytope. This algorithm is very fast and the simulations carried out suggest that it is stable with respect to the presence of noise. However, the convex combination reduces the search region in each iteration; although a desirable property at first sight, it has the handicap that if the solution is left outside the search region in an iteration, it is not possible to include it in the search region again. To cope with this problem, there are several options; the most natural is to consider the set of vertices as the initial population of the algorithm and use as mutation operator the convex combination of the measure with one of the vertices; unfortunately, for the general case of fuzzy measures and also for many subfamilies, the number of vertices increases too fast. Thus, this solution is most of times unfeasible from a computational point of view. Then, the most practical way of proceeding is to consider as initial population a set of fuzzy measures selected randomly with respect to the uniform distribution on the set of all fuzzy measures. As stated before, for general fuzzy measures and many subfamilies, the problem of random generation consists in generating points randomly in a polytope. However, this is a complex problem that has not been satisfactorily solved for general polytopes; indeed, there are many different techniques to deal with this problem, as the grid method, the sweep-plane method and triangulation methods; in the case of fuzzy measures, it will become apparent below that the triangulation method is the most suitable one. This is because and many subfamilies are related to a partially ordered sets and, as we will explain below, the problem reduces to derive a procedure for selecting a linear extension of such posets. On the other hand, Brightwell and Winkler have shown that, for general posets, this problem, as well as the problem of counting the number of linear extensions, are ♯ P-complete problems, and consequently, it is not possible to derive a satisfactory procedure for solving these problems for general posets. Then, this problem has been treated by many researchers, aiming to derive either techniques for a particular class of posets, or to propose algorithms that are close to randomness and whose complexity is reduced. Indeed, this problem is a hot topic in Computer Sciences nowadays. In this paper we will deal with this problem for the subfamily of 2-symmetric measures. For this, we will apply some results coming from Combinatorics. This is indeed the novelty of the paper: the use of tools appearing in Combinatorics to solve a problem appearing in a very different domain. The rest of the paper goes as follows: We introduce the basic concepts about k-symmetric measures and order polytopes in next section. In Section 3 we introduce Young diagrams and Ferrers posets, together with the Hook theorem, that constitutes the basis of our algorithm for random generation. Section 4 shows the relation between 2-symmetric measures and these combinatorial objects. In Section 5, we develop the algorithm for random generation of 2-symmetric measures. Section 6 presents an application of these results, related to the problem of identification of general fuzzy measures. We finish with conclusions and open problems.", "section": "Methodology", "doi": "10.1016/j.fss.2019.01.017", "references": [1639032689, 1963855747, 1979423085, 1989817181, 1992657293, 2008450451, 2012928462, 2015981446, 2016000144, 2017449401, 2027636563, 2035145079, 2041563000, 2043075598, 2043605821, 2049637061, 2056239531, 2060907774, 2113984600, 2134824231, 2145498098, 2147632348, 2156032636, 2169850462, 2914659449]}
{"paragraph": "In realistic decision making processes, owing to the innate fuzziness of human thought and the complexity of decision making contexts, experts are notably prone to express their preferences in qualitative situations. To cope with such issues, the fuzzy linguistic approach expresses qualitative information as linguistic variables, and this approach enhances the flexibility and reliability of processing various linguistic expressions. However, considering that the form of linguistic variables is often expressed by single terms, the fuzzy linguistic approach is still insufficient in modeling every individual person’s complicated thinking. For instance, when estimating the programming ability of a job seeker, human resource experts can only express modest aptitude, but they cannot express higher than modest or between modest and competent ones. Therefore, considering that it is beneficial for experts to describe their respective opinions in multiple linguistic terms, the notion of HFLTSs was established by simultaneously overcoming the limitations of fuzzy linguistic approaches and hesitant fuzzy sets. Ever since the establishment of HFLTSs, substantial academic achievements have been realized in terms of solving HFL group decision making problems. In practice, experts are usually confronted with the following two challenges in HFL group decision making situations: each expert needs to explain a decision with respect to the scenario under which alternatives have been selected as the optimal one, i.e., the information analysis issue when experts make group decisions; all experts need to reach an agreement or a conclusion by referring to each decision result, i.e., the information fusion issue in the group decision making procedure. The rough set theory is widely known as a reasonable and efficient soft computing method for handling several decision making situations via attribute selections and rule acquisitions. Moreover, in the past decades, various generalized rough set models have been constructed in step with the actual demands of real-world situations. In this research, for the sake of enhancing the applicability of generalized rough set models in handling the two challenges mentioned above in HFL group decision making, we aim to explore a novel rough set model by means of the multi-granularity three-way decisions paradigm. Multi-granularity three-way decisions, which originate from the granular computing framework, construct multi-level problem solving methods by providing information analysis and information fusion rules for solution spaces in different granularity levels based on the three-way decisions theory. Among the most commonly used multi-granularity three-way decisions models that combine decision-theoretic rough sets with multigranulation rough sets, MG-DTRSs were initially constructed. Since then, many scholars have enriched MG-DTRSs from the viewpoints of multi-granularity three-way decisions. In particular, the motivations of utilizing DTRSs and MGRSs over two universes to address the challenges of information analysis and information fusion in group decision making can be summarized as follows. For solving information analysis issues in group decision making, three-way decisions theory has demonstrated its superior performances in each granularity level when constructing multi-level problem solving methods. Three-way decisions can be utilized to interpret three regions in rough sets, i.e., positive, boundary, and negative regions, which are regarded as the regions of acceptance, noncommitment, and rejection in ternary classifications. DTRSs are regarded as one of the most typical representatives of three-way decisions that have been proposed in recent years. Moreover, DTRSs have been proposed much earlier than three-way decisions, and the essence of DTRSs can be illustrated by three-way decisions from a broad perspective. Specifically, DTRSs are established by the ideas of acceptance, noncommitment, and rejection by introducing threshold-based induction rules to permit error tolerance. Thus, DTRSs build a link between rough sets and decision theory, especially for the purpose of providing a reasonable semantic interpretation for decision risks. With the support of DTRSs, by reasonably quantifying and minimizing the decision loss of wrong decisions, a more reliable ternary classification result can be obtained compared with those that use other existing models without focusing on decision risks. For solving information fusion issues in group decision making, MGRSs over two universes are regarded as one of the most reasonable and efficient tools for fusing solution space in different granularity levels. Specifically, the strengths of MGRSs over two universes are reflected in the following two levels. MGRSs regard each decision maker’s preferences as a single information system that can induce its related granular structures, and thus, MGRSs are able to transform the issue of information fusion into multigranulation fusion from multiple views and levels. MGRSs reportedly not only provide an effective scheme to discover knowledge by simultaneously processing multiple binary relations with actual requirements, but they also include optimistic and pessimistic MGRSs that can be utilized according to risk-seeking and risk-averse tactics. The consideration of two universes outperforms a single universe when depicting real-world decision making information systems, and hence, exploring decision making problems by means of rough sets over two universes is essential. Here, we aim to discuss a special correlation group decision making problem. As an important measure in data analysis, the correlation can reflect a complex relationship of two sets with the aid of a measure of interdependency of the two sets. In using granular computing to handle the aforementioned special decision making problems, particularly by developing the model of MGRSs over two universes, decision makers can conveniently and realistically describe useful inherent relationships between the two kinds of objects of these group decision making problems. On the basis of the above discussions, to handle HFL group decision making problems according to the multi-granularity three-way decisions paradigm, this work investigates a hybrid model named adjustable HFL MG-DTRSs over two universes from the viewpoint of theoretical foundations and real-world applications. We simultaneously combine the notion of HFLTSs, DTRSs, and MGRSs over two universes to overcome the shortcomings of existing MG-DTRS models in HFL group decision making. The combination mechanism can be summarized as follows: in enabling the proposed model to quantify and minimize the loss of wrong decisions in the HFL context, we construct a single membership degree by introducing DTRSs into HFL information systems; in enabling the proposed model to acquire adjustable capabilities according to one expert’s risk preferences when handling HFL group decision making problems, we expand the presented single membership degree to the background of multiple granulations by introducing an adjustable parameter. Then, the notion of adjustable membership degrees is established, and both optimistic and pessimistic versions of the proposed model are regarded as special cases of the adjustable version. Hence, by taking advantages of DTRSs and MGRSs over two universes, the proposed model can suitably cope with HFL group decision making situations despite decision risks. Compared with existing HFL group decision making studies, our critical contributions in this paper can be summarized as follows. According to the multi-granularity three-way decisions paradigm, we explore the notion of adjustable HFL MG-DTRSs over two universes, and some of their properties are also discussed. Different from other studies, the viewpoint of granular computing can help us design a novel three-way decision making approach. The concept of two universes is introduced to analyze the correlation of group decision making problems, which then can depict the complex relationship of two sets. The measure of interdependency of the two sets is considered, as it is a significant measure in data analysis. By taking full advantages of DTRSs and MGRSs over two universes, the proposed model can appropriately handle group decision making situations with risks in the HFL context. Considering that comparing and computing linguistic variables are ineffective, some studies have proposed the transformation of qualitative expressions into quantitative ones. Accordingly, we design a scheme to transform hesitant fuzzy linguistic elements into interval numbers. Moreover, by introducing an adjustable parameter for the expected risk preference of experts, the proposed model can help experts to make decisions based on their risk attitudes. We propose a general group decision making rule based on adjustable HFL MG-DTRSs over two universes in the background of person-job fit. The case study illustrates that the proposed group decision making rule can substantially enhance matching accuracies and decrease uncertainties of person-job fit. Moreover, we use a classical validity test to show the effectiveness of the proposed method. To facilitate the discussion, background on the concepts of HFLTSs, DTRSs, and MGRSs over two universes is presented. The proposed notion of adjustable HFL MG-DTRSs over two universes is then discussed. The construction of an HFL group decision making method by means of adjustable HFL MG-DTRSs over two universes is presented. The findings from a case study, a comparative analysis, and a validity test are presented to prove the validity of the developed decision making rule. The paper ends with several concluding comments.", "section": "Related Work", "doi": "10.1016/j.ins.2019.01.033", "references": [101345952, 881110586, 964455177, 1131999718, 1422679120, 1511669739, 1564232874, 1606022329, 1969463949, 1978062397, 1980471025, 2006873874, 2065043952, 2070813883, 2084718108, 2089923511, 2097423137, 2107566963, 2131390763, 2153676086, 2162755671, 2170755382, 2172368975, 2196468142, 2215524969, 2288301540, 2297889545, 2340020088, 2346610881, 2350506101, 2549504134, 2550409005, 2586356223, 2598771954, 2772332129, 2772794611, 2803341335, 2806445482, 2887485161, 2887596728, 2887732625, 2896905879, 2905271694]}
{"paragraph": "Multiobjective optimization problems often involve multiple conflicting objectives to be optimized simultaneously in real-world applications. Due to the conflicting nature of the objectives, MOPs do not exist one single solution that is able to optimize all the objectives. Instead, they aim at finding a series of best possible trade-off solutions termed Pareto optimal solutions, whose objectives cannot be improved further more. The Pareto optimal solutions are known as the Pareto front in objective space and the Pareto set in decision space, respectively. Evolutionary multiobjective optimization algorithms, as a class of population based search heuristics, have been successfully applied in various bi-objective and tri-objective optimization scenarios. Based on various acceptance rules of selecting offspring solutions, EMO algorithms could be generally divided to three branches: Pareto dominance-based approaches, Decomposition-based methods, and Indicator-based algorithms. Pareto dominance-based algorithms rely on the relationship of Pareto dominance among solutions to approach the PF, Decomposition-based methods decompose the original problem into several subproblems and utilize scalarizing function to conduct selections in each subproblem. However, they encounter various kinds of difficulties when the number of objectives increases to more than three, which are known as many-objective optimization problems. In contrast to the two other groups of EMO algorithms, indicator-based algorithms use performance indicators to guide the evolution. Prominent examples of this kind of EMO algorithms are IBEA, SMS-EMOA, MO-CMA-ES, and R2-EMOA. In this paper, we mainly focus on the hypervolume indicator based EMO algorithms. Hypervolume indicator is a volumetric measurement in geometry that calculates the union volume of a dominated region which was produced by a set of points. It was originally proposed as a set quality indicator called size of the space covered to quantitatively compare the populations of different EMO algorithms. This concept was later denoted as hypervolume measurement. Because hypervolume indicator has a significant attribute that keeps fully sensitive and effective to both Pareto dominance and population diversity, it is now one of the most important indicators in many-objective optimization. Hypervolume indicator based EMO algorithms such as HypE, SMS-EMOA, FV-MOEA, HAGA and MOPSOhv have been reported to be very promising in solving MaOPs. For instance, SMS-EMOA is designed to maximize the exact hypervolume of populations by means of discarding the least contribution solution. This idea in essence utilizes the hypervolume indicator to measure the quality of solutions. The superiority of SMS-EMOA has been verified by a plethora of research studies. Unfortunately, it is still unpractical to apply SMS-EMOA to high-dimensional MaOPs due to the costly computational resources of hypervolume indicator calculation. It has been proved that hypervolume calculation is a NP–complete problem. It is expected that no polynomial calculation algorithm exists since this would imply . Therefore, the main problem of this kind of EMO algorithms can be attributed to find a method that calculates the hypervolume indicator efficiently. Without a fast and quality hypervolume calculation algorithm, the hypervolume based EMO algorithms would either have high computational complexity or compromised performance. A lot of methods have been proposed to calculate the hypervolume indicator, and generally, two classes can be seen. The first class is the exact calculation method, which is mainly realized by means of the recursive computation. It divides the hypervolume calculation problem into several subproblems with the same property as the original problem and solves them respectively. This kind of method is featured by high accuracy and exponentially increasing complexity. Some influential algorithms of this kind include hypervolume by slicing objectives algorithm, HOY, Quick hypervolume, HBDA and Walking Fish Group, etc. The second class is to estimate the hypervolume value by some statistical methods. Monte Carlo simulation is a representative method, and its effectiveness in high dimensions has been proved by some performance analyses. The first attempt in this direction is presented previously. Later an efficient FPRAS algorithm for hypervolume calculation was presented. Its complexity is O(dn/ϵ) with an error. Though this method can reduce the complexity of hypervolume computation, the accuracy and running time have to be compromised. The main reason is the huge number of sampling points needed to reach the condition of dependable accuracy, which seriously influences the running time. Therefore, how to get it a further performance boost is still one of the major research problems. In this paper, we propose a new hypervolume approximation method, which consists of two parts: segmentation and approximation. The basic idea of this method is to reduce the number of Monte Carlo sampling points by a new segmentation strategy. This segmentation strategy recursively segments the original geometric problem into a hypercube and several subproblems which are similar to the original problem. After segmentation, a modified Monte Carlo simulation is used to approximate these rest subproblems, which is called in corners part. Due to the quite small size of these in corners part, when applying modified Monte Carlo simulation, the number of sampling points would be much less than the original method. With the reduction of the number of sampling points, the running time is also reduced. This method is a combination of the exact calculation method and the approximation method, and thus, we call it partial precision and partial approximation. To better understand the proposed hypervolume approximation method, theoretical analysis about this method is given in this paper. Meanwhile, a series of experiments are conducted to systematically investigate its efficiency. To be specific, we compare the proposed method with three exact calculation methods and an approximation method on four widely-used test instances. Simulation results show that the proposed algorithm has low running time in high dimensional hypervolume calculation. The proposed algorithm can normally achieve about ten times faster than the FPRAS algorithm which is the most efficient algorithm in more than 10 dimensional space so far. Furthermore, we propose a method to find the solution that contributes the least to hypervolume in a population, which is often called the incremental version. We apply the proposed method into the framework of SMS-EMOA and compare it with NSGA–III, MOEA/D, HypE, IBEA and the incremental version of IWFG and exQHV in the framework of SMS-EMOA. By comparing, the proposed algorithm can achieve an acceptable performance in respect of both runtime and the quality of solutions. In summary, we have the following contributions and innovations in this paper. A fast hypervolume approximation algorithm is proposed that accelerates the hypervolume calculation with a steady accuracy error, through a segmentation strategy and the Monte Carlo simulation. Theoretical studies of the proposed hypervolume approximation method and some others are conducted. Based on the above fast approximation method, an incremental algorithm is proposed for many objective optimization. This paper is organized as follows. Section 2 briefly introduces the basic concepts of multiobjective optimization and hypervolume. Section 3 gives a detailed description about the proposed hypervolume approximation method. Theoretical analysis is presented in Section 4. Simulation experiments are conducted in Section 5. Section 6 concludes this paper. Appendix A proposes an incremental hypervolume algorithm of the proposed method, and conducts comparative experiments with other EMO algorithms.", "section": "Introduction", "doi": "10.1016/j.ins.2019.02.054", "references": [82483302, 138500685, 1558919105, 1564638059, 1588375755, 1657799094, 1817091417, 1979908631, 1990341344, 2000825106, 2009678220, 2022485595, 2038420231, 2045750029, 2050914643, 2058142975, 2059217302, 2059819407, 2067544246, 2069307406, 2074308169, 2090795732, 2090797242, 2095792542, 2108968575, 2110586983, 2117362819, 2126105956, 2126273906, 2127474646, 2143185749, 2143381319, 2147026702, 2151953637, 2160088187, 2187701960, 2295944640, 2312339748, 2344880386, 2558334374, 2564148772, 2609579687, 2889750846, 2962987192]}
{"paragraph": "Interactive simulation involving soft deformable objects still often suffers from stability problems such as impulsive force response or oscillations in the haptic device held by the user. Early studies have focused on the stability problem caused by the hard stiffness of a fixed virtual wall. A representative approach is to use the virtual coupling. The penetration depth is measured from the proxy constrained on the contact surface of the rigid objects. The feedback force proportional to the penetration depth is computed by using the virtual coupling. The advantage of using the proxy-based virtual coupling is that the stability of the simulation is guaranteed by restricting the parameters of the virtual coupling by using the passivity condition or the absolute stability condition. The proxy-based virtual coupling is extended to the simulation involving interaction with deformable objects. The proxy is updated by using the rigid body dynamics because the geometric constraint is difficult to be applied on the surface of deformable objects. Brown and Colgate analyze the minimum mass satisfying the passivity condition according to integration methods. Several research works tried to increase the update rate of the haptic feedback for the transparent haptic renderings. Peterlik et al. proposed a multirate compliant mechanism that updates the proxy and the coupling force at high rates based on the measured position of the haptic device and the compliance matrix computed from the low-rate loop. A local model is proposed to generate a feedback force proportional to the penetration depth into an intermediate model such as a plane and sphere. The maximum stiffness of the local model is restricted by the time step of simulation and the equivalent mass, damping, and stiffness of the contact surface of the interacting object. A linear contact model was proposed to simplify the contact force computation between the proxy and deformable objects. The impedance transparency is enhanced by reducing the mass of virtual tool and increasing the stiffness of the virtual coupling. The impulsive contact force between the proxy and deformable object is resolved by clustering the multiple contact points. The proxy-based virtual coupling is extended to the simulation involving interaction between the deformable tool and deformable objects. Garre and Otaduy divided the deformable tool into the tool and the rigid handle. The parameters of the virtual coupling are determined by the interaction force between the tool and the handle. Multirate approaches based on the virtual coupling have limitations in impedance transparency because the contact force of the virtual object is conveyed through the virtual coupling and the proxy. Kim and Lee analyzed the stability and transmitted impedance of the simulation involving interaction between the dynamic proxy and deformable objects. Analysis result shows the effect of the mass and numerical integration methods of proxy on the stability and the impedance transparency. Another approach is the time-domain method that is widely used in the field of teleoperation and simulation because of its simplicity. The time-domain passivity absorbs the excessive energy from the virtual environment to maintain the passivity condition in the time domain by adding an artificial damping force to the output force. The artificial damping force, however, causes the discontinuity in the output force. Several research works tried to reduce this discontinuity in the output force by reducing the amount of energy absorbed by the passivity condition. The scattering matrix and wave variables are also used to resolve the stability problem caused by the slow update rate of the simulation. The time-domain methods are inadequate for the simulation involving interaction with deformable objects because of the discontinuity. The stability of the simulation can be easily guaranteed by indirect transfer of the impedance of the interacting objects by using the virtual coupling. The impedance transparency can be improved by increasing the stiffness of the virtual coupling. The impedance transparency has been improved by increasing the update rate of the haptic feedback because the small time step increases the maximum stiffness satisfying the passivity or absolute stability condition. The impedance transparency is, however, highly affected by the virtual coupling. This paper proposes a haptic rendering not using the proxy and virtual coupling based on the stability and impedance transparency analysis by using the bilateral control architecture. The proposed haptic rendering guarantees the stability and directly transfers impedance of the interacting objects.", "section": "Methodology", "doi": "10.1109/TIE.2019.2903755", "references": [1480135040, 1964366420, 2001852229, 2068794080, 2096147331, 2113661693, 2119024373, 2124034026, 2125137771, 2144415816, 2153631465, 2153681681, 2322516402, 2344076301, 2523702564, 2899133161]}
{"paragraph": "In finance, credit scoring is one of the crucial challenges and a core responsibility for lenders in their risk management. For example, the financial crisis of 2007/2008 impressively shows possible impacts of faulty risk management. Started as local sub-prime crisis in the housing sector in the U.S., the crisis spread around the world and severely infected the global economy as a whole. Even a decade later, the aftermaths of the crisis are still virulent, challenging economies around the world. As a lesson of the financial crisis, the rules for the assessment of credit ratings and the capital requirements for financial institutions have been tightened all over the world. Credit scoring ranges from the rating of countries and global international companies to ratings of small enterprises applying for credits. Furthermore, individuals are regularly assessed regarding their creditworthiness, whether they are applying for a large home loan or just seeking for a small mobile phone contract. The impacts of credit scoring are significant. On the one hand, approving credits to clients who fail to repay, may lead to affordable losses in the best case but may also jeopardize a lender’s business or even have severe impacts on macroeconomic levels like in the financial crisis in the worst case. On the other hand, strict rules for credit approvals may damage an economy that could be strengthened by affording credits. Consumer credits may increase the demand for goods whereas investment credits may support the supply side of an economy. When performing credit scoring three results are normally obtained. Some clients are solvent at the first sight while some other clients instantly fail to be creditworthy. The remaining clients need to be assessed in more detail before a final decision on the approval or disapproval of the credit can be made. More generally, the intensities of assessment required are not constant for all clients but they vary from client to client. They depend on the amount of information that is needed for a particular decision. The principles of such three-way decisions are widely applied in a diverse range of areas, such as medical diagnosis, computer vision, or recommender systems. However, only recently Yao proposed a formal framework for three-way decisions that is derived from probabilistic rough sets. Yao proposed the theory of three-way decisions to obtain cost efficient categorizations of objects into such three classes; the positive and negative ones and the objects whose final decision should or must be postponed. The objective of this paper is to propose a methodology for credit scoring that minimizes the decision-relevant costs using three-way decisions with probabilistic rough sets. Furthermore, we underline this methodology’s potential via a real-world application, where we analyze a data set that was collected by a Chilean bank. It consists of more than 7000 credit applications from small and micro-companies and their evaluations. In our analysis, we take the perspective of financial accounting as it is relevant for banking regulation. In Section 2, we review the theory of three-way decisions and present some of their applications. In Section 3, we develop our framework for credit scoring using three-way decisions and design decision rules to take the final deterministic decision for the case of credit granting. The subsequent section contains the results of applying this framework to the before-mentioned data. The paper concludes with a summary in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.08.001", "references": [1877823, 95749022, 101345952, 110592617, 112052679, 154229132, 837378864, 1004203976, 1041128124, 1511669739, 1537207111, 1756509722, 1861960977, 1973948212, 1975251650, 1982120517, 2001692054, 2007929615, 2009653394, 2036349451, 2050237791, 2060952501, 2067145099, 2070813883, 2080404663, 2090687467, 2137919530, 2159875572, 2191555075, 2217596628, 2278789126, 2310435784, 2549504134, 2600072788, 2792234535, 2793566013]}
{"paragraph": "Multi-view data are very common in some scientific data analytics problems such as computer video, social computing and environmental sciences, due to the use of different measuring methods or of different media, like text, video and audio. Multi-view clustering, which makes use of the complementary information embedded in multiple views to improve clustering performance, has attracted more and more attentions. In the existing methods, spectral clustering is a popular one for multi-view data because it represents multi-view data via graph structure and makes it possible to handle complex data such as high-dimensional and heterogeneous as well as it can easily use the pairwise constraint information provided by users. In some practical applications, particularly in the multimedia domain, a view is usually represented in a high-dimensional feature space. For high-dimensional data, the feature distribution is usually more sparse, the traditional similarity measurement methods based on distance measures become inapplicable. In order to solve this problem, the approaches based on low-rank representation try to learn a common low-dimensional subspace from the high-dimensional multi-view data and each object can be represented linearly by others objects in the same subspace, which contributes to reduce the computation cost and improves the robustness to noise corruptions. On the other hand, we find that a problem of the existing cluster analysis methods has not been addressed satisfactorily is the uncertain relationship between an object and a cluster. It is obviously that there are three relationships between an object and a cluster, namely, belong-to definitely, not belong-to definitely and uncertain. In most of the existing work, a cluster is represented by a single set, the set naturally divides the space into two regions. Objects belong to the cluster if they are in the set, otherwise they do not. Here, only two relationships are considered, no matter in hard clustering or in soft clustering. They are typically based on two-way decisions. Let us observe the third relationship, which means the object may or may not belong to the cluster. We just cannot make decisions based on the present obtained knowledge or information. We can make further certain decisions when we have further information. It is a typical idea of three-way decisions. Inspired by the theory of three-way decisions, a framework of three-way cluster analysis has been introduced. The previous results on three-way clustering provide us with a tool for studying the problem of clustering with uncertainty. In this paper, we focus on a general framework based on the theory of three-way decisions, which is appropriate for soft clustering or hard clustering. This three-way representation with two sets brings more insight into interpretation of clusters. Objects in the core region certainly belong to the cluster, objects in the trivial region definitively do not belong to the cluster, and objects in the fringe region maybe or may not belong to the cluster. Obviously, the two-way representation with a single set is a special case of three-way representation with two sets when fringe regions are empty. Compared with the supervised learning, clustering process lacks the user guidance or the class label information and may not produce the desired clusters. Thus, some semi-supervised clustering methods are proposed. These methods that use certain weak supervision form, such as pairwise constraints, can significantly improve the quality of unsupervised clustering. Pairwise constraints describe two objects whether they should be assigned to the same cluster or the different clusters. However, choosing the supervised information is random in most of existing methods, and it does not produce positive effect on improving the clustering result when the algorithm itself can find the prior information or there are amounts of noises in the prior information. Therefore, the active learning method is introduced to optimize the selection of the constraints for semi-supervised clustering. Hence, our work considers active learning of constraints in an iterative framework based on spectral clustering. In each iteration, we determine objects with the most important information toward improving the current clustering result and form queries accordingly instead of choosing the information randomly. The responses to the queries are then used to update the clustering. This process repeats until we reach a stable solution or we reach the maximum number of queries allowed. Such an iterative framework is widely used in active learning for semi-supervised clustering. The measurement of information is designed based on the entropy concept. Besides, to take the advantage of three-way representation of clustering, it is reasonable to choose pairwise constraints in fringe regions instead of the universe, which will improve the search efficiency. In this paper, we address the problem of clustering on multi-view data of high dimensionality. The main contributions are summarized as follows: A novel active three-way clustering method via low-rank matrices for multi-view data is proposed. It considers the diversity of multiple views and to improve the quality of clustering for multi-view data. A multi-view information fusion algorithm is presented via low-rank matrix representation for high-dimensional multi-view data, and the weights are adjusted adaptively on each view during solving the optimization problem of objective function. A three-way clustering representation is utilized to reflect the three relationships between an object and a cluster, namely, belong-to definitely, uncertain and not belong-to definitely. The three-way clustering approach provides us with a tool for studying the problem of clustering with uncertainty. An active three-way clustering algorithm is developed by taking the advantage of three-way representation of clustering, which can produce the three-way results as well as two-way results accordingly. The idea of farthest-first traversal scheme is used to construct the cores of clusters, then to expand cores to fringes by using the idea of k nearest neighbors, and the rules to adjust the consensus similarity matrix are also introduced. The proposed method is flexible for semi-supervised clustering as well as unsupervised clustering. We evaluate the proposed method with some other algorithms on seven real-world datasets. The results of comparative experiments demonstrate the effectiveness of the proposed method and show that it is appropriate for multi-view data of high dimensionality. The rest of the paper is organized as follows. In Section 2, we review existing clustering algorithms for multi-view data, clustering approaches for uncertain relationships and active learning approaches for clustering. In Section 3, we give a detailed description of the proposed method termed Active Three-way Clustering via Low-rank Matrices. In Section 4, we report on an extensive experimental evaluation of the proposed method. Finally, we summarize the present study in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.03.009", "references": [101345952, 201974436, 1041128124, 1511669739, 1558962159, 1596382552, 1670132599, 1965450189, 1997201895, 2004524042, 2013029404, 2016384870, 2023750115, 2029677459, 2056187338, 2070412788, 2077775960, 2079402740, 2080404663, 2085989833, 2088431846, 2103972604, 2105709960, 2135914502, 2137184539, 2142674578, 2153839362, 2154415691, 2159091719, 2165874743, 2166782149, 2169529055, 2210977594, 2294062232, 2297889545, 2405459681, 2512066509, 2518081091, 2527290711, 2716026328, 2951085447]}
{"paragraph": "For its special properties such as vertical taking off and landing (VTOL), rapid manoeuvring, and precise hovering, the quadrotor UAV has attracted great attention from both military and civil applications in recent years. But control design for quadrotor faces many challenges, such as limited payload ability, input delays, nonlinear dynamics, state coupling, underactuated property, and so on. Various control methods have been developed for the quadrotors, such as the signum of the error (RISE) method, L1 adaptive control method, and so on. Most recently, an increasing interest in the research area for quadrotor is the quadrotor aerial transportation system, especially the composite system which consists of a quadrotor with a suspended-payload, due to its advantages such as high payload capacity, security assurance, operational flexibility, etc. The potential applications of the system of a quadrotor UAV with a suspended-payload include package delivery, construction of building, disaster relief operation, etc. Some research works related to the quadrotor UAV suspended-payload system have been conducted in the last few years, such as lifting and landing control design, trajectory planning and obstacle avoidance, multiquadrotor UAV cooperating transportation, and so on. In particular, the antiswing motion control of the payload during the transportation procedure is a critical problem for the safety of the quadrotor suspended-payload composite system. Antiswing control design for the system of the quadrotor with a suspended-payload faces great challenges. With eight degrees of freedom but only four independent control inputs, the system of quadrotor UAV with a suspended-payload is a typical underactuated system to which most existing control techniques developed for the full actuated system cannot be applied directly. Along with the parametric uncertainties and the inherent nonlinearity of the system, the underactuated property makes the design of high-performance control a very challenging task. In order to solve the previous problems, different control techniques have been developed. A nonlinear model-predictive controller is designed for the simplified quadrotor UAV suspended-load dynamic model. A control design using the nested saturation approach is presented. A passivity-based control for the system of a quadrotor UAV with a suspended-payload is proposed, where two control strategies are presented. The first one depends on the feedback of the payload's swing angle, and the second one does not. For these control methodologies, most of them use a simplified dynamic model with the motion of the quadrotor and the payload being restricted in the transverse plane. Thus, the control design is away from the actual dynamic properties of the system. To address the aforementioned issue, different linear control strategies in three-dimensional (3-D) space have been presented. To reduce complexities associated with the control development, some researchers utilize linearized dynamic models for application of existing control methodologies. An iterative linear quadratic regulator based optimal control design is proposed based on a linearized dynamic model of the quadrotor transportation system. For the helicopter-payload system, an anti-swing trajectory generation strategy combining with a linear tracking controller is presented. Although these control designs perform well in the numerical simulation verification, the good performance of linear control strategy cannot be guaranteed when the system states are away from the equilibrium point. Especially, when the payload's swing angle becomes too large, the performance of a linear controller may deteriorate considerably. To deal with the disadvantages associated with linear control strategies, various nonlinear control methods have been developed. Sreenath et al. establish the dynamic model of a quadrotor UAV with a cable-suspended payload as a differentially-flat composite system and present a nonlinear control law in which the parametric uncertainties are not considered. By considering the motion of the payload as a modeled disturbance, the authors design a feedback linearization control law for the quadrotor–payload system. Liang et al. introduce a control scheme via utilizing the energy-based analysis method in the position-loop and a backstepping control method in the attitude-loop of the quadrotor UAV transportation system. In these works, the length of the cable is included for the control strategies. However, the swing angles of the payload are not considered in the control design procedure, which will decrease the performance for antiswing control. The energy-based control methodology has been employed for the control development for different underactuated mechanical systems, such as the overhead cranes, the rolling–balancing systems, and the quadrotor–payload systems. In this paper, a new nonlinear adaptive control design for antiswing control of the quadrotor aerial transportation system is proposed. First, to deal with the difficulties associated with underactuated properties of the dynamic system, an energy-based methodology is employed for the control design. Second, the energy-based function is modified so that the controller possesses a more powerful ability of swing-motion rejection for the payload. Finally, to increase the robustness with respect to the parametric uncertainties, the adaptive design is introduced. Despite the difficulty of stability analysis for the underactuated system, the stability of the closed-loop is proven via Lyapunov-based stability analysis. Furthermore, to validate the performance of the proposed control design, the nonlinear adaptive controller is verified via real-time experiments performed on an indoor quadrotor UAV flight testbed. The main contributions of this paper can be summarized as follows: (1) In this paper, a 3-D dynamic model of a quadrotor UAV with a suspended-payload is employed for the control development, the underactuated issue is well-addressed via the help of a novel energy-based methodology. (2) The unknown length of the connecting cable is compensated with a new adaptive design while most existing works have not considered about this problem. (3) Three groups of real-time experimental results are implemented on a quadrotor-suspended payload flying testbed to verify the performance of the proposed strategy while most existing works only provide numerical simulation results. And the payload's initial swing angles are set to be about 30 to 40 degrees which are larger than the experiments in most existing works. This has verified the good robustness of the proposed control strategy. This paper is organized as follows. Section II presents the dynamic model of the system of a quadrotor with a suspended-payload. In Section III, an energy-based adaptive control strategy is developed. And in Section IV, the equilibrium of quadrotor suspended-payload system is proven to be asymptotically stable via a Lyapunov-based analysis. In Section V, the real-time flight experimental results are provided. Finally, Section VI concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2902834", "references": [1517700591, 1568784080, 1966714274, 1969064048, 1986045690, 2028066543, 2044894195, 2058475458, 2079496302, 2109797433, 2160363664, 2565478864, 2581292955, 2591002471, 2756177107]}
{"paragraph": "Fault diagnosis is crucial to guarantee reliability and avoid significant economic loss in industry application. In the last years, data-driven intelligent fault diagnosis methods have been widely developed and applied, due to the development of advanced sensing technologies and its advantage that need not construct complicated physical model over model-based methods. Recent research works about data-driven intelligent diagnosis methods mainly focus on two aspects. The first aspect is to propose new feature extraction methods based on advanced signal processing techniques, such as spectral kurtosis method, entropy-based feature, and modified empirical mode decomposition method. The second aspect is to utilize and modify different machine-learning methods to learn the mapping relationship between features and fault modes automatically, such as support vector machine, k nearest neighbor, and neural network. Recently, deep learning based methods are widely studied and applied to addressing fault diagnosis problem, which can learn hierarchical representations from raw input without using hand-crafted features. For example, sparse filtering has been used to directly learn features from bearing vibration signals; then, the Softmax regression is employed to classify the health conditions. A novel variable-wise weighted stacked auto-encoder has been proposed, which can give better diagnosis performance than the traditional multilayer neural networks and SAE on an industrial debutanizer column process. A fault diagnosis and isolation method for wind turbines has been proposed, which implements long short-term memory networks for residual generator and applies the random forest algorithm for decision making. However, these conventional supervised learning methods follow a common assumption that training and test data must be drawn from the same distribution. If this assumption does not hold, the generalization ability of these methods will drop dramatically. Although deep learning based diagnosis methods can automatically learn remarkable features from raw signal, the generalization ability to congeneric diagnosis problems of the features learned by deep models is not discussed and guaranteed. Meanwhile, the identically distributed assumption should also be satisfied in the supervised identification stage because the conventional supervised methods are usually applied in the last layer of these deep models, such as neural network, Softmax regression, and random forest. Unfortunately, for actual diagnosis problem, holding this assumption is very difficult because it means that a training dataset should be collected before diagnosing from the same machine under the same operating condition and even noise environment. Therefore, those intelligent fault diagnosis methods cannot construct effective fault identification models for target machine in actual diagnosis scenario due to the unavailable of training data with same distribution. Moreover, an ideal and significant diagnosis model should not only accurately classify fault modes in a specific dataset, but also show a remarkable generalization ability in related diagnosis tasks. Actually, the fault data for training identification model are usually collected from different operating conditions or even other same-type machines. Although the a priori distribution estimation of test data is unknown and the distribution of test data might be different from training data, they are similar and related to each other because of the same working principle and the similar failure mechanism. Based on the related but distribution-different data, several research works have been studied to build fault diagnosis models using transfer learning or domain adaptation methods. TrAdaBoost has been applied to improve diagnosis performance of rolling element bearing under different operation conditions. Similarly, gearbox diagnosis performance has been enhanced by reusing historical data under various operating conditions, and a feature-based transfer method, transfer component analysis, was utilized to transfer knowledge from different operating conditions. Following the feature-based transfer strategy, several research works combined deep learning and transfer learning for machinery fault diagnosis. A deep domain adaptation method under the auto-encoder framework has been proposed and applied to bearing and gearbox fault diagnosis problem. A similar method based on sparse auto-encoder is also proposed. A transfer learning approach for improving bearing fault diagnosis performance has been presented, which can transfer partial parameters from the neural networks trained using enough source data to the neural networks of the target task. A convolutional neural network based diagnosis algorithm has been designed that achieves high identification accuracy for bearing fault diagnosis under noisy environment and different working loads. The basic idea of these research works is to learn and transfer the diagnosis knowledge from training data to test data and improve the generalization ability of the identification model on target dataset further. In contrast to traditional machine-learning techniques that try to learn a new task from scratch, transfer learning or domain adaptation borrows knowledge from source domains to facilitate learning in a target domain, which has shown promising results in cross-domain learning applications. The abovementioned diagnosis methods using transfer learning have proved the feasibility of promoting diagnosis performance by the manner of knowledge transfer. But they can only consider one single source in learning process and may suffer some deficiencies in actual diagnosis scenario. First, the premise of effective transfer is that the source and target domains should be directly related. However, how to ensure positive transfer has not been discussed for fault diagnosis problem. One strategy to decrease the risk for negative transfer is to borrow knowledge from multiple related sources. By this way, the chance to learn beneficial diagnosis knowledge closely related to the target machine significantly increases. Second, in actual diagnosis scenario, the samples of target machine under each fault category cannot be collected before diagnosing, and usually only normal samples are available for training diagnosis model. Hence, the distribution discrepancies between the target domain and source domains cannot be estimated for choosing the most related source using current multisource transfer methods. The fault diagnosis based on multiple sources is more prone to a domain generalization problem than a domain adaptation problem. Therefore, how to discover general diagnosis knowledge from multiple sources and apply these knowledge to a new diagnosis task is the crucial issue for data-driven fault diagnosis. In light of the above discussion, this paper proposes a novel intelligent fault identification method based on multiple source domains. Our proposed method describes the discriminant structure of each source domain that is favorable for fault identification as a point on Grassmann manifold using a dimensionality reduction method, local Fisher discriminant analysis. Then, the method tries to discover the general diagnosis knowledge from those source domains by constructing the mean subspace on that Grassmann manifold. To our best knowledge, this paper is the first attempt to consider knowledge transfer from multiple sources for machinery fault diagnosis. In contrast to current multisource transfer methods that try to adapt source domains to target domain, our method is more prone to discover and transfer general diagnosis knowledge from source domains to target domain, which is more suitable to actual diagnosis scenario. In addition, two subspace-based domain adaptation methods are close to ours. They also explored the effective manners of knowledge transfer in multiple source scenarios. The first difference of these works from ours is that we use local Fisher discriminant analysis instead of principal component analysis to discover discriminant diagnosis structure of each source. Compared with principal component analysis that identifies the directions of variances maximization without considering label information, local Fisher discriminant analysis can learn the optimal subspace for discriminant analysis with preserving local structure of the data. By this way, the diagnosis structure that is associated with discriminant information will be represented more effectively. The second difference is that we directly embed target samples to common diagnosis subspace without sampling finite or infinite subspaces along the geodesic flow connected source and target datasets. The main reason is that the target domain cannot be embedded on the Grassmann manifold correctly without the fault data of target domain in training stage. The main contributions of our paper can be summarized as follows. This paper proposes a new data-driven fault identification method that can borrow diagnosis knowledge from multiple related source domains. The data of each source domain can be collected from different operating conditions or other same-type machines. The proposed method is very promising for handling actual fault diagnosis problem than conventional data-driven methods. The manner of knowledge transfer is a new attempt to achieve higher level of intelligent fault identification. The well-designed method in our paper is suitable to address the mechanical fault diagnosis problem. In the proposed method, local Fisher discriminant analysis is used to learn the optimal discriminant structure of each source domain from multimodal fault data. In addition, because of lacking fault data of the target domain in the training stage, we directly map target samples to common subspace without embedding it on Grassmann manifold. The proposed method can discover general diagnosis knowledge from multiple source domains and apply the knowledge to facilitate new tasks in target domain. The risk of negative transfer in learning process is reduced. This paper is organized as follows. Preliminaries about our proposed method and problem formulation are described. Then, the proposed method is presented in detail. Experimental results and analysis are illustrated. Finally, this paper concludes.", "section": "Related Work", "doi": "10.1109/TIE.2019.2898619", "references": [247715996, 1804110266, 1965491739, 1981658663, 1982696459, 1984672166, 2062179223, 2075728230, 2100664256, 2109531142, 2115403315, 2118414455, 2122838776, 2126017757, 2128053425, 2149466042, 2153635508, 2163406054, 2165698076, 2317595875, 2474392855, 2515979703, 2556013418, 2568819930, 2605346601, 2731372149, 2735735104, 2740570963, 2760844412, 2762355244, 2763583057, 2767498147, 2768753204, 2788805965, 2963756240]}
{"paragraph": "Graphs are commonly used for representing complex systems, such as interactions between proteins, data communications between computers' and relationships between people. Visualizing a graph can help better understand the relational and structural information in the data that would not be as apparent if presented in a numeric form. The most popular and intuitive way to visualize a graph is a node-link diagram, where the nodes are drawn as points, and the links are rendered as lines. Drawing a node-link diagram by hand is laborious; since the 1960s, researchers have devised a multitude of methods to automatically lay out a graph. The layouts of the same graph can vary greatly depending on which method is used and the method's configuration. However, there is no best layout of a graph as different layouts often highlight different structural characteristics of the same graph. For example, while one layout can emphasize connections between different communities of a graph, it might not be able to depict connections within each community. Thus, it is important to find a good layout for showing the features of a graph that users want to highlight. Finding a good layout of a graph is, however, a challenging task. The heuristics to find a good layout are nearly impossible to define. It requires to consider many different graphs, characteristics to be highlighted, and user preferences. There is thus no existing method to automatically find a good layout. In practice, users rely on a trial-and-error process to find a good layout. Until they find a layout that satisfies their requirements, users typically visualize a graph in multiple layouts using different methods and varying parameter settings. This process often requires a significant amount of the user's time as it results in a haphazard and tedious exploration of a large number of layouts. Furthermore, expert knowledge of layout methods is often required to find a good layout. Most layout methods have a number of parameters that can be tweaked to improve the layout of a graph. However, many layout methods—especially force-directed ones—are very sensitive to the parameter values, where the resulting layouts can be incomprehensible or even misleading. A proper selection of the parameter settings for a given graph requires detailed knowledge of the chosen method. Such knowledge can only be acquired through extensive experience in graph visualization. Thus, novice users are often blindly tweaking parameters, which leads to many trials and errors as they cannot foresee what the resulting layout will look like. Moreover, novices might explore only a fraction of possible layouts, choose an inadequate layout, and thus overlook critical insights in the graph. To help users to produce a layout that best suits their requirements, we present a deep generative model that systematically visualizes a graph in diverse layouts. We design an encoder-decoder architecture to learn a generative model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder generates layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space. By mapping a grid of generated samples, a two-dimensional latent space can be used as a what-you-see-is-what-you-get interface. This allows users to intuitively navigate and generate various layouts without blindly tweaking parameters of layout methods. Thus, users can create a layout that satisfies their requirements without a haphazard trial-and-error process or any expert knowledge of layout methods. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. Also, graph neural networks and Gromov–Wasserstein distance help the model better learn the complex relationship between the structure and the layouts of a graph. After training, our model generates new layouts considerably faster than existing layout methods. In addition, the generated layouts are spatially stable, which helps users to compare various layouts. In summary, we introduce a fundamentally new approach to graph visualization, where a machine learning model learns to visualize a graph as a node-link diagram from existing examples without manually-defined heuristics. Our work is an example of artificial intelligence augmentation: a machine learning model builds a new type of user interface i.e., layout latent space to augment a human intelligence task i.e., graph visualization design.", "section": "Related Work", "doi": "10.1109/tvcg.2019.2934396", "references": [58821284, 131619556, 170199606, 884650706, 1503013560, 1504716054, 1564330171, 1584092895, 1688966418, 1836465849, 1862093616, 1916693793, 1959608418, 1966406768, 1967140769, 1991701776, 1992709202, 1994616650, 2000856020, 2013307517, 2021053884, 2026729066, 2027842533, 2035080386, 2049335983, 2075220720, 2097998348, 2099471712, 2100083479, 2102664288, 2125910575, 2132964407, 2135415614, 2138959470, 2140063299, 2144803487, 2148249409, 2150770193, 2408539216, 2417433140, 2461158874, 2529996553, 2576577998, 2577946330, 2606712314, 2732947513, 2751731070, 2753738274, 2766453196, 2785523195, 2792210438, 2803533564, 2806115886, 2806351858, 2811124557, 2888650943, 2891814056, 2892118767, 2907366006, 2951659295, 2952574282, 2962711740, 2962760235, 2962767366, 2962937159, 2964015378, 2964108670, 2964114465, 2964222296]}
{"paragraph": "Multivalued logic (MVL) technology evolves from binary Boolean logic technology. In traditional Boolean logic systems a variable has only two values, 1 and 0, to represent two specific states, such as right and wrong in logic, on and off in circuits, and high and low in level. While in MVL systems, a variable can have many values to express many different states. For examples, in the Post algebra, it can express three symbols: 1) positive; 2) zero; and 3) negative; and in computer operating systems it can express five kinds of process state: 1) create; 2) ready; 3) execute; 4) block; and 5) exit. The MVL technology has the potential to increase the information capacity of each single line obviously and consequently it has a great advantage in the logic circuit design, reliability analysis, and optimization. With the expansion of circuit integration, the optimization of an MVL network can shrink the size of programmable logic arrays effectively, reduce the number of interconnections among chips in orders of magnitude, thereby improving circuit integration, speeding up hardware, and enhancing the ability of fault tolerance. Recently, the MVL network technology has also been applied in many other ways which are promising and attractive, such as pattern recognition, image processing, associative memories, and data mining. Especially it is also applied to multivalued neurons, which is based on the multivalued threshold logic and can match inputs and outputs arbitrarily by a partially defined multivalued function. It is necessary and important to optimize MVL networks. In order to optimize MVL networks, several kinds of methods, such as an algebraic method, circuit method, hyper-planes method, theorem-proving technique, and modular design approach have been used. Although these early methods can provide some optimized solutions, they are extremely time-consuming, even in orders of days and they can only solve small-size problems. As an alternative, direct cover (DC) can generate an efficient cover directly given an MVL network, omitting the intermediate procedure of creating prime implicants. DC requires much less computational time than those methods based on implicants. MVL networks optimized by DC contain no error just as the ones before optimizing. However, this technique aims at accuracy, regardless the number of optimized gate circuits. In other words, DC is poor in saving the cost of manufacturing and integration of circuits. Subsequently in recent years, DC is introduced into some intelligent algorithms, which can simplify MVL networks. Especially an ant colony optimization (ACO) algorithm combined with DC can improve the performance of MVL networks, which is called ACO-DC. ACO-DC is a state-of-the-art algorithm, which first uses some ants to decompose an MVL network, and then uses DC to mimic each subnetwork. It completes the cover of MVL networks successfully, and reduces the number of gate circuits effectively, yet not enough. Meanwhile a new learning MVL network is put forward to improve the performance of the networks based on algebra, and several kinds of technologies, such as error back-propagation and nonback-propagation, are introduced into the model. It can accumulate a priori knowledge about targets and processes, so as to evolve more effectively. In fact, a learning MVL network using error back-propagation requires some presuppositions: 1) the simulated function curve must be continuous and differentiable at each point and 2) the values of some parameters, e.g., weight and threshold are preassigned. It is obvious that these conditions involve some uncertainies. On the other hand, a learning MVL network based on nonback-propagation such as local search and simulated annealing needs no such prior knowledge, but it has its disadvantages: it is easy to fall into a local minimum and easy to converge prematurely. To jump out of a local minimum, a great deal of research has been carried out. A stochastic dynamic factor is introduced into a traditional local search method, which can permit the search to go a little farther along the error direction in order to escape from a local minimum when the search encounters a trap. A clonal selection algorithm incorporated with chaos is proposed to maintain the diversity of population and accelerate its evolution. The above-mentioned search methods have proved to be better than the traditional local search through a large number of experiments. They can deal with the local minimum issue, run smoothly in the global field, then reach some high-quality solutions in a reasonable time and show their robustness. Nevertheless, they have a common flaw: their measure of the solutions is unique and incomplete. In other words, they are all based on a single objective function which is used to guide their search in the solution landscape. Real-world optimization problems need usually be evaluated with several criteria, but not only one, and these criteria maybe in conflict with each other. Specifically, the optimization of MVL networks includes two objectives: 1) accuracy and 2) optimality. Accuracy is the most important measure of a system and optimality reflects the cost of a system. The methods mentioned above only take accuracy into consideration, but ignore optimality. They always try their best to find the best solutions with the least error, regardless of the cost of their solutions, which is clearly unreasonable. A modified error function combining accuracy and optimality is presented to evaluate the individuals of MVL networks by using a weighted sum. It must be emphasized that these two evaluation criteria used in the evaluation function are totally unequal: the accuracy plays a dominant role, while the optimality starts to work only when the accuracy of an individual is the same or almost the same as that of the other. This algorithm is strongly influenced by the weights that are tough to be determined objectively. Hence, it is a single-objective one in essence. In this paper, we for the first time propose a true multiobjective method to optimize MVL networks and try to find the optimal solutions. First and foremost, two independent objective evaluation functions are applied to the problems. They are similar to the previous definitions, yet we utilize them completely equally in our research, without any subjective parameters. Second, in single-objective problems the best solution appears when the objective function is the best while in multiobjective problems there are complex internal relations among various objectives. It would not be a subjective decision whether a solution is abandoned or not in solving MVL network problems. Sufficient optimal solutions should be collected in order to provide the needed decision support for practical applications. They include two kinds, the elite solutions with no error in accuracy and the Pareto optimal solutions whose accuracy and optimality are compromised with each other. The solution with both best accuracy and best optimality may not be turned into reality for the reason of cost or technical limitations. Thus, the best accurate solutions may have some different optimality for potentially different applications. Furthermore, some products hope to reduce the cost by allowing some noncritical errors. Therefore, Pareto optimal solutions should also have some different ranks. Altogether, two kinds of solutions should be uniformly distributed along the two evaluation functions. In MVL networks, the objective functions have simple phenotypes in which their variable values are discrete and limited, while the decision vectors corresponding to the objective functions have rich genotypes in which a decision vector has a number of variables. Because of these features, a novel algorithm based on traditional differential evolution (DE) is proposed to optimize MVL networks. A novel data structure and characteristic updating method for archive population are built to store elite solutions with different optimality and Pareto optimal solutions with different ranks. The simulation results indicate that our algorithm is significantly superior to the existing algorithms. The contributions of this paper are: Modeling MVL network learning task as a multiple-objective optimization problem and considering error and optimality of solutions simultaneously. Discriminating Pareto optimal solutions with several different ranks to provide fruitful solutions for practical applications. Designing a novel data structure and updating rule for the archive population to maintain the diversity and quality of solutions. Testing a set of MVL instances to evaluate the proposed approach and therefore giving more evidences about its superior performance over other existing algorithms including single-objective methods, several variants of multiple-objective methods, and DC-based methods. The remainder of this paper is organized as follows. In Section II, we give some basic knowledge in order to understand the algorithm to be proposed. In Section III, we illustrate a bi-objective algorithm based on DE and its characteristics in details. In Section IV, a great number of simulations are implemented and the results are analyzed. In Section V, we summarize this paper and put forward a further research plan.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868493", "references": [1544309315, 1561333824, 1563525949, 1584198977, 1595159159, 1956801889, 1975507646, 1984029699, 1991458509, 1994125175, 2002280630, 2025152094, 2026904884, 2026923346, 2026958463, 2034585384, 2051159673, 2054208673, 2056655435, 2061510426, 2084855067, 2092673054, 2092886720, 2095708000, 2105972436, 2106371135, 2112626954, 2120514998, 2126105956, 2129691920, 2137779163, 2141522269, 2148324414, 2150046657, 2151017751, 2154338144, 2156194072, 2156735594, 2156753779, 2158115054, 2163636073, 2193409108, 2342596926, 2576677224, 2611421169, 2751605210, 2764251381, 2805172798]}
{"paragraph": "Modern systems are composed of several distributed components that work in real-time to satisfy a given specification. This makes them difficult to reason about manually and encourages the use of formal methods to analyze them automatically. This in turn requires the development of models that capture all the features of a system and still allow efficient algorithms for analysis. Further, to bring formal models closer to real world implementations, it is important to design robust models, i.e., models that preserve their behavior or at least some important properties under imprecise time measurement. In this paper, we consider Petri nets extended with time constraints for modeling real-time distributed systems. For timed variants of Petri nets, many basic problems are usually undecidable or algorithmically intractable. Our goal is to consider structural restrictions which allow us to model features such as unbounded resources as well as time-deadlines while remaining within the realm of decidability and satisfying some robustness properties. Time Petri nets are a classical extension of Petri nets in which time intervals are attached to transitions and constrain the time that can elapse between the enabling of a transition and its firing date. In such models, the basic verification problems considered include: reachability, whether a particular marking can be reached in the net; termination, whether there exists an infinite run in the net; boundedness, whether there is a bound on the number of tokens in the reachable markings; and firability, whether a given transition is fireable in some execution of the net. It turns out that all these basic problems are in general undecidable for time Petri nets, though they are decidable for the untimed version of Petri nets. The main reason is that time Petri nets are usually equipped with an urgent semantics: when the time elapsed since enabling of a transition reaches the maximal value of its interval, a transition of the net has to fire. This semantics breaks monotony. Indeed, with a time Petri net, one can easily encode a two-counter machine, yielding undecidability of most of the verification problems. Decidability can be obtained by restricting to the subclass of bounded time Petri nets, for which the number of tokens in all places of reachable markings is bounded by some constant. Other ways to obtain decidability are by weakening the semantics or restricting the use of urgency. Another important problem in this setting is the question of robustness. Robustness can be defined as the preservation of properties of systems that are subject to imprecision of time measurement. The main motivation for considering this is that formal models usually have an idealized representation of time, and assume an unrealizable precision in measurement of time which cannot be guaranteed by real implementations. Robustness has been studied extensively for timed automata and more recently for time Petri nets, but decidability results are only known in a bounded-resource setting. The definition of the semantics plays an important role both to define the expressive power of a model, and to obtain decidability results. When considering unbounded nets, where multiple and a possibly unbounded number of tokens may be present at every place, one has to decide whether transitions should be considered as multiply enabled, and if so, fix a policy to handle multiple instances of enabling. This becomes even more complicated when real-time constraints are considered. Indeed, several possible variants for the multiple enabling semantics have been considered. In this paper, we fix one of the variants and consider time Petri nets equipped with a multi-enabling urgent semantics, which allows to start measuring elapsed time from every occurrence of a transition enabling. This feature is particularly interesting: combined with urgency, it allows for instance to model maximal latency in communication channels. We adopt a semantics where time is measured at each transition's enabling, and with urgency, i.e. a discrete transition firing has to occur if a transition has been enabled for a duration that equals the upper bound in the time interval attached to it. Obviously, with this semantics, counter machines can still be encoded, and undecidability follows in general. We focus on a structural restriction on time Petri nets, which restricts the underlying net to be free-choice, and call such nets Free-choice time Petri nets. Free-choice Petri nets have been extensively studied in the untimed setting and have several nice properties from a decidability and a complexity-theoretic point of view. In this class of nets, all occurrences of transitions that have a common place in their presets are enabled at the same instant. Such transitions are said to belong to a cluster of transitions. Thus, with this restriction, a transition can only prevent transitions from the same cluster to fire, and hence only constrain firing times of transitions in its cluster. Further, we disallow forcing of instantaneous occurrence of infinite behaviors, that we call 0-delay firing sequences. This can easily be ensured by another structural restriction forbidding transitions or even loops in time Petri nets labeled with such constraints. Our main results are the following: we show that for a free-choice time Petri net under the multi-enabling urgent semantics, and in the absence of 0-delay firing sequences, the problem of checking firability of a transition and of termination are both decidable. The main idea is to show that, after some pre-processing, we can reduce these problems to corresponding problems on the underlying untimed free-choice Petri net. More precisely, we are able to show that every partially-ordered execution of the underlying untimed net can be simulated by the time Petri net, i.e., it is the untimed prefix of a timed execution of the time Petri net. To formalize this argument we introduce definitions of untimed and timed causal processes for unbounded time Petri nets, which is another contribution of the paper. Finally, we address several robustness questions. The problem of robustness for time Petri nets has previously been considered, but shown decidable only for bounded classes of nets. We show that the problem of robustness of firability with respect to guard enlargement, i.e., whether there exists a value such that enlarging all guards of a time Petri net by that value preserves the set of fireable transitions, is decidable for free-choice time Petri nets without 0-delay firing sequences. We also consider the same question for guard shrinking, i.e., existence of a value such that shortening the guards of a time Petri net by that value preserves the set of fireable transitions. We show that this problem is also decidable in our model. Finally, we consider robustness of termination with respect to guard enlargement or shrinking, and show that this question is decidable as well. To the best of our knowledge, these are the first decidability results on robustness for a class of unbounded time Petri nets. Related work. Verification, unfolding, and extensions of Petri nets with time have been considered in many works. Other than time Petri nets, a different yet common approach to integrate time to Petri nets is to attach time to tokens, constraints to arcs, and allow firing of a transition if all constraints attached to transitions are satisfied by at least one token in each place of its preset. This variant, called Timed-arc Petri nets, enjoys decidability of coverability, but cannot impose urgent firing, which is a key issue in real-time systems. A variant of Timed-arc Petri nets with place invariants is proposed in the TAPAAL tool. This allows for the modeling of urgency, but at the cost of decidability of coverability in unbounded nets. In time Petri nets, weak semantics have been proposed, where clocks may continue to evolve even if a transition does not fire urgently. With this semantics, time Petri nets have the same expressive power as untimed Petri nets, again due to lack of urgency, which is not the case in our model. Recently, variants of time and timed-arc Petri nets with urgency have been considered, where decidability of reachability and coverability is obtained by restricting urgency to transitions consuming tokens only from bounded places. This way, encoding of counter machines is not straightforward, and some problems that are undecidable in general for time or timed-arc Petri nets become decidable. The free-choice assumption in this paper is orthogonal to this approach and it would be interesting to see how it affects decidability for timed Petri nets with urgency. Finally, partial-order semantics have been considered in the timed setting. A notion of timed process for timed-arc Petri nets has been defined and a semantics to timed-arc nets with an algebra of concatenable weighted pomsets has been given. However, processes and unfoldings for time Petri nets have received less attention. An initial proposal was used to define equivalences among time Petri nets. Unfoldings and processes were refined to obtain symbolic unfoldings for safe Petri nets. The closest work to ours defines processes to reason about the class of free choice safe time Petri nets. However, this work does not consider unbounded nets, and focuses more on semantic variants with respect to time-progress than on decidability or robustness issues. This paper is an extended version of results presented previously. With respect to this first contribution, it contains all the proofs in full details, and an extended comparison between the single and multi-enabling of time Petri nets. It also establishes new results on robustness of firability and termination, that are now considered with respect to guard enlargement as well as shrinking. The paper is organized as follows: Section 2 introduces notations and defines a class of time Petri nets with multi-enabling semantics. Section 3 defines processes for these nets. Section 4 introduces the subclass of Free-choice time Petri nets and relates properties of untimed and timed processes. In Section 5, this relation is used to prove decidability of firability and termination for free-choice time Petri nets and, in Section 6 to address robustness of firability and termination to guard enlargement and shrinking. Section 7 discusses the assumptions needed to obtain decidability, and issues related to decidability of other problems in free-choice nets, before conclusion.", "section": "Related Work", "doi": "10.1016/j.jlamp.2018.11.006", "references": [63786410, 101037006, 1495551332, 1509494512, 1546958647, 1562433349, 1576317809, 1823152540, 1878074184, 1937565299, 1965237160, 1987557885, 2018780789, 2040633651, 2061288205, 2083567135, 2101508170, 2129140418, 2133170941, 2136315358, 2144938038, 2145580394, 2151612633, 2151720555, 2174804035, 2289622999, 2402880788, 2506850889, 2952981239]}
{"paragraph": "The problem of recovering a one-dimensional (1-D) signal from its Fourier transform magnitude, known as phase retrieval, is of paramount importance in various engineering and scientific applications, such as X-ray crystallography, optics, astronomy, blind channel estimation, and blind image deblurring. This problem has a long history and has been studied by many researchers. The phase retrieval problem originally arises from detectors that can sometimes only record the magnitude square of the Fourier transform of a signal. Due to the lack of Fourier phase information, some forms of additional information is required to identify the underlying signal efficiently. In this respect, the phase retrieval methods can be mainly classified into two categories based either on additional prior information, such as sparsity or additional magnitude-only measurements, including structured illuminations and masks, and short-time Fourier transform (STFT) magnitude-square measurements. The key idea of using additional STFT magnitude-square measurements is to introduce redundancy in the magnitude-only measurements by maintaining a substantial overlap between adjacent short-time windows. These phase retrieval methods have focused on recovering a single source from its Fourier transform magnitude. However, in certain cases, the problem of recovering multiple underlying images from multiple mixed Fourier transform magnitudes of images, called blind multiple-input multiple-output image phase retrieval (BMIPR), is ever-present in charge-coupled device (CCD) cameras and photosensitive films, such as astronomy or light field images. This problem is ill-posed due to the lack of the phase property and the mixing information. Recently, Guo et al. proposed a method for recovering the multiple one-dimensional (1-D) signals from the multiple mixed phaseless STFT measurements. Bendory et al. consider the problem of recovering a pair of signals from their blind STFT. Although these methods extend the research to a two-source scenario for 1-D signals, the existing phase retrieval methods cannot provide a solution to the problem of recovering the multiple underlying images from the multiple mixed phaseless STFT image measurements. Therefore, it is necessary to investigate the BMIPR problem. Extending the former research of Guo et al., a closely related problem of recovering the multiple underlying images from the multiple mixed phaseless STFT image measurements is considered. In this paper, our contribution is three-fold, which is as follows. BMIPR Model: A new model of the BMIPR problem is proposed in order to recover multiple underlying images from multiple mixed STFT image magnitude-square measurements, corrupted by noise. BMIPR Algorithms: Due to the absence of Fourier phase information and mixing information, we explore hybrid methods by introducing additional STFT magnitude-square measurements as well as estimating the mixing information. In view of the BMIPR model, an integrated algorithm is proposed, which combines a gradient descent (GD) algorithm by minimizing a nonconvex loss function with an improved complex maximization of non-Gaussianity (CMN) algorithm and a nonlocal means (NM) algorithm. At first, the mixed images can be recovered by the GD algorithm by minimizing a nonconvex loss function. Then, we use a composite algorithm that combined an improved CMN algorithm and an NM algorithm to estimate the mixing information and the underlying images from the mixed images. Initialization of the GD Algorithm: It is shown that the initialization of the GD algorithm can be obtained by minimizing a nonconvex loss function and equivalently posed as a constrained least squares (LS) solution with a penalty term. However, this method tends to limit the value range of model parameters and produce biases. To address this issue, we propose to use the principle eigenvector of a designed correlation matrix to initialize the GD algorithm that minimizes an LS solution with a penalty term. The new loss function may provide significant benefits in three aspects. First, it is more likely to get a sparser solution than the use of norm. Second, it has a better analytical structure than other norms. Third, it prevents over-fitting and improves generalization performance and relaxes the rank restriction of the regression variables. This paper is organized as follows. Section II formulates a mathematical model and gives the assumptions for the BMRP problem from the multiple mixed STFT image magnitude-square measurements. Section III discusses the uniqueness of the BMRP problem and presents the conditions under which it has a solution by combining a GD algorithm, an independent component analysis (ICA) algorithm, and an NM algorithm. This section also explores the initialization method for the GD algorithm. Section IV shows numerical experimental results. Section V concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2901661", "references": [1960538562, 1969483602, 2078397124, 2088085634, 2108746992, 2138002536, 2140867429, 2255081686, 2255090779, 2579955770, 2620752075, 2775677614, 2808012550, 2888257719, 2962941403, 2963095544, 2963249967]}
{"paragraph": "Traditionally, computer researchers have used the geometric mean of performance ratios of two computers running a set of selected benchmarks to compare their relative performances. This approach, however, is limited by the variability of computer systems which stems from non-deterministic hardware and software behaviors or deterministic behaviors such as measurement bias. The situation is exacerbated by increasingly complicated architectures and programs, both of which can negatively impact performance reproducibility. Wrong conclusions could be drawn if variability is not handled correctly. Using a simple geometric mean cannot describe the performance variability of computers. Recently, computer architects have been seeking advanced statistical inferential tools to address the problem of performance comparisons of computers. The two common statistical approaches of comparing two populations are the hypothesis test and confidence interval estimation. As we know, most of the parametric tests such as t-tests require population distribution normality. Unfortunately, computer performance measurements are often not normally distributed but either skewed or multimodal. We can see that the distributions of performance measures for the benchmarks are non-normal; benchmarks “gcc” and “mcf” are skewed to the right, while “bzip2” is multimodal. This non-normality observation was first observed by Chen et al. who tackled with a non-parametric statistics method named hierarchical performance testing. In this paper, we propose three statistical resampling methods to evaluate and compare computer performance. The first is a randomization test used to compare the performance between two computers; the second is a bootstrapping confidence interval method for estimating the comparative performance measurement, i.e., speedup, through a range; and the third is an empirical distribution method to evaluate the distributional properties of computer performance. The basic idea of resampling methods, as the name implies, is to resample the data iteratively, in a manner that is consistent with certain conditions. Specifically, we first resample the data according to the purpose of each method. Second, for each iteration, we calculate the statistic of interest, such as the ratio of geometric means between two computers. Third, we repeat the previous two steps a number of times. Then the distribution of the calculated statistic is used as an approximation of the underlying distribution of the statistic under the assumed condition. Hence, the resampling methods set us free from the need for normal data or large samples so that Central Limit Theorem can be applied. Note that the proposed three methods all follow the three steps described above. However, the resampling and calculating steps within each iteration are different according to the individual purpose for each method. In summary, the main contributions of this paper can be listed as follows: First, we propose and implement a randomization test for testing the performances of two computers, which provides an accurate estimate of the confidence of a comparison when the performances of two computers are close to each other. Second, we propose and implement a bootstrapping-based confidence interval estimation method to estimate the confidence interval of the ratio of geometric means between two computers. Third, as a generic framework, the proposed method can directly be applied to arithmetic and harmonic means. We demonstrate that the arithmetic mean is very sensitive to outliers while geometric and harmonic means are much more stable. Fourth, we point out that a single test is not enough to reveal the nature of the computer performance in some cases due to the variability of computer systems. Hence, we suggest using empirical distribution to evaluate computer performance and use five-number-summary to summarize the computer performance. Fifth, we investigate the source of performance variation by predicting the performance and relative variation of machines running the SPEC 2006 benchmark suite using published hardware descriptions and environment variables. Sixth, we demonstrate the effectiveness of the proposed sampling methods on Big Data benchmarks which have more variation behaviors than traditional CPU benchmarks like SPEC or PARSEC. Finally, we use a Biplot visualization tool for computer performance comparisons which can visualize the results.", "section": "Introduction", "doi": "10.1007/s11704-018-7319-2", "references": [1555915743, 1651603302, 1985938075, 2032593675, 2059710695, 2072916763, 2097879961, 2102201375, 2103648619, 2120909947, 2140163235, 2141386638, 2149207009, 2150478767, 2153530968, 2166693749, 2168155345, 2169875292, 2496760721, 2526539616, 2902245301]}
{"paragraph": "In uncertain industrial environments, due to aging in equipments, and under external stochastic disturbances, sensors often suffer from extra errors, especially when measurement data are transmitted through public wireless networks. To account for such phenomena, one way is to introduce a disturbance vector vn, which represents all possible errors experienced by a sensor, and include it additively to the measurements equation yn=gn(xn)+vn, in which xn denotes the process state vector and gn(xn) is a nonlinear function mapping xn into the measurement vector yn. To apply methods of optimal filtering, vn is typically assumed to be zero mean Gaussian with some covariance Rn governing the uncertain degree of yn. Of importance is that the noise vn intensity may vary with time (e.g., impulsive or slow changing) as caused by a number of external and internal factors. That makes its covariance Rn time varying, especially under harsh industrial conditions. To illustrate this point, Fig. 1 taken from sketches a measured trajectory of a robot traveling circularly within the ranges of two radio-frequency identification (RFID) tag/sensors T1 and T2. A specific of this example is that the measurement noise intensity grows when the robot approaches the line linking two RFID tag/sensors. Note that the effect can be reinforced in uncertain environments, which may affect the sensor noise and lead to localization faults. To avoid problems with time-varying noise intensities, sensors should be monitored to indicate their normal or abnormal operation and draw a quantitative picture about measurements (mainly show the uncertainties) when something went wrong. In recent decades, a number of methods have been proposed for sensor monitoring, such as the principle component analysis-based methods, parity space approaches, robust state observers, identification-based methods, and data-driven methods of process monitoring. Among these and other methods, the approach based on Kalman filtering (or state observer based approaches for deterministic systems) is recognized as fundamental. Its key concept is to consider a residual signal, present the inconsistence between observed and expected behaviors, and then provide a residual analysis and/or statistical tests for decision making. However, the Kalman filter (KF) based solutions require the measurement noise covariance in advance, which is practically unavailable in most of the cases. But even if to account for some abnormalities by choosing an appropriate noise covariance, important information about the potential abnormalities will still remain missed, such as about location, types, and magnitude. In view of the abovementioned methods, a challenging problem is to learn the noise covariances from measurements in real time. Some related results can be found in, where solutions are found under the concept of “adaptive filtering”. For example, in one study, both the process noise covariance and the observation noise covariance are estimated by analyzing the innovation sequence. In another, an optimization procedure is incorporated into the KF with a purpose of estimating the process noise covariance. By imposing the prior conjugate distribution, the posterior distribution of the measurement noise covariance is inferred for linear and nonlinear processes, respectively. However, these methods were developed only for estimation purposes and cannot be applied for sensor monitoring. A solution to the latter problem has been proposed under the assumption that all of the sensors are uncorrelated and the model is linear. But still no results were addressed to nonlinear process. In this regard, let us notice that not only unexpected fluctuations in sensors can be reflected satisfactorily, but also a quantitative description of abnormalities or faults can be drawn in linear and nonlinear systems, if to provide estimation of measurement noise covariance on finite horizons. In this paper, we remove both the linear and uncorrelated assumptions made earlier and propose a new sensor monitoring algorithm for a wider scope of applications. An important specific of the proposed method is that the estimated probability density function (PDF) of the state is overlapped by a set of weighted particles, while the PDF of the measurement noise covariance is represented analytically by the inverse-Wishart distribution. The main contribution of this paper is summarized as follows. The sensor monitoring algorithm is derived for correlated sensors in nonlinear state space, from which some important information for trouble shooting is efficiently extracted. The particle approximation technique is incorporated into the variational Bayesian (VB) inference to project the intractable VB marginal of the state to the alternative empirical distribution. The state PDF is estimated in the presence of sensor faults and/or abnormal operation without extra efforts that makes the proposed algorithm useful for real-time applications. The rest of this paper is organized as follows. In Section II, some preliminaries are introduced and the problem is formulated. In Section III, the sensor monitoring algorithm is derived by estimating the PDFs of the state and the measurement noise covariance simultaneously by using the particle approximation. A numerical example of localization and an experiment conducted with rotary flexible joint are presented in Section IV to demonstrate the effectiveness of the proposed method. Finally, conclusions are drawn in Section V.", "section": "Related Work", "doi": "10.1109/TIE.2019.2907505", "references": [1482540049, 1948379568, 1984786788, 1985865685, 2010922160, 2015826698, 2026755043, 2061893297, 2072664415, 2102832680, 2122246449, 2126790865, 2147129131, 2160337655, 2171911691, 2319186869, 2343246112, 2549039965, 2607855551, 2806411441]}
{"paragraph": "Classification is an important topic in machine learning. By removing irrelevant or redundant features, a feature selection (FS) algorithm can effectively reduce the dimension of data, shorten the learning time, and improve the classification performance. During the past decade, numerous FS algorithms have been proposed. Among them, meta-heuristic methods have shown a lot of advantages in dealing with feature selection problems due to their powerful exploration capabilities. Meta-heuristic feature selection methods include genetic algorithms, ant colony optimization, particle swarm optimization, firefly algorithm, memetic algorithm, artificial bee colony, grasshopper optimization algorithm, evolutionary gravitational search, etc. Generally, feature selection can be modeled as a multi-objective combinatory optimization problem, which mainly contains two objectives, the number of the selected features and the classification accuracy. In some cases, removing irrelevant and redundant features from the original datasets can improve the classification accuracy of classifiers. In other words, the feature size can be reduced while the classification accuracy is improved. However, when a dataset only contains the key features that must be used by classifiers, deleting any feature may increase the error rate of these classifiers. Here, the two objectives are conflicting with each other. Moreover, obtaining the report values of different features often needs different level costs, such as time, money, or other resources. A large number of features usually means a high cost. Therefore, reducing the number of the selected features is also an important indicator. Moreover, formulating a feature selection problem as a multi-objective optimization one is beneficial in obtaining a set of optimal feature subsets so as to meet various requirements of decision-makers. Multi-objective evolutionary algorithms can seek multiple solutions lying in the Pareto optimal front in a single run. The aforementioned algorithms have been widely applied in handling the feature selection problems. Because of its simplicity and efficiency, differential evolution (DE) is a very popular evolutionary algorithm used in the feature selection problems. However, most of the current study focuses on the single-objective case, maximizing the classification accuracy. There is little work on applying the DE to the multi-objective case. Multi-objective DE has been used in feature selection and applied to tasks such as multi-label feature selection, entity extraction in biomedical texts, and facial expression recognition systems. The results obtained show the effectiveness of the multi-objective DE in handling the feature selection problems. However, these approaches have the following disadvantages. Most of them adopt the DE/rand/1/bin strategy to generate candidate individuals, where the base vector in the mutation is randomly chosen from the population. Due to the randomness of the base vector, this strategy makes the population yield a good exploration performance but slows down the overall convergence. The elite individuals in the population only take responsibility for guiding the search of other individuals. Improving the self-learning ability of the elite individuals may enhance the exploration of the whole population. Therefore, how to boost the global exploration of the DE without sacrificing its convergence needs new effective strategies to be developed. To improve the capability of the original DE in dealing with the multi-objective feature selection, the present paper studies a new binary differential evolution with a self-learning strategy, namely MOFS-BDE. In our algorithm, the DE is responsible for exploring the search space and finding potential regions, while a self-learning strategy is utilized to effectively exploit these potential areas. The main contributions of this paper are as follows: a binary differential evolution algorithm with a self-learning strategy, MOFS-BDE, is proposed to attack the multi-objective feature selection problems; a new binary mutation operator based on the probability difference is designed to generate fresh solutions. Since the base vector is always the best one among the three randomly generated vectors, this operator can guide individuals to locate potentially optimal areas in a fast way; a new problem-specific self-learning strategy, namely one-bit purifying search, is proposed to refine the elite individuals in the population. Thus, the elite individuals not only take the responsibility of guiding the search of other individuals, but also have the self-learning capability; an efficient non-dominated sorting combined with crowding distance is employed to select appropriate parent individuals so as to reduce the time consumption of the selection operator in the regular differential evolution. The structure of our paper is as follows. Section 2 introduces the background of the feature selection problems, and Section 3 reviews some of the existing evolutionary feature selection approaches. The standard and modified DE are discussed in detail in Sections 4 and 5, respectively. Section 6 presents experimental results of the proposed MOFS-BDE. Finally, a few conclusions and remarks are given in Section 7.", "section": "Methodology", "doi": "10.1016/j.ins.2019.08.040", "references": [1418108976, 1607660464, 1833634424, 1964537798, 1970032027, 1970769764, 2013885787, 2019511760, 2027425155, 2058118608, 2069928051, 2070556583, 2083945868, 2093010752, 2105223996, 2106334424, 2126105956, 2131090729, 2143381319, 2156194072, 2162043657, 2165885026, 2210287443, 2254043540, 2343420905, 2397430692, 2411885377, 2561576489, 2588659412, 2593421093, 2604772753, 2616650140, 2724365251, 2736583283, 2739135567, 2742990887, 2745573163, 2754840697, 2757662287, 2765937321, 2776226778, 2790793279, 2791239441, 2801536506, 2932046728, 2945163040, 2953877793]}
{"paragraph": "Detecting line segments in images is a long standing problem in computer vision. A classical approach to the problem relies on a global Hough transform. More efficient methods implement in various ways the principles of perceptual grouping, by grouping local clues, which are usually obtained from the gradient of the image. Recently, it is proposed to combine a global Hough transform and local grouping. The most recent methods rely on deep learning, but to the best of our knowledge, none of these methods addresses the point of line segment detection in images with strong noises. One domain where the levels of noise are extremely high is Synthetic Aperture Radar imaging, where images are impacted by very strong speckle noise, a noise that is inherent to all coherent imaging systems. While the detection of linear features in SAR images has received a lot of attention, typically in view of the detection of road networks, reliably detecting line segments is still an open problem. Nevertheless, line segments are very important features in SAR images, mostly because many man-made objects like buildings, farmlands or airports can be described by line segments. Besides, most geometric structures can be approximated by line segments. In addition, line segments can be extracted as low level features and then be used for tasks such as image registration and target recognition. Due to the strong speckle noise, methods that are effective for optical images cannot be straightforwardly applied to SAR images. First, the usual assumption that noise is additive and Gaussian is wrong. Second, and more importantly, the strong level of noise encountered in SAR images makes most optical approaches inefficient. Taking the logarithm of the amplitude or intensity of SAR images can change multiplicative noise to additive noise but this does not allow the plain application of optical methods, as we will see in the experimental section in the case of the LSD detector and a recent line segment detector AFM which is based on deep learning. The usual way to detect line segments in SAR images is global and relies on the Hough transform. First, a constant false alarm rate edge detector is applied to the image, followed by a Hough transform to detect lines. Then, post processing steps are applied to localize Hough lines into line segments. Many methods of this kind have been proposed for SAR images following the early work, in the context of different applications. In some cases, line segments are extracted by the Hough transform and then used to reconstruct buildings from meter-resolution multi-aspect SAR images. An optical-to-SAR image registration method has been proposed, relying on line segments that are detected using a ratio-based gradient and the Hough transform. The same idea was previously explored. In other cases, edge detection using phase symmetry and wavelet correlations is followed by a Hough transform in order to detect ship wakes. A common limitation of these approaches is that the performance of the Hough transform critically relies on both a preliminary edge detection and on the selection of parameters. The input of the Hough transform is usually a binary edge map. Many dedicated methods have been proposed for SAR images to compute gradients, but extracting a binary edge map necessitates a difficult compromise between suppressing false alarms due to speckle and preserving edges of low contrast. Besides, the corresponding threshold choices are strongly image-dependent. An interesting approach, which was recently proposed, detects lines from the magnitude field instead of a binary edge map, but the subsequent detection tasks still require non-trivial parameter tunings. The LSD detector, which is based on the a contrario model and the Helmholtz principle, is considered to be one of the most celebrated line segment detector. The goal of the present paper is to develop a LSD-like line segment detector for SAR images, which results in a very challenging task. Indeed the LSD detector, as most a contrario approaches, relies on a null hypothesis against which segments are detected. Unfortunately, this null hypothesis is completely inadequate for SAR images. More precisely, local orientations, each of which being defined as the direction perpendicular to the gradient orientation, are grouped against the hypothesis that they are uniformly distributed and mutually independent. Both these hypotheses appear to be structurally wrong in SAR images. First, classical ways to compute the gradient in SAR images yield a non uniform distribution of the orientation, even in the absence of geometrical structures. Second, the speckle noise imposes the use of strong filtering schemes, implying strong structural dependencies between nearby orientations. In the proposed LSDSAR approach, we replace the gradient computation with a ratio based method, which yields robust and unbiased local orientations at each pixel. Further, we replace the crucial independence hypothesis between local orientations by a first order Markov chain modeling, which in practice is enough to counterbalance the effect of filtering and yields an efficient control of the number of false detections. The result is a generic line-segment detector adapted to the specific structure of SAR images. This paper is organized as follows: in Section 2, we give a description of the original LSD algorithm. In Section 3, the new line segment detector is provided. In particular, the computation of the gradient is detailed, as well as the use of a first order Markov chain in the a contrario model. Line segment detection results on both synthetic and real SAR images are given in Section 4 and are compared to the results of state-of-the-art line segment detectors. We finally conclude in Section 5 and present some perspectives.", "section": "Related Work", "doi": "10.1016/j.patcog.2019.107034", "references": [1504108853, 1983184501, 1990200755, 1995376165, 2004491626, 2021278996, 2030233108, 2038563371, 2055568811, 2070041365, 2095905764, 2101853874, 2113654450, 2119764380, 2121796624, 2135049578, 2138055993, 2138106400, 2139381235, 2140753314, 2142572935, 2144506334, 2144572173, 2145023731, 2145158371, 2156953558, 2160072137, 2167566599, 2294634632, 2307048845, 2737353725, 2752347180, 2798943056, 2902517736, 2913455019]}
{"paragraph": "Quadrotors have received significant attention from the research fields because of their low cost and easy maintenance. An important application of quadrotors is payload transportation especially in areas without a suitable landing zone, such as forests and mountains. Conventionally, payloads are contained in a cargo compartment attached to the bottom of the aircraft. In recent years, delivering the payload by connecting it to the vehicle with cables has also been researched extensively. In these papers, the payload is assumed to be a point mass connected to the quadrotor by a cable. Quadrotors can load and unload cable-suspended payloads without taking off and landing, saving time and energy. Therefore, a cable-suspended payload configuration provides potential benefits to extend the usage of such vehicles. The lift vector of a quadrotor is fixed along the z-direction of the body fixed frame, so the quadrotor propellers usually can only control the translational motion by tilting the vehicle to a reference direction. Meanwhile, the cable and the payload form an uncontrolled pendulum system. Therefore, the entire system is underactuated, giving the control design many challenges. For such systems, commonly a translational outer loop controller is first determined, and then, an attitude tracking controller is used to asymptotically point the quadrotor to a reference attitude. This methodology is called cascade design technique. The controller first drives the cable to a reference direction so that it can provide the correct force to manipulate the payload motion, and then, the actual lift is a torque control on the quadrotor. Since the control law needs the payload position and velocity information, an estimation-based control has been proposed to make the control law suitable for practical use. Retrospective cost adaptive control has been proposed to deal with payload mass uncertainty. A method to manipulate the load with a flexible cable under fixed disturbances on the quadrotor has been proposed. A partial feedback linearization control law has been proposed to deal with the off-centered tethered payload. Recently, a controller without payload motion feedback has been proposed to facilitate the position stabilization. Wind disturbances caused by air drag are the main external forces that affect the performance of the system, so it must be considered in the control design for accurate transportation task. While it is complicated to model the drag on an irregular quadrotor, approximating the drag as a linear function of the airspeed is a reasonable simplification at low speed. Since the airspeed of an object is the sum of its inertial velocity and the wind velocity with respect to the ground, we can decouple the total wind disturbance as the sum of the cruising drag caused by the inertial velocity of the object and the wind force caused by the wind velocity. The airspeed of a quadrotor can be in any direction relative to the body fixed frame, so additional wind sensors are required to provide an accurate reading of the airspeed in all directions, which increases the cost and system complexity. For quadrotors with powerful motors, the propeller vortex is intense and can affect the sensor readings. Therefore, no wind sensor is used in this paper. The quadrotor motion is measured by an inertial measurement unit and GPS. The payload swing motion can be measured by a camera system. The complete velocity profile of a wind field is complicated. However, a changing wind field can be viewed as a constant wind velocity plus a zero-mean time-varying gust. A reasonable practice in many published works is to treat the gust as its mean, i.e., 0, and to approximate the wind field with a constant wind velocity or constant drag. The performance of the controller with this approximation in a changing wind field is tested in simulations or experiments. Studies have been done to estimate and compensate the wind disturbances. Adaptive control is the prevailing way to deal with constant exogenous disturbances and unknown system parameters. A nonlinear H∞ controller combined with the Lyapunov redesign method has been proposed to achieve path tracking without load swing. The flapping effects along with a computational approach to estimate the wind field have been introduced. While adaptive control is suitable to deal with constant unknown parameters, it introduces large fluctuations into the system. The adaptive law is valid under the assumption that the unknown parameters are constant, so the robustness of the adaptive control is limited under changing wind forces. The uncertainty and disturbance estimator provides an alternative way to design the estimator as a filtered result of the true disturbances. Compared with computational methods and H∞ control, UDE requires less computational power and is less restrictive in performance. By introducing a stable linear filter together with the system dynamics, we can obtain the estimation law with only state feedback. In addition, UDE can capture both the constant and the low-frequency components of the disturbances, making it more robust than adaptive control in dealing with time-varying disturbances. This paper provides a novel UDE-based path-following control law for a quadrotor UAV with a cable-suspended payload under wind disturbances using the cascade design methodology. The equations of motion of the system are adopted. The cable is attached on the center of mass of the quadrotor so that the coupling between the attitude and translational dynamics do not need to be considered as previously pointed out. Even if the tether point offsets slightly from the center of mass, the model is reasonable when the angular acceleration of the quadrotor is small. We formulate the problem as a path-following task along a set of interconnecting straight lines approximating a curved path to obtain a simplified and practical control law for engineering implementation. The novelty includes the following. A novel asymptotically stable UDE-based translational control law is proposed. The translational controller can simultaneously eliminate the effect of the cruising drag and the wind forces. Moreover, at near hovering state where the cruising drag can be neglected, the closed-loop system is almost globally asymptotically stable. The reduction theorem is used. Conventionally, after adding an attitude tracking controller to facilitate the translational control law, an extended Lyapunov function is required to show the stability of the complete system. With the help of the reduction theorem, we can guarantee the stability without a new Lyapunov function and decouple the design of the translational control law and the choice of the attitude tracking law, giving more room for future modifications. The rest of this paper is organized as follows. Section II provides mathematic preliminaries. Section III contains the system model and the problem formulation. Section IV is the control architecture. Section V contains the stability analysis. Sections VI and VII are simulation and conclusion, respectively.", "section": "Methodology", "doi": "10.1109/TIE.2019.2905811", "references": [1500759886, 1501199254, 1988444843, 1996454263, 1997402176, 2044894195, 2063554272, 2132785145, 2134069001, 2140515911, 2491164576, 2649974662, 2756177107, 2762267649, 2783345620]}
{"paragraph": "Dealing with discrepancies between properties of the real plant and its mathematical description has become one of the main problems of modern control theory, and many nonlinear control techniques have been developed to reduce the adverse effects of external disturbances, unmodeled dynamics, and parameter uncertainties. Control design problems caused by such discrepancies may be approached in at least two ways. One involves robust control and the uses of feedback principles to suppress disturbances. However, such methods can present difficulties in terms of estimation of the disturbances or may tend to overestimate their upper bounds. Also, the resulting gains of the state-feedback controllers tend to be large in order to provide enough control effort to reduce the effects of the disturbances. It appears that such robust methods can lead to a worst-case-based design and the large values of gain factors can yield unsatisfactory dynamic and steady-state performance within the closed-loop system. The second approach is based on composite control principles and generally involves two steps. The first step is to design a baseline feedback controller to satisfy the desired performance specifications for the nominal system (i.e., the nonlinear system without considering the disturbances). The second step is concerned with disturbance attenuation and, in the approach considered here, a disturbance observer (DOB) is used to estimate the disturbances. The estimated value is then taken as a feedforward term for compensation. The baseline feedback controller plus the feedforward term constitute the composite controller. It is obvious that the feedforward term attenuates the adverse effects of the disturbances, whereas the baseline feedback controller provides the required steady-state and dynamic performances and also suppresses any remaining disturbances. Owing to the feedforward nature of the compensation and satisfaction of the prescribed specifications through use of the baseline feedback controller, the composite control schemes should provide excellent tracking performance and smooth control actions without the use of large feedback gains. The fundamental idea of the DOB is to bring together all the internal uncertainties, external disturbances, parameter uncertainties, and unmodeled dynamics as a single lumped disturbance term and then estimate this term by designing an observer. As described in the DOB was developed by Ohnishi et al. in the early 1980s. The basic DOB block diagram was first proposed in terms of the frequency domain. One important element of this block diagram is a low-pass filter, which allows the disturbance to be estimated in a low- and medium-frequency range but with high-frequency measurement noise filtered out. Methods for design of this low-pass filter, which is of central importance in the frequency-domain DOB approach, can be found. It should be noted that the conventional frequency-domain DOBs require the linear systems being considered to have minimum-phase properties. The DOB design method has been extended recently to nonminimum-phase systems. However, it should also be noted that the systems considered are required to be linear or, alternatively, their nonlinear parts must be lumped together as a part of the disturbance variable. For some inherently nonlinear systems, the estimation performance with the linear DOB approach, outlined above, may not be satisfactory, since the design of the linear DOB is based on the linearization model of the nonlinear systems. Indeed, for many practical systems, the nonlinear dynamics are partially known. If these nonlinear dynamics can be taken into account, the performance in terms of estimation of the unknown lumped disturbances may be significantly improved. The first nonlinear DOB was developed by Chen, where a nonlinear DOB was constructed to estimate the disturbance torques caused by unknown friction in nonlinear robotic manipulators. Lyapunov analysis was used to verify the stability of the proposed nonlinear DOB system. The nonlinear DOB described was investigated further in the context of control performance improvements in a missile autopilot system. This paper led on to a proposal for a general framework for nonlinear DOB design, where the disturbance is generated by an exogenous system. Another method, which may also be applied to the DOB design problem, involves reduced-order multiple observers. The main advantage of the reduced-order multiple observer approach relates to the decrease of the number of system sensors required. The concept of the reduced-order multiple observer was developed first where a reduced-order multiple observer was constructed for Takagi–Sugeno (T–S) systems with unknown inputs. Later, through the results, a new reduced-order multiple observer was developed to achieve the finite-time reconstruction of the system's states associated with T–S multiple models. In addition, DOB techniques have been applied widely to many practical applications, such as the guidance law design for a hypersonic gliding vehicle, the attitude control of a spacecraft, and the direct yaw-moment control of an electric vehicle. It may be seen that the approaches presented require system state variables to be available. This implies that if some state variables are not measurable, these nonlinear DOB design methods cannot be applied. It should be pointed out that an output feedback design approach for single-input single-output (SISO) systems has been studied, where a nonlinear DOB is constructed without measurements of the state variables. The proposed nonlinear DOB recovers not only the steady-state performance, but also the transient performance of the nominal closed-loop system in the presence of plant uncertainties and input disturbances. The results have also been extended to the multi-input-multi-output (MIMO) case. However, the systems considered are characterized by some special structures and this restricts the application of this DOB design method. Motivated by these important observations, this paper aims to develop a nonlinear DOB design method that uses only the output of the nonlinear system. The basic block diagram structure for a DOB system of this kind is shown in Fig. 1 where the plant output variable provides the input to a nonlinear inverse model block. In general, inverse dynamic models allow time histories of input variables to be found that correspond to a given set of output time history requirements. Thus, if one can construct the inverse dynamics of a nonlinear system, the disturbance can then be directly estimated using the type of approach suggested by the block diagram of Fig. 1. Inverse models of nonlinear systems have received much attention in recent years and several inverse simulation methods are available that allow inverse dynamic models to be implemented. Some of these were developed for specific application areas such as fixed-wing aircraft and helicopter handling qualities and agility investigations but have also been used with success for a number of other types of problems. The most widely used approaches have involved iterative methods and a useful review of these techniques, as developed for aeronautical applications, has been provided. Other approaches have been developed that are based on continuous system simulation principles and the most important of these has origins that can be traced back for more than 60 years to the use of feedback principles for operations, such as division and inverse function generation, in electronic analog computers. In more recent years, the method was developed further and applied to more general problems of inverse modeling and simulation, as described. This approach has been adopted for the application considered in this paper, leading to development of a new form of nonlinear DOB, which can be constructed for a class of nonlinear systems represented by input–output differential equations. As described, the inverse of the nonlinear system can be developed in a direct fashion using the ordinary differential equations, which describe the plant model simply through the addition of high gain feedback. It has been shown that analysis based on linear minimum-phase models can be extended to the nonlinear case, not only for smooth nonlinearities, but also for saturation and rate-limited dynamics. In the context of DOB design, those results imply that the disturbance may be estimated by subtracting the control input in the DOB block diagram from the reconstructed input obtained from the inverse simulation model. This offers the possibility of extending the inverse simulation approach based on feedback to provide a new form of nonlinear DOB for system described by nonlinear input–output ordinary differential equations. There are two main contributions in the paper. The first one is that a new DOB design method has been developed using only the information from the output and the control input, whereas the conventional nonlinear DOB requires the information relating to all the state variables. The second contribution is that the basic idea of developing the proposed nonlinear DOB is to construct the inverse of the original system, which is analogous to the conventional frequency-domain DOB.", "section": "Methodology", "doi": "10.1109/TIE.2019.2898585", "references": [2058740270, 2073336319, 2086767939, 2093219038, 2107054843, 2119936308, 2167808181, 2230652337, 2559541357, 2568361892, 2594781583, 2595738348, 2742569709, 2742592180, 2748648378, 2759033590, 2768549578, 2769090091, 2787616751, 2790841337, 2792683898, 2793526435, 2799616795, 2805674684, 2883558842, 2888308857]}
{"paragraph": "As is well known, the Markovian jump systems have been widely employed to model many practical engineering systems, such as manufacturing systems, power systems, economic systems, and communication systems. As such, it is not surprising that the control problem for the Markovian jump systems has become a tremendous hot topic. For the purpose of dealing with the uncertainties, a number of control schemes have been reported, such as the H-infinity control schemes, disturbance-observer-based control schemes, and sliding mode control schemes. However, it should be noticed that the mentioned literature restricts its attention on the Markovian jump systems without nonlinearities or with the nonlinearities obeying the Lipschitz condition. To handle this, very recently, several adaptive sliding mode control schemes have been designed for the non-Lipschitz Markovian jump nonlinear systems. Note that in these works, the mismatched or unmatched disturbances have not been considered which is pervasive in engineering practice. To overcome such a limitation, the so-called backstepping approach has been put forward. Despite the progress, in the aforementioned literature, it has been implicitly assumed that the unknown parameters never vary with the switching modes. Clearly, for the Markovian jump nonlinear systems with randomly jumping unknown parameters and Markovian switching unknown functions, which are encountered more often in practice, the control approaches are not capable of achieving desired control performances. Moreover, it should be pointed out that, to the best of the author’s knowledge, the Markovian jump nonlinear systems in strict-feedback form have never been investigated. On the other hand, the actuator failures are also often encountered in a wealth of realistic applications, such as robotic systems, spacecrafts, and hypersonic vehicle systems. It has been well recognized that the actuator failures are one of the main causes which could degrade the control performances. To solve this problem, a lot of effective control methods have been proposed in the past few decades. Several passive fault-tolerant control structures have been developed according to robust control theory. Furthermore, to achieve the adaptive capability for actuator and sensor faults, a number of active fault-tolerant controllers, which possess reconfigurable structures, have been proposed. Note that in the aforementioned results, the randomly changing actuator failures which often exist in practice have been rarely investigated. Most recently, two fault-tolerant schemes have been developed for Markovian jumping actuator faults. However, the parameters and the structures of the controlled systems in these works are never allowed to vary with the Markovian modes. Therefore, the designed controllers in these studies are not capable of achieving the desired performances for the concerned Markovian jump nonlinear systems with randomly switching actuator failures. Furthermore, to obtain the wanted control performances, the unmodeled dynamics has to be adequately handled. As is well known, the unmodeled dynamics, also denoted as the ignored dynamics or dynamic uncertainties, widely exists in engineering practice, which might lead to the instability of the controlled systems. In recent years, a great deal of literature has been reported on control of systems with unmodeled dynamics. However, up to now, the control schemes have not been investigated for the dynamic uncertainties with Markovian switching parameters and randomly jumping structures. In this paper, a novel adaptive fuzzy control scheme is developed for a class of strict-feedback Markovian jump nonlinear systems with randomly varying uncertainties and Markovian jumping actuator failures. The main challenges stem from the immanent randomly jumping characteristics of the parameters and structures in the considered system. It is worth noting that it is difficult to estimate the unknown parameters which keep randomly changing and few results have been reported. Additionally, the mismatched and random features of the uncertainties bring about more challenges in the controller design. In this paper, by estimating the upper bounds of the unknown parameters, the difficulties caused by the Markovian jumping unknown parameters and actuator failures are circumvented. Meanwhile, the unknown nonlinearities are handled by the fuzzy logic systems. An adaptive fuzzy control approach is finally synthesized in the context of the adaptive backstepping technique. Compared with the existing literature, the main contributions of the proposed method are highlighted as follows. The established control method first solves the tracking control problem of the strict feedback Markovian jump nonlinear systems with randomly changing actuator faults and Markovian switching dynamic uncertainties. As far as the authors know, it is also the first attempt to tackle the tracking control problem for the pure Markov jumping strict-feedback systems in the sense that the parameters and structures of the Markovian jump nonlinear systems are permitted to be Markovian switching. By using the proposed method, the transition rate matrix is allowed to be completely unknown. The structure of this paper is provided herein. Section II provides the problem description and several imperative definitions. In Section III, the adaptive fault-tolerant tracking controller is designed. The convergence analysis is also given in this section. The simulation results are shown in Section IV. In this paper, AT represents the transpose of matrix A, and the norm represents the Euclidean norm. The probability space is represented by the sample space, the sigma-algebra of subsets of the sample space, and the probability measure on the sigma-algebra, respectively. The expectation operator represents the mathematical expectation, and the probability operator represents the probability.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2865677", "references": [1564089118, 1842211470, 1967964192, 1971410285, 1984285111, 2010108030, 2024677911, 2034320245, 2054500645, 2066374800, 2069516953, 2083601177, 2132179668, 2133085788, 2217539471, 2341465751, 2343288771, 2344237528, 2411793894, 2462438590, 2512467232, 2560451446, 2603089571, 2619461977]}
