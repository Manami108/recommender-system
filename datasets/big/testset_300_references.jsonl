{"paragraph": "Generally, most of the protocols are designed for homogeneous WSNs where each SN is equipped with same energy level at the beginning of network. However, during network execution, these are later transformed into heterogeneous WSNs. This is due to the variation in energy dissipation of each SN due to radio communication features, occurrence of random events or morphological attributes of the field during network execution. This reason gives birth to the fact that an efficient clustering protocol must be capable of handling both homogeneous and heterogeneous WSNs competently. In this paper, we extend the optimized-HEED protocols for heterogeneous WSNs model based on varying levels of node heterogeneity (in terms of energy), i.e., 1-level, 2-level, 3-level and multi-level, and propose these as heterogeneous optimized-HEED (Hetero-OHEED) protocols. Hetero-OHEED protocols consist of heterogeneous HEED-1 Tier Chaining (hetHEED1TC), heterogeneous HEED-2 Tier Chaining (hetHEED2TC), heterogeneous ICHB-based HEED (hetICHB-HEED), heterogeneous ICHB-based OHEED-1 Tier Chaining (hetICOH1TC), heterogeneous ICHB-based OHEED-2 Tier Chaining (hetICOH2TC), heterogeneous ICHB-FL-based OHEED-1 Tier Chaining (hetICFLOH1TC) and heterogeneous ICHB-FL-based OHEED-2 Tier Chaining (hetICFLOH2TC) protocols. In 1-level of Hetero-OHEED protocols, each node has same energy level at the beginning of the network and behaves as homogeneous WSN. It consists of het1-HEED1TC, het1-HEED2TC, het1-ICHB-HEED, het1-ICOH1TC, het1-ICOH2TC, het1-ICFLOH1TC and het1-ICFLOH2TC protocols. For 2-level of Hetero-OHEED protocols, WSN is equipped with 2-level of heterogeneous nodes which consists of two types of SNs, initialized with different energy levels at the beginning of the network. It includes het2-OHEED protocols. Likewise, 3-level of Hetero-OHEED protocols contains three types of SNs with varying energy levels initialized at the beginning of the network. It consists of het3-OHEED protocols. In multi-level of Hetero-OHEED protocols, WSN is equipped with different heterogeneous nodes under a close set of varying energy levels. It includes mul-OHEED protocols. Here, we analyze the functional behavior and performance of Hetero-OHEED protocols in heterogeneous WSNs. During the simulation results and analysis, it has been observed that on increasing the level of node’s heterogeneity, the performance of Hetero-OHEED protocols improves far better. Notably, the stability region of each Hetero-OHEED protocol enhances with prolonged network lifetime. This confirms that Hetero-OHEED protocols are capable of providing a rich solution to those WSN’s applications, where stability region and network lifetime have equal importance.", "section": "Introduction", "doi": "10.1007/s00500-019-04000-8", "references": [1449224687, 1966957729, 1971282865, 1974201661, 1979044357, 1981606254, 1989022292, 2007485183, 2020919367, 2026717594, 2027984087, 2045034956, 2047990986, 2054897420, 2060274297, 2067028650, 2071443469, 2072822315, 2081989110, 2093770889, 2094117776, 2104846640, 2106335692, 2108880818, 2119106551, 2138373774, 2143938200, 2155492251, 2203808635, 2221535557, 2271178920, 2401736114, 2440402910, 2754711490, 2791802072, 2896165748]}
{"paragraph": "Recently, Evangelidis and Horaud introduced clustering as an effective means to solve the multi-view registration problem. Although this approach belongs to a probabilistic approach and may be accurate, its efficiency and robustness is suspicious. Given this consideration, we formulate multi-view registration as a joint clustering and alignment problem, solving it with a novel approach based on a well-known clustering paradigm, viz., the K-means algorithm. In what follows, we first describe our design of a new objective function and then present the proposed multi-view registration approach. Given initially aligned point sets, the proposed approach starts by estimating the initial centroids. Then, clustering is performed. This consists of assigning each point to a single cluster and then updating all of the cluster centroids. Insofar as the multi-view point sets are not well aligned, the shape composed of all the updated centroids can be used as a reference point set to estimate the rigid transformation for each point set sequentially by pair-wise registration. To obtain the desired results, clustering and transformation estimation are alternately and iteratively applied to all point sets. For 3D model reconstruction, K-means clustering was initially introduced by Zhou et al., where it was utilized to detect and merge the corresponding points of overlapping areas. However, their approach does not involve updating rigid transformations and can only be viewed as a post-processing step that proceeds after multi-view registration in 3D reconstruction.", "section": "Related Work", "doi": "10.1016/j.ins.2019.03.024", "references": [201974436, 1085399651, 1603720607, 1907775068, 1952323793, 1966383696, 1967368660, 1970765983, 1997201895, 2016003865, 2035237009, 2043863462, 2060318245, 2060558759, 2063549868, 2109323956, 2114467878, 2118026950, 2120436040, 2121603262, 2125989410, 2134236847, 2137699140, 2145828616, 2150190641, 2160821342, 2161160262, 2165874743, 2270425654, 2336961836, 2344109021, 2488386856, 2520913206, 2521851109, 2563669458, 2594495318, 2610614679, 2780100774, 2792346134, 2799460201, 2897298014, 2898233200, 2952529962]}
{"paragraph": "Multiple methods have been introduced that incorporate local intensity information in an energy framework for image segmentation. However, local intensity itself may not be sufficient for accurate segmentation in the presence of heavy noise and intensity inhomogeneity. Recently, Wang et al. characterized an energy equation using local Gaussian distribution fitting of image intensities. This energy was minimized using level set to achieve the brain matter segmentation. He et al. proposed a method that used local entropy feature derived from the grey level distribution of MR image instead of intensity, for segmentation. Further, Huang et al. combined voxel probability, image gradient, and curvature information to achieve brain tissue segmentation. Finally, Popuria et al. used various MRI modalities and their texture characteristics to construct a multi-dimensional feature set to achieve the same task. The above-mentioned studies have proven that a combined feature set is well suited for brain matter segmentation as opposed to a single feature. However, no explanation is provided as to what kind of features best captures the properties of the brain matter in MR images. In this paper, a two-stage segmentation pipeline is used to achieve brain matter segmentation from MR images. The first stage of the pipeline uses a proposed translation, rotation and scale invariant high-dimensional feature derived by combining three sub-features that best represent the structural and intensity properties of the brain matter. The proposed high-dimensional feature is then used by a novel variational level sets method to achieve the desired brain matter segmentation.", "section": "Introduction", "doi": "10.1007/978-3-319-54427-4_43", "references": [1484228140, 1495971627, 1563062848, 1573386842, 1962314142, 1979393293, 1996550590, 1997630379, 2027955893, 2030563091, 2051477566, 2052915721, 2084694925, 2086447102, 2109200236, 2113521994, 2113884510, 2116040950, 2117530999, 2118713142, 2129534965, 2132116135, 2134756983, 2135365788, 2136573752, 2139478903, 2145023731, 2147128996, 2150606601, 2152942156, 2158030993, 2160109814, 2160172739, 2160871494, 2161591846, 2162630772, 2165324514, 2165734775]}
{"paragraph": "Existing algorithms do not measure the transmission distortion in a comprehensive way. These algorithms are capable of working with videos encoded with only I and P frames; however, videos are always encoded with I, P and B frames to transmit over the wireless network. Moreover, these algorithms are not capable of finding the prediction direction and reference frame exactly for each of the MB in any frame. When a video sequence is encoded as I and P frames only, the immediate previous frame is taken as reference most of the time, as a frame is highly correlated with its immediate previous frame in a sequence. Therefore, these algorithms are also able to approximate the distortion. In order to overcome these limitations, a NR algorithm is proposed, which is capable of estimating the transmission distortion for the video sequence encoded as I and P frames and encoded as I, P and B frames also. The proposed algorithm also extracts the exact information about the referenced frame and prediction direction for each of the macroblocks in the frame, enabling the algorithm to be applicable to any GOP structure having varying number of B frames between the P frames. The algorithm takes into account the spatio-temporal dynamics of the video sequence effectively. The algorithm models the effects of the spatial and temporal error concealments, along with the effect of the temporal distortion propagation. The algorithm is compared with the mean squared error method as a measure of the video quality. The algorithm provides estimation of the video quality at the MB granularity level. Simulation results have verified that the proposed algorithm can estimate the transmission distortion effectively and in real time for the video sequence decoded from the H.264/AVC bitstream. The proposed methodology can be adopted for any of the video sequence encoded by using the motion compensated approach. Thus, the paper aims to present an analytical approach to estimate the channel induced distortion which is efficient, simple and deterministic.", "section": "Related Work", "doi": "10.1109/TSIPN.2015.2476695", "references": [2004748331, 2089994895, 2102509245, 2109527809, 2130660429, 2137298097, 2147000487, 2149138362, 2157099212, 2162458147, 2163939254, 2164486011, 2167454998, 2170636967, 2172073677]}
{"paragraph": "We propose scheduling algorithms for the collaborative task execution to conserve the energy on the mobile device under the delay deadline. The proposed scheduling algorithms are based on partial critical path analysis, which enable us to find out the critical path formed by a set of critical parents. Specifically, the critical parent is defined as the parent node of a task that results in the maximum value of the earliest start time of the task. After the identification of the partial critical path, we find its sub-deadline and schedule the tasks on the path using two algorithms based on different application scenarios. Notice that there could be some tasks that must be executed on the mobile device or on the cloud clone; we refer to this as execution restriction. For a special case without the execution restriction, we apply one-climb policy, in which there exists at most one migration from the mobile device to the cloud if ever for the minimum energy consumption. For a general case with the execution restriction, we adopt Lagrange Relaxation based Aggregated Cost algorithm to schedule the tasks on the partial critical path. We conduct performance evaluation for the collaborative task execution. We consider mesh, tree and more general topology as the task organization in the application for the special case and the general case without and with the execution restriction, respectively. Simulation results demonstrate that as the data rate increases, more tasks will be offloaded to the cloud for execution and more energy consumption can be saved for both the special case and the general case. Moreover, the collaborative task execution can save energy consumption compared to the local execution and is more flexible than the remote execution. Thus, by exploring the granularity of tasks in the application, we can have a more energy-efficient collaborative task execution for mobile applications. In addition, the collaborative task execution under the special case without execution restriction consumes less energy than that under the general case with the execution restriction. This suggests that one needs to have a reasonable partition of tasks within the mobile application for energy-efficient collaborative task execution.", "section": "Methodology", "doi": "10.1109/TCC.2015.2511727", "references": [182605591, 1544212906, 1607207171, 1658697598, 1986293551, 2012423440, 2019116263, 2023169239, 2023380813, 2032728524, 2068300842, 2073765234, 2101788345, 2104237724, 2116175219, 2129861682, 2135099885, 2140556610, 2156144168, 2159619460, 2524539616]}
{"paragraph": "A crucial issue in the regularization of ill-posed inverse problems is the choice of the regularization parameter, a subject to which much work has been devoted. The discrepancy principle chooses the regularization parameter so that the variance of the residual, i.e., the difference between the observed image and the blurred estimate, equals that of the noise; the discrepancy principle thus requires an accurate estimate of the noise variance and is known to yield over-regularized estimates. A recent extension of the discrepancy principle uses not only the variance, but also other residual moments. Local residual statistics have also been used to obtain locally adaptive TV regularizers for NBID. Two other popular criteria are generalized cross validation and the L-curve, which, although developed and mainly applied to linear methods, can also be used with non-linear methods, but are outperformed by more recent criteria based on Stein's unbiased risk estimate. Stein's unbiased risk estimate provides an estimate of the mean squared error, assuming knowledge of the noise distribution and requiring an accurate estimate of its variance. While methods for automatically adjusting the regularization parameter are relatively developed for denoising and NBID, the same is not true for BID, with most existing methods requiring the regularization parameters to be somehow tuned or empirically selected. For example, Stein's unbiased risk estimate-based approaches assume full knowledge of the degradation model, thus are not suitable for BID. There are a few methods that address the adjustment of the regularization parameter; however, some of those approaches were developed for Bayesian formulations and do not fit iterative BID algorithms. Finally, we should mention no-reference image quality measures; although proposed for adjusting the regularization parameter in denoising, they can in principle be used in NBID or BID methods.", "section": "Methodology", "doi": "10.1109/TIP.2013.2257810", "references": [1543711598, 1963701614, 1968735445, 1983041613, 1987075379, 1993102688, 1999174140, 2017812570, 2028349405, 2044810215, 2070073359, 2078186482, 2095106682, 2095775020, 2098535678, 2100556411, 2100705753, 2110505738, 2113920645, 2116641053, 2123142395, 2125542221, 2126252886, 2126607811, 2128428741, 2132132164, 2132256814, 2135089338, 2138204001, 2138881789, 2141115311, 2142005762, 2143820350, 2145319246, 2145568341, 2146842127, 2151289376, 2163940565, 2164278908, 2169253121, 2171125155]}
{"paragraph": "A prerequisite for the gain-cost evaluation to be prospectively meaningful as a tool for evaluating different predictors is that the results on the training data also extend to new data. Conformal prediction is a framework for generating confidence predictors that produce predictions with a fixed error rate. This is achieved through evaluating new predictions by comparing them to the predictions of known instances in a calibration set. For binary classification, labels are then assigned to the new instance in a way that can result in four different outcomes: the instance belongs to either of the two labels, both labels simultaneously or none of the labels. Two factors make conformal predictors highly suitable for bioactivity prediction: their ability to accurately predict minority classes, since in a screen there tends to be many inactive compounds for each active, and the ability to control the error rate and thereby limiting the number of false positives. Conformal predictors have previously been successfully applied for bioactivity modelling. As the efficiency, i.e., number of single label predictions, generated by the conformal predictor will vary depending on the confidence level applied, evaluating different confidence levels will identify if it is better to screen a small set of compounds with higher confidence or a larger set but with more uncertainty. This approach also does not require the user to decide on an exact number of compounds to screen in the next iteration, but instead, this will be provided by the predictor based on the selected confidence. For the conformal prediction framework to guarantee the error rate, the data considered needs to be exchangeable. In an iterative screening setup, this has implications on how to select the compounds for the initial round of screening, and the only way to guarantee exchangeability, as long as the sample is large enough, would be to draw a random sample of the available data.", "section": "Introduction", "doi": "10.1186/s13321-018-0260-4", "references": [2360098, 321037060, 1786598644, 1967019899, 1994249991, 2011006316, 2042110087, 2101234009, 2126091441, 2148125134, 2337224300, 2400913246, 2587942483, 2731306092, 2916534270]}
{"paragraph": "Topology control refers to selecting only a subset of the available communication links for data transmission, which has been widely used to construct networks with specific properties such as planarity, bounded node degree, the spanner property and low interference. Researchers are not only interested in minimizing the average interference on the nodes, but also the maximum interference, because the maximum interference is closely related to the time when the first node runs out of energy, which could mean a halt of the entire network’s operation. The problem of minimizing the maximum interference while preserving connectivity in two-dimensional networks has been proved to be NP-complete. One proposed algorithm could bound the maximum interference by O(√Δ) using the ε-net theory in computational geometry. Here, n is the number of nodes and Δ is the maximum node degree in the topology when each node is set to the maximum transmission range and connected to all the other nodes in its range. If all the nodes have the same maximum transmission range, the topology is actually a unit-disk graph. For minimizing average interference in 2D networks, another study developed an asymptotically optimal algorithm with an approximation ratio of O(log n). Researchers are also interested in the interference problem in 1D networks as there are many application scenarios for 1D networks, such as bridges and tunnels. For minimizing the maximum interference on the exponential chain, an asymptotically optimal algorithm was proposed and a tight lower bound of Ω(√Δ) was proved. Here the exponential chain means the nodes are distributed on a 1D line with the distances growing exponentially. Furthermore, for the general case in which the nodes are arbitrarily distributed on a line, known as the highway model, the minimum maximum interference was bounded by O(√Δ) and an approximation with ratio of O(√Δ) was presented.", "section": "Related Work", "doi": "10.1007/978-3-642-18381-2_43", "references": [1496521616, 1765990116, 1864223778, 2008979299, 2047964793, 2085446033, 2087196488, 2095808982, 2100806474, 2121101422, 2121976406, 2132963070, 2134229377, 2142852908, 2150722810, 2601751983]}
{"paragraph": "The characterization of the so-called constrained capacity, in view of the fact that the inputs are constrained to obey a certain practical distribution rather than the capacity-achieving one, poses a myriad of challenges due to the absence of explicit and tractable mutual information expressions. An innovative approach toward resolving this class of problems was introduced by exploiting connections between key quantities in information theory and estimation theory. By drawing upon the relationship between mutual information and minimum mean-squared error, known as the I-MMSE identity, researchers have studied optimal power allocation policies for parallel non-interfering Gaussian channels with arbitrary inputs. Others have studied optimal power allocation and precoding for interfering multiple-input multiple-output Gaussian channels with arbitrary inputs. Optimum power allocation for multiuser OFDM systems with arbitrary signal constellations, both in scenarios where the transmitter knows the fading channel state and in scenarios where the transmitter knows only the fading channel distribution, has also been addressed using identical techniques. Other applications of the connections between mutual information and MMSE have been explored as well. This paper instead pursues the characterization of the constrained capacity of multi-antenna fading coherent channels with arbitrary discrete inputs. The coherent channel model, where the receiver is assumed to know the exact channel state but the transmitter is only assumed to know the channel distribution, is particularly relevant because the use of bandwidth-limited feedback channels between the receiver and the transmitter only enables the transmission of statistical, rather than instantaneous, channel state information in fast fading mobile communication systems.", "section": "Methodology", "doi": "10.1109/TIT.2013.2287507", "references": [1979975371, 2046817139, 2049285480, 2072184935, 2080450804, 2097695636, 2101300273, 2109286020, 2111616148, 2116485279, 2118040894, 2118259605, 2118433700, 2129766733, 2130192438, 2130509920, 2133475491, 2136464164, 2137736549, 2147431204, 2153521822, 2159312879, 2166595933, 2167455364, 2168089577, 2169813715, 2170738062, 2181292907, 2295513759]}
{"paragraph": "This paper provides an overview of a special-topics software engineering course that adopts a Java-based approach for teaching concepts of software adaptation. The course examines the principles of meta and reflection as they support the concept of adaptation from several contexts. The general goals of the course are the following. Provide an introduction to the history and literature of reflection and metaprogramming. This includes the introspective Java Reflection API, as well as research extensions to Java that support more powerful intercessional forms of reflection. Motivate the area of aspect-oriented software development as a direct result of past research into reflection. The majority of the semester is spent in this section of the course where students gain experience in applying AOSD ideas using enhanced Java translators, such as AspectJ. Introduce the issues surrounding meta-modeling and domain modeling within model-integrated computing. The generic modeling environment is used as a platform for exploring model-based synthesis of Java applications. Expose students to applications of the above techniques for supporting adaptation of distributed object computing. In particular, various research efforts in the area of reflective middleware are presented, in addition to modeling tools for generating middleware configurations.", "section": "Introduction", "doi": "10.1016/j.scico.2004.02.005", "references": [6853780, 1493267071, 1500250067, 1510687364, 1513136098, 1581016894, 1606614601, 1626032987, 1654534824, 1971860760, 1972328015, 1988940012, 1991604845, 1997502136, 2005940397, 2014019774, 2019952623, 2021188780, 2031334538, 2056553417, 2057334711, 2058104334, 2065614429, 2067057008, 2076638640, 2078434561, 2083803628, 2097370855, 2101730275, 2102194774, 2111386044, 2115842798, 2117132408, 2132084165, 2134119432, 2138363365, 2139453480, 2139897701, 2146740093, 2150765606, 2162885111, 2167023209, 2168287017, 2172069600]}
{"paragraph": "This paper presents PKorat, a parallel test generation framework based on Korat. The key insight is that even though it is not feasible to fast-forward Korat search effectively, the systematic exploration of the input space can still be parallelized by exploring non-deterministic field assignments in parallel. PKorat search backtracks by generating several candidate inputs that are explored in parallel. A key aspect of PKorat is that it explores exactly the same number of candidates that Korat does and still enables a scalable parallel implementation. PKorat uses a master-slave configuration to implement its parallel search. When delegating exploration to a slave processor, PKorat provides a small amount of meta-information that prevents different slaves from exploring the same candidates and avoids redundant exploration. Previous work on parallelizing Korat provided techniques that efficiently parallelize test execution, while test generation was mostly sequential. While this paper focuses on a parallel search that implements the Korat algorithm, the technique applies to parallelizing other systematic analyses that are based on non-deterministic assignments, such as symbolic execution. Future work will generalize this parallel technique and evaluate its effectiveness in the more general context of symbolic execution. This paper makes the following contributions: a parallel algorithm for test generation by presenting PKorat, a scalable parallel algorithm based on Korat for generation of structurally complex test inputs; an implementation in C++ using MPI, evaluated on a Linux cluster; and an evaluation on a variety of commonly used data structures, including complex structures such as red-black trees and Java programs. The results show the scalability of PKorat and the significant speedups it provides over Korat.", "section": "Related Work", "doi": "10.1109/ICST.2009.48", "references": [107427404, 1505732339, 1523720513, 1548575501, 1720848645, 1895387792, 1920006546, 1996528637, 2009489720, 2009966597, 2012223233, 2017518635, 2025304834, 2033072042, 2043562807, 2065675749, 2095850105, 2096449544, 2101512909, 2109276114, 2115309705, 2118467295, 2148821745, 2152949134, 2155024733, 2155786233, 2159933174, 2162120832, 2165413178, 2168498016, 2171480813]}
{"paragraph": "At present, there have fortunately been rather few attacks on either domestic and workplace IoT systems or increasingly networked industrial automation. The reasons for this are primarily that this is a new technology, much of it bespoke, and specialist knowledge is required to launch effective attacks, which have nevertheless managed to cause damage to Iranian nuclear plants, an explosion in a blast furnace, and remove power from a city in the Ukraine, amongst others. This is changing, and while academics and companies are focused on the challenging problem of creating functional future systems, little is being done to explore the attack space—other than by attackers. It is our contention that evolutionary computing techniques have the potential to revolutionize the way we engineer future systems, from individual devices up to integrated systems, by facilitating the automated creation of attackers against which one can test and adapt defensive strategies. While some work has been undertaken in using evolutionary computing in creating defensive systems for wired networks, to the best of our knowledge, this study is one of very few attempts to explore the attack space for wireless networks. Given the complexities of causing many heterogeneous systems to operate together, not to mention the novelty of IoT and lack of prior information about attack, we believe that evolved attackers are a powerful tool that will augment more conventional forms of security analysis. Naturally, they might also provide attackers with new routes to attack complex systems—in which case a failure to use them proactively becomes a failure to prepare effectively. Here, we explore a limited problem to demonstrate the feasibility of evolving attackers. In this paper, we examine the problem of protecting wireless sensor networks, a form of IoT system constrained not to have actuators, that use an artificial immune system devised by Wallenta et al. We evolve attackers against it and demonstrate their effectiveness by comparing our attackers to the hand-crafted attacker used to evaluate the original system. The main contributions of this paper are: a novel approach to developing attackers for wireless sensor networks using genetic programming; improving the design of the intrusion detection system to fit the proposed scenario; defining a trustworthiness metric in the context of the system and using that metric to improve the overall results; and a full implementation and evaluation of the approach, which demonstrates that the proposed solution performs better than the original, in terms of both the attack and the defence, and provides the basis for further research.", "section": "Methodology", "doi": "10.1049/iet-wss.2016.0090", "references": [162192759, 1557652964, 1942111125, 1976540969, 1993842826, 1995566852, 2003544295, 2007727504, 2038194220, 2045009954, 2050067204, 2056347873, 2057840591, 2080258232, 2118599924, 2120417050, 2132159317, 2136074299, 2141929795]}
{"paragraph": "We develop ConceptCloud, an interactive tag cloud visualization engine for software repositories that aggregates commit data and lets users easily construct uniform visualizations of many different aspects of the project history. ConceptCloud makes use of a novel combination of tag clouds and an underlying concept lattice to support exploratory search tasks on software repositories. When users have no previous knowledge of a project or have not yet formulated a direct query, their task becomes one of exploratory search instead of direct search or retrieval. While there are already approaches supporting specific retrieval tasks and visualizing aspects of software repositories, support for exploratory search in software repository data remains unavailable. The goal of our work is to build a flexible and interactive visualization engine that allows users to visualize different aspects of a project interactively and therefore supports exploratory search tasks, instead of presenting the user with one static, pre-configured view. An exploratory search approach can provide an overview of the repository data and allow the user to further investigate any aspects of the project which they might find interesting. Therefore, exploratory search approaches can support new developers on a project in understanding the project history and team structure. An exploratory approach can also be used to answer more general questions, such as which developers collaborate, which cannot be formulated as a single search query as would be possible if the question were more focused, such as who collaborates with a specific developer.", "section": "Introduction", "doi": "10.1016/j.infsof.2016.12.001", "references": [32482440, 1576040511, 1851672339, 1967714462, 1990566545, 1993032259, 1994605000, 1998084297, 2001606680, 2008164297, 2017103330, 2032929460, 2037802158, 2038899190, 2060704087, 2080010225, 2080534028, 2086474969, 2086723374, 2096061896, 2098162425, 2112486798, 2112501342, 2114461832, 2115919801, 2115941721, 2119410041, 2120933861, 2124961838, 2125083169, 2125889680, 2127898536, 2129200997, 2132414768, 2133032989, 2139248042, 2150375547, 2155223492, 2157353183, 2160715838, 2164721131, 2244428951, 2436001372]}
{"paragraph": "In the IaaS model, infrastructure requests are mainly served by allocating virtual machines to cloud users. Successful live migration of virtual machines from one host to another without significant interruption of service results in dynamic consolidation of virtual machines. However, highly variable workloads can cause performance degradation when an application requires increasing demand for resources. Besides power consumption, we need to consider performance as it affects Quality of Service, which is defined via Service Level Agreement. It is clear that maintenance of cloud computing involves an energy-performance trade-off—we have to minimize energy consumption while meeting the Quality of Service. In order to address this problem, in this work, multiple virtual machine placement algorithms are proposed based on the solution to the bin packing problem. Previously, adaptive heuristics were proposed for energy and performance-efficient dynamic virtual machine consolidation. These included various methods for detecting host underload or overload to choose virtual machines to migrate from those underloaded and overloaded hosts. A modified version of best fit decreasing was proposed for virtual machine placement. In our work, we follow the heuristics stated in previous research for dynamic virtual machine consolidation, but instead of using the modified best fit decreasing algorithm for placement, we propose new algorithms based on other bin packing solutions with custom modifications. We also introduce a new technique that forms clusters of virtual machines to migrate by taking into account both CPU utilization and allocated RAM. We implement and study the performance of our algorithms against the default virtual machine placement algorithm designed in CloudSim to determine whether our proposed algorithm can achieve improved performance compared to the existing one.", "section": "Related Work", "doi": "10.1186/s13677-015-0045-5", "references": [7574676, 184075017, 1497339356, 1562916533, 1564665474, 2000268015, 2004354923, 2061880373, 2086102640, 2103375760, 2103998655, 2110374615, 2113566137, 2118955868, 2123590764, 2148459868, 2157016369, 2158468884]}
{"paragraph": "There are two main aspects in garbage collector design, namely, the partitioning of heap space and the algorithms for garbage collection. For better memory management, a modern high-performance garbage collector usually manages large and normal objects separately, such that the heap is divided into large object space and non-large object space. However, the object size distribution varies from one application to another and even from one execution phase to the next within the same application, making it impossible to predefine a proper heap partitioning for large and non-large object spaces statically. Current garbage collectors with separate allocation spaces mostly suffer from the problem of not adapting well to dynamic variations in object size distribution at runtime. This leads to imbalanced space utilization and negatively impacts overall garbage collector performance. Beyond large object and non-large object partitioning, imbalanced space utilization is also a concern in any garbage collector with multiple spaces. For example, a generational garbage collector typically has a young object space and a mature object space. The space partitioning for both also requires careful design to achieve maximal efficiency. To address fragmentation, copying or compacting garbage collectors are introduced. Compaction algorithms are now widely used in garbage collector designs. Compaction eliminates fragmentation by grouping live objects together in the heap and freeing up large contiguous spaces for future allocation. As multi-core architectures become common, parallel compaction algorithms have been developed for improved time efficiency. However, none of the proposed parallel compaction algorithms can be applied to large object compaction. Large object compaction is difficult to parallelize due to strong data dependencies, where the source object cannot be moved to its target location until the object originally at the target location has been moved. This is particularly problematic when only a few very large objects exist, making parallelism inadequate.", "section": "Methodology", "doi": "10.1145/1531743.1531749", "references": [1583464938, 1702434361, 1971233256, 1973638350, 1987079491, 2025899651, 2062372058, 2064700219, 2084659024, 2117391835, 2134377938, 2137736443, 2140638092, 2156225894, 2158525816, 2160068354]}
{"paragraph": "Any dynamic voltage scaling solution must be able to identify the minimum supply voltage that guarantees error-free operation at the requested clock frequency. One approach proposed exploiting the programmable nature of FPGAs by using an offline, design-specific, per-chip calibration method to identify this voltage. A CAD tool was developed that works in conjunction with a commercial CAD tool to identify timing-critical paths of an application. It then generates a calibration bitstream that contains these timing-critical paths, testing circuitry, and heaters to evaluate delay faults at different temperatures. The output of this calibration process is a table that stores the minimum safe voltage at different clock speeds and temperatures. This information is used in a closed-loop feedback system to scale the supply voltage dynamically based on chip temperature and desired speed. However, the testing procedure assumes that the source and sink registers of all timing paths have observable inputs and outputs. While this assumption holds for registers in the soft fabric of an FPGA, it does not apply to registers in hard blocks such as DSPs and BRAMs. This work builds upon the existing approach to achieve a voltage scaling solution that can support any application using BRAMs. The focus is on BRAMs because they differ from other FPGA components due to their analog nature, and scaling supply voltage can result in failures, such as parametric issues in weaker SRAM cells. Furthermore, previous studies and experimental results indicate that many timing-critical paths in applications either start or end at BRAMs, making it essential to measure delays in these paths. The proposed voltage scaling solution ensures that all BRAM cells used by the application avoid parametric failures during voltage changes and robustly tests for delay faults in the BRAM interface logic.", "section": "Introduction", "doi": "10.1109/fpl.2018.00020", "references": [1965193075, 1978124838, 1981384041, 2005027393, 2022202043, 2024381286, 2037524288, 2045704975, 2098762322, 2108165851, 2123009614, 2123453286, 2132621842, 2137769675, 2143192260, 2144511081, 2156499539, 2164340799, 2167040116, 2170735577, 2525415380, 2583145172, 2610319614, 2762572888]}
{"paragraph": "A built-in self-test scheme for circuits based on time redundancy has been proposed. Critical permanent failures are distinguished from uncritical temporary ones by repeating the test in case of a faulty outcome, a process known as signature rollback. To minimize yield loss and overall test time, the test is partitioned into shorter sessions. If a faulty signature is observed at the end of a session, a rollback is triggered, and only that particular session is repeated. It has been shown that as the number of sessions increases, both the average test time and yield loss decrease. However, the area overhead for storing the reference signatures increases with the number of sessions. In this paper, signature analysis is combined with an additional compaction of the signatures into a short parity sequence. This method allows a significant reduction in storage requirements while maintaining nearly constant quality. In contrast to other methods of extreme compaction, which apply space compaction followed by signature analysis, this approach does not require special test generation procedures.", "section": "Related Work", "doi": "10.1109/ICCD.2010.5647648", "references": [141121412, 197391467, 612784192, 1541483005, 1957023732, 2043318181, 2098695358, 2099971661, 2104309038, 2104955033, 2108785072, 2110578688, 2125169487, 2128456472, 2145136265, 2147814275, 2151814994, 2152652532, 2163094833, 2164754947]}
{"paragraph": "Many protocols have been proposed for data-gathering or communication among wireless sensor nodes. Most of the proposed protocols have focused on static wireless sensor networks. However, in many applications, sensor nodes can move either due to external forces or their own mobility components, such as when they are attached to moving objects for tracking or scattered across the sea. Among the existing protocols, the cluster-based structure provides an effective architecture for data-gathering in wireless sensor networks. This paper considers such an architecture and proposes distributed clustering algorithms for a cluster-based data-gathering protocol in a wireless mobile sensor network. There is no explicit mobility model proposed and studied specifically for wireless mobile sensor networks so far. Since a wireless mobile sensor network is essentially a type of wireless ad hoc network, we refer to the mobility models used in mobile ad hoc networks. Furthermore, the mobility pattern of a sensor node in a wireless mobile sensor network can be programmed in advance before deployment. Therefore, we select the following three mobility models to evaluate the proposed data-gathering protocols: Random Walk Mobility model, Random Direction Mobility model, and Simple Mobility model. More details about these mobility models are provided later. To measure the performance of the proposed distributed algorithms in terms of energy consumption, we consider the system lifetime of a wireless sensor network. The system lifetime refers to the duration in which the wireless sensor network remains functional and relates to both energy consumption and quality of service. An energy-efficient data-gathering protocol can extend the lifetime of the wireless sensor network. A longer network lifetime reduces the need to redeploy or reset the wireless sensor network frequently.", "section": "Methodology", "doi": "10.1016/j.jpdc.2007.06.010", "references": [1497763074, 1553037668, 1989651117, 2058322021, 2073079977, 2076399305, 2091936226, 2096200917, 2102188208, 2104846640, 2106335692, 2106665154, 2109089704, 2112577636, 2119362555, 2124076101, 2135401919, 2142129619, 2143514830, 2145417574, 2148135143, 2148251644, 2155492251, 2162650992, 2167641880, 2171398629, 2171821354]}
{"paragraph": "Researchers have proposed several algorithms to compute minimal models of propositional CNF theories, stable models of logic programs, and answer sets of disjunctive logic programs. Some implementations based on these algorithms, such as smodels and dlv, perform very well in practice. However, very little is known about the worst-case performance of these implementations. In this paper, we study the three computational problems mentioned earlier. We focus on t-CNF theories and t-programs, which are theories and programs consisting of clauses containing no more than t literals. Such theories and programs often arise in the context of search problems. Given a search problem, its specifications are often encoded as a disjunctive DATALOG¬ program or a set of propositional schemata—universally quantified clauses in some function-free language. A propositional program or CNF theory corresponding to a specific instance of the search problem is then obtained by grounding the disjunctive DATALOG¬ rules or propositional schemata using constants from the instance description. Since the initial program or set of schemata is independent of specific problem instances, grounding produces propositional programs or CNF theories with clauses of bounded length—that is, t-programs or t-theories for some typically small value of t. In many cases, t equals 2 or 3, as seen in several problems once domain predicates are simplified.", "section": "Introduction", "doi": "10.1017/S1471068405002607", "references": [1568972291, 1569596742, 1639901403, 1645675097, 1655990431, 1672891595, 2002476082, 2011124182, 2041526687, 2066749637, 2076698873, 2087432892, 2103797177, 2123059908, 2152131859, 2167440926, 2171680292, 2176981155, 2945259913]}
{"paragraph": "The idea of predicting the number of faults is attractive because it provides the probability that a certain number of faults will occur in a given software module, capturing the fault occurrence pattern of the software system. This type of prediction can be very useful for the quality assurance team to focus testing efforts on modules likely to contain more faults. Count models such as Poisson regression and negative binomial regression can be used to estimate the probability of fault occurrences in each software module. In addition to count models, other prediction techniques such as multilayer perceptron, decision tree regression, and genetic programming can also be used for fault prediction. Previous studies have evaluated count models and found them capable of accurately predicting the number of faults. Some researchers have also investigated the use of genetic programming, decision tree regression, and multilayer perceptron for this purpose and reported promising results. However, no comprehensive investigation is available that compares count models with other fault prediction techniques such as genetic programming, multilayer perceptron, and decision trees for predicting the number of faults in software modules. One comparative study was conducted using five count models, applying hypothesis testing and statistical measures to evaluate their relative performance. However, that study did not assess the full potential of count models for fault prediction, nor did it include a comparison with other prediction techniques.", "section": "Related Work", "doi": "10.1007/s00500-016-2284-x", "references": [1542582976, 1587315015, 1639032689, 1680392829, 1838241330, 2000400110, 2023061571, 2024209326, 2027483357, 2028349769, 2041507839, 2074805796, 2081917329, 2102782753, 2120457925, 2133300800, 2139506929, 2143637886, 2145091807, 2146474553, 2147105902, 2159610968, 2296517388, 2302881059, 2305460223]}
{"paragraph": "All this leads to the loss of visual capacity with age and the development of pathologies such as cataracts, glaucoma, macular degeneration, and dry eyes. In addition, estimates indicate a greater aging of the population in Asia, and the situation of blindness in India is significantly more severe compared to Europe or other developed countries. It is expected to increase further due to growing risk factors such as population aging and the rising prevalence of diabetes. The prevalence of diabetes in India is also higher than in the EU. Facial recognition technology has made significant improvements in recent years and is approaching high levels of accuracy. These advances have also benefited people with disabilities. For example, an intelligent cane using facial recognition has recently been introduced for the blind. However, facial recognition systems remain vulnerable to spoofing attacks, where someone presents a photograph of another person to the camera instead of their own face. This presents a significant problem, as obtaining such photos is relatively easy. This issue is particularly critical for people with visual impairments. Based on these premises, the objective of this paper is to propose, build, and validate an architecture based on facial recognition and anti-spoofing techniques that can be integrated into both a video door entry system and a mobile application. The aim is to provide blind and visually impaired individuals with a tool that improves their quality of life and enhances both their security and sense of safety at home or during social interactions. The proposed architecture has been validated with real users in a real environment, simulating conditions encountered by video door entry systems and mobile devices used by visually impaired persons. The contributions are as follows: First, an algorithm for robust facial normalization is proposed to correct for rotation and imbalance in facial detection. This improves the success rate of facial detection algorithms. Second, a spoofing detection algorithm based on texture analysis is proposed to enhance system security, taking advantage of the prior normalization step. Third, a lightweight architecture optimized for devices with limited computing resources has been designed and implemented. The algorithm is developed in C# using a DLL generated in MATLAB and ported to Android via the Java Native Interface for integration with computer vision algorithms. Finally, the architecture is complemented with a text-to-speech tool to enhance accessibility.", "section": "Methodology", "doi": "10.1109/icccnt.2018.8493971", "references": [1905338716, 2039051707, 2048127762, 2106852298, 2107227001, 2131081720, 2152826865, 2153635508, 2161969291, 2163352848, 2163487272, 2163808566, 2164598857, 2325168940, 2588064259]}
{"paragraph": "It is well known that within-class spectral variation may lead to the overclassification of spectrally homogeneous areas, resulting in salt-and-pepper noise in the classification maps of these areas. Therefore, incorporating geometric priors in the dimensionality reduction of hyperspectral image data is important, as it can help overcome overclassification in such regions. In this paper, we explore the geometric similarity of samples in hyperspectral image classification to make the reduced hyperspectral vectors more discriminative. The local geometric similarity of data is examined in both the manifold domain and the image domain, and a semi-supervised dual-geometric subspace projection (DGSP) approach is proposed, utilizing both labeled and unlabeled samples. First, the local geometric information in the manifold domain is captured using a sparse coding-based geometric matrix. Then, a local consistency-constrained geometric matrix is defined to represent the geometric structure in the image domain. Second, unlabeled samples are used to refine the geometric structure by defining a pairwise similarity matrix. Third, three scatter matrices are derived from these similarity matrices to identify the optimal subspace projection that captures the most significant properties of the subspaces for classification. Experiments conducted on a real hyperspectral image dataset demonstrate the effectiveness of the proposed method.", "section": "Introduction", "doi": "10.1109/TGRS.2013.2273798", "references": [1564277727, 1596717185, 1969184429, 1976359033, 1991382078, 2013793344, 2049365101, 2097900616, 2100495423, 2104290444, 2107196683, 2129812935, 2134262590, 2135496738, 2136554445, 2138038253, 2139206670, 2141567889, 2143130611, 2145376937, 2150990614, 2151288205, 2151599207, 2153409933, 2162698522, 2171500336, 2442791468]}
{"paragraph": "In this paper, we describe a novel gait recognition technique that derives classification features directly from XYT patterns. Specifically, it computes the image self-similarity plot, defined as the correlation of all pairs of images in the sequence. Normalized feature vectors are extracted from the self-similarity plot and used for recognition. Related work has demonstrated the effective use of self-similarity plots in recognizing different types of biological periodic motions, such as those of humans and dogs, and applied the technique for human detection in video. We use them here to classify the movement patterns of different people. We argue that the self-similarity plot encodes a projection of planar gait dynamics and provides a two-dimensional signature of gait. Whether it contains sufficient discriminative power for accurate recognition is the focus of our investigation. As in any pattern recognition problem, these methods typically consist of two stages: a feature extraction stage that derives motion information from the image sequence and organizes it into a compact representation, and a recognition stage that applies a standard classification technique to the motion patterns, such as K-nearest neighbor, support vector machines, or hidden Markov models. In our view, the core of the gait recognition problem lies in perfecting the feature extraction stage. The challenge is to find motion patterns that are sufficiently discriminative despite the wide variability in gait, and that can be extracted reliably and consistently from video. The method proposed in this paper addresses these requirements. It is based on the self-similarity plot, which is robust to segmentation noise and can be computed without correspondence from low-resolution images. Although the method is view-dependent due to its appearance-based nature, this limitation is addressed using view-based recognition. The method is evaluated on several datasets with varying degrees of difficulty, including a large, surveillance-quality outdoor dataset of 54 people and a multiview dataset of 12 people taken from eight different viewpoints.", "section": "Related Work", "doi": "10.1155/S1110865704309236", "references": [152533846, 1480289567, 1490760466, 1498220216, 1499877760, 1502651450, 1505602316, 1539977911, 1565880995, 1575198668, 1578415806, 1622986818, 1840252612, 1848363921, 1975443308, 1989702938, 2012352340, 2041475884, 2069126929, 2098873533, 2106393952, 2113856781, 2121647436, 2121899951, 2124925761, 2129887056, 2132549764, 2134262590, 2135285359, 2135340719, 2137806997, 2141294180, 2141560585, 2142532896, 2143023146, 2145421913, 2146968264, 2153691007, 2153754923, 2156795973, 2161315665]}
{"paragraph": "In this work, we focus on the design of multilevel quantizers for distributed detection of a weak signal. Specifically, the problem involves detecting a mean parameter corrupted by Gaussian noise in a wireless sensor network, where the communication channels between local sensors and the fusion center may be imperfect. The original observations are first quantized into multi-bit data, which are then encoded into binary codewords and transmitted to the fusion center through distortion channels modeled as binary symmetric channels. Using the quantized data received at the fusion center, we propose a generalized likelihood ratio test detector to make a global decision about the presence or absence of an unknown signal. An asymptotic performance analysis of this detector under the weak-signal assumption reveals that the quantization thresholds significantly affect detection performance. In particular, the detection probability increases monotonically with the Fisher information of the unknown signal. Based on this insight, we propose a quantizer design method that maximizes the Fisher information with respect to the quantization thresholds. Since the Fisher information is a nonlinear and non-convex function of the quantization thresholds, making gradient-based optimization ineffective, we employ a particle swarm optimization algorithm to search for the optimal thresholds. This algorithm is a stochastic global optimization technique inspired by social behaviors such as bird flocking and fish schooling. It does not require gradient information and is also easy to implement.", "section": "Methodology", "doi": "10.1109/TWC.2014.2379279", "references": [1974469944, 1981127284, 1983750483, 1992340830, 2007137388, 2019304957, 2022451981, 2032599725, 2052515690, 2073148077, 2099705786, 2104009246, 2108290973, 2109808436, 2113085878, 2133412337, 2138319443, 2143219333, 2149976859, 2154828437, 2158284593, 2168747298]}
{"paragraph": "In order to advance the understanding of the global behavior of learning classifier systems (LCS), we aim to develop a framework that enables the study of LCS as a whole through the combination of interacting models of its subcomponents. The primary objectives of this work are to: (i) include all LCS components within the framework to allow for systematic analysis of the components and their interactions; (ii) design the framework flexibly so it can support the development of extensions to existing LCS methods; and (iii) adopt notation and concepts from related fields to enable direct comparison and facilitate the translation of new developments into the LCS domain. The framework is developed by decomposing LCS into three components that align with other machine learning techniques and are studied both individually and in combination. Although this paper focuses on only one of the components, it is examined with integration in mind. Through the construction of this framework, we aim to develop LCS approaches that not only achieve improved performance but are also grounded in theoretically justified methods, in contrast to the commonly used heuristics that often require extensive parameter tuning.", "section": "Introduction", "doi": "10.1007/s10994-007-5024-8", "references": [1821722, 120328374, 161400130, 332851652, 1502893368, 1507222174, 1513999961, 1527476591, 1555782997, 1591645366, 1599347704, 1617964729, 1963721479, 1989101984, 1993254737, 1999128321, 2008052365, 2021247827, 2044462844, 2064957497, 2065759098, 2121863487, 2124175081, 2134367663, 2157456991, 2169107971, 2469703414, 2911546748]}
{"paragraph": "Recently, automated workflows to assemble specialized software into a comprehensive and organized data flow have begun to emerge in the field of bibliometrics. These workflows are particularly well suited to multi-step analyses involving different types of software tools. In this paper, we propose a unique tool developed in the R language, which follows a classic logical bibliometric workflow that we have reconstructed. We have designed and implemented an R-based tool for conducting comprehensive bibliometric analyses. R is a language and environment for statistical computing and graphics that offers a wide range of statistical and graphical techniques and is highly extensible. In addition to supporting statistical operations, R is an object-oriented and functional programming language, allowing for automated analysis and the creation of custom functions. Its open-source nature ensures strong community support, with new functions regularly contributed by users, many of whom are expert statisticians. Because it is developed in R, the proposed tool is flexible, can be rapidly updated, and can be integrated with other statistical R packages, making it especially useful in the evolving field of bibliometrics.", "section": "Related Work", "doi": "10.1016/j.joi.2017.08.007", "references": [767067438, 1799889357, 1963753783, 1966104031, 1966538856, 1975107573, 1976620775, 1977714176, 1981340255, 1981886524, 1989669128, 1992669535, 1996229960, 2002117998, 2005207065, 2027998681, 2034800299, 2042852414, 2045108252, 2052710775, 2056147580, 2066888021, 2069134922, 2071983778, 2072897447, 2075220720, 2094864959, 2096630656, 2097148950, 2098162425, 2108680868, 2132948308, 2135455887, 2146695321, 2150220236, 2150999626, 2171458572, 2225280195, 2248369874, 2285085441, 2408216567, 2472756135, 2922415744, 2963453445]}
{"paragraph": "Dynamic textures are a research topic of highly growing interest. The number of publications on dynamic textures in major computer vision conferences has risen sharply in recent years. Dynamic or temporal textures were introduced by the pioneer works of Nelson and Polana, where a first dynamic texture definition was given. Different methods were thereafter proposed for dynamic texture characterization with a steep increase since 2003. This growing interest can be explained by both the democratization of video acquisition and processing systems, and by a large field of potential applications. Among them, one can mention the following. Video indexing: the goal is to perform elaborate queries associating features of semantic nature. For example, one can search for videos of turbulent water, or a fire, a calm lake, a tree waving in the wind, and so on. Video surveillance: in some image sequences, dynamic texture is an important characteristic of the scene. To detect an accident or a risky behavior in traffic, to supervise and characterize the motion of a crowd, to detect forest fires or smoke are all examples where a robust description of dynamic textures is necessary. Spatio-temporal segmentation of image sequences: being able to segment a video sequence with respect to dynamic textures can enrich the comprehension of a scene. It can enable us to detect a perturbation in a given dynamic texture, to help building video summaries, or to better compress videos according to their texture content. Dynamic background subtraction: in such applications, the background can be composed of dynamic textures, such as moving trees, and a precise characterization of dynamic textures can improve the efficiency of background subtraction algorithms. Tracking: being able to track dynamic textures in image sequences could enable us to follow and analyze the evolution of phenomena such as fluid flows or vortices, fire. Video synthesis: realistic dynamic texture synthesis is necessary for video games, animations, or video inpainting.", "section": "Methodology", "doi": "10.1109/TCSVT.2011.2159430", "references": [1501891604, 1517404776, 1532450414, 1534780913, 1578285471, 1736371956, 1976566382, 2009573558, 2045559171, 2075487205, 2098667639, 2098710354, 2105795585, 2105854624, 2115528090, 2121651107, 2122661899, 2139916508, 2150806910, 2150907858, 2151134568, 2154996879, 2155359665, 2156328381, 2157126810, 2162616721, 2168813398, 2170025675, 2912155302]}
{"paragraph": "The statistic learning based methods such as Principal Component Analysis, Fisher Linear Analysis, Kernel Principle Component Analysis, and Kernel Fisher Analysis are popular ways to extract global features. When using these methods to extract local features, the researchers often apply them on local regions instead of on the global image. Different from linear methods such as PCA and Fisher analysis, KPCA and KFA are more effective on nonlinear feature extraction. However, a common limitation of the traditional KPCA and KFA for feature extraction is that they have to reserve the training samples or part of the training dataset, which causes the storage space problem for complex pattern recognition systems. Similarly, SVM classifiers also need to save support vectors of the training database. Traditionally, a kernel-based method uses kernel functions to calculate the inner product of different input image samples in a high-dimensionality space. Different from the existing works, this paper proposes a new Local Kernel Feature Analysis method for local feature extraction, which calculates the local kernel similarity from a single input image without the training samples saved. LKFA is proposed to capture the nonlinear relationship among local neighborhoods by using the kernel function. It is proved in this paper that the proposed LKFA method is robust to noise. Moreover, the LKFA can be further combined with Fisher Linear Analysis for better performance in applications with multiple training examples per class.", "section": "Introduction", "doi": "10.1016/j.neucom.2010.09.008", "references": [1515999713, 1998292402, 2054891869, 2095739675, 2098305432, 2105622852, 2111308925, 2111956627, 2114214092, 2115084764, 2117513046, 2120515362, 2121647436, 2124386111, 2137659841, 2139140682, 2141583337, 2144119003, 2144990628, 2145023731, 2159727956, 2162854380, 2163352848, 2163808566, 2165731615, 2169364766]}
{"paragraph": "In Avatar, a mobile user owns one or more mobile devices and an associated avatar hosted in the cloud. An avatar is a per-user software entity which acts as a surrogate for the user's mobile devices to the extent possible, thus reducing the workload and the demand for storage and bandwidth needed on the mobiles. The avatars are instantiated as virtual machines in the cloud in order to provide resource isolation and to simplify per-user resource management. Avatars run the same operating system as the mobiles and can thus run unmodified app components. Implicitly, they save energy on the mobiles and improve the response time for many apps by executing certain tasks on behalf of the mobiles. The avatars are always available, even when their mobile devices are offline because of poor network connectivity or simply turned off. Four of the features listed above—high availability, low latency, resource isolation, and resource savings on mobiles—are implicitly offered through the Avatar concept. The other three features, namely programming model, cloud scalability, and privacy, represent the major research challenges of this system.", "section": "Related Work", "doi": "10.1109/MobileCloud.2015.22", "references": [1409892002, 1499686117, 1734019305, 1852007091, 2009657469, 2009751454, 2023380813, 2088692353, 2100538831, 2101788345, 2111034077, 2129861682, 2135099885, 2143087446, 2151100432]}
{"paragraph": "One of the main challenges in the design of such an event-triggering mechanism is to guarantee the desired control performance, for example in terms of L2-stability, together with a positive minimum inter-event time despite the presence of disturbances. These two properties are essential in the context of event-triggered vehicle platooning. The control performance guarantee is needed to establish a string-stable platoon, and a positive minimum inter-event time is required to avoid Zeno behavior—an infinite number of events in finite time—and to enable practical implementation of the event-triggered control system. In addition, the resulting systems should be robust to time-varying communication delays induced by the DSRC channel. Although many event-triggered control methods are available, it has been shown that many of the proposed mechanisms do not have a positive minimum inter-event time that is robust in the presence of external disturbances. To address this issue, recent works either use time regularization, meaning that the next event transmission can only occur after a specific waiting time has elapsed since the last transmission, or periodic event-triggered control, meaning that the triggering condition is checked at fixed periodic sampling time instants with a defined sampling period, ensuring that the minimum inter-event time is bounded. However, none of these works provides an output-based event-triggered control method that guarantees both L2-stability and robustness with respect to time-varying delays, which is required in the context of cooperative adaptive cruise control.", "section": "Methodology", "doi": "10.1109/TITS.2017.2738446", "references": [118497390, 1493077431, 1966254600, 1973378833, 1978518835, 1991638565, 1995549873, 1998151680, 2008151824, 2011753154, 2014153391, 2035890084, 2048153718, 2048186868, 2062298332, 2079764542, 2084988223, 2086001768, 2100409338, 2103896722, 2111990577, 2125472350, 2137505842, 2137813237, 2141333364, 2141462254, 2146411117, 2317290577, 2490856144, 2528247373, 2602572350, 2963713359]}
{"paragraph": "It has been discovered that the retino-cortical mapping in the primate vision system can be modeled by a log polar geometry, and it motivated investigations to determine any apparent advantage that can be derived from this geometric formation. A vergence control model is proposed using a coarse-to-fine disparity estimation in the log polar space. This computational architecture utilizes the inherent foveal magnification properties of the log polar transformation to systematically focus towards the foveal information, eventually providing a stable vergence. This proposed method can be used reliably for real-time vergence control. This paper discusses the rationale for the system implementation and further illustrates the performance of the proposed vergence control model through the experimental results obtained. The understanding of three essential components is necessary for conceptualization of the robust vergence. These include the significance of the log polar transformation, the log polar-based normalized cross correlation, and the coarse-to-fine image pyramidal search strategy. Section 2 is a review of some existing vergence control methods. Section 3 presents the proposed log polar correlation model using image pyramid. Section 4 presents experimental results and Section 5 concludes the paper.", "section": "Introduction", "doi": "10.1016/j.imavis.2010.08.005", "references": [1794343390, 1954950839, 1964580108, 1994574341, 2029629437, 2037098133, 2054536235, 2067376725, 2100558374, 2100880139, 2125994734, 2132043085, 2142552333, 2144813526, 2155457151, 2165388334, 2170934089, 2171370357]}
{"paragraph": "Traditionally, the term “spectrum sharing” means that unlicensed devices, or secondary users, are allowed to utilize a frequency band on the condition that they do not cause harmful interference to licensed devices, or primary users, of that band. Hence, spectrum sharing is also referred to as cognitive radio, dynamic spectrum access, and opportunistic spectrum access technology. Cognitive radio techniques have been extensively studied, and several comprehensive survey papers summarize historical and state-of-the-art work in this area. Different from these definitions, in this paper, spectrum sharing, or coexistence, is defined as the general scenario where multiple, potentially different RF systems operate in the same frequency band. In this sense, cognitive radio technology is only one of the spectrum sharing technologies considered in this paper, which particularly focuses on spectrum sharing between a primary system and a secondary system. This paper covers a wider range of potential approaches to improving spectrum usage efficiency in current RF systems. We investigate existing spectrum sharing methods dealing with the coexistence of multiple RF systems in the same frequency band. The contributions of this paper are threefold: to the best of our knowledge, this is the first comprehensive survey studying general spectrum sharing methods for the coexistence of multiple RF systems in the same frequency band; we classify existing works on spectrum sharing into a number of general methods and discuss their applications in coexistence scenarios; and we explore open research issues on spectrum sharing methods and provide potential approaches to addressing these issues.", "section": "Related Work", "doi": "10.1016/j.adhoc.2016.09.009", "references": [1504889437, 1561338055, 1575228144, 1592634599, 1595523644, 1918798494, 1970240007, 1972786070, 1976875481, 1979754998, 1983643512, 1985004753, 1987205825, 1993602208, 1997150858, 2000101573, 2002937495, 2011062047, 2013209511, 2013523011, 2014958431, 2016495152, 2017889396, 2018669777, 2018932238, 2019698800, 2021449636, 2022000523, 2023324197, 2023684823, 2027519294, 2030157318, 2034004659, 2041187894, 2041718520, 2041792217, 2042693614, 2049468127, 2049599648, 2049783329, 2050373698, 2050668030, 2053905714, 2054252056, 2054550666, 2057394983, 2061914652, 2063485510, 2063569994, 2063773941, 2063914254, 2065320671, 2066604477, 2068282294, 2070418635, 2070824572, 2071695061, 2073043073, 2073752401, 2076855206, 2077372400, 2079158693, 2086348038, 2088780783, 2088936779, 2091005538, 2096558171, 2097382990, 2102350526, 2103619774, 2104532745, 2107710215, 2112109709, 2113303683, 2114424829, 2114596241, 2115003817, 2117924085, 2120144288, 2120192162, 2122294647, 2122329317, 2123378808, 2124326683, 2126596623, 2129543395, 2132254987, 2132344484, 2132463147, 2135169717, 2135620784, 2136168907, 2136595898, 2136679483, 2136957911, 2138020867, 2142819538, 2143535483, 2144369278, 2144763537, 2145590812, 2148158480, 2149946041, 2151537887, 2154574806, 2156957527, 2157206506, 2158061833, 2159664891, 2160081863, 2161176671, 2162636480, 2163900592, 2164987783, 2165272767, 2165376755, 2165908403, 2167230435, 2167518331, 2167748975, 2167818879, 2168180948, 2168456675, 2168702829, 2170020038, 2170991283, 2171637444, 2171757753, 2172171145, 2257221609, 2436937229, 2519687049, 2964583846]}
{"paragraph": "Since the l1 norm is not differentiable everywhere, it results in a more challenging problem than l2 regularization. In this paper, we describe and analyze a simple but efficient iterative algorithm to solve this problem. In each iteration, we derive a gradient descent method to update the weights. We show that the objective function monotonically decreases with respect to the number of iterations. The major contributions of this paper include: (1) We propose a framework for the sparse learning-to-rank problem. We model the sparse ranking problem as an optimization problem with l1 regularization and propose a simple iterative algorithm to efficiently solve the non-differentiable problem. (2) We empirically show that sparse models can significantly improve ranking accuracies, especially when compared to dense models. (3) We prove the convergence rate of our algorithm and propose a new way to set the Lipschitz constant. To further improve the training time, we also adopt a line search scheme to adaptively find the Lipschitz constant.", "section": "Methodology", "doi": "10.1016/j.knosys.2013.06.001", "references": [1660390307, 1988790447, 1998635907, 1998850041, 2000745937, 2002658919, 2007815473, 2014975478, 2029330199, 2045682932, 2059001985, 2069870183, 2071664212, 2091158010, 2105180921, 2107890099, 2108862644, 2118152081, 2120391124, 2127176025, 2128186735, 2142537246, 2143331230, 2156145910, 2171541062, 2171743041]}
{"paragraph": "In this paper, we aim to capture the trade-off between the mobile users’ uplink energy consumption and their perceived quality of service and employ such a relationship to design a balanced dynamic planning framework that can save energy for network operators while providing service quality guarantees for mobile users with data calls in the uplink and downlink. Specifically, the contributions of this paper are summarized as follows: we quantify the impact of dynamic planning on the service quality of mobile users in the uplink and downlink. The uplink mobile users’ quality of service is measured in terms of supporting the minimum required throughput, which is related to the transmission power, the available energy at the battery, and the number of uplink users, and satisfying a maximum threshold on call dropping probability due to battery depletion. The downlink mobile users’ quality of service is measured in terms of satisfying the minimum required throughput, which is related to the base station transmission power and the number of downlink users. We formulate a balanced dynamic planning framework as a two-timescale decision problem whose objective is to save energy for the service provider in a heterogeneous cellular network while ensuring the quality of service of mobile users. The two-timescale problem decouples base station operation and user association into two separate timescales: a slow timescale capturing long-term fluctuations in call traffic load, and a fast timescale capturing short-term fluctuations due to user arrivals and departures. This formulation offers a novel base station switching criterion, as switching off may be infeasible due to service quality degradation in the uplink. Due to the computational complexity of the decision problem, we present a sub-optimal dynamic planning framework following a feasibility analysis. We prove that this feasibility-based framework results in an optimal base station switching decision from the downlink perspective and a sub-optimal decision from the uplink perspective. To further reduce complexity, we propose a tunable heuristic algorithm that iteratively activates small cells until uplink and downlink quality of service parameters are satisfied on average. We perform extensive simulations to assess the performance of the proposed framework and compare it with an unbalanced dynamic planning approach that does not account for uplink quality of service in switching decisions. Simulation results indicate that the balanced framework can save energy for network operators compared with keeping all base stations active, while ensuring target quality of service for both uplink and downlink users, unlike the unbalanced approach.", "section": "Introduction", "doi": "10.1109/JSAC.2016.2624098", "references": [1526166825, 1972470182, 1986966935, 1989269224, 2008783708, 2009495780, 2014084212, 2024379753, 2034518457, 2036581396, 2037656568, 2039297968, 2044775111, 2069119628, 2080289841, 2090500089, 2098473710, 2098563066, 2110519532, 2112170521, 2114102402, 2122855803, 2139111642, 2144239375, 2147437672, 2149865317, 2155153696, 2155303957, 2171617713, 2281592530, 2294547139, 2323646721, 2337454810, 2373139161, 2524056225]}
{"paragraph": "In the real world, decision-makers often encounter multi-objective linear programming problems where the degree of priority among the objective functions is important. Therefore, the objective functions should be arranged based on their importance, leading to the use of the lexicographic method to solve such problems. These are known as lexicographic multi-objective linear programming (LMOLP) problems. A number of methods have been proposed to solve LMOLP problems. Solving the LMOLP problem was first proposed using the weighted method. Many researchers have used this method to solve their lexicographic optimization problems. One application described the optimization of water resources planning for Lake Verbano in northern Italy using the LMOLP framework. The goal was to determine an optimal policy for water supply management over a planning horizon, with the objectives being: maximize flood protection (first priority), minimize supply shortage for irrigation (second priority), and maximize electricity generation (third priority). This order of objectives was legally prescribed, giving the problem a lexicographic nature. An algorithm was suggested for solving LMOLP problems based on the duality theorem and a sufficiently large value. Another implementation involved sensitivity analysis of LMOLP problems when the priorities of the objective functions are changed. One notable feature of this algorithm is that the preemptive optimal solution of the LMOLP problem may be obtained by solving a single-objective linear programming problem. Further work also studied the transformation of a lexicographic multi-objective nonlinear programming problem into an equivalent single-objective problem to find the preemptive optimal solution.", "section": "Related Work", "doi": "10.3233/IFS-130906", "references": [1585268704, 1735048081, 1964545832, 1966884570, 1975939055, 1982897083, 1990819810, 1991901402, 2011033465, 2028873714, 2031024216, 2033536064, 2039052464, 2053912621, 2055044850, 2055877691, 2059083350, 2143231526]}
{"paragraph": "To resolve such limitations, we propose a multi-level hypergraph-based full video object co-segmentation method, which accounts for high-order correlations, incorporates multiple object proposals per video frame, and is robust to videos with large portions of empty frames. The hyperedge computation in our method benefits from a hybrid object model incorporating multi-modality information, with one high-level model focused on video semantics and a separate low-level model dedicated to video appearance, motion, and saliency. Specifically, the high-level object model merges multiple object proposals to generate a more reliable object region per frame, thus producing more robust high-level features. The low-level features, including appearance, motion, and saliency, naturally complement the high-level ones, jointly contributing to a better video representation. The hypergraph cut algorithm is subsequently utilized to achieve the final object co-segmentation. The contributions of this paper are as follows: (1) the method is robust against a large percentage of empty video frames that lack common objects; (2) it incorporates a multi-level hypergraph-based hybrid object model that accounts for both high-level semantics and low-level features; and (3) a new, challenging video co-segmentation dataset is collected with both ground-truth categorical labels and pixel-wise foreground labels.", "section": "Methodology", "doi": "10.1109/icip.2018.8451806", "references": [88469699, 140330156, 1555385401, 1586994488, 1772076007, 1903029394, 1989348325, 1990342138, 1990802205, 1998588337, 2030346542, 2046561909, 2113708607, 2117539524, 2118877769, 2121947440, 2128700566, 2131720600, 2145584811, 2155598147, 2162813810, 2170057991, 2194775991, 2463175074, 2518874898, 2616689446, 2767198899, 2777774479, 2964253156]}
{"paragraph": "Originally, peer-to-peer solutions were designed without much attention to their energy demands. For example, the most popular protocol for P2P file sharing and distribution, BitTorrent, has no internal mechanism for energy efficiency. The main reason for this design choice is that in P2P systems, energy consumption is distributed among a large number of peers, unlike traditional client-server architectures where energy is mainly consumed at data centers. In P2P systems, energy costs are not borne by a single organization but are shared among many users. As the energy costs of P2P systems grow with their popularity and share of Internet traffic, and as awareness of energy and environmental issues increases, developing energy-efficient solutions has gained growing attention. Additionally, with the proliferation of mobile devices such as smartphones and tablets, many P2P applications and services have been extended to mobile platforms. Mobile devices are typically battery-powered with limited energy, making energy efficiency vital to extending their lifetimes. Designing energy-efficient P2P solutions is not a trivial task. Since energy consumption is distributed among peers, addressing energy efficiency is generally more challenging in P2P systems than in traditional client-server architectures. Introducing energy efficiency in P2P audio or video streaming is even more difficult because, in addition to saving energy, the quality of service constraints imposed by the application must also be satisfied. Typically, energy savings and quality of service provision are conflicting goals.", "section": "Introduction", "doi": "10.1145/2835374", "references": [30096953, 180044563, 205484651, 807056160, 1408735241, 1481358901, 1481882649, 1894778686, 1973386063, 1974284667, 1977767697, 1978211940, 1978837428, 1981749469, 1981992464, 1984248305, 1985031560, 1990476939, 2002615004, 2005514800, 2009539526, 2012573873, 2014644972, 2016489836, 2019586910, 2022900869, 2031918306, 2039123032, 2041664850, 2042615960, 2044690078, 2048343758, 2051661326, 2057745027, 2058322021, 2059389412, 2060656154, 2065064467, 2069021661, 2072457093, 2077363378, 2077419187, 2077439840, 2085437001, 2085482525, 2085770399, 2087230725, 2090769298, 2092162881, 2094431417, 2095456274, 2099574482, 2099730848, 2104480072, 2104780755, 2106823605, 2114751856, 2115286336, 2116684263, 2117638871, 2119788997, 2122912623, 2126402853, 2129636357, 2132713601, 2135861925, 2137637970, 2138830906, 2139041001, 2139377163, 2141482534, 2143589620, 2146976342, 2147961310, 2149416793, 2149706718, 2151893328, 2153044774, 2155290773, 2156012409, 2156305837, 2157613522, 2158417086, 2160960486, 2163513873, 2165380761, 2166245380, 2169986911, 2296427920, 2539645636]}
{"paragraph": "However, clinical pathway knowledge has not been fully integrated into the treatment process, and the knowledge accumulated in current information systems has not been fully utilized to improve medical quality. Even though intelligent decision support has been implemented in some hospital IT applications, these systems can only support certain parts of medical practice rather than the entire treatment process. One of the identified challenges in knowledge management is the integration of decision models within business processes. To address this challenge, we propose a medical quality management method based on clinical pathways and an intelligent knowledge base. This research aims to utilize clinical knowledge to support and optimize medical processes. First, we extract patterns for medical quality improvement by integrating the full cycle of medical knowledge creation, usage, and updates into clinical pathways, based on which an architecture for intelligent clinical pathway systems is proposed. Next, we elaborate on the intelligent knowledge base, which classifies knowledge related to clinical pathways to provide comprehensive decision support throughout the treatment process. Finally, we demonstrate how the intelligent knowledge base improves treatment quality through our knowledge-based clinical pathway system, which manages the classified clinical pathway knowledge to monitor and optimize medical behaviors.", "section": "Related Work", "doi": "10.1007/s10796-011-9307-z", "references": [181622867, 1520542685, 1562720392, 1565633174, 1569026615, 1993999581, 1995413076, 1996734986, 2025978739, 2064661416, 2097633432, 2101978547, 2111658156, 2124175028, 2171188883, 2179133185, 2183983593, 2275899958]}
{"paragraph": "In the past year, systems have begun to emerge that combine handheld camera tracking capability with dense surface reconstruction modules, enabling more sophisticated occlusion prediction and surface interaction. Most recently, iterative image alignment against dense reconstructions has also been used to replace point features for camera tracking. While this work is very promising for augmented reality, dense scene reconstruction in real time remains a challenge for passive monocular systems, which assume the availability of the right type of camera motion and suitable scene illumination. Meanwhile, algorithms for estimating camera pose and extracting geometry from images have been evolving alongside advances in camera technology. New depth cameras based on time-of-flight or structured light sensing offer dense measurements of depth in an integrated device. With the arrival of consumer-level depth sensors, such sensing has become widely accessible. The opportunities for simultaneous localization and mapping and augmented reality with these sensors are clear, but current algorithms have not yet fully leveraged the fidelity and speed that these devices offer.", "section": "Methodology", "doi": "10.1109/ISMAR.2011.6092378", "references": [1187244281, 1514909517, 1517576992, 1543549646, 1883517952, 1891947214, 1972671602, 1991793962, 2009422376, 2044233231, 2046333466, 2049981393, 2063549868, 2071992612, 2094363303, 2099244020, 2099940712, 2108134361, 2113243634, 2118104180, 2126053368, 2150375237, 2150794573, 2151290401, 2160014001, 2164280484, 2167335287, 2247810280]}
{"paragraph": "So far, the only sub-problems solved within the weakest setting are the circle-formation problem, where robots must form a regular n-gon, the gathering problem, where robots must move toward a common point, and the case of asymmetric configurations. Further research directions concern randomized approaches. Instead of exploring another specific case, we investigate a different branch—the epf problem without chirality. Intuitively, having fixed points instead of chirality or randomness increases the difficulty of designing distributed algorithms. In fact, the main challenges usually arise from symmetries. In this respect, chirality or randomness are powerful tools to break possible symmetries. Fixed points, on the other hand, might be helpful in cases where the matching between robots and fixed points is trivial, but generally this is not the case, and the proposed strategies must handle unbreakable symmetries. It turns out that some configurations for the epf problem are unsolvable, meaning they do not admit any deterministic algorithm. This unsolvability result holds even when considering synchronous robots. Any algorithm designed to solve the epf problem must account for this result, as unsolvable configurations must be avoided by robots during their movements.", "section": "Related Work", "doi": "10.1007/978-3-662-53426-7_7", "references": [4878893, 179805188, 182867387, 627224437, 1016708617, 1479990946, 1507757834, 1696966082, 1880301761, 1975531816, 2078130991, 2082183470, 2087073465, 2284162315, 2485686185, 2963504389]}
{"paragraph": "Many publications have studied spontaneous speech effects in dialog situations like human–machine or human–human interaction, but much less has been published about spontaneous monologues like presentations or dictations. This work is concerned with spontaneous medical reports, coming from an interaction-free dictation scenario in which the speakers expect to be transcribed by human transcribers. This important application area for automatic speech recognition has hardly been addressed in recent years. Compared to conversational speech, the spontaneous dictation task exhibits a more professional dictation speaking style, a larger rate of speech fluctuations, and different disfluency characteristics. Another important difference is the existence of repetitive passages in the dictations, which are known to both parties—speaker and transcriptionist—beforehand. In these parts, the speaking style is often sloppy with an extremely high rate of speech.", "section": "Methodology", "doi": "10.1016/j.specom.2005.08.003", "references": [2308727, 5457946, 11539304, 21917968, 93383444, 108207029, 151705834, 153989523, 187680888, 200325626, 205500098, 1517982047, 1577618146, 1579752022, 1608231129, 1618325928, 1675297740, 1763005514, 1764939238, 1783786615, 1884028435, 1910206629, 1975800736, 1976795180, 1979136262, 1979447841, 1992155534, 2050492193, 2077804127, 2093729408, 2100734170, 2103127925, 2107086364, 2115900267, 2122028591, 2123524045, 2127646729, 2127894527, 2130628335, 2135185008, 2147794814, 2154226865, 2165504593, 2166810516, 2169883442]}
{"paragraph": "In recent years, with the emergence of computational intelligence, intelligence-oriented algorithms such as genetic algorithms, simulated annealing, and tabu search have been employed to solve scheduling problems. Another important method in simulating biological processes is the immune algorithm, which imitates the defense process of the immune system against invaders in a biological body. There has been growing interest in immune algorithms and their applications. However, it is rare to find applications of immune algorithms in production planning and scheduling. It has been cited that the merits of immune algorithms and other probabilistic optimization algorithms such as genetic algorithms include the following: (1) immune algorithms operate on the memory cell, which guarantees fast convergence toward the global optimum; (2) they include an affinity calculation routine to embody the diversity of the real immune system; and (3) the self-adjustment of the immune response can be represented by the help or suppression of antibody production. In this paper, an immune evolutionary algorithm is proposed for solving sequence-dependent setup time hybrid flow shop problems. The structure of the paper is as follows: Section 2 provides a literature review of hybrid flow shop scheduling; Section 3 describes the problem; Section 4 introduces the proposed immune algorithm; Section 5 presents the experimental design, comparing the results achieved by the proposed immune algorithm with those achieved by a past genetic algorithm; and Section 6 presents conclusions and future work.", "section": "Introduction", "doi": "10.1016/j.amc.2005.11.136", "references": [152978804, 1639032689, 1923211672, 1969926095, 1970329178, 1976720470, 1991511510, 2019704988, 2022864263, 2025873924, 2048873858, 2068471948, 2071511809, 2102785645, 2123161631, 2128973482, 2137272305, 2142183404, 2164934768]}
{"paragraph": "The key to improving the performance of parallel sparse LU factorization on second-class message passing platforms is to reduce inter-processor synchronization granularity and communication volume. In this paper, we examine the message passing overhead in parallel sparse LU factorization with two-dimensional data mapping and investigate techniques to reduce such overhead. Our main finding is that this objective can be achieved with a small amount of extra computation and slightly weakened numerical stability. Although such trade-offs may not be worthwhile on systems with high message passing performance, these techniques can be very beneficial for second-class message passing platforms. In particular, we propose a novel technique called speculative batch pivoting, in which large elements for a group of columns across all processors are collected at one processor, and the pivot selections for these columns are made together through speculative factorization. These pivot selections are accepted if the chosen pivots pass a numerical stability test; otherwise, the scheme falls back to the conventional column-by-column pivot selection for this group of columns. Speculative batch pivoting substantially decreases the inter-processor synchronization granularity compared with the conventional approach. This reduction is achieved at the cost of increased computation, specifically the cost of speculative factorization.", "section": "Related Work", "doi": "10.1145/1088149.1088196", "references": [143438160, 1981404639, 2003105834, 2004656897, 2017431061, 2023924583, 2027880019, 2038205735, 2041720103, 2069849438, 2070299075, 2111912335, 2119197001, 2141331848, 2145118117]}
{"paragraph": "This problem is tackled on the theoretical side by introducing the semantic framework of coalgebraic modal logic, which covers all logics mentioned above and many more. It turns out that coalgebraic modal logic allows the design of generic reasoning algorithms, including a generic tableau method; this generic method can in fact be separated from the semantics and developed purely syntactically. Generic tableau algorithms for coalgebraic modal logics, in particular one such algorithm, have been implemented in the reasoning tool CoLoSS. As indicated above, it is a necessary feature of generic tableau systems that they potentially generate multiple successor nodes for a given modal demand, so that in addition to the typical depth problem, proof search also faces a noticeable problem of breadth. The search for optimization strategies to improve the efficiency of reasoning thus becomes increasingly important. Here we present one such strategy, which is generally applicable but particularly effective in reducing both depth and branching for the class of conditional logics. We exploit a notable feature of this class—namely, that many of the relevant rules rely heavily on premises stating equivalence between formulas. Thus, conditional logics are a good candidate for memoising strategies, applied judiciously to short sequents. We describe the implementation of memoising and dynamic programming strategies within CoLoSS and discuss the outcomes of various comparative experiments.", "section": "Introduction", "doi": "10.1016/j.entcs.2010.04.012", "references": [160240489, 1554477044, 1583784794, 1982431152, 2039834595, 2062285040, 2086035610, 2097313873, 2104166006, 2108446683, 2109058602, 2121827343, 2167981859, 2173324610, 2250074872, 2610670723]}
{"paragraph": "Additional information flow inside the network is required to resolve singularity-related issues. ResNet has practically demonstrated that adding additional information flow to the network architecture can offer better performance and mitigate depth-related issues. Skip connections are described as extra links among different layers, carrying information from top layers to bottom layers. Skip connections have significantly improved the training of very deep neural networks since their introduction. Besides offering diverse and additional information to the final network layers for better classification, skip layers also contribute to eliminating singularities. Residual networks of residual networks are another variant of ResNet that aim to resolve the diminishing feature problem. They add level-wise shortcut connections to the original residual network, which improves its learning capacity. As a result, they exhibit excellent performance in image classification tasks compared with the original ResNet architecture, although they lack diversity of visual features essential for final detection tasks. To mitigate network singularities, this research proposes a network design that introduces a novel communication flow and offers a diversity of features for the final assessment task. The proposed network architecture uses a different method to transfer valuable details from top to bottom layers. The skip connection design and placement resolve three types of singularities. These additional and diverse communication links are based on a well-assessed feature flow architecture, trained and tested for performance improvement. We propose a general three-level skip connection architecture that can be applied to any convolutional neural network by positioning skip connections at three different levels of the architecture. The proposed hypothesis was initially tested with ResNet and resulted in improved performance with fewer network parameters compared with early benchmark approaches. It also provides a shallow and wide CNN design that avoids feature diminishing problems. Experiments were performed on popular benchmark datasets CIFAR-10 and CIFAR-100, and the results were compared with those of similar approaches to evaluate the proposed method.", "section": "Methodology", "doi": "10.1007/s11390-019-1950-8", "references": [104184427, 1533861849, 1665214252, 1677182931, 1690739335, 1799366690, 1904365287, 1994197834, 2001689396, 2036317923, 2095705004, 2097117768, 2107878631, 2117539524, 2125911849, 2147800946, 2152424459, 2155893237, 2168894214, 2194775991, 2274287116, 2300242332, 2302255633, 2331143823, 2418863753, 2498789492, 2549139847, 2618530766, 2753301142, 2896910005, 2949117887, 2950005703, 2951627457, 2963551763]}
{"paragraph": "Even when considering weak labels, all multilabel methods we are aware of still produce complete predictions as outputs. However, given the complexity of the prediction task and the likely presence of missing data, it may be sensible to aim for cautious yet more trustworthy predictions. That is, it may be beneficial for the learner to abstain from making a prediction about a label whose relevance is too uncertain, so that the final prediction is partial but more robust—in the sense that a prediction is made only for those labels about which there is sufficient information. Such partial predictions can help identify which instances are hard to predict, thereby highlighting where more information is needed. They could be used in active learning settings or simply to warn the user or analyst that the current information does not allow a reliable prediction for a particular instance. For example, when trying to detect protein or gene functions, it could be useful for a domain expert to know which function requires additional data collection. Various approaches have been proposed to obtain such partial predictions. A classical approach is to implement a reject option or a partial version of it. More recent methods include the use of probability sets, where the size of the set reflects uncertainty, or conformal prediction. Other proposals have addressed the problem of making partial predictions for label ranking, of which multilabel problems can be seen as a special case.", "section": "Introduction", "doi": "10.1016/j.patcog.2015.04.020", "references": [80699098, 1483977290, 1507048016, 1847363402, 1963986132, 1999954155, 2021968313, 2036376268, 2040389268, 2051995220, 2063756363, 2084465406, 2118712128, 2121826357, 2127570136, 2140335411, 2146241755, 2149684865, 2154775390, 2156935079, 2158216734, 2161619158, 2161813894, 2161824996, 2162168660, 2165766797, 2166912588, 2171585602, 2171871229, 2278792521]}
{"paragraph": "Real-time systems have a very important non-functional requirement, which is the concern about timing aspects such as deadlines, maximum jitter, worst-case execution time, tolerated delays, and others. The complexity related to the non-functional analysis of these systems increases when they become distributed and embedded. To address some of these non-functional requirements, some proposals suggest the use of aspects. Aspects can help manage crosscutting concerns in the design of distributed embedded real-time systems by modularizing their handling. In addition to modularization, more abstract aspects are easier to reuse because they define how to handle a concern at a high level and describe how it can be applied or how it affects the system without imposing implementation or platform constraints. This work presents the Distributed Embedded Real-time Aspects Framework, which provides an extensible set of aspects to manage non-functional requirements of distributed embedded real-time systems at a high level of abstraction. In other words, it is a set of implementation-independent aspects to handle non-functional requirements during the creation of real-time UML models at the design phase. In this initial version, the framework addresses the following non-functional requirements: timing, precision, performance, distribution, and embedded behavior. It was created to be extensible so that support for other important aspects, such as fault tolerance, can be added. However, any new aspects must follow the high-level and implementation-independent nature of the framework. It is important to highlight that despite the high level of abstraction, aspects within the framework must be implementable; that is, the realization of each aspect must be feasible.", "section": "Related Work", "doi": "10.1007/978-3-540-76811-1_4", "references": [1490892580, 1991003521, 2004191252, 2029414465, 2063162028, 2073781861, 2096646560, 2108267083, 2112021951, 2141727300, 2143259336, 2148849581, 2154843963, 2158864412, 2168561921, 2389684430]}
{"paragraph": "Since UWB information-bearing pulses are ultrashort, timing offset may significantly deteriorate bit error rate performance. Hence, timing synchronization is a major challenge for the implementation of UWB receivers. Currently, there are very few studies on the timing synchronization issue in the literature concerning the FM-DCSK UWB system. However, several papers have addressed the timing synchronization problem for coherent UWB by correlating the template signal with the received signal to achieve timing acquisition. Since FM-DCSK UWB is noncoherent, timing synchronization algorithms specific to noncoherent UWB systems must be developed. Existing algorithms for noncoherent time-reversal UWB systems rely on two main operations: correlation between two neighboring symbols and selecting the peak from a large set of correlation values. However, the correlation operation is not applicable to the FM-DCSK UWB system for two reasons: first, a chaotic waveform varies from symbol to symbol, even if the same bit is transmitted repeatedly, which is a key difference between FM-DCSK UWB and conventional noncoherent time-reversal UWB; second, noise-like chaotic signals have low intersymbol correlation values. As a result, the peak-picking operation generally cannot be performed correctly.", "section": "Methodology", "doi": "10.1109/TIE.2009.2038402", "references": [1974598773, 2096169224, 2100052814, 2110243397, 2111075016, 2123658061, 2127535650, 2138464758, 2138634465, 2146716166, 2156456259, 2158356902, 2161563959, 2163850973, 2171593097, 2171821911, 2172212285]}
{"paragraph": "This type of architecture guarantees scalability, as specific functions can be defined in the controllers to establish their role within the overall network. In this way, only the traffic destined for other networks is sent to the controllers at the top level. In a hierarchical architecture, each controller manages its domain and distributes the necessary data to other controllers. To ensure good performance in the hierarchy, proper communication among controllers is essential. To the best of our knowledge, there is no standard for SDN controller communication. However, several works focus on formal concepts and theoretical architecture. In this paper, we aim to contribute to the design of the hierarchical SDN control plane by developing a testbed composed of SDN controllers federated through a data distribution service. Our implementation represents a first step toward ensuring a robust control plane for 5G networks, where control elements can distribute network information and respond quickly to network and controller failures. The main contributions of this paper are as follows: a communication mechanism to logically distribute SDN controllers using either a flat or hierarchical architecture; auto-discovery of SDN controllers and detection of controller failures in a short time frame; and an authentication procedure to prevent unauthorized publishers from introducing incorrect messages.", "section": "Introduction", "doi": "10.1109/ICCCN.2019.8847035", "references": [192446467, 1531707100, 1579339470, 1769222792, 1829787244, 2010148125, 2022904807, 2074616737, 2087946700, 2089939717, 2094656022, 2118404561, 2524169362, 2798915702, 2807667769]}
{"paragraph": "In view of the above problems, a web service recommendation method that combines word embeddings with topic models is proposed in this paper. First, the method uses word embedding technology to train a word embedding model on Wikipedia to obtain semantically similar words. Then, it applies a GPU-accelerated Dirichlet Multinomial Mixture model to mine latent topics of web services for similarity calculation. Finally, multi-dimensional features such as similarity, popularity, and co-occurrence are modeled using deep factorization machines to predict and recommend Top-N web APIs. The proposed method uses a real dataset crawled from the Programmable Web platform. The main contributions of this work are as follows: we present a topic vector model based on word embedding, which uses Wikipedia to construct a high-quality word embedding model for identifying semantically similar words, and integrates these into topic derivation to obtain a more effective topic distribution using a GPU-accelerated DMM strategy; we model multi-dimensional information—including similar web APIs, similar mashups, popularity, and co-occurrence of web APIs—using deep factorization machines to rank and predict the Top-N web API recommendation set; and we conduct experiments on real crawled data. Compared with existing web service recommendation methods, the experimental results show that our approach achieves better performance in recall, precision, F-measure, and NDCG.", "section": "Related Work", "doi": "10.1109/bdcloud.2018.00133", "references": [66295247, 1714665356, 2011336438, 2054705465, 2061922307, 2076219102, 2091381870, 2097089247, 2112465256, 2123476762, 2139317750, 2140597141, 2171836785, 2340381866, 2611274474, 2754483871, 2756226611, 2951001079]}
{"paragraph": "Within the field of trust research, it is generally agreed that trust is a multi-dimensional construct. However, there is no consensus on the specific dimensions of trust. Some researchers believe that trust is composed of three elements: benevolence, honesty, and competence, while others suggest that specific beliefs such as integrity, ability, and benevolence are the foundations of general trust. Additionally, many researchers have examined the conceptualization of trust. For example, one study shows that common synonyms for trust include competence, credibility, confidence, faith, hope, loyalty, and reliance, while another defines trust as a three-part relationship in which one person trusts another to perform a specific action. Various trust models have been developed from different perspectives. A review of these models shows that researchers have approached trust modeling from both systemic and technical viewpoints in computer and information sciences, resulting in numerous models. Alongside these, there are many generic models of trust that focus on functional trust and the factors influencing trust formation in daily activities. Typologies and frameworks of trust have also been proposed; however, none of the existing models have taken a generalized approach to modeling trust at the societal level. Although prior models have provided detailed reviews of trust constructs, they often lack a relational approach that addresses different actors and levels of trust in society. As a result, ambiguity remains in identifying what type of trust corresponds to what level of societal relationship. This gap has been noted, with calls for future research to explore trust at the macro level. While acknowledging the contributions of previous studies in defining trust and its constructs, this study does not aim to present a new definition of trust. Instead, it identifies emergent types and concepts of trust in society. The goal is to establish a common ground for communicating research findings and to improve understanding of the different forms of trust in societal contexts. This foundation can help researchers effectively share insights across studies. Accordingly, this study seeks to answer the following questions: What are the different types and aspects of trust in previous studies? Is it possible to develop a more generic model for trust?", "section": "Methodology", "doi": "10.4018/IJEGR.2016070104", "references": [560557252, 1553725291, 1562663307, 1584583063, 1974573010, 1979710488, 1982369977, 1984099372, 1998812248, 1999376348, 2020625361, 2026694085, 2032852354, 2042214850, 2071680986, 2072273878, 2073782236, 2074738896, 2076452960, 2090278726, 2094747961, 2130006747, 2131896026, 2134798318, 2153804514, 2159350444, 2163067277, 2169361415, 2407409432, 2649135381]}
{"paragraph": "In the MRAG model, nodes are represented by the centroid of each region as a result of the initial oversegmentation, and weighted edges are defined based on the spatial distance between nodes, their corresponding brightness mean value, and the corresponding region sizes. The minimum semantic information is assumed to be given by each watershed region, so the mapping from the original oversegmented image to the MRAG is straightforward, with each region mapped to one node. Next, an optimal bipartition that minimizes the normalized cut value for the image graph is computed. This process is successively repeated for each of the two resulting regions using a binary splitting scheme until a termination condition is met. A clear computational advantage is obtained when describing the image as a set of regions instead of pixels in the MRAG structure. This enables faster region merging in oversegmented images with higher spatial resolution than reported in previous literature. The definition of MRAG and the application of a hierarchical social metaheuristic to efficiently solve the normalized cut problem form the core of the proposed method. Our approach to modeling and solving image segmentation as a graph-partitioning problem is related to prior work that used a computational technique based on a generalized eigenvalue problem for computing segmentation regions. Instead of that method, we found that higher quality segmentation results can be obtained by applying hierarchical social algorithms through an iterative solution of the normalized cut problem.", "section": "Introduction", "doi": "10.1016/j.patrec.2005.07.022", "references": [1575739010, 1579376917, 1589880601, 1649464328, 1972969203, 1975442866, 1999478155, 2003004389, 2063266501, 2066356146, 2101309634, 2107941094, 2114224867, 2121947440, 2124260943, 2126140058, 2126863699, 2129872026, 2132603077, 2134737843, 2141578730, 2150010279, 2168990496, 2294259878, 2649919590, 2898704628]}
{"paragraph": "Since using either kind of criteria alone is not sufficient to obtain optimal results, several works have attempted to query unlabeled samples with both high informativeness and high representativeness. These approaches are often heuristic in designing specific query criteria or ad hoc in measuring the informativeness and representativeness of samples. More recent work has attempted to integrate both discriminative and representative information into a unified optimization framework. One approach used uncertainty as the query criterion and incorporated unlabeled data in a semi-supervised learning setting to boost learning performance. Another batch mode semi-supervised active learning formulation extended batch mode active learning under a min-max optimization framework. However, these methods may fail to preserve the original data distribution. If the data structure does not satisfy the semi-supervised learning assumptions, they may not perform well. In this article, we extend the empirical risk minimization principle to active learning and present a novel active learning framework. In this framework, we adapt maximum mean discrepancy to measure distribution differences and derive an empirical upper bound for active learning risk. By minimizing this bound, we approximately minimize the true risk under the original data distribution. We propose a practical batch mode active learning algorithm that queries a subset of unlabeled samples to help minimize generalization risk. The samples we query not only help to reduce empirical risk on the training data but also preserve the original data distribution, resulting in strong generalization for unseen samples. This allows us to simultaneously leverage both discriminative and representative information. Moreover, our method naturally handles scenarios with or without initial labeled samples and achieves high efficiency in both cases. We also propose a multiclass active learning algorithm under this framework. Experiments on benchmark datasets and real-world applications demonstrate the effectiveness of the proposed method compared to state-of-the-art batch mode active learning approaches.", "section": "Related Work", "doi": "10.1145/2700408", "references": [1483816357, 1484084878, 1520757484, 1528361845, 1599935123, 1968200975, 1978633512, 1991357106, 1991589570, 1993202648, 1994389483, 2006484294, 2016405781, 2080021732, 2102873759, 2104848109, 2109059049, 2114188922, 2114338449, 2124331852, 2132948751, 2133864802, 2137795521, 2138032594, 2139212933, 2139238311, 2140524605, 2152798516, 2153635508, 2153818524, 2164278908, 2164943005, 2165484066, 2167595980, 2178357350, 2212660284, 2426031434]}
{"paragraph": "However, for a Markov chain grey model, it is not easy to determine the number of states and their bounds—these parameters are usually specified in advance based on experience, and the modification range for a predicted value derived from the original grey model is identical to its corresponding predicted residual from the Markov chain. These factors can affect prediction performance. To address this, and because of the advantages of combining grey prediction with neural networks, we propose a residual modification model based on neural networks, called NN-Grey-Markov. This model incorporates a functional link network with strong function approximation capabilities to estimate the modification range corresponding to a predicted residual from the Markov chain. A genetic algorithm is employed to determine the connection weights of the network, the number of states, and the bounds of each state to construct the grey prediction model with high prediction accuracy. Foreign tourist forecasting can be regarded as a grey system problem since tourism demand is influenced by various uncertain factors such as exchange rates, security, and disease. The exact impact of these factors is difficult to quantify. The diversity and volatility of the international tourism market make foreign tourist prediction a challenging task for tourism administrators. The global tourism industry significantly affects national economic development, and accurate forecasting of foreign tourists plays a crucial role in forming tourism development plans for cities and countries. This motivates us to examine the prediction performance of the proposed residual modification model in the context of foreign tourist forecasting.", "section": "Methodology", "doi": "10.3390/info8040126", "references": [1634072176, 1994934343, 2011864101, 2012638612, 2014615732, 2019123741, 2033613958, 2043792085, 2088368357, 2119712504, 2194513117, 2287571625, 2494507164, 2553848975, 2566102230, 2586921189]}
{"paragraph": "Hidden algebra is based on the notion of equational behavioral satisfaction, meaning that hidden specifications characterize how objects behave in response to a given set of experiments. Hidden algebra can handle some of the most complex features of large systems, including concurrency, nondeterminism, and local states, as well as features common to the object-oriented paradigm, such as classes, subclasses (inheritance), attributes, and methods. It also supports logical variables, abstract data types, and generic modules. CCS is a calculus used to specify the behavior of interactive systems. A CCS process expresses interactions between subsystems and the system’s ability to interact with other concurrently running systems. Essentially, a CCS process describes communication scenarios the designed system should be capable of performing, and thus, we use CCS to specify communication requirements. We extend the algebraic specification with synchronization elements similar to CCS communication channels; such channels link the object initiating a method invocation with the corresponding object method. The formal operational semantics of hiddenCCS integrates the state transition semantics of hidden algebra with the CCS reduction rules using these synchronization elements. In this way, we provide a foundation for complex derivations and a reasoning system that combines hidden logic with Hennessy-Milner logic. This new specification technique is highly flexible, allowing the reuse of both object specifications and coordinating CCS modules.", "section": "Introduction", "doi": "10.1007/978-3-540-24756-2_17", "references": [1503973138, 1506096824, 1559723191, 1603799276, 1871480302, 1971107784, 2001487918, 2033809864, 2052867443, 2056488621, 2130176451, 2131784087, 2134287022, 2170673019, 2340735175]}
{"paragraph": "With the functional dependency Player + Team, adding the tuple (tm) to the last relation does not change the set of weak instances. The reason is that any relation on attributes Player, Team, and Manager that contains tuples (p, t, ml) and (p, tl, m) for some tl and ml must have t = t1 to satisfy Player + Team. In Section 3 we define two legal states of a database scheme to be equivalent if they have the same set of weak instances. We show that two states are equivalent if and only if their associated tableaux, constructed as suggested by Honeyman, are equivalent tableaux. We show that every equivalence class of states contains a unique maximal element, called the completion of the class, that contains all the other states in the class. The completion of a state can be obtained by chasing the associated tableau and then taking the total projections of the chase onto each of the relation schemes. It might be objected that the set of weak instances for a database state is too large to be meaningfully associated with the state. For an extreme example, if the database state is empty, then any universal relation that satisfies the constraints is a weak instance. It is intuitively more appropriate to consider only the weak instances that are minimal, in a sense defined in Section 3. We also show in this section that two states have the same set of minimal weak instances if and only if they have the same set of weak instances. This result justifies the use of sets of weak instances as representatives of the information content of a database state. In Section 4 we discuss how our notion of state equivalence relates to the intuitively appealing idea that two states are equivalent if they produce the same answer to every reasonable query. We show that, under the query semantics most appropriate to the context of this work, the notion of equivalence proposed in Section 3 coincides with query-based equivalence.", "section": "Related Work", "doi": "10.1145/329.318579", "references": [1566656512, 1973125170, 1975714036, 1979514837, 1985581502, 2001665375, 2011239742, 2020640185, 2027009365, 2039610696, 2046736543, 2053741160, 2061462629, 2084927067, 2089954064, 2122147831, 2139238107, 2177275277, 2232207765]}
{"paragraph": "Feature selection often depends on a joint consideration of two conflicting aspects: computational cost and achievable performance. The best choice usually represents an optimal tradeoff between these factors. The challenge lies in how to reach a useful dimension reduction while conceding minimum sacrifice on accuracy or other desired performance. Feature selection methods can be divided into two categories: filter and wrapper. In the filter method, feature selection and classifier design are separated in that a subset of features is first selected and then classifiers are trained based on the selected features. For example, the discriminative power of features can be ranked according to their discriminant ratio so that features with high ranks are retained. The wrapper approach, on the other hand, uses classification accuracies or criteria derived from the classifier to rank the discriminative power of all or part of the possible feature subsets and select the subset that produces the best performance. Therefore, the selected features are bound to the type of classifier that was used in the feature selection process. Both filter and wrapper methods have their own merits and limitations. For example, although filter methods are simple and fast, the selected features may be highly correlated because the feature set may contain many highly discriminative features with almost identical characteristics. Moreover, most ranking criteria do not take the combined effect of features into account. This paper proposes fusing the filter and wrapper methods to leverage the advantages of both methods. Evaluations on a subcellular localization task and a gene selection problem demonstrate that the two feature selection approaches are complementary to each other.", "section": "Methodology", "doi": "10.1016/j.neucom.2008.04.024", "references": [1492753585, 1583700199, 1973714307, 1995918845, 2017337590, 2026478983, 2108728387, 2112733344, 2116811581, 2118142823, 2124225314, 2127103767, 2128036349, 2143426320, 2154053567]}
{"paragraph": "Nonparametric Bayesian models are powerful probabilistic signal modeling tools that have been extensively employed in the context of compressed sensing. These models have also been used to model nonlinear data manifolds in various applications. As discussed later, such manifold models—namely Mixture of Probabilistic Principal Component Analyzers and Mixture of Factor Analyzers—are capable of representing complex data structures such as spatial heterogeneity. Because these manifold models are flexible enough to fit nonlinear data spaces, such as those defined by a training image, and to tightly capture the data space, they provide important modeling information that enables reconstruction of the complete signal from a small number of samples. However, the location of samples significantly affects reconstruction performance. This sensitivity can be leveraged to identify sampling locations that are most likely to provide useful spatial information. It should also be noted that many other probabilistic modeling tools developed in image processing literature could be used to address the sampling design problem based on training images. The main contribution of our paper is to employ multiple-point statistics for sampling design. We believe that two important factors must be considered in selecting the location of samples: the signal behavior, which can be modeled using a manifold model based on the training patterns of a training image; and the locations and values of previously drawn samples.", "section": "Introduction", "doi": "10.1016/j.cageo.2016.03.009", "references": [1949082156, 1966740773, 1967277558, 1980961028, 2004465977, 2050460554, 2063114804, 2071284784, 2098316308, 2102129292, 2109320267, 2113421750, 2113459360, 2115429828, 2132792959, 2132804635, 2134033146, 2139183155, 2142940228, 2150045166, 2153663612, 2157278886, 2161610387, 2164329028, 2167404137, 2224876336, 2296616510, 2568283273]}
{"paragraph": "Compared to desktop computing, designing hardware and software for mobile computing presents a host of unique challenges, particularly because location, environment, connectivity, and other important factors are commonly unpredictable and dynamic. The strategies that have been demonstrated to be effective for desktop computing are only minimally useful for mobile computing. Clearly, different design and evaluation paradigms need to exist for mobile computing devices and environments. One study cites the inadequacy of the desktop metaphor for mobile computing for information presentation. This is merely a single example of the dissonance between effective desktop and mobile computing strategies. Another source notes that human-computer interaction has developed a good understanding of how to design and evaluate forms of interaction in fixed contexts of use, but this is not the situation for mobile computing. This highlights the issue of differences between desktop and mobile computing in terms of contexts of use. It has been pointed out that for traditional desktop computing applications, tasks take place within the computer, while for mobile computing, tasks typically reside outside of the computer, such as navigation or data recording. Thus, in many mobile computing interactions, there are multiple tasks taking place, often with the mobile task being secondary, which is why the context of use must be considered.", "section": "Related Work", "doi": "10.1007/s00779-006-0063-x", "references": [206199432, 1492397400, 1500396425, 1538394126, 1548428445, 1566248475, 1569698616, 1971733068, 1985951810, 1991250949, 2004411368, 2005117804, 2020721196, 2030690140, 2031218379, 2041545805, 2070988579, 2080957047, 2092478745, 2096964310, 2115321629, 2122058736, 2129480985, 2148926335, 2163396398, 2169732913]}
{"paragraph": "The challenge further escalates when our auction model involves operational costs of servers in the computation of social welfare and profit, even in expectation. Most existing auction designs ignore such production costs of resources and consider social welfare as only the overall value of accepted bids and profit as the overall payment. Significant difficulties arise when resource costs are deducted in calculating social welfare and profit, especially in the online setting. The allocation problem with server costs involves a combination of packing and covering constraints—such problems are more challenging than those with only packing constraints as in previous models without server costs. Further, we aim to consider more generic server cost functions that are convex rather than linear, and appropriate techniques for handling such non-linear costs have only emerged recently. This paper leverages a recent development in primal-dual online algorithm design and randomized reduction techniques to design a set of truthful, polynomial-time online auctions for social welfare maximization or profit maximization in expectation with good competitive ratios. Our mechanisms consist of two main modules: an online primal-dual optimization framework for virtual machine allocation to maximize social welfare while accounting for server costs, and for revealing payments through dual variables to ensure truthfulness; and a randomized reduction algorithm to convert the social welfare maximizing auctions into ones that yield maximal expected profit for the provider, with competitive ratios comparable to those for social welfare.", "section": "Methodology", "doi": "10.1109/TNET.2016.2619743", "references": [1806207712, 1860570137, 2001024980, 2009569367, 2022575296, 2025616337, 2041211404, 2072548042, 2077124610, 2078453919, 2079367169, 2081931751, 2091175924, 2101658191, 2109546606, 2125053293, 2129123493, 2129542763, 2131726714, 2142339200, 2147300694, 2148459868, 2153998850, 2266750254, 2281104701, 2310680702, 2780261041, 2953214254]}
{"paragraph": "Another recent example is a study that proposed a fast computation method by combining certain properties of the Tchebichef polynomial with a block segmentation of the images. It is important to stress that some of these improvements are for the 2-D Tchebichef moments, and not for the radial Tchebichef moments. To the best knowledge of the authors, there is no deep analysis of the discriminative characteristics of the radial discrete Tchebichef moments used in pattern recognition of large datasets. Different images have been used to demonstrate the reconstruction characteristics of the Tchebichef and other moments. However, the same images are not appropriate for the discriminative characterization due to the limited number of images for a single object or class. The main contribution of this paper is to show the performance of the Tchebichef moments computed from grey-scale images for two large datasets, MNIST and ICDAR 2013. This is achieved by training several multi-class classifiers using an implementation of AdaBoost.MH. The choice of AdaBoost is based on the fact that it is one of the best off-the-shelf machine learning algorithms and is very robust against overfitting.", "section": "Introduction", "doi": "10.1145/2683405.2683433", "references": [1974177088, 2032210760, 2037364089, 2072419029, 2098280924, 2110458324, 2112256856, 2116886646, 2124041826, 2133990480, 2135883983, 2137401668, 2147800946, 2154579312, 2184241488]}
{"paragraph": "The focus on minimizing store and runtime impedance strongly impacts the implementation of Platypus and directly led to the development of a zero-copy buffer cache implementation. The zero-copy buffer cache presents a number of technical hurdles, notably in relation to the recovery algorithm and controlling page write-back. The construction of a flexible, robust, high-performance transactional store is a major undertaking and, in our experience, is filled with challenges. In this paper, we touch on just a few aspects of the construction of Platypus. We address the system architecture because it embodies decisions that were key to achieving both flexibility and performance. We also discuss major performance issues encountered during implementation, as the solutions we developed are novel and likely to be applicable to other systems. For other details of the construction of Platypus that are not novel, such as the workings of the recovery algorithm and the choice of buffer cache eviction policies, the reader is referred to the extensive literature on database and store design and implementation.", "section": "Related Work", "doi": "10.1007/3-540-45498-5_10", "references": [101498165, 1506285740, 1514938825, 1557189078, 1586721701, 1705155330, 2010042648, 2027775657, 2076162755, 2082151259, 2099117306, 2100012073, 2104954161, 2110631345, 2130055503, 2151791866, 2205031991, 2468851175]}
{"paragraph": "Compositionality appears to be a helpful paradigm not only for coping with complexity when designing and implementing embedded systems but also when analyzing them in early design phases. The framework of modular performance analysis is a formalism for the performance evaluation of distributed real-time systems. Its methodology originates from real-time calculus, which uses functions on the time-interval domain to abstractly represent system workloads and the availability of computation and communication resources. Component interaction is modeled by sets of flow-bounding functions rather than concrete streams of events or data packets. The corresponding interaction between abstract components leads to a compositional approach to compute key performance metrics of system-level designs, such as buffer spaces, throughput bounds, and end-to-end delays. However, the obtained worst and best case properties, such as backlog sizes and response times, are only tight bounds if the model of computation inherent to real-time calculus is an adequate abstraction of the system under analysis. The present paper describes various methods to combine computational, state-based modeling and analysis methods with compositional, analytic methods. This way, the expressiveness of state-based modeling formalisms can be leveraged while keeping computational complexity restricted to the intra-component level. Embedding into an inter-component analytic analysis framework enables scalable, compositional analysis of system-wide properties. It appears that interfacing model classes with different abstraction mechanisms is far from trivial. In the following, we describe the necessary basic concepts that allow embedding models from classical real-time scheduling theory, finite state machines, and timed automata into the modular performance analysis framework.", "section": "Methodology", "doi": "10.1109/DATE.2011.5763143", "references": [59365162, 1936438023, 1962072139, 2046593656, 2058252772, 2113301111, 2113488668, 2116142540, 2124262702, 2125634417, 2136711971, 2147206873, 2153269726, 2157779731, 2164751066, 2169965763]}
{"paragraph": "In asynchronous gossip, our first result demonstrates the inherent cost of asynchrony and crashes. Indirectly, this result indicates that the techniques developed in the synchronous world cannot be efficiently transferred to an asynchronous environment. Specifically, we show that any asynchronous gossip protocol—either deterministic or against an adaptive adversary—that tolerates faults has either high message complexity or high time complexity. Notice that the trivial gossip algorithm, in which each process sends its rumor directly to everyone else, has quadratic message complexity and time complexity linear in network delay. Thus, if the number of faults is proportional to the number of processes, any protocol that improves on the trivial solution in message complexity requires a time complexity linear in the number of possible faults. This contrasts with deterministic algorithms for synchronous networks that complete in only polylogarithmic rounds using a polylogarithmic number of messages, despite tolerating many failures. In many ways, the lower bound is surprising, as epidemic-style algorithms appear relatively timing independent. Underlying our lower bound proof lies a strategy for the adversary to hinder the spread of a rumor by adaptively choosing how to delay computation and when to fail processes. The strategy forces processes to keep spreading the rumor for a long period or to increase the number of times the rumor needs to be spread. By manipulating the relative process speeds, the adversary can trick many processes into believing that the remaining ones have failed. These remaining processes face a dilemma: if they send too many messages, message complexity becomes high; if they send too few, the adversary can isolate them, resulting in a slow completion time.", "section": "Introduction", "doi": "10.1145/2450142.2450147", "references": [161179119, 1505927490, 1526359699, 1611108134, 1990476939, 2003214215, 2006699751, 2008636585, 2014644972, 2038562061, 2040993775, 2063105299, 2098319168, 2117905067, 2129095072, 2137358897, 2151350366, 2152135680, 2157004711, 2159505833, 2159716194, 2296427920, 2899702797]}
{"paragraph": "In this general case, the main questions are: how to choose the intermediate nodes that form a multi-hop path from the source node to the destination node, and how to configure each hop at the physical layer with respect to the security and throughput constraints of the path. Specifically, the problem considered in this paper is how to find a minimum cost path between a source and destination node in the network while guaranteeing a pre-specified lower bound on the end-to-end secrecy and goodput of the path. The cost of a path can be defined in terms of various system parameters. In a wireless network, transmission power is a critical factor affecting the throughput and lifetime of the network. While increasing the transmission power results in increased link throughput, excessive power results in high levels of interference, thereby reducing network throughput due to inefficient spatial reuse. With cooperative jamming at the physical layer, transmission power becomes even more important due to the additional interference caused by jamming signals if they are employed. Thus, in this work, the amount of end-to-end transmission power is considered as the cost of a path, with the objective of finding secure paths that consume the least amount of energy. Such paths, by minimizing interference in the network, also result in higher throughput. Note that solutions using power only at the nodes transmitting the messages (and not employing cooperative jamming) are included in the optimization space; therefore, if it is more efficient to not use cooperative jamming, such a solution will be identified by our algorithms.", "section": "Related Work", "doi": "10.1109/TMC.2014.2354031", "references": [1964250960, 1971146648, 1973384884, 1982170863, 1984661114, 2007529001, 2028074180, 2035357213, 2043769961, 2056746002, 2061914652, 2064310069, 2071331544, 2073516681, 2074117023, 2101560327, 2106833918, 2108247956, 2109394932, 2117420234, 2118623524, 2120890744, 2124841881, 2128153585, 2129457795, 2131603212, 2135189806, 2137775453, 2142102521, 2144007657, 2146794521, 2148967920, 2153646424, 2156214717, 2159362188, 2168527529, 2171006634, 2397223326]}
{"paragraph": "A class of concurrency bugs that are extremely difficult to debug are race conditions related to correlated variables. These race conditions involve multiple variables. Studies have shown that over 30 percent of all race conditions involve correlated variables. To the best of our knowledge, no existing work addresses the creation of parallel unit tests for data races related to correlated variables. There is no unit test framework available that supports race detectors capable of considering correlated variables in their analysis. Given that non-deterministic thread scheduling makes such races generally hard to reproduce, there is a strong need for unit tests that cover race conditions on correlated variables. In this work, we aim to combine the benefits of automatic parallel unit test generation with the advantages of race detection for correlated variables. To achieve this, our work builds on the existing unit test generator AutoRT. In this paper, we introduce an extension called CorrRT, which enhances AutoRT by identifying possible correlation violations in method pairs that access correlated variables. The higher the number of correlations found, the higher the probability that a potential race condition violates a variable correlation. We automatically generated 81 parallel unit tests for correlated variables on eight different applications. After analyzing the unit tests, a race detector for correlated variables reported more than 85 percent of the race conditions as violating variable correlations. Furthermore, we were able to reduce the number of redundantly generated tests by up to 50 percent in comparison to the original AutoRT approach.", "section": "Methodology", "doi": "10.1007/s10766-015-0363-8", "references": [1502049067, 1590406138, 1882061689, 1887412317, 1999343420, 2025001147, 2100010684, 2101161997, 2111378743, 2111759889, 2123468567, 2137422315, 2138546883, 2147109705, 2156858199, 2157539713, 2178564501]}
{"paragraph": "Evolutionary multi-tasking optimization algorithms, known as multifactorial evolutionary algorithms, are proposed to address multi-task optimization. The main features of these algorithms include unified individual encoding, cross-domain decoding, inter-task knowledge transfer, selective evaluation, and rank-based individual fitness evaluation. The core of these algorithms is inter-task knowledge transfer, primarily based on assortative mating and vertical cultural transmission. Many experimental studies have shown that these algorithms achieve better performance than their corresponding single-task evolutionary algorithms in terms of population convergence and global search ability, thanks to positive knowledge transfer among tasks. However, these algorithms are prone to getting stuck in local optima, especially when the optimal solutions of different tasks are far apart. In such cases, the failure occurs because the inter-task knowledge transferred is represented by individuals or points in a high-dimensional decision space. To address this issue, this paper proposes a new inter-task knowledge transfer strategy based on search direction rather than individuals. The proposed strategy generates offspring by summing an elite individual from one task with a difference vector from another task. The elite individual helps accelerate population convergence, while the difference vector from another task enhances search diversity. Experimental studies have demonstrated the effectiveness and efficiency of the proposed strategy compared to the classical approach.", "section": "Introduction", "doi": "10.1109/CEC.2019.8789959", "references": [873008480, 1532651649, 2023685757, 2105217850, 2131694312, 2165698076, 2294818690, 2329749247, 2413527939, 2479340554, 2589105738, 2595502370, 2616650140, 2761893695, 2763433602, 2809555178, 2892336140]}
{"paragraph": "This paper proposes a mutual-information-graph regularized sparse transform algorithm by considering both feature sparsity and the underlying manifold structure. The feature transform is defined using a transform kernel and a bias matrix. A mutual information graph is introduced to represent the underlying manifold structure of the observation data. The transform kernel and bias matrix are learned under the regularization imposed by the mutual information graph. The proposed method possesses both sparsity and local structure preservation properties, which ensure strong discriminative power and robustness in practical applications. The main contributions of this paper are as follows: a mutual information graph is introduced to represent the underlying manifold structure of the observation data; based on this graph, a sparse transform model is proposed for unsupervised feature learning; and the proposed method demonstrates superior performance compared to existing unsupervised feature learning models.", "section": "Related Work", "doi": "10.1109/ISPACS.2018.8923407", "references": [1491719799, 1677409904, 1964357740, 2025768430, 2070127246, 2097117768, 2097308346, 2108119513, 2113606819, 2117228865, 2129812935, 2139427956, 2151103935, 2154872931, 2156718197, 2163605009, 2163808566, 2295125894, 2296616510, 2499468060, 2519796145, 2527569769, 2557283755, 2766669005, 2885904013, 2962690605, 2963114249]}
{"paragraph": "To improve spectral efficiency and receiver performance, noncoherent techniques, including noncoherent data detection and blind or semiblind channel estimation methods, have been proposed. These techniques are appealing because they use only a few pilots, or even no pilot, for data detection and channel estimation, making them well-suited for fast time-varying channels. However, most existing works focus on two separate scenarios: coded flat-fading space-time systems without OFDM, and uncoded OSTBC-OFDM systems without channel coding. While it is possible to extend previous works on flat-fading systems to coded OSTBC-OFDM by treating each OFDM subcarrier as an individual flat-fading channel, such extensions require the channel to remain static over many OFDM blocks. Similarly, many blind channel estimation methods for uncoded OSTBC-OFDM also require the channel to remain unchanged for several OSTBC-OFDM blocks. These approaches are therefore better suited to slow or moderately fast fading channels. Another noncoherent approach involves using differential space-time coding schemes; however, these suffer from a 3 dB signal-to-noise ratio loss compared to coherent receivers. In our previous work, a noncoherent OSTBC-OFDM detection method based on a deterministic blind maximum-likelihood criterion was proposed. It was shown that this method can achieve near-coherent performance using only one OSTBC-OFDM block, making it suitable for fast time-varying channels. However, channel coding was not considered in that work.", "section": "Methodology", "doi": "10.1109/TWC.2012.071612.111936", "references": [1973382494, 1982812403, 1988892531, 2003774343, 2091343073, 2109153506, 2110659753, 2114115739, 2114518213, 2115786126, 2117033416, 2120211641, 2120896095, 2124098952, 2128359040, 2129893919, 2131828276, 2136347775, 2140577326, 2145257226, 2160828051, 2162747867, 2166066512, 2167242540]}
{"paragraph": "Based on the above discussion and analysis, we propose a new routing protocol called IRQV. In this paper, we treat dynamic route selection with optimal QoS as an optimization problem and apply the ant colony optimization algorithm to solve this NP-hard problem. First, based on the movement direction of the communicating pairs and the distance to neighboring intersections, IRQV determines the terminal intersection. Then, using global QoS and derived local QoS, several forward ants are sent to explore available routing paths between two terminal intersections, while backward ants update global QoS and select the optimal route. During data packet forwarding, IRQV dynamically selects the best next intersection. When packets are forwarded between adjacent intersections, they use a simple but efficient greedy carry-and-forward mechanism, which reduces the impact of individual vehicle movement on routing paths. Compared with other RREQ-RREP-based or intersection-based routing protocols, the main contributions of IRQV are as follows: (1) The intersection-based routing paths explored by IRQV are between terminal intersections rather than end-to-end communicating vehicles. Thus, maintaining complete routes between vehicles is unnecessary, and different communicating pairs with the same terminal intersections can reuse previously explored paths. This significantly increases routing stability, reduces link failures, minimizes redundant route discovery, and relieves channel congestion in heavy traffic. (2) Forward ants are sent opportunistically based on local and global QoS instead of blind flooding, which reduces exploration time, mitigates the effects of traffic variability, and prevents congestion. (3) IRQV uses the ACO algorithm to enable different communicating pairs to collaboratively update routing information, improving adaptability to topology changes and large-scale urban environments. (4) We propose connectivity and transmission delay models for two-lane road segments to estimate local QoS. Compared to using historical traffic or periodic hello messages, our method reduces processing time, lowers congestion, and enhances QoS estimation accuracy.", "section": "Introduction", "doi": "10.1007/s11036-015-0577-4", "references": [1667165204, 1982002312, 2006339402, 2010978324, 2023262636, 2060067830, 2067028650, 2067747711, 2073892795, 2083990685, 2090435272, 2101963262, 2109094293, 2110499865, 2124523096, 2134900464, 2135035173, 2143487272, 2145417574, 2150671917, 2153107279, 2153897821, 2167071526]}
{"paragraph": "In parallel with the precision medicine initiative, an emerging computing paradigm in recent years is cloud computing. More and more data owners, such as hospitals or public health agencies, may wish to outsource their data to an external service provider that manages the data on their behalf. When datasets are outsourced to a cloud server, many security issues arise, including the need to protect the confidentiality of the data from the cloud server. To ensure data privacy, data owners must apply data anonymization or cryptographic techniques to their datasets before outsourcing them to an untrusted cloud server. One approach is to anonymize sensitive datasets prior to outsourcing. This can be done using either traditional data de-identification or anonymization techniques or differential privacy techniques that provide stricter mathematical privacy guarantees. While these techniques are effective for privacy-preserving data sharing, they typically require data generalization or perturbation, which can reduce the utility of the resulting dataset. Another approach is to use cryptographic techniques, where the data owner encrypts the dataset before outsourcing, enabling certain types of query processing on the encrypted data. To preserve query privacy, authorized clients must also encrypt their queries before sending them to the cloud server.", "section": "Related Work", "doi": "10.1007/978-3-319-41576-5_5", "references": [1589843374, 1971286892, 1972508246, 1977899329, 1993141132, 2031533839, 2041480327, 2043113437, 2049864887, 2063695313, 2067636847, 2074356711, 2074423396, 2075783041, 2075818205, 2115016714, 2116396741, 2119219356, 2130795633, 2132172731, 2134167315, 2134414115, 2134479759, 2136114025, 2137101044, 2137619903, 2138414767, 2142406320, 2151789982, 2159024459, 2169134473, 2170188482, 2200949486, 2293703278, 2401646429, 2911978475]}
{"paragraph": "In this paper, we present a new symmetrical buffered clock-tree synthesis flow with supply voltage considerations. Since IR drops vary across different positions in the power network, maintaining identical supply voltages is virtually impossible. To address this, we propose a solution called supply-voltage alignment to optimize skew by minimizing the differences in supply voltages used. For each buffer, we identify potential buffer-placement positions, known as buffer embedding regions, to determine corresponding supply-voltage candidates. Using this technique, we first cluster sinks level-by-level in a bottom-up manner to define the tree topology, which specifies the connection order of sinks without actual routing. During this phase, we focus on aligning supply voltages and minimizing wirelength. Then, in a top-down manner, we place buffers and route wires by determining buffer locations such that their supply voltages align with an expected voltage value, derived from the voltage ranges of all buffers at that level. Experimental results show that our method achieves an average clock skew reduction of 76% compared to a baseline without supply voltage consideration, and 40% compared to an extended version of the top-down flow with voltage awareness. These improvements are achieved with minimal resource and runtime overheads. Our method satisfies the stringent 7.5 ps skew constraint set by the 2010 ISPD contest for high-performance circuit designs under the commonly accepted 5% IR-drop tolerance, which other methods fail to meet. This work highlights the importance of addressing practical design issues like IR drops in real-world high-performance clock-tree synthesis.", "section": "Related Work", "doi": "10.1109/ASPDAC.2013.6509637", "references": [1986726876, 2021856858, 2023981464, 2025594056, 2032244362, 2032722667, 2055372196, 2068121061, 2068670364, 2122533670, 2122944491, 2124891222, 2125692303, 2133492470, 2138708698, 2140367260, 2144558399, 2148609278, 2158130267, 2170009145]}
{"paragraph": "This work aims to study and expand the theory of interaction nets, particularly the symmetric interaction combinators. These are especially interesting because of their universality—any interaction net system can be translated into the symmetric interaction combinators. Therefore, within the graph-rewriting paradigm defined by interaction nets, the symmetric interaction combinators serve as a prototypical language, much like the lambda calculus is the prototypical language of the functional paradigm. More specifically, the contribution of this work is twofold and can be seen as a methodology for addressing the following two questions: Observational equivalence: Given two nets in the system of symmetric interaction combinators, when can we say that they behave in the same way? Denotational semantics and full abstraction: Any answer to the above question yields an equivalence on nets; can we denotationally characterize this equivalence? In other words, can we find an abstract interpretation of the syntax so that such equivalence becomes an equality?", "section": "Methodology", "doi": "10.2168/LMCS-5(4:6)2009", "references": [1520717734, 1578801263, 1646840510, 1980647766, 1985185257, 2020295593, 2040745167, 2063179190, 2064276662, 2072347726, 2072956997, 2074116409, 2090981365, 2093062902, 2093541252, 2093561140, 2136240238, 2138520644, 2142286062, 2165446401, 2184268586, 2294530123, 2911303149, 2911865844]}
{"paragraph": "Indeed, the existing formalisms do capture basic properties such as privacy of inputs and correctness of outputs against coordinated attack. However, as has been observed in the past, there exist security concerns that are not naturally captured using the centralized adversary approach. Consider for instance the collusion-freeness concern: a protocol is collusion-free if even misbehaving protocol participants cannot use the protocol to exchange disallowed information without being detected. Centralized simulator formalisms do not capture the inability of parties to collude. That is, with a centralized adversary, a protocol might allow collusions between corrupted parties even when it realizes an ideal task that is collusion-free. An additional known limitation of standard security notions is cryptographic implementations of game-theoretic mechanisms. In contrast to cryptography, game theory considers rational players that behave according to their individual goals. In many realistic settings, the incentive structure depends on whom players can collaborate with and the cost of this collaboration. Security with a centralized adversary does not guarantee that the incentive structure with respect to collaboration is preserved when moving from the ideal protocol to the one that realizes it. Consequently, it does not correctly capture the incentive structure and does not suffice for preserving game-theoretic solution concepts that restrict the formation of coalitions.", "section": "Introduction", "doi": "10.1007/978-3-642-32928-9_16", "references": [1498307416, 1499934958, 1537539847, 1562226375, 1667994295, 1777311972, 1875574498, 1907580662, 1970606468, 1973801321, 1997859100, 2058014492, 2078907844, 2085140050, 2099137371, 2107510526, 2113085788, 2117376962, 2124885906, 2143215034, 2143929067, 2161670619, 2951618218, 2953145142]}
{"paragraph": "Currently, research on visual tracking methods focus on generative approaches which locates the target between frames based on highest similarity using a search window or template. In generative trackers targets are often represented by a series of templates which do not include background information limiting the ability of these trackers to achieve reliable results in complicated environments. Another commonly used approach is discriminative methods which distinguish objects from the image background using classification. Babenko applies positive samples to the tracked object and negative samples to the background in order to iterate a discriminative tracker model. On the other hand Wang adopts an online algorithm, based on local sparse representation, which learns sparse codes for robust object tracking. Correlation filters are widely used for track-by-detection applications and have been proven to provide more robust performance than most generative trackers. Correlation filters are also computationally efficient due to their ability to transfer the operator into the Fourier domain by element-wise multiplication. Based on the online object tracking benchmark the Kernelized Correlation Filter provides superior speed over top used methods such as Struck, TLD and MIL. The original KCF method has been further developed by researchers to improve accuracy and reliability. For example, Yao proposed a part-based visual tracker that improved tracking performance during occlusion and deformation. Huang adopted a KCF based tracker that enables scale and aspect ratio adaptability for performance enhancement. However, for both of these methods the tracking speeds decreased in order to improve overall performance.", "section": "Related Work", "doi": "10.1109/icra.2017.7989514", "references": [818325216, 1908905119, 2001785244, 2016075127, 2066513304, 2089961441, 2098941887, 2125202975, 2139132812, 2147533695, 2154889144, 2156445964, 2158917775, 2167089254, 2312882201, 2317058546]}
{"paragraph": "Bisimulation minimization algorithms partition a state space into equivalence classes such that states in the same class agree on whether the invariance holds and on their next-state transitions to other classes. These algorithms follow a common outline. First, they partition the states into two blocks: those which satisfy the invariance and those that do not. Next, they repeatedly split existing blocks into new ones until all states in a block agree on their next-state transitions to other blocks. If some states in a block reach states in another block and some do not, the latter is called a splitter of the former. The minimized system contains one state from each block in the final partition. The naive bisimulation minimization algorithm has two shortcomings in the context of symbolic model checking. First, it computes the relation, rather than the individual equivalence classes. The BDD for the relation requires twice as many variables as the BDDs for the classes. Second, the naive algorithm fails to distinguish between reachable and unreachable blocks. Several bisimulation algorithms address one or both of these problems. Our work considers the algorithms by Paige and Tarjan, Bouajjani, Fernandez, and Halbwachs, and Lee and Yannakakis. We chose these algorithms for the following reasons: Paige and Tarjan has the best provable worst-case running time of bisimulation minimization algorithms that do not distinguish between reachable and unreachable blocks. Bouajjani, Fernandez, and Halbwachs improves on Paige and Tarjan by choosing only reachable blocks to split on each iteration; however, it may split an unreachable block that was split off from the reachable block being split in the current iteration. Lee and Yannakakis improves on Bouajjani, Fernandez, and Halbwachs by never splitting an unreachable block. By splitting few, if any, unreachable blocks, the latter two algorithms are tailored to verification contexts. The Paige and Tarjan algorithm, although not so tailored, is interesting because it chooses splitters instead of blocks to split. None of the algorithms is fully symbolic. Lee and Yannakakis and Bouajjani, Fernandez, and Halbwachs operate on a symbolically-represented transition system and produce an explicit-state minimized transition system. Paige and Tarjan is originally expressed for explicit state systems; we converted it to the same hybrid symbolic-explicit style as the others.", "section": "Methodology", "doi": "10.1007/3-540-48153-2_29", "references": [1507985299, 1517157359, 1534252212, 1550110278, 1553233117, 1574045977, 1608609850, 1728798265, 1758995468, 1822320350, 1971107784, 1978469611, 2009965218, 2021473546, 2057706113, 2068361557, 2071152819, 2114092850, 2124563781, 2128871797, 2137865376, 2170486827, 2401501606, 2913663755]}
{"paragraph": "Aggregated search techniques allow to search and to assemble information from a variety of sources, within a single interface. Examples of aggregated search include Universal Search by Google, Alpha Yahoo, Ask’s X and Microsoft’s Live. Users have then access to different types of results, all in one page. They incorporate information from a variety of vertical searches including videos, images, news, maps, books, and websites into a single set of results. The issue of aggregation of elements from a collection of XML documents has received a little attention in the literature. Indeed, the proposed approaches that address this issue are limited to Web documents. This paper is more concerned with aggregation in XML document. The idea behind the selection of aggregated elements instead of element alone comes from the fact that we believe that an element alone might not be totally relevant for a given query, it might be partially relevant to a query, therefore if we aggregate these partially relevant elements together then one might produce a better answer to the user.", "section": "Introduction", "doi": "10.1145/2254129.2254214", "references": [51503012, 1993320088, 2007197665, 2015512514, 2019976352, 2032195233, 2034540658, 2035814200, 2088499566, 2102618089, 2118302148, 2132314908, 2140116426, 2151335566, 2168859760]}
{"paragraph": "We feel that it is important to understand children’s privacy behaviours and conceptualisations not only because it might identify conceptual gaps in understanding that lead to better ways to educate and protect children online, but also because such behaviours have been shown to be indicative of later behaviours and attitudes. To this end, we build on recent work examining children’s perceptions of online privacy risks and extend existing literature in two ways: first, by examining in detail how children describe certain common kinds of risks, for the purpose of understanding their conceptualisations of them; second, we examine the risk coping strategies taken by the children for each distinct risk context. Specifically, we focus on the following three research questions: Do children care about privacy online? When they do, how do they describe privacy risks? How do children recognise risks? What information do they use in the process? How do children apply their existing knowledge in responding to risks in different scenarios, including threats they never experienced before? What do such responses suggest about information and knowledge that might be needed? We report our results based on 12 focus groups with 29 children, aged 6–10, from UK schools undertaken between June and August 2018. We found that children in our study had a good understanding of risks related to inappropriate content, the approach of strangers, and oversharing of personal information online. However, they struggled to fully understand and describe risks related to online game/video promotions and personal data tracking. Moreover, children’s risk coping strategies depended on their understanding of the risks and their previous experiences: effective risk strategies were applied only if children recognised certain risks or when they felt something untoward. These findings demonstrate the importance of learning about potential risks through a multitude of channels, such as school, parents, friends, siblings, and personal experiences.", "section": "Related Work", "doi": "10.1145/3290605.3300336", "references": [794140516, 1804339065, 1964646547, 1971295515, 2000758863, 2033624312, 2098849717, 2104359975, 2143953012, 2148080935, 2161361013, 2303157508, 2401762417, 2486072240, 2501170812, 2588334051, 2588634505, 2588759517, 2609588687, 2611292243, 2611603913, 2734587602, 2735599528, 2773560169, 2777008125, 2794852036, 2795211868, 2795909404, 2796419887, 2796498984, 2797171593, 2808082055, 2808087006]}
{"paragraph": "The increase in the number of layers, the depth, of the ResNet model resulted in a small improvement in the accuracy of image recognition. However, this increased the processing time. Wide-ResNet was developed from ResNet, decreasing the depth and increasing the width of the residual network, providing significantly higher accuracy than ResNet but with high processing time. This means that Wide-ResNet is not suitable for devices with limited resources. This limitation led to the development of the SqueezeNet model with the number of parameters approximately 50 times lower than AlexNet model, while maintaining the same accuracy. The SqueezeNet model offers a squeeze block that uses a kernel to squeeze the number of output filters of the previous layer to a smaller size to reduce the number of parameters. The output data from the expand block with kernel size and are concatenated to obtain one Fire module. The SqueezeNet model uses 9 Fire modules for complete modeling. Using the squeeze block decreases the number of parameters over the AlexNet model. The squeeze block concept from the SqueezeNet model was introduced into the NUF-Net model, thereby significantly reducing the number of parameters. A convolution layer of different kernel size was also added to the NUF-Net model to allow the processing of images with varying kernel size. The usage of different kernels helps to create a variety of map features which contributes to increased accuracy. In addition, the NUF-Net model uses an identity mapping technique from the ResNet model to preserve data from previous layers to enhance accuracy. Both of these methods can improve the accuracy of the training but will result in a higher number of parameters and greater processing time.", "section": "Methodology", "doi": "10.1007/s11042-019-08332-3", "references": [1975053445, 2072751480, 2097117768, 2117309138, 2163605009, 2168894214, 2194775991, 2253429366, 2274287116, 2279098554, 2302255633, 2341528187, 2511730936, 2518108298, 2531409750, 2549139847, 2582248455, 2620983607, 2779335303, 2783165089, 2809653416, 2895295102, 2949117887, 2962971773, 2963446712, 2964137095, 2965618718]}
{"paragraph": "Triangle meshes are extensively used to represent 2-manifold meshes as the only reliable approximation of continuous surface. Scanners which produce large amount of point sets also introduce noise and sampling errors which make the reconstructed surface ragged. Large point sets are typically irregularly sampled and non uniformly distributed. Improvement of such scans is desired for many applications. Noise present in every acquisition method should be removed while preserving important features such as sharp edges and corners. With large number of acquired points from 3D scanner, post-processing reduce dense areas of points to prescribed density and thus introduces artifacts on edges. On decimated and uniformly distributed meshes this is seen as high frequency noise in the position of the vertices. Denoising can be applied by just adjusting vertex positions. Such approach preserves connectivity and topology of the mesh if vertices are uniformly distributed over the mesh.", "section": "Introduction", "doi": "10.1145/2614348.2614372", "references": [1578998830, 1626653188, 1912568406, 1974582493, 1984522470, 1990816220, 2000214666, 2006502673, 2058524213, 2079017595, 2114275307, 2123454054, 2130470231, 2158569910, 2165904679, 2171542563]}
{"paragraph": "The combination of multiple classifiers was shown to be suitable for improving the recognition performance in difficult classification problems. Also in handwriting recognition, classifier combination has been applied. Recently methods for the generation of multiple classifier systems, called ensemble methods, have been proposed in the field of machine learning, which generate an ensemble of classifiers from a single classifier automatically. Given a single classifier, the base classifier, a set of classifiers can be generated by changing the training set, the input features, the input data by injecting randomness, or the parameters and the architecture of the classifier. Another possibility is to change the classification task from a multi-class to many two-class problems. Examples of widely used methods that change the training set are Bagging and AdaBoost. Random subspace method is a well-known approach based on changing the input features. Although the popularity of multiple classifier systems in handwritten recognition has grown significantly, not much work on the use of ensemble methods has been reported in the literature. An exception addresses only the classification of isolated characters, while the focus of the present paper is on the recognition of cursive words. Some papers of the authors addressed the use of ensemble methods for handwritten word recognition. In this paper we propose a new ensemble method. This ensemble method specializes the individual classifiers on different writing styles. In Section 2 the new ensemble method is described in detail. Section 3 contains information about the handwritten word recognizer, which is used as the base classifier in the experiments. Those experiments are then discussed in Section 4 and, finally, conclusions are drawn in Section 5.", "section": "Related Work", "doi": "10.1007/978-3-540-28640-0_27", "references": [1525230573, 1534477342, 1560013842, 1676820704, 1985887223, 1988790447, 1992152861, 1992419399, 2025124369, 2092858021, 2099996526, 2107645346, 2113242816, 2121604559, 2126358082, 2143060414, 2152928267, 2164568552, 2171752105, 2912934387]}
{"paragraph": "The comprehensive annotation and compilation of all known biological interactions in a computable form is essential for network-based approaches to understanding biological systems and human disease. The Biological General Repository for Interaction Datasets (BioGRID) was established in order to help capture biological interaction data from the primary biomedical literature and to provide this data in a readily computable format. BioGRID collects and annotates genetic and protein interaction data from the published literature for all major model organism species and humans. When available, data on the influence of protein post-translational modifications, including phosphorylation and ubiquitination, is also captured. The complete BioGRID dataset is freely accessible through a dedicated web-based search portal and is also available for download in various standardized formats. BioGRID data content is updated and permanently archived on a monthly basis, and in addition to the BioGRID web interface, is disseminated to the research community through model organism database partners and other biological resources and meta-databases. The interaction datasets in BioGRID thus provide a resource for biomedical researchers who study the function of individual genes and pathways, as well as for computational biologists who analyze the properties of large biological networks.", "section": "Methodology", "doi": "10.1093/nar/gku1204", "references": [1566952502, 1985213868, 1989277387, 2003516452, 2015593638, 2018570668, 2051001559, 2051315170, 2068948802, 2079121387, 2094726706, 2096525273, 2102221598, 2105217514, 2105694586, 2105924489, 2107644675, 2112543617, 2116443392, 2117513900, 2128889848, 2131848047, 2135230723, 2147526198, 2147834895, 2148362397, 2149608537, 2159092541, 2163256056, 2164298117, 2169382766, 2169425537, 2170283197, 2605068739]}
{"paragraph": "As already mentioned, a key novelty of our approach consists in the utilization of deep learning techniques. Deep learning is a recent trend in machine learning that offers higher flexibility in learning nonlinear dynamics in large datasets. These methods have gained significant momentum due to their state-of-the-art performance in various scientific fields, including computer vision and natural language processing. Deep learning techniques essentially comprise a new generation of artificial neural networks that employ statistical modeling arguments to overcome problems that plague traditional ones, such as the vanishing gradients problem and their overfitting tendencies. Finally, a main goal of this paper is to explain the causes of negative co-exceedances in stock markets. We effect this analysis by introducing financial economic covariates to account for interdependence in normal times and external shock variables; these allow the study of contagion effects in crisis periods. On this basis, we employ a series of machine learning techniques to forecast tail events in the global stock markets. This is in contrast to existing early warning systems, which usually employ error-prone, biased, and oversimplistic heuristically defined macro-indicators. Our suggested approach is considerably more robust in modeling nonlinear behavior in financial data, can better model the interaction effects between leading financial turbulence indicators, and can capture long and subtle temporal patterns that are elusive to human analysts.", "section": "Introduction", "doi": "10.1016/j.eswa.2018.06.032", "references": [1443712405, 1994085451, 2005346797, 2015780725, 2074250525, 2079553537, 2083060785, 2084072817, 2095705004, 2304517173, 2345563409, 2424889563, 2523498403, 2557283755, 2607150089, 2789364533, 2911964244, 2912934387]}
{"paragraph": "In this work, to reduce the mismatch between training and operating environments, multi-SNR models have been trained to develop LID systems. In training multi-SNR models, training data is augmented with multiple versions of noisy data synthetically generated by adding noise to the training corpus at different SNR levels, and the augmented dataset is used for developing LID systems. Training method plays a crucial role in the convergence, generalization, and performance of neural networks. In this study, curriculum learning based learning schedules have been explored in training multi-SNR models for developing robust LID systems. Curriculum learning refers to the task of training a neural network with examples in some specific meaningful order rather than randomly sampling the training dataset. Better curriculum learning strategy helps the model to converge better and eases the optimization of the neural network. The use of curriculum learning based training strategies has been explored for a variety of applications such as probabilistic linear discriminant analysis for noise robust speaker recognition. To assess the proposed method's performance, experimentation was done on two different and challenging LID datasets: a dataset built from an Indian language database for LID and the Oriental Language Recognition database. Thus, first, the proposed approach is tested in a practical environment; and second, the same is verified in a familiar and standard evaluation framework for the LID community. In both cases, it is focused on test utterances having 5s duration.", "section": "Related Work", "doi": "10.1016/j.eswa.2018.06.004", "references": [126852431, 1558802035, 1972300018, 2021483907, 2032036568, 2056119007, 2106284094, 2131703294, 2136188588, 2146194791, 2150769028, 2172287020, 2183984815, 2294381517, 2296073425, 2397940721, 2399033287, 2406392101, 2408021097, 2518527613, 2527255948, 2735439328, 2766245080]}
{"paragraph": "Publications associated with RoboCup have appeared in the RoboCup Symposium proceedings since 1997, in individual team reports, and in many different journals and conference proceedings related to robotics or machine learning. In the present paper, we extend our pilot study from 2004 and approach the above questions by focusing on the four-legged league since it started in 1998. There is a similar body of literature covering machine learning in the other leagues of RoboCup. Previous surveys on multiagent systems included links to the history of RoboCup and emphasized the involvement of machine learning. The present paper is intended for the researchers in interdisciplinary fields that combine machine learning and robotics and areas associated with RoboCup or the Federation International Robosoccer Association. Due to the growing impact of these initiatives, the paper could also be helpful for researchers with a more general interest in current developments in artificial intelligence or cybernetics.", "section": "Related Work", "doi": "10.1109/TSMCC.2006.886964", "references": [18923964, 111857891, 187361493, 1479973828, 1481164811, 1501513361, 1502633609, 1506713141, 1508969806, 1512990288, 1517523385, 1525400443, 1546266526, 1565175369, 1568829386, 1580572138, 1582331576, 1594931361, 1596568669, 1705675503, 1741217179, 1767458536, 1807600159, 1832542730, 1837920285, 1892723389, 1904781439, 1904934947, 1976410891, 2006038922, 2019056455, 2037574298, 2054043069, 2060052814, 2087347434, 2095910046, 2097863147, 2098049033, 2104641222, 2107193468, 2108734173, 2112002204, 2112598971, 2119821739, 2120712855, 2121863487, 2124135442, 2132504164, 2132870739, 2136557331, 2139053308, 2140219596, 2143258943, 2147492008, 2147759175, 2153192722, 2156629996, 2156909104, 2170396035, 2171565529, 2521106013, 2913016321, 2915049281, 2971424655]}
{"paragraph": "One way to solve the problems discussed above is to define enterprise architecture IT standards to guide IT departments and business units in their technical choices and project-level decisions related to data and application design. EA standards are a set of policies, rules, and guidelines that form unifying principles and practices across projects and business units; they provide the organizing logic for applications, data, and infrastructure technologies. The use of EA standards enables organizations to influence the actions of subunits without dictating exactly how they handle all of their information-processing activities. Many practitioners, researchers, and standards organizations have advocated the use of EA standards to manage IT resources throughout the whole organization. While the use of EA standards can potentially help organizations to solve coordination problems across business units, standardization involves trade-offs. While standardization has long been associated with an increase in overall efficiency, it often restricts the options of business units, thus resulting in a less optimal local solution. Hence, not all organizations choose to adopt a standardization approach. Nevertheless, the use of EA standards to plan and coordinate IT resources across an enterprise has been considered a useful approach since the early 1990s. It has taken on a more significant role recently with the increasing popularity of technologies such as Web services that build upon well-defined architectures. Many research and practitioner articles highlight the potential benefits of using EA standards. However, little empirical research has been conducted to verify these claims. Hence, in this study, we examine whether the use of EA standards helps organizations achieve the objectives of better sharing and integration of IT resources.", "section": "Methodology", "doi": "10.2753/MIS0742-1222230307", "references": [125336254, 309246951, 1487531252, 1487665605, 1496112752, 1530624724, 1533396945, 1539089137, 1545181001, 1567733789, 1573420096, 1583120786, 1590502543, 1608836379, 1660606661, 1675256116, 1696601253, 1715153959, 1742818702, 1966382716, 1974613913, 1975213073, 1981507201, 1985191658, 1985809248, 1987357925, 1990282415, 1995987809, 2005681154, 2007178886, 2017103945, 2031762953, 2037037629, 2050965286, 2063728486, 2064842558, 2068046153, 2068786172, 2091949669, 2098685541, 2102716596, 2103340114, 2122593986, 2132568829, 2142464495, 2142824088, 2146531730, 2156700354, 2162822465, 2164551223, 2169855821]}
{"paragraph": "Recently, a Sparse Dictionary based algorithm in which keyframes are considered as a dictionary that can sparsely reconstruct all the frames of a video has been demonstrated to be more effective than many popular video summarization algorithms. However, the convex relaxation method to solve this problem, in which a norm-based constraint is imposed to enforce the selected keyframes as sparse as possible, cannot ensure the sparsity of the reconstruction coefficients directly. As a result, keyframes are selected by identifying local maximums of an importance curve generated according to the sparse coefficients, limiting it to be a local representative video summarization algorithm. Thus, the results of the Sparse Dictionary method may not be optimum, which results in redundant adjacent frames. In this paper, inspired by the Sparse Dictionary algorithm where it is expected that the original video can be reconstructed from the keyframes as accurately as possible, a selection matrix is proposed to model video summarization problems and the norm of such selection matrix is imposed to ensure sparsity directly. As a result, an Orthogonal Subspace Projection based Iterative Keyframe Selection algorithm is proposed to obtain the selection matrix for video summarization. In order to summarize a video sequence with different length, a Percentage Of Reconstruction criterion is also proposed to enable the proposed algorithm to adapt to different kinds of videos. Finally, experiments on a popular dataset are conducted to demonstrate the effectiveness of the proposed algorithm.", "section": "Related Work", "doi": "10.1109/ICIP.2014.7025581", "references": [1971994349, 1987366351, 1989863986, 1991531520, 2019339419, 2019712573, 2043784772, 2083498763, 2094325573, 2094998392, 2095116991, 2105174364, 2106910805, 2111494488, 2117051369, 2118210476, 2129291408, 2134577448, 2144577430, 2152433968, 2163461766]}
{"paragraph": "In order to answer the research questions, we opted for an empirical/experimental approach. Data on modeling activities of novice modelers (86 students in total) have been collected by means of experimental logging functionality of the JMermaid3 modeling environment. Students’ group works over one semester period of time were observed. For data analysis we opted for process mining techniques motivated by the fact that process mining has built a reputation of being capable of analyzing rich data trails and activity streams in various contexts. In addition, process mining diagrams make it easier to visually extract useful information and quantify relevant properties on process-oriented modeling approaches. We further elaborate the findings with quantitative and qualitative analysis. While findings showed that certain behavioral patterns can indeed be associated with better or worse outcomes in terms of reaching a satisfactory model quality, further examinations are needed to identify more generic patterns. The results of the study can be used to provide recommendations on process-oriented feedback. This study presents first insights to support research on learning analytics as well as artificial intelligence in the domain of conceptual modeling. In addition, this study can be inspirational for the application of process-oriented learning analytics outside of the topic of conceptual modeling, as learning event data is becoming more readily available through digital learning systems and other educational information systems.", "section": "Methodology", "doi": "10.1016/j.chb.2014.09.054", "references": [17548582, 30814344, 62300523, 1486594979, 1565132002, 1567844191, 1614190211, 1791587663, 1929882576, 1937768634, 1961143152, 1976191438, 1985271641, 1997383729, 2006444123, 2008857148, 2047701910, 2063042352, 2063670358, 2075886324, 2078482049, 2079966199, 2080408219, 2089438716, 2093914167, 2100379340, 2101268022, 2110143060, 2126512988, 2131566236, 2133341045, 2139865360, 2149112225, 2159835652, 2404425559]}
{"paragraph": "Over the past two decades, efforts to characterize and mitigate the effects of noise in BOLD fMRI time series have played an integral role in the development of fMRI acquisition and analysis approaches. Advances in methods to distinguish signal from noise have led to improvements in the ability to detect and estimate brain activity. In this paper, we will review the primary sources of noise in fMRI, with a focus on the noise components that appear in fMRI time series signals. In-depth treatments of the various noise sources are provided elsewhere in this special issue. In the analysis of fMRI studies, there are additional sources of noise, such as inter-scan, inter-subject, and inter-site variability, but these sources will not be considered here. We will begin by reviewing a basic signal model for BOLD fMRI and considering the various ways in which noise affects the elements of the model. This will be followed by an examination of the mechanisms through which various processes, such as cardiac and respiratory activity, can act as noise sources. We will conclude with an overview of approaches for separating signal from noise in fMRI.", "section": "Introduction", "doi": "10.1016/j.neuroimage.2017.05.031", "references": [1731560309, 1964625711, 1972690852, 1976781461, 1978694642, 1979043849, 1981804517, 1986387796, 1990134753, 1993227560, 1997260622, 2020333990, 2022925177, 2031235352, 2033865693, 2035809725, 2044423747, 2047453615, 2048857243, 2049325056, 2050780724, 2052615191, 2053749151, 2053838731, 2055720818, 2065442416, 2065895453, 2066494844, 2068782491, 2076678298, 2078789899, 2082391156, 2089066085, 2096672020, 2106822401, 2108384452, 2117805716, 2130010412, 2132175842, 2136543288, 2149240084, 2152758478, 2157446241, 2160988655, 2169787465, 2255433468, 2290306878, 2305898258]}
