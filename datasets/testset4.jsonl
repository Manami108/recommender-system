{"paragraph": "It has currently become accepted practice in the steel manufacturing industry that the overall process should be properly monitored in spite of using advanced control methods, since even a small fault or unpermitted change, if ignored, may lead to tremendous safety and economic losses. One solution to this problem is the application of fault detection (FD) methods, which can determine not only whether and when the process under supervision is not operating well, but also to determine the potential root-cause once an abnormal operation is detected. Although model-based FD method can be implemented if process model exists, obtaining such models for steel manufacturing processes is often difficult due to the complex nature of the process. Instead, data acquisition and management systems are commonly installed in modern industrial plants. For example, in the hot tandem rolling mill (HTRM) process, the rolling and bending forces, the gap between two working rollers, the rotating speed of rollers, and different control signals can be measured, transmitted, and, finally, stored in the plant database. With the rapid growth in amount of available data, process engineers are now with the challenge of dealing with big data. Furthermore, common processes, such as the HTRM process, are increasingly becoming interconnected. Thus, using process data to monitor complex industrial systems has recently become a topic of interest. In fact, the operating conditions of an industrial process can be monitored using data analytics-based FD methods. First of all, historical data are properly modeled by various data modeling methods to extract features that can reflect normal operating conditions. Then, the online operating condition can be decided by examining whether any change occurs in features. Among different data analytics methods, the commonly used ones focus on determining the boundary features of normal data, such as support vector and nearest neighbor-based methods. Once the distance from the real-time data to the boundary feature is larger than a threshold, a fault alarm can be triggered. When the obtained features fit the actual boundary of normal operation data, better detection performance will be achieved. Another widely used technique is the multivariate statistical methods, such as principal component analysis (PCA), independent component analysis (ICA), and partial least squares (PLS)-based methods, as well as their variants and extensions. Unlike above methods, these methods focus on determining features of the normal operation data using low-dimensional latent variable data. It is also assumed for these methods that a clearer and more detailed interpretation of the normal data leads to a lower missing alarm rate (MAR). Then, the real-time data that cannot fit to the established latent variable model will be an indication of a fault. Considering the uncertainties in the data due to noises, FD statistics like the T2 and the generalized likelihood ratio (GLR) test statistics are often used for the decision-making process. Note that once a fault has been detected using a multivariate statistical FD method, contribution analysis- and causality analysis-based methods can be performed to identify the potential fault root-cause. Due to the simplicity and efficiency in processing huge amount of process data, multivariate statistical methods are nowadays widely accepted in practice. Above statistical FD methods are developed considering the entire uncertainties corrupted in process data. Recently, modifications have been made to provide a priori fault information for modeling, such that uncertainties in original data can be, to some extent, reduced, and then the FD performance is accordingly improved. For example, if the latent variables affected by a given fault are known beforehand, then, solely monitoring these affected latent variables would be straightforward and effective to detect this fault. However, in many practical cases like the HTRM process, the amount of available information about the faults is considerably limited, especially compared with the massive amounts of normal operation data. Thus, it can be difficult to apply this approach to most industrial applications. It is often the case in process industry that parts of uncertainties like noises are normal variations within the process. They mainly come from upstream processes or caused by the local controller, which, compared with the measurement uncertainty, are always predictable. Reducing the influence of such uncertainties by means of data processing methods would improve the FD performance, in particular for statistical methods. Previous applications of data-driven FD methods to the HTRM process monitoring have focused on two different schemes. In the first one, simultaneously monitoring of the whole plant is under consideration. That is, FD methods are developed using the data from multiple stands. Using these methods, the whole plant operation can be tracked, while detailed operation of an individual stand is ignored. Differently, the second one is dedicated to monitoring the behavior of each stand, and data from an individual stand are used for developing FD methods. However, since the upstream stands can determine the off-line setting and online performance of the systems in downstream stands, the operation of a stand can be better monitored if the information from its upstream stands is used. Motivated by above observations, objectives of this paper are to demonstrate in the context of T2 test statistic that the fault detectability can be improved when the covariance matrix of the noises is reduced; to propose a correlation-based statistical FD method that reduces the covariance of the noises; to extend the proposed method to a distributed form that makes it applicable to the monitoring of an HTRM process; to show the applicability of the proposed method using a hardware-in-the-loop (HiL) simulation.", "section": "Methodology", "doi": "10.1109/TIE.2019.2901565", "references": [1504881615, 1966863755, 1973164954, 1979706043, 2039042371, 2063823978, 2066315659, 2072857564, 2100028154, 2158958729, 2293880708, 2296641122, 2341771485, 2520249960, 2611311248, 2618709819, 2741385306]}
{"paragraph": "The external disturbances have been recognized as one of major issues in the design of industrial control systems. In the past decades, the anti-disturbance feature has been a key objective in control engineering and applications. In general, the antidisturbance control strategies can be classified into two main categories, i.e., the feedback suppression and the feedforward compensation. As a refined method, the feedforward strategy or the disturbance/uncertainty estimation and attention (DUEA) method concentrates on estimating the accurate magnitudes of the disturbances, and the control law is constructed with the disturbance estimator/compensator. Many promising results have been reported, such as the disturbance observer, the unknown input observer, and the extended state observer. The main advantage of the DUEA method is releasing the conservatism of the feedback suppression strategy. Accordingly, various types of disturbances have been considered. A generalized disturbance observer was proposed to estimate higher order disturbances in the time-series expansion. The extended state observer was used to address the effect of unknown dynamics. The disturbance estimation problem was considered for nonlinear systems with multiple sources of disturbances and uncertainties. Meanwhile, different estimators have been proposed. A sliding mode disturbance observer was presented and the finite-time convergence performance can be achieved. An internal model-based disturbance observer was proposed to estimate the disturbances in both the low-frequency region and the harmonic frequencies. The nonsmooth function fal(⋅) was utilized to construct an extended state observer. Up to now, the DUEA method has been applied to various applications, such as mechatronic systems and aerospace systems. Although the stability of these mentioned estimators can be guaranteed, the performance of disturbance estimation may not be satisfactory enough, especially when the estimation errors exist or before the full convergence of the estimation errors is achieved. This characteristic might cause doubt whether disturbances could be fully compensated or not. Recently, the interval observer technology was proposed. It is based on the differential inequality theory that allows one to design a Luenberger observer and estimate the upper and lower boundaries of the system states at any time instant. Originally, the interval observer was constructed for the linear system where the observed system need to be a positive one. This technology was generalized to be utilized in the stable linear system based on a time-varying coordinate transformation that can transform an autonomous system into a positive one. The design problem of coordinate transformation was transformed to solve Sylvester equation, and a time-invariant coordinate transformation was obtained. The aforementioned publications have become the basis of solving more related problems, such as fault diagnosis and isolation problem. However, in the existing literatures, the interval observer technology has been rarely addressed on the disturbance attention for the general nonlinear system. In this paper, a disturbance interval estimation approach is proposed for nonlinear systems by utilizing the interval observer technology. Distinct from those aforementioned feedforward estimation schemes aiming to estimate the magnitudes of the disturbances, the disturbance interval estimation approach can provide the upper and lower boundaries covering all possible values for each element of the current disturbances. On the other hand, comparing with the boundaries utilized in feedback strategy, these upper and lower ones are generated by taking full account of the disturbance characteristics rather than the worst case. Moreover, the width between two boundaries represents the estimation accuracy rather than the maximum magnitudes of disturbances. Basically, a key advantage of the disturbance interval estimation is that it addresses a crucial issue of the end user: The interval width provides a quantized guidance on the estimation accuracy at any time instant. It is feasible to choose any value in the interval as the compensator, and the certain interval width can be employed to design the feedback control law. For the sake of convenience, we refer the proposed approach as disturbance interval observer (DIOB) in this paper. The DIOB consists of two particular boundary estimators possessing the structure of the standard disturbance observers. These two estimators are utilized to approximate the time-varying upper and lower boundaries, respectively. By introducing a time-invariant coordinate transformation, the dynamics of the boundary estimation errors are constructed as positive systems. Accordingly, the boundary estimation errors can be guaranteed to be nonnegative and bounded based on the differential inequality theory and ultimate boundedness one. To the best of authors’ knowledge, it is the first time to design the disturbance observer with the interval estimation approach. Furthermore, to achieve the anti-disturbance objective, an integrated feedback and feedforward anti-disturbance control scheme is summarized based on the proposed DIOB. As an illustrative case, the anti-disturbance control method is developed by integrating the DIOB and the sliding mode control (SMC). The mid value and the width of the generated interval are employed to construct the feedforward compensator and the gain of the feedback sliding mode item. Upon the reliability of the estimated upper and lower boundaries, the asymptotical stability of nonlinear system can be obtained straightly. The rest of this paper is organized as follows. The problem formulation and preliminaries are given in Section II. The design procedure of the DIOB is presented in Section III. The anti-disturbance control method is constructed in Section IV. The numerical simulation and experimental validation are provided in Sections V and VI, respectively. Finally, Section VII concludes this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2898575", "references": [1544497799, 2004231187, 2010351321, 2073278424, 2075133506, 2084628204, 2095189861, 2103904919, 2110047814, 2113307062, 2126061438, 2129608559, 2135493101, 2137994854, 2142205210, 2142428281, 2230652337, 2542001067, 2600970829, 2615101682, 2623064590, 2676821827, 2734300891, 2748127821, 2769087263, 2787504052]}
{"paragraph": "Telecom service provided by the service operator and mobile phone handset made by the handset manufacturer are indispensable and complementary elements when a consumer chooses a mobile phone from the mobile phone market. This complementarity has made it viable for firms to introduce the bundled channel to realize a bulk of sales, that is, consumers buy the contractual handset and the telecom service as a bundled package from either the manufacturer or the operator. At the same time, firms also sell handset models through an unlocked channel, that is, consumers get the unlocked handset from the manufacturer and subscribe to the telecom service from an arbitrary operator separately. In many countries, firms have deployed the dual sales channels to deliver both handset and service to end consumers. For example, in China, many manufacturers’ handset models such as Huawei Honer 7, Samsung Galaxy 7, and Apple iPhone 6s have been sold with subscription to China Unicom's telecom service, and these handset models also have been sold without any telecom service subscription. The dual sales channels will likely cause substitutable competition. It is, therefore, very likely that this phenomenon also occurs in the dual sales channels of the mobile phone industry. In the dual-channel distribution setting, manufacturer and operator provide essentially the same product and service, especially in the functional aspect. Consumers can get what they want from either channel. Thus, the unlocked and bundled channel may compete for the same group of consumers. On the other hand, the perceived experience difference between the unlocked and bundled channel induces consumers to choose the channel that is better suited to their needs. In the bundled channel, for example, operators require that the contractual handset should work exclusively with their own services within 12–36 months so as to gain the future subscriber revenues and offset the cost of subsidies, that is, consumers have to be tied up to the particular operator for a fixed period of time. Because exclusive arrangement is not convenient and the value of convenient plays the most important role in mobile phone usage, people looking for convenience will like the unlocked channel better. The substitutable competition between the unlocked and bundled channel poses vexing challenges for the manufacturer and operator. On one hand, although leveraging the bundled channel may enable greater market penetration, it may cannibalize the sales in the unlocked channel. In such a setting, how players decide their pricing strategies as well as the subsidy strategy to resolve the problem and maximize their profits will be an important factor in their success. On the other hand, the cannibalization of the dual sales channel leads to a more complex channel relationship and causes channel conflict. In general, the channel conflict tends to be a negative force that may hurt profits for all parties. For this, various contracts have been widely implemented for alleviating the channel conflict. For instance, subsidy contract is often implemented as a control mechanism to increase the sales volume as well as to stimulate the manufacturer and operator to collaborate with each other. Thus, how to manage coordinating and contracting problem among the manufacturers and operators becomes increasingly meaningful and challenging. To our knowledge, the extant literature has sparsely addressed the relevant problem for the rapid-developing mobile phone sector, even less effort has been devoted to examining the relevant problems to substitutable competition. This paper intends to contribute to fill this research gap by investigating a range of operational decisions and supply chain coordination issues. More specifically, we propose to answer the following research questions: How do the operator and the manufacturer make their optimal decisions as a whole on bundled and unlocked retail prices in a centralized system with substitutable competition? What are the operator's subsidy policy and the manufacturer's optimal pricing policy in a decentralized system with substitutable competition? Can a subsidy-only contract coordinate the competitive dual sales channels that possess both complementarity and substitutability? If not, what contractual terms should be designed to improve the overall performance? To address these questions, we will start with the demand model setting that can capture the features of the dual sales channels of the mobile phone industry. Then three scenarios are analyzed in a game theoretic framework: centralized scenario, which mainly as a benchmark for coordinating analysis; decentralized scenario, which is presented to verify the coordination failure of the subsidy-only contract; decentralized scenario with a two-way revenue sharing contract, which is presented for designing a properly and practical coordinating contract. Finally, after presenting equilibrium strategies and the relevant sensitivity and coordination analyses, we further analyze the impact of two-way revenue sharing contract and telecom service price on channel profit, as well as the impact of telecom service price on the two-way revenue sharing contract. Our key contributions to the literature are the following. First, this paper enriches the operations management studies of the mobile phone supply chain by analyzing three models in a Stackelberg game theoretical framework, as well as taking into consideration of the substitutable competition. We specifically investigate the channel members’ operational decisions for each model, which are well founded as well as practically relevant to real-world operations in some way. Second, this paper extends the supply chain coordination and contracting literature by modeling a subsidy-only contract and a two-way revenue sharing contract for the mobile phone supply chain. Our result implies that the widely used subsidy-only contract fails to coordinate the mobile phone supply chain due to the competition between the bundled and unlocked channel, thus we propose a two-way revenue sharing contract to achieve the optimal overall supply chain performance. The remainder of the paper is organized as follows. In Section 2, we provide a review of the related literature. Section 3 details the dual sales channels of the mobile phone industry. In Section 4, we present our models and analyses, and then focus on the coordination issues. We conduct the numerical analysis in Section 5, and provide two extensions in Section 6. The summary of our key findings and future research directions are presented in Section 7.", "section": "Methodology", "doi": "10.1111/itor.12451", "references": [1500626379, 1789954534, 1965007576, 1982862737, 2001330444, 2017767555, 2020907614, 2025887266, 2028308112, 2038476837, 2061660901, 2077800159, 2082011667, 2104052944, 2111107475, 2112482196, 2126741666, 2139753030, 2149035559, 2157406401]}
{"paragraph": "Electroencephalogram (EEG) is an electrophysiological monitoring method to record the brain electrical signals. In this typically noninvasive method, the electrodes are placed on the scalp to record the brain’s spontaneous electrical activity over a period of time. Arguably, analysis of the recorded EEG signal is a complicated task. The reason is that this type of analysis involves many decision variables making it a large-scale optimization problem. In order to deal with such large-scale optimization problems with real-time requirements, a fast optimization technique is required. In this context, modern intelligent optimization techniques and in particular evolutionary algorithms (EAs) are considered as viable tools and efficient alternatives to the traditional optimization techniques. However, it is well-known that the performance of the EAs decreases sharply for large-scale optimization problems with numerous decision variables. Many efforts have been made towards improving the current EAs or designing new EAs in order to successfully address the large-scale optimization problems. The complication degree of an optimization problem exponentially increases as the problem size increases. Up to now, few multi-objective evolutionary algorithms (MOEAs) have been successfully deployed to solve the large-scale optimization problems. An in-depth parametric study is a key to the success of MOEAs. On the other hand, for large-scale optimization problems, a fast process is also a critical factor for any algorithm. EAs have two important operators which heavily influence their convergent speed: mutation and crossover. Several studies have been carried out in the area of designing more efficient mutation and crossover operators for EAs. Recently, a reference-point-based many-objective evolutionary algorithm, called non-dominated sorting genetic algorithm - the third version (NSGA-III), has been proposed. In the present paper, this powerful algorithm is applied to solve complicated high-dimensional multi-objective optimization (MOO) problems originated from the EEG signal processing with thousands of decision variables. These large-scale optimization problems are proposed in IEEE CEC 2015. In the basic NSGA-III algorithm, only the SBX crossover operator is used. In this study, the effect of different crossover operators (SBX, SI, and UC) on the performance of the NSGA-III algorithm is fully investigated. In addition, improved crossover operators are proposed by combining SBX, SI, and UC with Stud. The new operators are then incorporated into NSGA-III in order to generate the offspring population for the next generation. Subsequently, ten variants of NSGA-III algorithms are developed and further verified on the large-scale optimization problems. The current paper is structured as follows: Section 2 reviews several most representative work on large-scale optimization problems and MOEAs. In Section 3, the basic NSGA-III algorithm is described. The proposed crossover operators are described in Section 4. The description of a large-scale optimization problem is given in Section 5. Section 6 provides the simulation results on large-scale optimization problems proposed in CEC 2015 big data competition. Finally, Section 7 provides a summary of the present work.", "section": "Introduction", "doi": "10.1016/j.ins.2018.10.005", "references": [1580723439, 1968219458, 1969803830, 1976442029, 1980048226, 2003961265, 2022485595, 2024008934, 2028885867, 2053498776, 2053900989, 2058361915, 2087812386, 2106334424, 2116661285, 2126105956, 2143381319, 2204362653, 2295576181, 2344916885, 2514388731, 2521918431, 2533800621, 2547860982, 2556421959, 2558150887, 2570235502, 2579361096, 2602908436, 2604129326, 2605049507, 2616346973, 2739608398, 2745261836, 2752606981, 2757662287, 2774381817, 2778143478, 2780124552, 2782424305, 2799163655, 2802576758, 2808993793]}
{"paragraph": "There are many situations in which a corrupting additive component (noise) needs to be removed from an a priori unknown digital signal, for instance a two-dimensional image. Whilst this problem has no universal solution, natural and synthetic images have features which are sufficiently different from typical noise to enable the development of many practically useful noise-reduction algorithms. The performance and characteristics of such algorithms are critically dependent on how they each define the difference between ‘noise’ and ‘signal.’ Such definitions are wide-ranging, for instance based on transformed domains (noise has higher frequency), data level (signal has greater difference in data value), pattern-matching (signal structure is more repeatable) or even the historical record (signal is previously known). In this context, the novel concept of using signal ‘bitonicity’ for differentiation was recently proposed, where the signal is deemed to be anything containing one maximum or one minimum over a given spatial range. This definition is crucially independent of data value, being based instead on data ordering or rank: hence it applies equally to both smooth and disjoint signals. The bitonic filter was developed as a combination of rank-based (robust morphological openings and closings) and linear (Gaussian filtering) operators, in order to remove non-bitonicity (noise) from corrupted signals, with an initial application in medical computed tomography. It was shown to have good noise-reduction performance across a range of noise levels, surpassing other morphology-based alternatives, and even competing in some cases with more well known linear filters such as anisotropic diffusion or non-local means, particularly for images with varying noise levels. Whilst, for one-dimensional signals, the spatial range over which bitonicity is imposed is defined purely by the filter window length, for two-dimensional (2D) images, a 2D structuring element or ‘mask’ defines which image pixels locally contribute to the data ranking. In the bitonic filter, this mask was fixed to a circle with a chosen diameter. Since the mask shape imposes a structure on the signal, and yet the structure of the signal is not expected to be constant over an image, it is clear that allowing the shape to vary locally could offer significant performance improvements. This concept of structurally varying morphological operations (also known as adaptive morphology) has been the subject of recent research with well developed mathematical foundations. There are a variety of ways to change the shape of the mask, but the extent to which the mask is allowed to shape itself to the data is related to the noise reduction performance. Strong prior definitions of shape are required if high noise reduction is desired, since otherwise the mask can conform to the noise as well as the signal. Pre-defined flat masks are hence the most appropriate for inclusion in the bitonic filter, with ellipses the simplest extension from a circle. Having defined the type of structurally varying mask, its orientation and specific shape needs to be able to adapt to the image data. The orientation is usually derived from the gradient, often in the form of a structure tensor also called the average squared gradient. The local anisotropy can be used to control the shape, for instance setting the aspect ratio of an ellipse. It is the aim of this paper to develop the bitonic filter so that it can make use of structurally varying masks. Structural variation is a relatively new development within mathematical morphology, and, other than the median filter, mathematical morphology itself is a less widely used technique than linear image filtering, perhaps due to complexity in implementation and analysis. Hence the paper covers sufficient detail to show how the structurally varying bitonic can be implemented efficiently, without presuming familiarity with the field. In order to develop this filter, several extensions to structurally varying morphology are also proposed. Firstly, the bitonic filter unusually involves robust (not involving maxima or minima) morphological openings and closings, and efficient implementations of such operations do not exist. In addition, a better definition of mask orientation is developed, making use of trials over multiple masks rather than solely relying on the structure tensor. The incorporation of data thresholds, and the inclusion of the whole in a multi-resolution framework, is also demonstrated. For the new bitonic filter, an alternative to the Gaussian filter which can structurally match the varying morphological operations is also required. Since the bitonic filter is a recent concept, its performance is placed in context by comparison with a wide range of more well known linear filters, on a range of images, across multiple levels of noise. It is seen that the structurally varying bitonic can achieve competitive noise reduction, with practical processing times, across the entire noise range, though with particularly promising results in very high noise.", "section": "Related Work", "doi": "10.1109/TIP.2019.2932572", "references": [1670866629, 1966984322, 1979369422, 1990336127, 2017837798, 2021586009, 2023534903, 2056370875, 2058512501, 2069027079, 2099244020, 2109853861, 2119041295, 2125188192, 2136396015, 2143750255, 2150134853, 2157691228, 2161859983, 2476975422, 2528621788, 2915008565]}
{"paragraph": "Multi-agent systems are collections of autonomous agents that operate according to some local rules and a limited mutual awareness. They are a convenient formalism for representing several classes of complex systems and can support formal reasoning about them. An issue that arises when considering a multi-agent system is how to determine whether a global property of interest emerges from the combination of the local behaviors of the different individual agents. The availability of a formal description of a multi-agent system allows one to apply automated verification techniques and can be instrumental for obtaining strong guarantees about its global behavior. Simulation-based approaches, on the other hand, may be more effective when dealing with larger multi-agent systems, due to the considerably large state spaces resulting from their distributed and asynchronous nature. Therefore, the two approaches should be considered complementary to each other. In this paper, we introduce a language for describing multi-agent systems that lends itself to an intuitive design of local specifications and that can be used as the basis for automated analysis. The language, which we call LAbS for Language with Attribute-based Stigmergies, is simple yet versatile enough to model several interesting classes of systems. It combines stigmergic interaction with attribute-based communication. A key concept of the language is that of virtual stigmergy, a distributed data structure that can model global knowledge. Each agent operates only on his local copy of the stigmergy, that stores his own partial knowledge of the system. Individual knowledge is then asynchronously propagated across other local stigmergies. Thus, changes by an agent may indirectly affect the behavior of another. In the originally proposed version of the virtual stigmergy, agents are concrete entities, each at a specific position in space, that can communicate across the stigmergy only if they are within a given distance from each other. To increase expressiveness, we generalise stigmergic interaction to arbitrary predicates over exposed features, referred to as attributes, of the agents. In fact, our language has no explicit concept of position for agents, and thus of neighborhood. An agent can have instead local attributes, and predicates over these attributes can express the conditions for two agents to be allowed to exchange knowledge. Movement is no longer seen as a specific action; an agent may update the attribute that encodes its position by performing a standard update action. The generalisation of stigmergic interaction outlined above increases the flexibility of the language and allows to model a wider class of systems. However, it is still not sufficiently expressive to naturally model those classes of multi-agent systems where the global environment plays a crucial role. To address this shortcoming, we extend our language with tailored primitives to explicitly model actions on the environment. This work extends our original presentation of the language in several ways. The new syntax and semantics support multiple stigmergies and improve the specification of situated systems. Multiple stigmergies allow us to naturally describe further interesting classes of systems, where agents can communicate in different ways. For instance, they can be used to directly model multi-robot systems where robots have multiple sensors and communication devices, and decide the equipment to use depending on specific environmental conditions. As for situated systems, environment variables may now directly occur in expressions and guards. This makes it easier to describe systems where agents also interact via the environment. New case studies have been added to those related to birds flocking, robots foraging and opinion formation. To vindicate the flexibility of our language and its ability of expressing different interesting classes of systems, in this paper we also model Boids, and population protocols. The former is a widely-used model of flocking behavior as observed in different classes of natural systems. It extends the flocking case study by allowing additional interaction strategies, for getting closer and avoiding collisions, and not only for moving in the same direction. The latter are a type of gossip protocols that rely on a distributed communication paradigm inspired by the spreading of epidemics and by the gossip phenomenon observed in social networks. For both classes of systems, we do provide experimental results about their automated analysis. Our modeling of Boids systems shows the benefits of adding multiple stigmergies to the language and, to the best of our knowledge, our work reports the first results about formal verification of such systems; previous investigations only exploited simulation-based techniques. The rest of the paper is organized as follows. In Section 2 we present a revised version of the formal semantics of the core language, allowing us to define systems where agents interact indirectly through multiple stigmergies. In Section 3 we demonstrate the features of the language by modeling the Boids system, and include preliminary results about its automated analysis together with a discussion about the impact on verification of the different parameters of the specified system and of the used verification tools. In Section 4 we further enrich the language with environment-oriented primitives and show how LAbS can naturally model other classes of systems dealing with robot foraging, opinion formation and gossiping. In Section 5 we summarise our main achievements, compare our work with others, and suggest directions for future research.", "section": "Introduction", "doi": "10.1007/978-3-030-04771-9_26", "references": [24701999, 59714529, 64119988, 121297392, 191696817, 233122475, 292160745, 1060861436, 1482847502, 1501097462, 1507006488, 1508927144, 1544985403, 1549166962, 1566132409, 1571641412, 1613817315, 1631091010, 1729887387, 1750856813, 1864649459, 1951714113, 1973501242, 1976092684, 1991609163, 1995124372, 2017150251, 2021089599, 2031522732, 2044484214, 2059505795, 2082511574, 2095156325, 2098579316, 2098914525, 2108943074, 2118331730, 2122875695, 2129538349, 2132182116, 2142219372, 2150312211, 2161253570, 2167340365, 2177603835, 2254102208, 2260243663, 2281788661, 2284655150, 2395613050, 2404253444, 2460389227, 2567377301, 2775821128, 2799135505, 2883472767, 2884484164, 2888321432, 2894761520, 2950142800]}
{"paragraph": "Since the introduction of the term “Granular Computing (GrC)” by Zadeh and T.Y. Lin in 1997, we have witnessed a rapid development and fast growing interest in this topic. It is well accepted that the theories of fuzzy sets and rough sets are two primary contributions that have occurred from the emergence of GrC. In recent years, shadow set and three-way decisions are regarded as special models of Granular Computing to solve the problem of information granularity selection. GrC can be used in many fields, such as variable granulation, system granulation, concept granulation, cluster analysis, intelligent systems, and data processing. In set theory, for rough sets or a cluster analysis, a granule may be interpreted as a subset of a universal set. In programming, a granule can be a program module. Many GrC models and methods have been proposed and studied. GrC may be studied based on two related issues: granulation and computation. The former is related to the construction, representation, and measurement of the granules. The latter is related to the computing and reasoning with granules and granular structures. In general, two types of granulation processes exist: functional granulation and relational granulation. If the process is based on the attributes of the objects, it is known as functional granulation because the attributes are mathematical functions from the set of objects to the set of values. The granulation process is relational if it is based on the relationship between objects. Unlike a GrC-based framework for mining relational data found in the literature, where information granules derived from an information system are defined based on the notion of related sets, it is based on an information system defined for relational data. Herein, relational data implies the relationship between objects. For example, in a network, an edge between two nodes represents a relational tie. Relational granulation is particularly useful for the analysis of network data because a network is a relation between nodes. Liau formalized social networks into a fifth GrC model, known as the relational GrC model. In their work, they obtained logical characterizations of different positional equivalence relations and subsequently transformed them into a functional granulation process. The maximum flow problem was first presented by Fulkerson and Dantzig in 1955, and was solved by Ford and Fulkerson in 1956. Over the past few decades, researchers have proposed many efficient algorithms for solving the maximum flow problem. However, the size of real-world network has grown far larger than the amount of available memory in conventional machines. Therefore, more efficient algorithms are required to solve this problem. Two primary types of methods are used to handle the increasing scale of networks. The first solution is parallel computing or cloud computing. However, this naturally requires solving distributed programming issues, e.g., data distribution, load balancing, scheduling, fault tolerance, and communication, among other factors. The other solution is to simplify the flow network. In the maximum flow problem, Lee and Rieger studied the relation between the maximum flow and the substructure in complex networks. They showed that the flow efficiency is related to particular topological properties of the network. Liers and Pardella proposed the shrink max edge (SME) method by compressing the maximum capacity edge to reduce the network size. However, this method cannot significantly reduce the mass of the network, and is not applicable to a large number of networks. Scheuermann and Rosenhahn presented a method for graph simplification, which constructs a slim graph by merging nodes that are connected by simple edges. It also demonstrates that the maximum flow of the much smaller graph remains identical. In our work, we introduce a method for simplifying a complex problem. This method reduces the complexity of the problem and effectively solves the maximum flow problem. However, developing a method for granulating the network, aiming at finding the approximate optimal solution of the maximum flow on the original network, remains a challenge. Based on the discussion above, a novel method, called the maximum flow based on Quotient Space Theory (MF-QST), for solving the maximum flow problem based on Quotient Space Theory is proposed herein. The primary idea of the quotient space is forming a suitable granular space to describe and solve the problem, as well as constructing the relation of different granular spaces to simplify the problem. The motivation of Quotient Space Theory, proposed by Zhang & Zhang, originates primarily from Hobbs’ idea that the world can be conceptualized at different granularities and easily translated from one level of abstraction to another, i.e., they can be handled hierarchically. In the proposed method MF-QST, a community as a specific substructure is first given. Subsequently, the relational granulation criterion is discussed in detail to filter and retain the appropriate substructure. Next, the construction of the quotient network based on Quotient Space Theory is formed. Finally, the maximum flow algorithm is used to compute the maximum flow on the quotient network as the approximated maximum flow on the original network. The remainder of this paper is organized as follows. Section 2 introduces some concepts to formalize the maximum flow problem. Related studies on Quotient Space Theory are also described. The proposed method, MF-QST, is discussed in Section 3. Section 4 describes the experimental results of the proposed method, and discusses some qualitative and quantitative characterizations. Finally, some concluding remarks are provided in Section 5.", "section": "Introduction", "doi": "10.1016/j.ins.2018.12.009", "references": [1512189463, 1984627379, 2018804864, 2036841467, 2040753112, 2082272625, 2113137767, 2152216760, 2153676086, 2154500141, 2170755382, 2235896546, 2340020088, 2537382886, 2652860524, 2751469631, 2792234535, 2806445482, 2883737195, 2898660825, 2912565176]}
{"paragraph": "Granular computing (GrC) is an emerging computing paradigm of information processing with multiple granularity levels, and its studies are closely related to several existing fields such as rough sets. GrC has a basic issue regarding the information granule number and its interpretation, and three-way decisions (3WD) utilize three parts or granules for problem solving. To enrich the studies of GrC and 3WD, this paper mainly focuses on attribute reduction of rough sets by introducing information theory, and we will particularly establish three-way information reducts within the basic framework of three-way granular structures. By virtue of GrC, attribute reduction serves as a fundamental topic of rough sets, and it utilizes effective simplification or feature selection to implement the classification and reasoning in information processing. Attribute reduction usually needs a basic data background of a decision table, and the three-layer granular structures of a decision table were revealed. The mainstream attribute reducts remain at the Macro-Top, and relevant classification-based reducts adopt basic research pathways of the rule region, discernibility matrix, and information measure; moreover, there are various other approaches of reduct construction such as incremental, dynamic, parallel, approximate, hierarchical reducts. The traditional classification-based reducts depend on the set operation and region structure, and thus they adhere to the so-called algebra viewpoint. In contrast, the classification-based reducts have in-depth investigations via the introduction of the information theory, so they also have the information viewpoint. For example, one study offered the informational representation of knowledge reduction and decision reduction, where the entropy and mutual information are highlighted; another study adopted the conditional entropy to develop heuristic reduction algorithms; another used the conditional entropy to define approximate reducts; moreover, another presented the relative decision entropy to propose a feature selection algorithm. In particular, aiming at the classification-based reducts, one study conducted a comparative study from the algebra and information viewpoints, where the conditional entropy acts as the main information tool, so the algebra and information types gain their explanation and difference. The classification-based attribute reducts implement complete optimization to, on average, fit all of the decision classes, but their compromise might not necessarily be suitable for specific optimization with regard to every decision class. Thus, one study first introduced a notion of attribute reducts for a decision class based on the positive region of the decision class; in particular, another study deeply analyzed three relevant blind spots of classification-based reducts and further stood at the Meso-Middle with only one decision class to establish the class-specific attribute reducts, mainly from the viewpoint of the positive region or acceptance rules; recently, another study enriched the class-specific reducts from the 3WD perspective, mainly by adding the negative region or negative rules and positive-and-negative region or rule-out rules. The class-specific reducts become a new sort of attribute reduction, and their difference from the local attribute reducts has also been clarified. Regarding practical applications, the class-specific reducts can utilize the concrete decision class to simplify the class-pattern recognition or class-decision rules and further integratedly complete the entire classification and reasoning tasks in a low-dimensional space. However, the class-specific reducts currently offer only the algebra interpretation on regions, so their fundamental information construction and further comparison between the algebra and information types become novel and valuable. This paper mainly sets about these relevant works, and the search for appropriate information measures becomes a starting point. The information theory provides an effective approach for uncertainty analyses. Now, relevant information measures have been introduced into the rough set theory for uncertainty representation and measurement. In particular, as mentioned above, the classification-based reducts depended on information measures to gain the information interpretation. There, the information theory directly generated the entropy, conditional entropy, and mutual information, but the three information measures are located at the Macro-Top to develop only the usual classification-based reducts. In our previous research, the three-way informational measures based on the three-layer granular structures were systematically and hierarchically established from the rough set itself based on the decision table and attribute reduction; as a result, the previous information measures at the Macro-Top achieved their hierarchical construction mechanisms and Bayes-based systematic relationships, and three relevant measures with uncertainty measurement also emerged at the Meso-Middle, i.e., the likelihood, prior, and posterior weighted entropies. Clearly, the three-way weighted entropies can be fully utilized to underlie our discussion on the information class-specific reducts here. The above background is clarified in a figure. Targeting the class-specific attribute reducts, this paper mainly establishes three-way information types by using the three-way weighted entropies with uncertainty measurement, and it further compares these new information types with the existing algebra type. The two basic research contents are also marked by two question marks in the figure, and they are actually related to GrC and 3WD, except the above information theory. GrC has a trialistic framework to act as a fundamental structural methodology and can effectively process hierarchical information. GrC basically concerns information granulation and is extensively applied in rough sets; moreover, the granulation monotonicity in knowledge coarsening plays an important role in attribute reduction. In particular, one study utilized GrC to define orthopairs and a hierarchy on them, and thus three levels determined by orthopairs were discussed to give a kind of three-level granular structures. Herein, the class-specific reducts are related to the three-layer granular structures, so they adhere to GrC. The class-specific attribute reduction also concerns a basic GrC process of knowledge coarsening, and the relevant granulation monotonicity and equality condition are particularly required. In contrast, the GrC methodology of multi-granule, multi-level, and multi-view can be fully utilized to probe the concerned information measures and constructed multiple reducts. 3WD serves as an important decision-making methodology with extensive applications, especially in knowledge discovery and management. 3WD was first introduced in rough sets, and it gradually moved to a more general tri-partition framework. At present, 3WD has already become a research hotspot to exhibit in-depth results, and these recent studies show that 3WD can be viewed in a broad sense as thinking in threes. In particular, one study basically discussed generalized and special 3WD; furthermore, another rationally established the more general 3WD theory to embrace multiple ideas including rough sets, and thus the essential 3WD thinking is through trisecting and acting. As a result, the three-level analysis falls into the 3WD category, so our three-layer granular structures are related to 3WD. Moreover, our three-way information class-specific reducts and their systematic relationships will be explored by the three-way weighted entropies and three-way regions, and thus these relevant studies also become a good example for the enrichment of 3WD. According to the three-way weighted entropies and their granulation monotonicity, three-way information class-specific reducts can be naturally established by preserving the weighted entropy value, and this preservation reduction target is essential to maintain a kind of certainty optimization, which is initially reached by the entire condition attributes. In fact, the existing algebra class-specific reduct also aims to obtain a sort of optimization, mainly regarding the positive region. Therefore, mining and comparing the optimization preservation conditions of the three-way weighted entropies and the positive region become the research key and difficulty to reveal class-specific reducts’ relationships from the information and algebra viewpoints. In this core process, we would effectively utilize the related GrC technology, and three auxiliary probability conditions for granular merging are also provided, i.e., the LPE-Condition, IP0-Condition, and IPS-Condition. Against the above content and thought, the research developed in this paper is concretely presented as follows. The algebra class-specific reduct is reviewed by the positive region. Then, the likelihood, prior, and posterior class-specific reducts are systematically and respectively established by the likelihood, prior, and posterior weighted entropies. Optimization preservation conditions of the positive region and weighted entropy are deeply mined, and they are effectively represented by the granular merging and three-way regions. All four types of class-specific reducts gain their strong-weak relationships, consistency or inconsistency degeneration, and information systematicness. The algebra and information class-specific reducts and their relationships are illustrated by a consistency example and an inconsistency example. In summary, the four types of class-specific reducts adopt distinctive algebra and information viewpoints to present different emphases and mutual relationships. This study provides an in-depth insight into GrC-based attribute reduction, mainly by the information theory and 3WD theory, and this contribution is also clarified in the figure. The remainder of this paper is organized as follows. Section 2 provides preliminaries to review the three-layer granular structures, the three-way informational measures, and the algebra class-specific reducts. Section 3 constructs the three-way information class-specific reducts via the three-way weighted entropies. Section 4 first mines optimization preservation conditions of the positive region and weighted entropy and then analyzes multiple relationships of class-specific reducts. Section 5 provides an illustrative example via a consistent and an inconsistent decision table. Finally, Section 6 concludes this paper.", "section": "Related Work", "doi": "10.1016/j.ins.2018.06.001", "references": [101345952, 831516024, 1798724659, 1861960977, 1971515969, 1974313046, 1994743372, 2040753112, 2045810864, 2049204733, 2059228994, 2070416356, 2070860530, 2074056315, 2082272625, 2098093602, 2130367040, 2131956306, 2162755671, 2206296687, 2270306881, 2297889545, 2346184948, 2483885539, 2517012834, 2551396410, 2557802193, 2560804083, 2568228263, 2568232622, 2588487610, 2612102978, 2619471640, 2707389940, 2745134596, 2753804762, 2760880172, 2770194470, 2792691650, 2793566013, 2794856943]}
{"paragraph": "The first geometry software tools, such as the Geometric Supposer, appeared in early 1980s, followed by Cabri, the Geometer's Sketchpad and a number of others, like Cinderella, DrGeo, Eukleides, GCLC, KSEG, to name just a few. Geometry tools have found their way to classrooms worldwide and became an irreplaceable component of mathematical education. The most popular geometry, and not only geometry, educational tool GeoGebra has tens of millions of users. Over the years, geometry tools got additional features, including intelligent ones, such as automated proving based on a range of methods of properties of constructed geometric figures. Recently, functionalities of increasingly popular touch-based devices, such as mobile phones and tablets, got supported by some geometry tools, for instance, Sketchometry and QuickDraw. The key problem here is recognition of geometric figures drawn on a screen by fingers or pens; this feature is related to but still significantly different from recognition of geometric objects and their relationships from scanned images. Most of geometry software tools are dynamic geometry tools, built from the very beginning and still around one central scenario: the user chooses several free points and, using them, constructs some other points and other geometric objects. Then, the user can move, drag, a chosen free point and explore how the constructed points and other objects change. This feature is very simple but very appealing and effective in mathematical education: instead of exploring static geometric figures, students can explore closely related figures obtained by dragging some free points and guess some properties about them. We are not aware of any DGS tool that allow dragging the constructed points. That is certainly not trivial, unlike the standard scenario where it is easy to recompute all constructed objects based on new locations of the free points. Namely, in order to drag one constructed point while keeping some other two points fixed, one has to solve a possibly complex, and sometimes even unsolvable using straightedge and compass, construction problem. Let us consider a simple example: the user chooses three free points A, B, and C and then, using angle bisectors and intersections, constructs the incenter I of the triangle ABC. If the user drags, for instance, the point A, the construction of the point I is simply replayed with the new points A, B, and C, and the new point I is obtained. Let us suppose now that the user wants to drag the point I while keeping B and C fixed, and allowing A to move: for that, the following non-trivial construction problem has to be solved: given a point B, a point C, and a point I, construct the point A, such that I is the incenter of the triangle ABC. This functionality that we are aimed at is illustrated in a figure. In this paper we describe our prototype dynamic geometry tool Touch&Drag that enables dragging constructed points. It uses solutions of hundreds of location construction problems generated automatically by our solver ArgoTriCS. Compared to most of other popular geometry tools, the second distinctive feature of Touch&Drag is that it uses functionalities of touch based devices, but the functionality for dragging constructed points can be integrated also into other sorts of dynamic geometry tools.", "section": "Introduction", "doi": "10.1016/j.jsc.2018.12.002", "references": [1534758548, 1976053863, 1984350501, 2006962766, 2010315317, 2032405865, 2053860863, 2064030682, 2093871241, 2099684934, 2131408699, 2294949920, 2400850545, 2795900304, 2950855305]}
{"paragraph": "Motion capture, also called motion tracking, is a kind of technique that can record and analyze the movement of objects, animals, or human beings. This technology is widely used in different fields such as entertainment, sports science, and medical applications. For motion capture, optical measurement is a common method to track the motion of objects. This measurement method uses the data captured from multiple calibrated cameras to get the overlapping images for calculating the position and the posture of objects through the triangulation methods. Although the optical measurement systems can be a reliable and accurate method, this method is expensive because of several high-performance cameras and the calculation system and has poor portability. Another serious problem is that the optical measurement method has the problem of line-of-sight shielding. Once such phenomenon occurs during the measurement process, the measured result will be inaccurate or even impossible to measure. This phenomenon usually appears while measuring complex and compact objects such as the motion of hand. In recent years, there has been an increasing interest in hand movements in various application fields. Hand movements are significant for human because people can accomplish a large variety of tasks during daily life through their hands. According to research, the classification of human hand functional gestures can be classified into 33 different types. How to measure complex movements in a compact space, like hands, is a worthwhile thing to be studied. Conventionally, angles data can be acquired using optical encoder or potentiometer in many mechanical designs. The reason that the encoder is widely used is that it has several advantages such as high resolution, high repeatability, and good antiinterference ability. These features of an encoder are usually used in many mechanical devices for measuring accurate angles data. However, it is difficult to install encoders on hands because of compact space, coupled with degree of freedom and multiple joints. For accurately measuring the gesture of hands and angles of fingers during movement, many methods have been developed over the past few years such as soft sensor, acoustic method, and flexible bending sensor. Among them, bending sensors have been widely used in many commercially available sensing devices due to its low price and ease of use, working like a potentiometer, in the recent years. However, the bending sensor is not linear with the change of the flex angle, and its sensitivity is low at small angles change. Moreover, its resistance value is quite susceptible to temperature. Even though these methods have their advantages, there are still many problems that still need to be solved in various measurement methods such as performance, wearability, and portability. A frontier motion capture system based on inertial measurement units has been widely studied in the recent years. With the development of microelectromechanical systems manufacture technique, inertial measurement unit sensor ship can be downsized as small as fingernails. This technology is suitable for creating a low-cost, highly wearable, and portable motion capture system. The conventional inertial measurement unit is composed of accelerometer and gyroscope, which may measure acceleration and rotation speed. With these two pieces of information, the pitch and roll angles of the subject can be calculated. For the heading information, a magnetometer sensor needs to be integrated together. Because the IMU sensor measures the orientation of the object through the acceleration, angular velocity, and magnetic force, the sensor can be mounted anywhere on the object unlike an encoder that must be mounted in an axial direction. On the other hand, using the IMU sensor to measure an orientation of the object, unlike the vision-tracking method, does not have the problems caused by cameras’ field of view and line-of-sight shielding. However, calculating an accurate and precise orientation of the object would face many challenges. These problems are mainly how to eliminate the influence of noise on accelerometers and magnetometers and reduce the effect of integrating errors of gyroscopes. In this paper, a simple sensor fusion algorithm is proposed to get better performance on the orientation measurement of objects by joining two types of orientation expressions. One of the expressions is updated from accelerometer and magnetometer through the rotation matrix. Another one is expressed as a quaternion format. This expression is updated by integrating the angular velocity from a gyroscope. The performance of the sensor fusion algorithm is verified by optical encoders to ensure that the orientation of the objects estimated from IMUs is reliable and can be applied to capture the motion of human hands.", "section": "Introduction", "doi": "10.1109/TIE.2019.2912765", "references": [940303641, 1980448216, 2002107854, 2015742518, 2055217114, 2060725197, 2079897949, 2110331760, 2121095571, 2122750542, 2134584018, 2138525521, 2148297478, 2282481780, 2295829093, 2557696962, 2616510615, 2735943270, 2799429034]}
{"paragraph": "Compressed Sensing (CS) has been extensively studied over the last decade as an attractive way to perform dimensionality reduction. According to CS theory, signal acquisition and compression can be jointly performed by means of random projections allowing for sub-Nyquist acquisition rates. In more detail, a K -sparse signal, i.e. a signal with K non-zero entries, can be exactly recovered with overwhelming probability from its random linear measurements if some assumptions on the sensing matrix are satisfied. The ability of performing low complexity and low energy consumption acquisition is one of the main reasons behind the rise of CS in the last years. Works such as showed its advantages over traditional acquisition methods. Furthermore, in the authors consider the specific application of CS to different Internet of Things (IoT) scenarios. It has then become evident that, because of its structure, CS could also provide some notions of secrecy. If the sensing matrix is not known, the original signal cannot be recovered and the CS framework acts as a private key cryptosystem. In this regard, the sensing matrix entries are the secret key, thus only shared among trusted parties, the original signal is the plaintext and the measurements are the ciphertext; the encryption is performed by means of CS acquisition while the decryption corresponds to CS recovery. This means that all applications making use of CS can also provide some kind of privacy, with little or no added cost. This is extremely advantageous for the wide range of low complexity devices which may acquire sensitive data, e.g. in the internet of things scenario, and might not be able to cope with standard encryption schemes such as AES. Moreover, these devices are oftentimes heavily constrained in terms of representation precision, see e.g.. Furthermore, it has to be highlighted that any practical system in which the sensing matrix has to be stored, automatically implies that its entries are represented with finite precision. If the sensing matrix entries are sub-Gaussian, then also its finite-precision counterpart will follow a sub-Gaussian distribution. In turn, this implies that a sparse or compressible signal acquired with such sensing matrix and enough measurements can be recovered with high probability. Nevertheless, if we focus on the security aspects, it is not clear how fast the secrecy may decrease when the representation precision is reduced. This indeed motivates us to explore the secrecy of CS cryptosystems exploiting sensing matrices under finite-precision representation. Starting from the preliminary experimental results on the secrecy of finite-precision sensing matrices presented in, with this work we formalize and extend the results from a information-theoretic perspective. This will allow us to address the gap existing in literature between the secrecy provided by CS cryptosystems exploiting sensing matrices under infinite precision representations and more practical scenarios requiring finite precision. To the best of our knowledge, this is the first work in literature dealing with finite precision representations lying on a larger-than-binary alphabet. More specifically, we prove that a CS cryptosystem exploiting quantized and truncated Gaussian entries can achieve a secrecy which is a function of the available representation precision. Next, we show that the same result can also be extended to the case of sensing matrices with i.i.d. entries drawn from a discrete Gaussian distribution. More in detail, we derive upper bounds for the secrecy in the worst case scenario of cryptosystems employing 1) quantized and 2) discrete Gaussian sensing matrices. If equal energy signals are considered, then we show that there exists a regime condition for which the secrecy exponentially increase with the number of bits employed for the quantization of the sensing matrix entries. On the other hand, in case of signals having arbitrary energy, we prove that the secrecy is not only related to the representation precision of the sensing matrix entries, but it also depends on the energy mismatch and derive an upper bound on this latter term. Lastly, the validity of the obtained bounds is evaluated through extensive experimental simulations. As a remark, it is important to consider that the results of this paper can also be exploited in works which rely on known secrecy properties of specific sensing matrices constructions. As an example, in, where the focus is put on practical and secure sensing matrix generation schemes, the results are based on the assumption of using sensing matrices for which the secrecy has already been proven. The seminal paper of Rachlin and Baron was the first study related to the secrecy capabilities of CS. Further studies such as focused on the asymptotic secrecy properties of compressive encryption, showing that measurements of equal energy signals become indistinguishable as the size of the original signals tends to infinity. Non-asymptotic analysis of the distinguishability of measurements sampled from Gaussian i.i.d. sensing matrices was carried out in. In this latter work, the authors show that normalizing to unit energy the signal leads to perfect secrecy under the assumption of one time sensing (OTS) acquisition, i.e. the sensing matrix is re-generated at each encryption. A similar analysis was also extended to the case of circulant sensing matrices in, where the authors characterize the increased information leakage of the measurements due to the structured nature of the sensing matrix. Further, a more comprehensive review on the secrecy properties of different classes of sensing matrices and signals was given in. From all the above works, it has emerged that, because of the linearity of the sensing process, the measurements will always reveal at least the energy of the original signal. The best case, in which only the original signal energy is leaked in the non-asymptotic case, is that of sensing matrices made of real-valued Gaussian i.i.d. entries. In order to overcome this problem, proposed solutions consider either to normalize the signal to unit energy as in or to obfuscate the energy as in. In this latter work, the authors propose a method to obfuscate the energy of the original signal through scalar multiplication, avoiding the encryption and transmission burden related to the transmission of plaintext energy. Interestingly, the authors show that this method also allows trusted parties to perform basic signal processing operations in the encrypted domain, i.e. anomaly detection. Differently, in the author considered the effect of the energy mismatch in case of compressive cryptosystems employing both Gaussian and Bernoulli sensing matrices. In more detail, they obtain Total Variation bounds on the measurements distinguishability and show that in the case of signals with unequal energy, the measurements are nearly distinguishable based on the system parameters. Other works shifted the emphasis on the encryption models. In, similarly to standard private key cryptosystems, modes of operation for compressive encryption are introduced which, along with the use of Bernoulli sensing matrices, make the considered scenario suitable for practical implementations. On a similar line, in different encryption models, including a model based on optical imaging, are discussed. In addition, the authors also show how a practical parallel compressed sensing scheme with random permutation can achieve asymptotical spherical secrecy. Lastly, in the authors discuss how the secrecy notions of CS can be effectively used in applications such as multimedia, cloud computing and IoT. Regarding practical systems, other works targeted a specific case of practical sensing matrix: the one made of Bernoulli distributed entries. As an example, in, the authors also consider Bernoulli sensing matrices and prove their asymptotical spherical secrecy. However, higher dimensional finite alphabets were only considered in from an experimental perspective. The remainder of this paper is organized as follows. In Section II we provide some background and notation about compressed sensing, the finite-precision Gaussian distributions and the secrecy metrics we will adopt for the rest of the paper. The main results of this work are presented in Section III. The experimental results and their discussion are shown in Section IV.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2918089", "references": [121367636, 1624366496, 1994790157, 2014035956, 2015606516, 2017761965, 2060430274, 2081122055, 2082476922, 2083042020, 2101444129, 2120365466, 2127271355, 2145096794, 2154423986, 2169382889, 2177125333, 2190983062, 2296616510, 2535450343, 2786204441, 2793404470, 2800879247]}
{"paragraph": "It is well-known that modal logic is a fragment of first-order logic with very appealing model-theoretic and computational properties, namely the small model property, decidability of satisfiability, and the tree model property. The latter is a consequence of bisimulation invariance of any property expressible in modal logic; in fact, a first-order property of nodes in a Kripke structure is definable in modal logic if and only if it is bisimulation-invariant. These positive features come at a price: the expressive power of modal logic is very limited. Many attempts have been made at finding logics with greater expressive power that retain decidability and other desirable properties. Prominent examples are temporal logics like CTL or more expressive branching logics which retain decidability, bisimulation invariance, tree and small model properties. Another closely related example is the modal μ-calculus which extends modal logic with special second-order fixpoint quantifiers. It also possesses these properties. The term hybrid logic refers to a family of logics which enrich modal logics with certain first-order constructs. This takes their expressiveness beyond the limitations of bisimulation invariance. Typically, hybrid logics extend modal logic with features like nominals—names for particular states in a Kripke structure or atomic propositions that hold true in a unique node—as well as restricted existential quantification in the form of the binder and the jump modality. The requirement on a nominal to be true at a single state is a very strong one. This is what destroys bisimulation invariance: clearly a formula can distinguish two copies of a state in the tree unfolding of a Kripke structure when only one of them can carry a particular name. Another intuitive interpretation of nominals and binders is given as reference points for temporal properties, and this is the name under which those logics were originally invented. However, despite increasing the expressive power vastly, one can typically easily define model checking procedures or other reasoning methods for these hybrid logics. In some cases, this can even be done without increasing the computational complexity. This raises the question of how hybrid extensions of more expressive logics like temporal logics behave. In this paper we consider a combination of two of these extensions: adding hybrid features and going from modal to temporal logics. We consider the hybridisation of the full computation tree logic and its prominent fragments CTL, fair fragments, with nominals, binders and jumps and study the complexity of their model checking problems. This logic allows arbitrary linear-time formulas underneath a path quantifier as opposed to CTL, which couples every linear-time temporal operator directly with a path quantifier. As a consequence, the syntax of this logic naturally features path and state formulas such that every state formula is also a path formula. They are correspondingly interpreted over paths and states of a Kripke structure. The syntax of CTL only features state formulas, and this is why the following considerations do not arise when studying only hybrid extensions of CTL. The presence of these two kinds of formulas gives some choice for the introduction of the hybrid operators: the unary binding and jump operators could be allowed in front of path formulas or could be required to only precede state formulas. Not every combination is meaningful. First, consider a binding formula, which intuitively reads as “name the current state of evaluation x and then proceed to checking whether φ holds under this assumption.” Clearly, φ can naturally be a state formula, but this statement also makes sense when it is a path formula because paths naturally have a first state to which x would get bound. The situation for the jump modality is different, read as “continue evaluating φ at the node x.” Note that x is supposed to be bound to a node. Hence, this is meaningful if φ is a state formula. It would only be meaningful for a genuine path formula if x was bound to a state on a particular path beforehand. Then the evaluation of φ can continue at the state x on that path. Syntactically, this is for example guaranteed when no path quantifier occurs between the binder and the jump. These considerations lead to three different hybrid extensions, depending on what kind of formulas are preceded by the binders and jumps. The reason for the additional syntactic restriction is purely the need for a well-defined semantics. For the same reason, we cannot consider the case in which a binder precedes a state formula and the jump precedes a path formula. We study the model checking problems for these logics over finite Kripke structures. Recall that model checking the full logic is PSpace-complete while model checking CTL can be done in P and related fragments are also tractable. We show that the complexity of the full logic extends to its hybridisation. On the other hand, model checking HCTL already becomes PSpace-hard. Allowing genuine path formulas under binders increases the complexity: model checking this logic is ExpSpace-complete. Finally, the complexity of the hybrid logic depends on the number of variables that are being used: with a single variable it is also ExpSpace-complete, with at least two variables it is only non-elementarily decidable. We also refine the analysis of the combined complexity of model checking these hybrid variants and give upper and lower complexity bounds for the data and expression complexity, i.e., the complexity of model checking a fixed formula, respectively a fixed Kripke structure. The model checking problems for various hybrid logics have been considered before. Some examined hybrid logics with an LTL-like syntax which are, nevertheless, interpreted over Kripke structures like simple branching-time logics. Past-time operators are featured in this logic as well, so the logic is in fact a hybrid extension of a variant of Propositional Dynamic Logic with Converse. The complexities obtained for various fragments range from P to PSpace. Improvements over those techniques have been made. Earlier, polynomial-time decidability of model checking of the hybrid extension of modal logic with a single variable has been noted. Hybrid extensions of genuine temporal and similar logics have been considered before but the focus has typically been on the complexity and decidability of their satisfiability problems. Some studies have explored hybrid extensions of simpler branching-time logics and obtained some decidability results in the area of satisfiability checking. As a rule of thumb, two variables make satisfiability undecidable, and the logics with a single variable tend to stay decidable but the complexity of satisfiability checking is one exponential worse than that of the underlying temporal logics. There has also been an attempt at defining and studying the hybrid extension of this logic but no considerations on the interplay between path formulas, binders and jumps have been made. Claims about the non-elementary complexity of satisfiability for the one-variable fragment have to be questioned because the lower bound proof uses formulas that are syntactically not well-formed and therefore cannot be given a well-defined semantics. Some have studied hybrid versions of the modal μ-calculus but these extensions feature nominals only; binders and jumps are not included. Furthermore, they only look at satisfiability. The full logic can be translated effectively into the modal μ-calculus at an exponential blow-up. Thus, a formula can in principle be model checked by translating it into the modal μ-calculus first and then using some model checking algorithm for this logic. This does not necessarily yield an optimal procedure, though. The model checking problem for the fully hybrid μ-calculus, the extension of the modal μ-calculus with nominals, binders and jumps, has recently been studied and its model checking problem has been identified to be ExpTime-complete. This, however, does not yield an upper bound on the model checking problem for the full hybrid temporal logic and therefore also not for its fragments, not even a non-optimal one, as it was also shown that this logic can express certain properties that the fully hybrid μ-calculus cannot. Model checking for μ-calculus with nominals is very briefly discussed with the result that it is basically the same as model checking the modal μ-calculus since nominals without jumps can simply be treated like atomic propositions. The aim of this article is to provide a comprehensive study of the complexity of the model checking problem for hybrid extensions of temporal branching-time logics. It extends a preliminary version with full proofs and the aforementioned study of data and expression complexity. The rest is organised as follows. Section 2 introduces the full logic and its fragments CTL and related extensions as well as their hybridisations. In Section 3 we then investigate their model checking problems. We give optimal upper and lower bounds not only for their combined complexities but also for data and expression complexities. Section 4 then identifies a family of fragments with better model checking complexities while still extending the branching time logics with hybrid operators.", "section": "Related Work", "doi": "10.1016/j.jlamp.2018.11.007", "references": [142499984, 1503647529, 1568275823, 1596995530, 1608094044, 1824565677, 1975657455, 1976513602, 1982129592, 2003227046, 2004306067, 2012935476, 2015640848, 2017460625, 2017759899, 2021711161, 2048355938, 2067441543, 2090612177, 2102548669, 2107444004, 2112476716, 2112535338, 2144726625, 2186597297, 2560471064, 2569214543, 2592897984, 2758859489]}
{"paragraph": "Citizen consultation is an essential step in the urban planning process to ensure, amongst other things, that the resulting buildings and changes to the city meet the needs and wishes of the inhabitants. There are long-established methods, such as public presentations or publicly displayed plans for this purpose but they suffer from a number of issues. These problems include a lack of participation from citizens in general and from specific groups, e.g. vulnerable or disadvantaged people, in particular. Standard ways to present urban plans, such as detailed architectural drawings or maps, are not necessarily easy to understand for everyone. The time and location where events take place or plans are exhibited, are also frequently only known to a small part of the population. Finally, participation often is realized at the lower end of Arnstein’s participation ladder, where citizens are merely informed about planned projects with little if any means to voice their opinions or make suggestions. Public displays as a technology have the potential to overcome many of these issues in participatory urban planning. First, they are increasingly ubiquitous, in particular in places that are visited by many people, e.g. shopping centers or transportation hubs. This would allow for pushing urban planning information to where people are, rather than citizens having to find out where/when such information is provided and then having to go there at the right time. Secondly, public displays are accessible to anyone who is present at their deployment location. This, in turn, can raise awareness of urban planning projects and empower people who rarely participate in traditional urban planning events. Finally, public displays can support different ways to present urban planning information and can facilitate different forms of interaction. These properties can help to make information more accessible to a broader audience and to provide them with means to participate beyond simply absorbing information. Taken together, these observations indicate that it makes sense to explore the potential of public displays for deeper participation in urban planning. In this article, we introduce a novel approach that relies on public displays to facilitate deeper participation in urban planning. We propose to combine an immersive public display with a simple mobile client to enable citizens to vote and comment on urban planning projects. The proposed projects are visualized using panoramic videos of the project site, in its current form, which are overlaid with 3D models of the planned buildings. We created a prototypical implementation that realizes our approach and evaluated it in a lab-based user study with 21 participants. In our study, we investigated standard usability aspects as well as comments participants provide on the urban planning projects. Our main contribution are a new approach for using public displays for voting and commenting in urban planning, and a multi-level evaluation of a prototypical system implementing the proposed approach. Our results can benefit designers of participatory public displays systems as well as urban planners wishing to deepen the level of citizen participation in their projects. The rest of this article first briefly reviews relevant related work and then introduces our approach in detail. The second half of the paper reports on the user study we conducted, presents key results, discusses implications and insights, and then summarizes the main findings of our work.", "section": "Related Work", "doi": "10.1080/10447318.2019.1606476", "references": [78594992, 157721646, 1557739949, 1964141240, 1971664989, 1972096067, 1977189158, 2002162197, 2013761708, 2035487231, 2042252751, 2052454986, 2063529501, 2083024233, 2085140749, 2085198316, 2085412990, 2104631017, 2106351401, 2121120672, 2121621580, 2122739475, 2123944407, 2125802131, 2141284070, 2146995712, 2159870351, 2160848194, 2336811793, 2624557858, 2753349643, 2761607258]}
{"paragraph": "These days, in real-world applications, many generated data sets comprise multiple representations or views. For example, in person re-identification, we may obtain photos of the same person from nonoverlapping cameras distributed at different physical locations. In document clustering, the same document may be translated into multiple different languages. In web retrieval, webpages contain not only the context information but also the hyperlink information. In general, these different views share some consistency and complementarity. Therefore, by integrating such information, we can significantly improve the performance of machine learning tasks compared with only using single-view information. To date, many multiview learning methods have been proposed, such as co-training-based methods, multiple kernel learning-based methods, and subspace learning-based methods. With the development of deep neural networks, many deep multiview learning methods are also proposed, such as deep restricted Boltzmann machine, deep autoencoder, and deep recurrent neural network. Although these multiview learning methods have been successfully applied in many scenarios such as computer vision, natural language processing, computational biology, web retrieval, and so on, a major problem is that most of these methods can not address the unpaired multiview data problem in which the pairwise relations of some instances in different views are not given and the incomplete multiview data problem in which some instances in one view miss corresponding instances in other views. However, it is often the case that in practice the available multiview data are unpaired or incomplete. For example, some but not all news articles published on one website may be also simultaneously published on other websites by different publishers. In the audio–video system, different sampling frequencies of sensors acquiring data may result in nonsynchronicity between signals from different channels. In the Alzheimer’s disease neuroimaging initiative, almost half of the patients lack cerebrospinal fluid measurements and many other patients may suffer from the lack of fluorodeoxyglucos positron emission tomography scans. Such unpaired and incomplete cases in all views make it difficult to learn better models for most of the above-mentioned methods. Thus, it is necessary to develop special multiview learning methods for both of these cases. Recently, some attempts have been made to address the unpaired and the incomplete multiview data learning problems, respectively. In this paper, we provide a taxonomy of these existing methods and roughly divide them into the following four main categories. Instances Imputation Methods: These methods are mainly designed for incomplete multiview data learning problems. They first fill in the missing instances with certain algorithms and then train the existing multiview learning algorithms on the imputative data sets. For example, some methods first fill in the missing samples in each incomplete view with the average feature values and then use the weighted non-negative matrix factorization with regularization for shared subspace learning. Other works embedded the view imputation step into the learning process and then simultaneously learn the missing instances and model parameters. Some proposed incomplete multiview clustering methods by using the kernel matrix in one view to fill in the missing kernel matrices of other views, instead of filling in the missing instances. Instance-Level Constraint Methods: These methods use the specially designed algorithms to ignore the missing information and just constrain the paired or nonmissing instances in each view to follow certain consistent assumptions. For example, some propose the incomplete multiview data clustering methods based on subspace learning by constraining the same instances in different views to have the same latent representations in the subspace. Others also used a similar constraint and propose a unified dimensionality reduction framework for partially paired and semi-supervised multiview data by utilizing both the global structural information captured from the unlabeled data and the local discriminative information captured from the limited labeled data. Another method proposed an incomplete multiview data classification method based on multitask learning by constraining the instances observed in each combination of views to belong to the same task. Further work proposed a transfer learning method for incomplete multiview data learning by constraining the different data sets and views to share a common subspace. Group-Level Constraint Methods: These methods carry out multiview learning by constraining the instance groups in different views to follow the consistent assumptions, and these groups can be obtained based on the label or cluster information. For example, one method proposed the group-level constraint method for a weakly paired multiview problem in which the full pairing between data samples may be not known, while pairing of a group of samples from one view to a group of samples in another view is known by jointly learning the projection matrices in each view and the pairing matrices. Another proposed a group-level constraint-based multiview dictionary learning method to simultaneously learn the dictionary and the pairing matrix. Co-Training: These methods train the single-view learning algorithms alternatively on different views and maximize the mutual agreement among these views. For example, one method first learns a base classifier for each view and then estimates the missing prediction scores by using certain missing value estimation algorithms which can be considered as maximizing the mutual agreement step of the co-training method. Another proposed a partially paired multiview data clustering method by using the co-EM algorithm to iteratively learn the clustering models in each individual view and then transferring the constraints across views. As described before, although the unpaired and the incomplete multiview data learning methods have many similarities, in general, the incomplete multiview data learning is more challenging than the unpaired multiview data learning, since the information loss of the former case where the instances in some view are missing is much more than that of the latter case where just the pairwise relations are unobserved. Especially, when dealing with multiview data which contain a relatively large number of missing instances, these above-mentioned methods often lead to poor results because of the less shared information caught by the models. To the best of our knowledge, only one work takes into consideration such circumstance by exploring the global structure over the entire training instances; however, it can only be used in two-view data and its performance is also not good enough, both of which limit its application scope. Recently, the Gaussian process-based latent variable models have been proposed to learn the complex low-dimensional manifolds and demonstrate their superiorities in nonlinear learning, uncertainty quantification, and so on. Furthermore, as a probabilistic nonparametric method, it needs no assumption on the concrete form of projection function and, thus, much flexibility is preserved, particularly in comparison to parametric methods, such as canonical correlation analysis and non-negative matrix factorization and so on. Apparently, it seems to be fairly straightforward to extend this model to the multiview learning case for gaining the merits of two worlds, that is, Gaussian process and multiview learning. But in fact, such a trivial extension of the previous Gaussian process-based multiview learning models just by using the instance-level constraint cannot obtain results as desired. In order to address this problem, in this paper, we propose a shared Gaussian process latent variable model for incomplete multiview data clustering. The key idea of this model is that, apart from using the instance-level constraint, we also learn a set of intentionally aligned representative auxiliary points among individual views to not only compensate for missing instances but also implement the group-level constraint. Thus, the shared information among these views can be explicitly built into the model in the form of a group-level constraint to improve the performance of incomplete multiview data learning. In the learning step, we use the variational inference methods to derive a lower bound for the joint posterior distribution of latent variables and then learn all of the hyper-parameters and the auxiliary points simultaneously by maximizing the lower bound. Compared with the existing methods, this model naturally inherits the properties of Gaussian processes and mainly has the following advantages. It inherits the flexibility of Gaussian processes in modeling without explicitly defining the concrete form of the projection function. It can elegantly carry out nonlinear learning by using the nonlinear kernel function in the Gaussian process. It not only learns more representative latent variables of multiview data for clustering but also gives the uncertainty measure of predictions that can be utilized by the follow-up tasks. It is also straightforwardly extended to cases with more than two views without adding any complexity in formulation. In the experiments, we compare our proposed method with the state-of-the-art methods for incomplete multiview data clustering. The results on four small and two large multiview data sets demonstrate the superiorities of our method.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2863790", "references": [66306528, 87822204, 137285897, 146815106, 154472438, 309263341, 1501486674, 1541825479, 1550614472, 1670132599, 1775792793, 1777124189, 1869602175, 1879834137, 1883346539, 1907775068, 1947481528, 1988569689, 1991311318, 1998450142, 2010755682, 2035299679, 2041262647, 2050997020, 2083196737, 2085789144, 2088801701, 2089494389, 2099680562, 2100560442, 2101324110, 2104563967, 2108193616, 2108502868, 2111440402, 2115601375, 2115936765, 2120340025, 2120405375, 2129625650, 2131144984, 2136111243, 2142674578, 2142742813, 2144304835, 2147070561, 2161638512, 2169410692, 2178987369, 2179793346, 2181159407, 2184188583, 2396665160, 2405459681, 2546561608, 2563908351, 2572663323, 2586899202, 2611879059, 2613146443, 2748391982, 2788125892]}
{"paragraph": "The five-phase permanent magnet synchronous motor PMSM not only has the advantages of high power density, high torque-to-inertia ratio, and high efficiency, but also has the advantages of low torque ripple and high reliability. It is especially suitable for the applications with high reliability requirements, such as electric vehicles, aerospace, and ship propulsion. The field-oriented control FOC and direct torque control DTC have been extended to multiphase motor drives. Recently, the model predictive control MPC has become a promising competitor of FOC and DTC because of its intuitive control structure, flexibility to handle multiple control targets, and fast dynamic response. The MPC methods can be categorized into continuous-control-set MPC CCS-MPC and finite-control-set MPC FCS-MPC. The CCS-MPC adopts a pulsewidth modulator to synthesize the optimal voltage vector VV that minimizes the predefined cost function. Due to the continuous optimal VV instead of limited discrete VVs in FCS-MPC, constant switching frequency and lower current ripple can be obtained by CCS-MPC. However, it is difficult for CCS-MPC to take the voltage and current constraints into consideration. The CCS-MPC that takes into account the voltage and current constraints suffers from heavy computational burden and is not easy to be implemented in real time. In order to solve this issue, an offline method is proposed. However, it needs a lot of memory space to store offline computing results. Then, an online iterative method that reduces the computational burden is proposed. However, it can only obtain a suboptimal VV. On the other hand, the FCS-MPC can easily and naturally consider voltage and current constraints by utilizing the discrete characteristic of the inverter and cost function. However, it suffers from heavy computational burden when an enumeration-based method is employed to select the optimal VV among the candidate VVs, especially for multiphase motor drives. In order to reduce the computational burden, some simplified FCS-MPC methods are proposed. In these, only largest VVs are treated as candidate VVs, so the computational burden is reduced by less iteration. A constrained search method is proposed to reduce the iteration according to the predefined constraints. However, since only one VV is applied to the inverter during the whole sampling period, the steady-state performance is deteriorated by large low-order current harmonics. Then, a number of multiple-vector-based model predictive current control MPCC methods have been proposed to effectively suppress current harmonics. Multiple VVs are selected to apply to the voltage source inverter VSI during one sampling period, so smooth current control performance can be obtained. The multiple-vector-based MPCC that selects one largest VV and one zero VV during one sampling period has been employed to dual three-phase induction motor drives. However, each switching state can simultaneously generate voltages in both subspaces for the multiphase inverter, so there are still large current harmonics. Then, the virtual VV V3 synthesized by largest VV and middle VV is employed to suppress the harmonic plane current in the DTC controlled dual three-phase PMSM drives, because the V3 can generate zero average voltage in the harmonic subspace during one sampling period. The concept of V3 has been extended to the FCS-MPCC method that only takes the V3s and zero VV into consideration. In order to further improve the current control performance, a one-step modulation V3-based FCS-MPCC method is proposed. This method selects one V3 and one zero VV applied to the drive system during one sampling period, resulting in well fundamental plane current control and zero average output voltage in harmonic plane. However, the V3-based MPCC method fails to suppress the harmonic plane current when the back electromotive force EMF of the PMSM is nonsinusoidal, because the harmonic content of the back EMF can also cause large harmonic currents. As it is well-known, the motor parameters vary with the operating condition and operating environment. So, it is possible that the motor parameters adopted in the controller do not match the actual motor parameters. Due to dependence on the accurate motor parameters, the control performance of the MPCC method deteriorates rapidly, such as large steady-state tracking error and large current ripples. The sliding-mode observer is embedded into the CCS-MPC method to estimate the disturbance caused by the parameter mismatch. Then, the steady-state tracking error can be reduced by compensating the disturbance. In order to avoid the chattering problem caused by sliding-mode observer, the extended state observer is utilized to estimate disturbance caused by inaccurate parameters. The disturbance observers have been normally used in relation to CCS-MPC schemes, but rarely, to the best of our knowledge, to estimate the disturbance in FCS-MPC techniques earlier. This paper proposes a new FCS-MPC method to deal with the aforementioned issues for five-phase surface-mounted PMSM with nonsinusoidal back EMF. The proposed method can simultaneously control the fundamental plane current and harmonic plane current, due to its optimal V3 selection and duty ratio calculation in both orthogonal subspaces. In order to avoid the enumeration-based optimal VV selection that results in large computational burden, the reference voltage calculated based on the principle of deadbeat current control is utilized to simultaneously select the optimal V3 and calculate its duration. In addition, carrier-based pulsewidth modulation CBPWM is used to synthesize the obtained voltage in both subspaces in order to avoid specific pulsewidth modulation PWM generation technique that might increase the computational burden. Correspondingly, the constant switching frequency can be achieved due to the use of CBPWM. Furthermore, a discrete time disturbance observer DTDO is designed to improve the robustness of the proposed method against parameter mismatch. The stability of the DTDO is proven and the guidelines for observer parameter design are also analyzed. Compared with the previous works, the main contributions of the proposed method are the achieved simultaneous current regulation in both subspaces for effective current harmonics suppression and the derived DTDO for improved robustness against parameter mismatch, and it is possible for the proposed FCS-MPCC method to inject third harmonic current to improve torque for five-phase PMSM with nonsinusoidal back EMF.", "section": "Methodology", "doi": "10.1109/TIE.2019.2907502", "references": [1578079046, 1641563937, 2032432078, 2106548061, 2113179375, 2144758358, 2146449635, 2153701652, 2154085245, 2162877829, 2167376829, 2324405668, 2343619647, 2595187144, 2626756724, 2744934252, 2768735896, 2772346488, 2786725104, 2789760935]}
{"paragraph": "In the past several decades, digital visual information and its quality measurement were widely used in various fields of our daily life which have fundamental importance and crucial role to numerous image processing applications. Due to the subjective nature of human visual perception, image quality assessment becomes a more complex and difficult problem in image processing applications. Unfortunately, images are inevitably subjected to a wide range of distortions during acquisition, compression, storage or transmission, which degrade the visual quality of images. These distortions may influence the perceptive comfort of human eyes and sometimes even disturb the accuracy of our observation. It is necessary for the imaging devices to automatically quantify the quality loss due to distortions. This gives rise to the demand for efficient objective image quality assessment algorithms in accordance with the human visual system. There are mainly two approaches of image quality measures. First is subjective image quality assessment approach according to which opinion scores are estimated with the help of human observers. This is the most natural way; it is hence reliable but complex, expensive, time consuming and impossible to be incorporated in automatic imaging systems. Second is objective image quality measurement approach based on mathematical expressions provided to estimate automatically the quality of image. These measures take into consideration the image itself and assume that all users look at the image in the same conditions and the fact that these conditions vary from one user to another is ignored, that is why another approach of image quality measures has emerged considering viewing conditions such as image resolution and viewing distance. From several years, a long panel of image quality assessment models has been proposed, claiming to have made headway in their respective domains. All classification schemes available in the literature agree that, according to the availability of information on the reference image, the objective image quality metrics can be classified into three different categories: full reference metrics for which both the original and the distorted images are required, reduced reference metrics for which a description of the original image into some attributes and the distorted image are both required, and no reference metrics where only the distorted image is required. Most of the traditional image quality assessment metrics are based on the full reference method such as mean squared error and peak signal-to-noise ratio. However, they often correlate poorly with subjective visual quality. In the literature, some other full reference algorithms have been established to achieve higher performance such as structural similarity index, multi scale structural similarity index, the visual information fidelity, the visual signal-to-noise ratio, perceptual similarity, and structural variation quality index. As a compromise, the reduced reference method provides a flexible solution for the condition when the reference image is not fully available, minimum amount of information about the reference along with the distorted image is useful in quality computation. Very recently, several popular reduced reference image quality assessment metrics have been developed based on image distortion modeling, human visual system modeling, natural scene statistics modeling, and finally machine learning oriented metrics. In the presented work, we are interested in no reference metrics; the existing ones can be categorized into: distortion-specific methods which assume that the image quality is affected by one or several particular kinds of distortions and extract distortion-specific features for quality prediction. The disadvantage of this approach is that it is distortion specific and hence also application specific, while the number of distortions introduced to images is large. Opinion aware methods train a model to predict the image quality score based on a number of features extracted from the distorted image. The first opinion aware method called blind image quality indices is based on natural scene statistics. It mainly consists of two steps: estimation of the presence of a set of distortions in the image and evaluation of the quality of the image according to those distortions. Later this method is modified and named distortion identification-based image verity and integrity evaluation which is a recent wavelet-based algorithm; it consists of a two-stage framework: the distorted images are classified into a distortion class using support vector machine, then support vector regression is applied to predict quality score. Then a new opinion aware model is proposed that works in the discrete cosine transform domain and is named blind image integrity notator using DCT statistics, where efficient features are calculated from a model of block DCT coefficients and used to map the quality to human rating via support vector regression model. Later this model is extended using more sophisticated DCT features. Another model that operated in the spatial domain is proposed namely blind reference image spatial quality evaluator. The image features are calculated using a model of mean subtracted contrast normalized, then these features are fit to a predictive score using a support vector regression model. Besides this model, another opinion aware method namely free energy based robust metric is developed using three groups of descriptors, including features of the free energy and structural degradation information, features inspired by the human visual system, and features measuring the deviation in naturalness in the distorted image. These features were transformed in a single value using support vector regression which will be compared with the subjective scores. Opinion unaware methods do not need training samples of distortions nor of human subjective scores. The first opinion unaware method called natural image quality evaluator extracts local features from an image in the spatial domain and fits the feature vectors to a single global multivariate Gaussian model. The image quality is calculated using a distance between the model of image in question and that of natural image. Another opinion unaware method called integrated local version of this method extracts five types of features from each patch of distorted image and used them to train a model, then a quality score is calculated over each patch and the final quality score is obtained by average pooling. A new type of solution for building opinion unaware methods has emerged to solve the problem where the subjective evaluation is not available or difficult to do especially for the big image database. A method was proposed for screen content images where four types of descriptors describing image complexity, statistics of screen content image, brightness and sharpness are extracted from each image, then a regression module is trained using the training samples labeled with a full reference image quality assessment model which achieved good performance when using with screen content images to replace the subjective judgment. Using the same previous procedure, another metric for image enhancement is built, a set of 17 features are extracted from each enhanced image by considering five factors: contrast, sharpness, brightness, colorfulness, and naturalness, these images are labeled using the most performing full reference method for enhanced images, then the support vector regression is used to transform these features to a global quality score. In this paper we propose a fast and efficient no reference image quality assessment in spatial domain for color images, a simple and few number of descriptors are extracted from Lab color space using two image databases, these descriptors were used as input data to relevance vector machine algorithm to get objective scores. A comparison of our metrics with seven state of the art no reference metrics show that these features achieve good results in terms of correlation and monotonicity. The remainder of this paper is organized as follows: related works in Section 2, description and analysis of the proposed method in Section 3, in Section 4 a description of the used regression module, validation of the algorithm in Section 5 followed by a conclusion in Section 6.", "section": "Methodology", "doi": "10.1016/j.matcom.2018.11.005", "references": [1526068416, 1545192369, 1964357740, 1964728608, 1977725648, 1982471090, 1988649016, 2001896471, 2004871019, 2009272644, 2025157389, 2046119925, 2057224480, 2063360098, 2068482139, 2102166818, 2107476778, 2124562516, 2129644086, 2133665775, 2137512539, 2140094223, 2141983208, 2153582625, 2161907179, 2162692770, 2163370434, 2340480757, 2430408220, 2579658395, 2594112328, 2621054742, 2741739307, 2757064044, 2767446501, 2767660127]}
{"paragraph": "In the conventional nine-to-five working model, people worked only in their workplaces. Typewriters remained on desks, telephones were wired, and computers were not portable. However, currently, working is not limited to specific hours and locations. With the advent of mobile devices, smartphones have enabled people to perform work activities anytime and anywhere. Smartphones enable us to perform work and diverse activities simultaneously. We can listen to the latest music, check e-mails, and search for a hotel close to our destination while riding on subways. According to a Google mobile apps report, the everyday use of smartphones has become a habit. Moreover, diverse applications, including games, entertainment, news, and sports, are being used in numerous places and at various times simultaneously. In addition to the technological benefits, a few problems that were non-existent earlier have arisen. As work and leisure activities are handled by one device – the smartphone – the boundaries between both domains have diminished. This has led to social problems and an increase in the negative consequences on work-life balance. In the office, cyberslacking, which is defined as the use of smartphones during business hours for unrelated purposes, is increasing. The problem of engaging in work-related phone calls during leisure time has also been occurring. In particular, as Korea has the highest smartphone penetration and more working hours, it can be inferred that large portions of working hours and smartphone usage time overlap. In this regard, in Korea, life without leaving the office and logging out has been discussed as a major social problem that causes stress. In response, there is ongoing debate on the right to disconnect. As the boundaries between work and leisure domains have blurred, people are compulsorily resorting to routine multitasking – in which several tasks are performed simultaneously. This consumes a user’s cognitive ability. Furthermore, according to the motivated cognition model, a user is like a student postponing homework and messaging his/her friend. Hence, problems arise because tedious activities are postponed as the user tends to select preferred activities while multitasking. Existing studies have focused on explaining self-regulation failures and media addictions including smartphones, which cause these problems. According to such studies, the blurred boundaries between work and leisure domains can be solved by banning smartphone use and improving self-regulation. It has been pointed out that people acquire the habit of checking their own devices repeatedly and referred to it as mobile data service addiction. Smartphone addiction is suggested to be a mental disorder that requires treatment. In this respect, behavioral problems such as smartphone addiction, and cyberloafing/cyberslacking should be mainly solved through self-regulation, including balance and control. However, considering that working environments and smartphones are linked closely, reducing smartphone use and blocking it at the individual level is not a realistic solution. We rarely have a chance to live without technologies – they are woven into the fabric of our lives. Even though smartphones are interfering with work and impairing work-life balance, their availability is increasing every day. People are accustomed to performing work through a virtual office, which saves all their data and work tools to an intermediate space, such as a cloud drive, connects the space with their smartphones, and performs work. Therefore, for current business realities, there is a requirement to find new alternatives to solve problems resulting from smartphone use in the work and leisure domains. For this purpose, a practical study should be conducted on the current use of smartphones in these domains. Based on this background, the aim of this study is to analyze the following problems on the manner and purpose of current smartphone usage in actual situations and to deliver solutions through a survey: Depending on work-related and non-work-related activities, what are the differences between users’ behaviors in terms of smartphone usage? Smartphone usage time, frequency, and change in application usage based on the characteristics of the users’ work. Determine how boundaries between work-related and non-work-related activities are blurred, and what are the patterns of these usages? In this study, users’ smartphone usage logs were collected and analyzed to understand their usage more specifically and practically. In comparison to the existing studies on smartphone usage, which mainly focus on surveys, this study is an attempt to practically analyze smartphone usage by examining the log data recorded automatically whenever users use their smartphones. This minimizes the misinterpretation of data that may occur when relying on users’ memories. The collected log data were divided into several sessions. Behaviors were classified into the following four types: non-work-related behaviors during work time, work-related behaviors during work time, work-related behaviors during non-work time, and non-work-related behaviors during non-work time. Then, depending on these behavioral types, the types of applications, the timeslot-based and day-to-day usage rate, switching rate, and pattern were analyzed.", "section": "Introduction", "doi": "10.1080/10447318.2019.1597574", "references": [1965851068, 1974962426, 2007419970, 2042700278, 2046521577, 2070561555, 2075743842, 2124555616, 2151126801, 2158628136, 2345413211, 2417457283, 2432781354, 2437053860, 2547815499, 2738969244, 2755178802]}
{"paragraph": "Rough set theory is regarded as a useful tool for analyzing various types of data. The main contributions of this theory are set approximation and attribute reduction. However, the classical rough set model has two limitations. First, this model is sensitive to noisy data. Second, this model is incapable of directly handling real data based on equivalence relation. Decision-theoretic rough sets, which were introduced by Yao et al., result from the integration of Bayesian decision theory and rough set theory. DTRSs use conditional probability to measure the degree overlap between two sets and provide a generalized framework to solve the first limitation. Yao et al. defined three quantitative probabilistic regions by introducing a pair of thresholds. The required thresholds for these regions can be systematically computed and interpreted. In the past 20 years, DTRSs have gained increasing attention from researchers across several fields and have become increasingly popular in various theoretical and practical fields, thereby producing many comprehensive results. As an extension of classical rough sets, the DTRS model cannot directly deal with numerical data. Different types of extended DTRSs have been proposed, including DTRSs with fuzzy sets, interval sets, fuzzy interval sets, interval fuzzy sets, intuitionistic fuzzy sets, and hesitant fuzzy sets, which defined all types of relations instead of equivalence relations in classical DTRSs, to overcome this limitation. In all types of real data, the intuitionistic fuzzy value is regarded as an intuitively direct extension of the fuzzy set value. Rough sets and intuitionistic fuzzy sets capture particular facets of the same notion of imprecision. Studies that combine intuitionistic fuzzy sets and rough sets have been considered a positive approach to rough set theory. For instance, some researchers presented an intuitionistic fuzzy rough set and discussed the covering reduction method in intuitionistic fuzzy graded covering systems. Although these generalized DTRSs can be utilized to handle real data with noise, their target concepts are approximated by lower and upper approximations through a single information table. On the one hand, Qian et al. considered lower and upper approximations through multiple relations, proposed the so-called multi-granulation rough sets, and discussed the use of multi-granulation DTRSs in knowledge acquisition within the context of multiple information tables. Since then, MG-DTRSs have become an important topic in rough set theory. Despite multiple meanings in granular computing, based on Qian et al.’s viewpoint, the multi-granulation means multiple relations or information tables in this study. On the other hand, in many real-life problems, an object can take on many values given that multiple scales are available under the same attribute in an information table. For instance, the English examination results of students can be recorded as real numbers between 0 to 100 and can also be classified into five grades: “Excellent,” “Good,” “Moderate,” “Bad,” and “Unacceptable.” If necessary, the grades may be further classified into two values: “Pass” and “Fail.” Hence, knowledge representation and discovery in hierarchically organized information granulations are two important tasks in real-life data mining. Some researchers investigated fuzzy concept lattice representation in fuzzy information systems. Others introduced the notion of multi-scale information tables and analyzed knowledge acquisition in multi-scale decision tables under different granulation levels. Two optimal scale selection algorithms for multi-scale information tables were presented. Another group discussed a local approach to rule induction in multi-scale decision tables. Some used sequential three-way decisions to investigate the optimal scale selection problem in a dynamic multi-scale decision table. Others considered a dominance-based rough set approach by applying an incremental learning technique for multi-scale information tables. The domains of DTRSs, intuitionistic fuzzy rough sets, and MGRS have been comprehensively described with the advancements in related studies. However, few studies have been conducted on their combination in multi-scale intuitionistic fuzzy information tables. For instance, in some practical applications illustrated as an example, we can see that the context of multi-scale in intuitionistic fuzzy backgrounds and intuitionistic fuzzy presentations is reasonable in reality. However, as illustrated in the example, three basic problems on MG-DTRSs have been encountered in multi-scale intuitionistic fuzzy information tables. The method for addressing DTRSs and MG-DTRSs should be examined in multi-scale intuitionistic fuzzy information tables. Optimal scale selection should be investigated from the viewpoint of MG-DTRSs in multi-scale intuitionistic fuzzy information tables. In an optimal scale, the use of fewer intuitionistic fuzzy relations should be investigated from the perspective of MG-DTRSs to achieve the same approximation results. Five candidates are applying for a faculty position at a university. The similarities among these candidates are characterized by three objective evaluation indices: “Comprehensive Ability,” “Scientific Research Achievement,” and “Costs”. Each of these five evaluation indices has two scales, which are originally recorded as real numbers between 0 to 1 and three grades: “Equivalent,” “Similar,” and “Different,” respectively. Furthermore, these real numbers and grades can be further evaluated by some experts and represented by intuitionistic fuzzy values. For instance, on the second scale of Comprehensive Ability, 20 percent of the experts thought that two candidates were equivalent, 60 percent of them considered that they were totally different with each other, and the other did not give any opinions. A subjective evaluation intuitionistic fuzzy set of the five candidates can be obtained on the basis of some interviewers’ opinions, where a tuple like ⟨candidate, 0.7, 0.2⟩ means that 70 percent of the experts thought that the candidate should pass the interview, 20 percent of them opposed, and the other did not give any opinion. Thereafter, three problems arise. How to present the relations between the objective and subjective evaluations? Which scale of objective evaluations is optimal for describing the candidates? Which candidate should be chosen? On the basis of the aforementioned analysis, although the use of MG-DTRSs in information analysis is significant, MG-DTRSs cannot handle intuitionistic fuzzy information systems and exhibit limitations in processing multi-scale information tables. Therefore this study aims to explore MG-DTRSs in multi-scale intuitionistic fuzzy information tables. In consideration of this objective, we study inclusion measure-based MG-DTRSs in multi-scale intuitionistic fuzzy information tables. The main contribution of this study is the construction of MG-DTRSs and the further exploration of their optimal scale selection and reduction processes in multi-scale intuitionistic fuzzy information tables. The rest of this paper is organized as follows. Section 2 briefly introduces the preliminary notions considered in this study. Section 3 presents the concept of the inclusion measure-based DTRSs in multi-scale intuitionistic fuzzy information tables. Section 4 proposes the inclusion measure-based optimistic and pessimistic MG-DTRSs in multi-scale intuitionistic fuzzy information tables. Section 5 discusses optimal scale selection based on MG-DTRSs and presents two optimal scale selection algorithms for multi-scale intuitionistic fuzzy information tables. Section 6 defines the concept of optimal scale reduct based on MG-DTRSs in multi-scale intuitionistic fuzzy information tables and develops their discernibility function-based reduction methods. Section 7 shows possible further generalizations related to the inclusion measure-based MG-DTRSs in multi-scale intuitionistic fuzzy information tables. Finally, Section 8 concludes this study.", "section": "Methodology", "doi": "10.1016/j.ins.2018.08.061", "references": [964455177, 984339528, 1131999718, 1470723969, 1969463949, 1977268486, 1977880445, 1986839581, 1992915331, 1997362234, 2013300239, 2023750115, 2031345712, 2048472139, 2071255876, 2075471994, 2079438262, 2122937613, 2131468036, 2159457601, 2162755671, 2170755382, 2172368975, 2175162522, 2215524969, 2330216853, 2340020088, 2345465422, 2409322758, 2516473705, 2519715111, 2519971059, 2549504134, 2555871690, 2588487610, 2588572297, 2601091105, 2605772523, 2707976390, 2763102103, 2767869620, 2800399611]}
{"paragraph": "Pricing Bermudan options in high dimensions requires Monte Carlo methods, and two simulation-based prices have been developed: lower and dual upper bounds. Specifically, Longstaff and Schwartz use a standard least-squares Monte Carlo approach to compute lower bounds. Likewise, upper bounds are also based on least squares and simulation. Although both bounds are widely used, upper bounds are hardly optimized, which is important because simulation is time consuming, demanding a smart approach. In this paper, we optimize recursive upper bounds and provide two new results. Lower and upper bounds generated by simulation depend on an exercise policy, whereby the upper bound is derived from a martingale based on this policy. First, we show a recursive upper bound is independent of the next-stage exercise decision and hence cannot be optimized. Therefore, we optimize the recursive lower bound, following a local least-squares Monte Carlo approach, and use its optimal recursive policy to evaluate the upper bound as well. We find these two bounds, which have a similar cost to the reciprocal bounds based on a policy estimated by the standard least-squares Monte Carlo method, are very tight. Second, we study separately an upper bound generated from a martingale based on continuation-value functions, a bound that is less time intensive yet more upper biased, and show how to reduce its bias as well. In our first approach, we consider a given family of exercise policies or stopping times. A local method maximizes a recursive lower bound with regard to this family at each exercise stage. An open question is which exercise strategy minimizes the upper bound. We show the exercise strategy that maximizes a recursive lower bound also minimizes not the recursive upper bound itself, but rather the gap between them. We provide a recursive expression for the gap, and show a recursive upper bound is independent of the next-stage exercise policy. Therefore, minimizing the gap is equivalent to maximizing the Bermudan price recursively. In the second approach, we consider a family of continuation-value functions. We show a recursive upper bound is independent of the next-stage continuation-value function as well. By factorizing the two martingales that are based on either stopping times or continuation values, the latter martingale includes a third error term, which ensures the process is actually a martingale yet implies more biased upper bounds. The other two terms of the martingale are those of the standard factorization of the American option into an early-exercise premium and the European counterpart. This third term, however, depends only on the option continuation value in the waiting or continuation region. The latter constraint is critical because Bermudan options are highly nonlinear near the exercise boundary but less so in the waiting region, and fitting a continuation-value function only in this region is easier. This new upper bound, based on a continuation value estimated only in the waiting region, is as accurate as an upper bound based on an exercise policy estimated by the least-squares Monte Carlo method, but in a fraction of the time. The new bound is especially accurate for at- or in-the-money options, which depend mostly on sample paths that cross the exercise region and that do not contribute to the martingale’s third error term. This dual waiting-region constraint is the reciprocal constraint of using in-the-money paths, and hence, the exercise region, to estimate the continuation value in the least-squares Monte Carlo or primal method. In the numerical exercise, we price up-and-out Bermudan max-options. The up-and-out barrier makes this option very sensitive to suboptimal exercise, providing a good test. From the local least-squares Monte Carlo method, we derive the optimal recursive exercise policy and compute the two bounds associated with this policy: the lower bound improves upon the reciprocal bounds based on the standard least-squares Monte Carlo and pathwise optimization by more than 100–200 cents; the upper bound yields a one-digit gap. This small gap implies the recursive policy and the associated martingale are near optimal and the two bounds are close to the true price. The local policy is so good that reducing the number of subsimulation paths by 20 decreases the time effort by a factor of 10, yet the upper bound increases only by a few cents. Notably, the upper bound based on the local least-squares Monte Carlo policy only changes marginally with the number of subsimulations and is robust to all refinements, implying the upper bound is tighter and closer to the true price than the lower bound. With other methods that yield a nontrivial gap, this claim cannot be made. This result agrees with the two-period Bermudan upper bound, which is independent of the one-period exercise policy. A tighter upper bound implies a midpoint between lower and upper bounds is lower biased. The duality approach in option pricing has been extended in many ways. Several studies examine optimal dual bounds, use a multilevel approach, study dual bounds based on regression methods, derive upper bounds using linear and semidefinite programming, or use a pathwise-optimization approach that is less time consuming. In a novel extension, one study uses an information relaxation that nests the perfect-information assumption of the martingale approach, and also applies to other problems in operations research. We tailor duality results to our optimal recursive setting, which yields such tight bounds. Specifically, in the former case based on continuation values, we improve the upper bound by computing the continuation value only in the waiting region. In the latter case based on stopping times, the upper bound is both tight and efficient if we use fewer subsimulation paths, but a near-optimal exercise strategy as in the local least-squares Monte Carlo approach. In both ways, we bring the overall cost of the martingale approach in line with pathwise optimization or information relaxation. Moreover, the factorization of the dual martingales in terms of the components of the Bermudan process is mostly new. Our tight bounds are a useful benchmark for new methods that try to improve upper bounds in terms of accuracy or time effort. Section 2 reviews the local least-squares Monte Carlo method and explains the exercise policies and continuation values needed later for dual bounding; Section 3 shows the independence of the recursive upper bound on the next-stage exercise policy; Section 4 shows an upper bound based on a continuation-value function only needs this function in the waiting region; Section 5 provides the complexity analysis and examples; Section 6 concludes. Proofs are left to the Appendix.", "section": "Related Work", "doi": "10.1016/j.ejor.2019.07.031", "references": [122275331, 1866485205, 2017001821, 2062659568, 2089124841, 2112795785, 2120301464, 2120790358, 2137824542, 2151786492, 2156168464, 2164674975, 2295954716, 2611766878, 2617918452, 2782351573]}
{"paragraph": "Optimization problems involving multiple conflicting objectives are often known as the multi-objective optimization problems (MOPs), which can be formulated as follows. The decision space stands for the domain of feasible solutions, and the decision vector denotes a specific choice of decision variables. The symbols represent the number of objectives and decision variables, respectively. The objective functions map the decision space to the objective space. For any two solutions of an MOP, one is said to dominate the other if all the objectives of the former are no worse than those of the latter and at least one objective of the former is better than the corresponding objective of the latter. A solution is Pareto-optimal if there exists no solution dominating it. Generally speaking, all the Pareto-optimal solutions are called the Pareto set in the decision space and the Pareto front in the objective space. The problem with at least four objectives is known as a many-objective optimization problem. In addition, if an MOP has at least one hundred decision variables, it is known as a large-scale MOP. Similarly, a large-scale many-objective optimization problem refers to an MOP having at least four objectives and one hundred decision variables. To solve MOPs and MaOPs, a large number of multi-objective evolutionary algorithms and many-objective evolutionary algorithms have been developed over the past two decades. However, most existing algorithms mainly focus on the problems with small-scale decision variables, and their performance often deteriorates rapidly as the number of decision variables increases. As a result, solving MOPs or MaOPs with large-scale decision variables is an ongoing challenge in the evolutionary optimization community. Recently, some works for solving large-scale MOPs and large-scale MaOPs have been proposed on the basis of the cooperative co-evolutionary framework, where the large-scale problems are decomposed into a group of small-scale problems and solved in a cooperative and co-evolutionary manner. To solve MOPs or large-scale MOPs, the existing multi-objective evolutionary algorithms, as well as the large-scale variants, try to approximate the set of Pareto-optimal solutions using one population, which requires that the evolutionary algorithms simultaneously strike a good balance between convergence and diversity. However, balancing convergence and diversity becomes more challenging as the number of decision variables increases. Therefore, this work proposes to solve the large-scale MOPs by using a series of subpopulations, where each subpopulation is made to converge to only one solution. Specifically, we adopt the CMA-ES as the local optimizer for each subpopulation. Compared to other meta-heuristic algorithms such as the differential evolution, genetic algorithm, simulated annealing, or particle swarm optimization, the CMA-ES works well with a much smaller population, thus costing much fewer fitness evaluations when performing local search. To be more specific, the main contributions of this work are summarized as follows. We propose a scalable small subpopulations based covariance matrix adaptation evolution strategy, namely S3-CMA-ES, for solving large-scale MOPs. Different from the existing algorithms that approximate the Pareto fronts of MOPs or large-scale MOPs by one single population, each subpopulation in the proposed algorithm attempts to search one solution using a small population. A diversity improvement strategy is proposed to select new solutions, which are generated on the basis of the solutions from the converged subpopulations. On the basis of the selected solutions, this strategy further generates new subpopulations. The rest of the paper is organized as follows. Section 2 summarizes relevant studies on multi- and many-objective evolutionary algorithms. Then, the proposed algorithm is described in Section 3, followed by numerical experiments to demonstrate its superiority in Section 4. Finally, the paper is concluded in Section 5.", "section": "Related Work", "doi": "10.1016/j.ins.2018.10.007", "references": [760913798, 1662894842, 1968219458, 2005650200, 2017299308, 2018344446, 2018712218, 2020320008, 2022485595, 2040622444, 2067544246, 2108968575, 2126105956, 2134154181, 2138537392, 2143381319, 2153654820, 2167145078, 2329749247, 2343601797, 2344916885, 2410777154, 2461861577, 2502932208, 2510493362, 2513211214, 2519112510, 2520973147, 2582096828, 2583230990, 2585632407, 2593887026, 2594344284, 2608705546, 2616257225, 2735649024, 2745599083, 2747918924, 2757662287, 2764251381, 2766133347, 2769746177, 2770118518, 2775348664, 2808165282]}
{"paragraph": "Software-defined network (SDN) separates the centralized control plane from the distributed data plane. With SDN’s support of flexible network management and rapid deployment of new functionalities, there is an increasing interest in deploying SDN in both inter-data center and intra-data center scenarios. While the concept of centralized control is the foundation of SDN, implementing it on a centralized controller does not provide the required levels of availability, responsiveness, and scalability. To improve scalability and avoid a single point of failure, some recent works have explored architectures for building distributed SDN controller plane. In generally, switches are statically assigned to one or multiple controllers in these distributed controller planes. However, static assignment between switches and controllers leads to long and highly varying controller response time, simply because traffic in data center networks (DCN) frequently fluctuates with both temporally and spatially. Spatially, switches in different layers of the DCN topology experience significantly different flow arrival rates. Temporally, the aggregate traffic usually peaks in daytime and falls at night. Moreover, traffic variability also exists in shorter time scales even when the total traffic remains unchanged. For example, measuring results over real data centers have shown that the peak-to-median ratio of low arrival rates is almost 1–2 orders of magnitude. All these factors cause hot spots among some controllers, resulting in excessively long response time for the switches they manage. Although the controller response time may not be significant for elephant flows, it fundamentally limits the network’s ability to quickly react to changes such as failures and may cause transient congestion to last for a long time. Therefore, for software defined DCN, using dynamic switch assignment is important to obtain lower controller response time and better utilization of controller resources. Dixit et al. propose an efficient protocol to enable switch migration across multiple controllers without message loss or observable delay. However, how to determine the assignment remains open. On the one hand, from the switch’s view, it prefers a controller with low response time to improve performance. On the other hand, from the controller’s view, it is more willing to manage topologically closer switches to reduce the control traffic overhead. This is critical when the communication between switches and controllers is frequent and occupies scarce bandwidth resources. These preferences are always intertwined, and make the problem especially challenging. Furthermore, the following challenges also need to be addressed: Centralization places heavy burden on the software-based controller. When a network event occurs, the control flow has to make an indirection via the controller. As a result, the reaction time can be orders of magnitude slower than an in-network reaction. This is particularly problematic in application scenarios that require fast response to frequent network dynamics. Recently, to the slow reaction problem for important network events, some works have begun to explore a radically different approach—offloading some latency-sensitive network management tasks to the data plane itself (stateful data plane), and implementing the tasks using only the existing rule-based infrastructure already implemented in the switches. This in-network approach is appealing due to two reasons: it eliminates the extra round trip to the controller, and it uses only a hardware-based implementation of forwarding rules, instead of software-based controller logic, bring a significant performance boost. In SDN, flows can be configured with proactive or reactive mode. The time associated with the reactive flow setup is the sum of the time it takes to send the packet from the switch to the controller, the processing time in the controller, and the time it takes to send the configuration message back to the switch. Therefore, there are two main factors influencing the establishment time of flow: the distance between switch-controller and the processing time of the controller. For the former, this paper attempts to place the controller closer to the switch based on the stateful data plane. Another key limitation of past works is that the mapping between a switch and controller is statically configured. This results in long and highly varying controller response time due to DCN traffic varying with both temporally and spatially as mentioned above. Since the initial proposal of the network function virtualization (NFV) concept, its relationship with SDN was argued to be complementary and potentially of benefit when both technologies are combined. Thus, this paper attempts to adjust adaptively the number of controllers according to the demand by the NFV. We demonstrate that a good selection of controllers may balance the load among them and also reduce the data loss in the control layer. As a result, the selected controllers can efficiently distribute the management duties among them to improve the scalability of the management process where each controller operates on its own abstract view of the network. This study attempts to make the switch to select the right controller based on the complex network community theory. To address the abovementioned challenges, this study proposes the adaptively adjusting and mapping controllers (AAMcon). The contributions of this study are as follows: We propose an elastic distributed SDN controller architecture for DCN, where the controller pool based on SDN+NFV dynamically expands or shrinks as the dynamic demand for controller due to the aggregate load changing over time. Where we propose a fast start/overload avoid algorithm to adaptively adjust the number of controllers according to the demand of switch. We propose an efficient switch-to-controller mapping scheme by building a community that contains a local controller on the switch using the complex network community theory. Where the local controller is placed at the most important node in the community and becomes the first mapping choice of the switch when it needs support from the controller. The controllers can respond to the switch with the least distance to minimize the communication time between the controller and switches. Simultaneously, AAMcon can achieve load balancing between controllers, avoid congestion, and is very tolerant to failures. This study proposes a stateful data plane to place the controller closer to the switch. The remainder of the paper is organized as follows. In Section 2, related works are introduced. In Section 3, the design and implementation for building a scalable control layer are presented. In Section 4, the experiment results of the proposed method are presented. Section 5 discusses some related issues. Finally, conclusion and future work are presented in Section 6.", "section": "Related Work", "doi": "10.1007/s11704-019-7266-6", "references": [166284719, 371964434, 1526212906, 1534304527, 1598444381, 1698388015, 1714884520, 1732817767, 1983720905, 2013958607, 2017158367, 2034677282, 2040074033, 2040340473, 2043255416, 2062596448, 2062784859, 2076469583, 2094739364, 2096548445, 2107944181, 2110738524, 2123347367, 2123372830, 2125008614, 2126822952, 2147118406, 2147802358, 2168595508, 2170803695, 2500120503, 2540165763, 2557120686, 2607260162, 2771556079]}
{"paragraph": "In 2016, Lucca et al. introduced the notion of pre-aggregation function (PAF), which fulfills the boundary conditions as any aggregation function, but, instead of being an increasing function, it is just directional increasing. That is, it increases along some specific ray (direction). Furthermore, the authors presented some methods to produce PAFs. One of them is by generalizing the Choquet integral replacing the product operator by a t-norm, obtaining, under some constraints, idempotent and averaging PAFs. This approach was used in Fuzzy Rule-Based Classification System (FRBCS), presenting excellent results, when the Hamacher t-norm is used for the generalization, overcoming the Choquet integral and classical averaging operators in classification systems. Those excellent results motivated us to explore a more general method for constructing PAFs based on the Choquet integral. For that, instead of using just a t-norm, we replace the product operator by a fusion function F that is left 0-absorbent, obtaining the integrals. Integrals are pre-aggregation functions, which, under certain conditions, may be idempotent and/or averaging functions. This allowed to analyze sub-families of integrals having or not the averaging behavior, showing that an integral does not need to be an averaging function when used in FRBCSs, since the non-averaging obtained more accurate results than the averaging ones. In the same line of this research, Lucca et al. investigated another kind of Choquet integral that leads to aggregation functions, instead of just PAFs. For that, the product operation of the standard Choquet integral was first distributed and, then, replaced by a copula, obtaining the CC-integrals, which happen to be averaging aggregation functions. This approach presented excellent results in classification, in particular, when the minimun t-norm was the considered copula, in which case it was called CMin-integral. See also the application. Recently, Luca et al. developed the concept of integral, which is a specific generalization of CC-integrals, based on two possibly different fusion functions satisfying some appropriate conditions, obtaining non-averaging Choquet-like integrals that were successfully used in the aggregation process of the fuzzy reasoning mechanisms of fuzzy rule based classification systems. Their performance was proved to be statistically equivalent to FURIA. The general aim of this paper is to generalize the concept of integrals, obtaining the so-called integrals, presenting a solid theoretical framework that gives the basis for applications. We shall define the integrals by distributing the product operation of the Choquet integral and, then, generalizing the two instances of the product operation by a pair of fusion functions. For that, we face two main problems: Which properties/constraints should be imposed on in order to guarantee a well defined concept, satisfying the boundary conditions and some kind of increasingness (increasingness, directional increasingness or ordered directional (OD) increasing)? This leads us to the concept of pseudo pre-aggregation function pair, that is, a pair of fusion functions satisfying some kind of boundary conditions, directional increasingness and dominance property. How can we deal with the problem of repeated elements in the input, which may cause ambiguity in the results (that is, the same input may produce different results when we change the order of the elements)? To solve this problem, we propose to collapse those repeated elements into one representant of the class, and to proceed to a problem dimension reduction. Then, the specific objectives of this paper are stated as: To introduce the notion of pseudo pre-aggregation function pair; To define a problem dimension reduction; Using dimension reduction, to introduce the notion of Choquet-like integral based on pseudo pre-aggregation function pairs, called integrals; To show under which conditions integrals based on pseudo pre-aggregation function pairs are (pre) aggregation functions; To show under which conditions integrals based on pseudo pre-aggregation function pairs are ordered directional increasing functions and satisfy the desirable boundary conditions; To study when integrals are averaging; To analyze several types of pseudo pre-aggregation function pairs, built from t-norms, overlap functions, copulas, and other functions that are not even PAFs, showing examples of different integrals. The paper is organized as follows. In Section 2, we present the basic concepts required to understand the paper. In Section 3, we introduce the concept of pseudo pre-aggregation pairs and analyze several properties. The concept of integrals is introduced in Section 4, where we also define the dimension reduction. In Section 5, we discuss when integrals are (pre) aggregation functions, and the related properties. Section 6 studies when integrals are not (pre) aggregation functions, but OD monotone functions. Section 7 is the Conclusion.", "section": "Related Work", "doi": "10.1016/j.fss.2019.01.009", "references": [1580821264, 1905771608, 1987898498, 1998690720, 2023837808, 2053519120, 2054976600, 2086721181, 2094811706, 2095312476, 2103327581, 2119436275, 2164400088, 2166599460, 2173773591, 2259867457, 2296349740, 2551128249, 2558711338, 2559077843, 2597236717, 2615591864, 2746640730, 2766388848, 2777602658, 2794327364, 2890484132, 2895842142]}
{"paragraph": "With the rapid development of sensor networks, Web services, and radio frequency identification techniques, uncertain data has become ubiquitous. Uncertain data exist for a variety of reasons, which are mainly divided into the following: uncertainty from original data, summary data, privacy preservation, and automatic or semi-automatic information extraction. In many applications, such as the economic, military, logistics, finance, telecommunications, meteorological, and oceanic fields, uncertain data plays an increasingly critical role. Typical uncertain data applications include sensor networks, RFID applications, Web applications, and location-based service. In the last 30 years, great interest has been devoted to uncertain data management. In fact, uncertain data processing techniques concern two aspects of uncertainty: Attribute-level uncertainty. In an uncertain database, the database has attribute-level uncertainty if one or more attributes is uncertain in following cases: Attribute values are from a set of discrete values, each of which is associated with an existence probability; attribute values are consecutively distributed possible values, and they associate with a probability density function. When performing queries in such databases, each record extracts one possible value from its uncertain attributes or distribution to form an instance table. Many applications such as sensors, electronic labels, and GPS values have attribute-level uncertainty in data records. Record-level uncertainty. In an uncertain database, if records do not have uncertain attributes, but each record in this database exists with some probability, the database contains record-level uncertainty. More complex record-level uncertainty also includes a group of generation rules, each of which contains a group of records that provides the constraint condition of this record group. Usually, there are two types of generation rules: exclusive rules, which require that this group of records cannot appear at the same time, and coexistence rules, which require that this group of records must exist together. We can point out some of the steps that have marked the history of uncertain data management. One pioneering contribution is the proposition of the lineage approach, which represents uncertain data by a combination of classical database relations and propositional formulas. The study on probabilistic databases has been continued since then. In 2007, a probabilistic database was modeled as a large graphical network. In 2008, a remarkable survey on probabilistic databases was carried out with this approach. In probabilistic databases, the tuple-independent model has been widely applied owing to its mathematical simplicity. This model assumes tuples are independent and queries are transformed into Boolean expressions. In probabilistic databases with independent tuples, every result tuple is associated with a Boolean formula lineage. A query evaluation is computable in polynomial time if the Boolean formulas can be factorized into a form in which every variable appears at most once, called read-once functions. Read-once expressions are helpful because they are computable in polynomial time, in contrast to being computationally intractable for arbitrary Boolean functions. Since then, several other cases computable in polynomial time and their extensions have been studied. For probabilistic database systems, there has been extensive work under the relational database scheme, such as several systems that deal well with uncertainty of data in an extended relational model and support SQL-based query language. Moreover, a system developed by a university can handle attribute and tuple uncertainty with arbitrary correlations, as well as discrete and continuous probability density functions. In contrast with relational probabilistic database systems, full-fledged semi-structured probabilistic database systems are still missing. Two XML document query processing systems have been proposed. Another is an RFID data management system. For uncertain data streams, some systems support stream processing for uncertain data using continuous random variables. For semi-structured data and stream data, techniques on uncertain data have made great progress and researchers have achieved a wealth of research results. Some surveys have been proposed on uncertain data management. However, the existing surveys focus on uncertain relational data problems and involve only a few uncertain XML data problems. A survey on probabilistic XML data management was proposed in 2013. With the rapid progress made by research in this area, many novel techniques for uncertain data management have been proposed. We propose this survey to summarize the state-of-art research progress on both relational and semi-structured uncertain data, as well as to provide future research directions in the area of uncertain data management. In this survey, we will cover the area of uncertain databases in a broad sense and give an overall view. The remaining sections of this paper are organized as follows: In Section 2, we present relational uncertain data management issues, i.e., uncertain models, basic data operators, query processing techniques, indexing techniques, and uncertain data mining. In Section 2.1, we summarize relational uncertain data models, such as the possible world semantics and the probabilistic graphical model. Possible world is the most popular system of uncertain querying semantics, which provides the theoretical basis for the state-of-art uncertain database, the probabilistic database. In Section 2.2, we present the basic operators, i.e., selection, aggregation, and join operation, for query processing of uncertain data. In Section 2.3, various types of queries are defined and studied by database researchers, and various indexes are designed according to the problem properties. In Section 3 and Section 4, we describe the uncertainty management issues with semi-structured data, i.e., XML documents and graph data. Section 5 presents uncertain data stream management issues and techniques, and in Section 6, we introduce the quality issues of uncertain data.", "section": "Related Work", "doi": "10.1007/s11704-017-7063-z", "references": [37148511, 199916997, 1486776102, 1491547607, 1496151277, 1499195461, 1505129060, 1510148020, 1512485814, 1515568087, 1545445857, 1559068944, 1561514023, 1568607842, 1570856340, 1574922246, 1594296362, 1603304162, 1707644991, 1716383435, 1772931857, 1794915560, 1943213095, 1962705364, 1963853643, 1968461475, 1969642980, 1973266211, 1977245156, 1977848225, 1979578660, 1981339597, 1986019774, 1990391007, 1991249762, 1991785464, 1992609556, 1996544809, 1996923081, 1997141048, 2002298107, 2005044405, 2005852566, 2006718332, 2008974204, 2011130689, 2013333366, 2013885128, 2022501110, 2024400846, 2025051251, 2034518740, 2037314220, 2038822667, 2041358936, 2041763948, 2042502381, 2043354831, 2045630074, 2045928315, 2050290319, 2050868968, 2054693333, 2057272970, 2063191945, 2064379477, 2064926388, 2068447118, 2075422000, 2076530298, 2078686663, 2083613374, 2084273580, 2084591860, 2085532670, 2085608749, 2086741590, 2093038664, 2093584660, 2095897464, 2097584995, 2097995023, 2099413512, 2099700811, 2101366610, 2101742950, 2102943360, 2103269600, 2103895255, 2106582507, 2109402524, 2110037957, 2110553125, 2110796598, 2112070477, 2112948115, 2114258210, 2115826669, 2115857253, 2115986770, 2116440837, 2118178019, 2118291925, 2118462151, 2119081050, 2120207388, 2120825705, 2121043820, 2121161268, 2121722315, 2123017611, 2124647958, 2125434953, 2125791539, 2126969584, 2127186572, 2127947301, 2129035130, 2133457275, 2133695619, 2133839490, 2134206624, 2137092177, 2137442080, 2137536943, 2138271690, 2138414767, 2140237757, 2140757415, 2141700159, 2142909274, 2144342293, 2144602362, 2146407749, 2150061878, 2153404040, 2153508518, 2153610999, 2154709738, 2157813460, 2158501189, 2159948839, 2162400408, 2165211504, 2166214052, 2166916904, 2166994031, 2167956717, 2170896764, 2170902325, 2170936641, 2171688288, 2171776999, 2171943719, 2172146718, 2294071733, 2338322935, 2951002752, 2952121399, 2953227115]}
{"paragraph": "As the balance of scientific evidence suggests, anthropogenic greenhouse gas emissions emanating from economic activities significantly impact the environment and amplify climate change, with the use of energy for electricity and heat production being the key source of these emissions. This, in turn, inarguably poses one of the largest sustainability threats of our time and affects not only the environment but various, if not all, sectors of the economy, both directly and indirectly. Motivated by the need to study the dynamic interactions between the economy, energy and climate dimensions, Integrated Assessment Models have long been used to support climate policy making. Although their contribution to this challenging task is uncontested, the research community has long been questioning the extent to which they have actually supported policy makers. Climate change as well as mitigation and adaptation policy feature significant levels and different types of uncertainties and risks. These regard issues ranging from future carbon accumulation levels to socio-economic developments that may promote or hinder the adoption and implementation of policy measures in this direction, such as the capacity to fund technological innovation or the levels of societal acceptance of policies. Uncertainty can be framed as a broad concept that refers to a general lack of knowledge or agreement upon possible outcomes and their probabilities. Risk can thus be defined as a negative possible outcome that stems from an uncertainty and largely depends on the focus of the study, regardless of whether it can be accurately quantified as a probability or be attributed a qualitative likelihood, for example based on stakeholders’ experience. From a scientific perspective and as with all modelling frameworks, Integrated Assessment Models inevitably fail to incorporate all relevant types of uncertainty and risks when used as stand-alone tools. This exclusion can have a significant impact on the model outcomes, but can be mitigated if experts’ knowledge, which can prove valuable in identifying such factors or reducing respective knowledge gaps, is elicited in a structured manner and effectively taken into account. At the same time, formalised modelling frameworks may impose representations and restrictions not justified by the underlying knowledge, may aggregate results, and cannot directly represent all policies, for instance soft measures aimed at human behaviour such as dissemination of knowledge through education and public information campaigns. This is an expected weakness of climate-economy modelling frameworks, which can only indirectly represent and calculate the economic impacts of climate policies. From a policy making perspective, climate policy makers usually find it hard to understand the complex processes of the very large number of existing Integrated Assessment Models, keep track of the various assumptions driving their modelling simulations, and trust their black-box nature. Furthermore, as with every other stakeholder group, policy makers are not always actively involved in the modelling processes and are only perhaps engaged in some limited, preliminary discussions towards formulating parts of the assumptions used. These weaknesses of climate-economy models require the adoption of additional frameworks for supporting climate policy design. These should include frameworks and tools that not only bring policy makers and other stakeholders closer to the process, but also enable the mobilisation of their tacit knowledge and experience towards taking into account various implementation and consequential risks and uncertainties. Among these, there can be found different levels of detail, capacity to identify and assess risks and uncertainties, and flexibility to include stakeholders, cover multiple sectors and customise the scope of study. These include Cost-Benefit Analysis, Fuzzy Cognitive Maps, Life Cycle Assessment, Multiple Criteria Decision Making, Portfolio Analysis, System Mapping, and Systems of Innovation frameworks, for example Technological Innovation Systems and Multi-Level Perspective. Cost-Benefit Analysis has been established as a decision support tool for assessing the economic efficiency of interventions in multiple domains, including climate policy; however, it appears to feature significant limitations regarding its capacity to incorporate uncertainty or to quantify non-market goods and social and environmental distributional impacts. Life Cycle Assessment and decision analysis have a long-established connection but, with regard to climate policy, dependence on data availability, poor definition of boundaries, time requirements and assumptions may limit its potential. Nevertheless, Life Cycle Assessment has been primarily used in environmental impact assessment studies, with climate-related implications for the power, transport, building, and agriculture and land use sectors. System Mapping is a purely qualitative, stakeholder-driven tool that features relative flexibility to look into various aspects of a system and its links to policy, and can help identify policy-related risks, but has only recently been framed in the climate policy domain. Both Technological Innovation Systems and Multi-Level Perspective focus on technological innovation and are, therefore, underexploited in this domain; they are employed primarily for socio-technical studies in the power and transport sectors. Despite their capacity to identify barriers and risks, however, both frameworks are strictly qualitative and appear to provide little insight into long-term policy and scale coverage, which are indeed important parameters in climate policy. In this study, we distinguish Fuzzy Cognitive Maps, Multiple Criteria Decision Making, and Portfolio Analysis, which a growing number of research publications seem to suggest as promising routes to bringing policy makers and stakeholders in general closer to the modelling processes, in response to the highlighted weakness of Integrated Assessment Models, as well as to supporting decision making in various climate policy-related fields and application areas. Acknowledging that these decision support frameworks are, compared to Integrated Assessment Models, significantly less detailed but can add value in other dimensions of climate policy assessment, the aim of this paper is to provide a detailed critical review of these frameworks and to explore their capacity to support decision making in specific climate policy problems. In addition, we propose a truly integrative scientific approach, appropriately exploiting blends of the three frameworks, to productively complement Integrated Assessment Models, towards enhancing climate policy support. To this end, the following section provides a concise overview of the various types of Integrated Assessment Models, while subsequent sections present a detailed review of Fuzzy Cognitive Maps, Multiple Criteria Decision Making and Portfolio Analysis respectively. Finally, a discussion on the findings of our review is provided, regarding the various manners in which these frameworks can potentially further contribute to climate policy, and strengthens this suggestion by means of a real-world climate policy problem. The study summarises the conclusions and notes that the publications reviewed were identified primarily on the basis of searches on academic search engines for various keywords, and included scientific journal articles, book chapters and conference papers. Furthermore, broader searches were performed in specific journals and books, after reviewing the number and thematic focus of the associated papers retrieved in the initial results.", "section": "Methodology", "doi": "10.1016/j.ejor.2019.01.017", "references": [32172252, 338388561, 1169237817, 1509866077, 1525018297, 1941780971, 1964956180, 1966579413, 1967709365, 1971547344, 1979474742, 1989025342, 1997098687, 2002162495, 2003336779, 2006114471, 2007323574, 2012058102, 2012908264, 2027010621, 2034105627, 2037034388, 2038553079, 2041942339, 2042102899, 2048165933, 2065109455, 2065957083, 2068431552, 2088599597, 2090930452, 2102117921, 2107415705, 2121442838, 2127470579, 2161818671, 2167165857, 2200039991, 2235818444, 2342736636, 2443285854, 2523627082, 2607821264, 2762939143, 2904810447]}
{"paragraph": "Online multilabel learning (OMLL) aims to deal with multilabel data streams in which samples arrive sequentially and are simultaneously associated with two or more labels. In real-world applications, there are many multilabel data stream scenarios. For instance, in social media analytics for personality where the samples are generated over time, each individual usually possesses multiple traits, for example, open, enthusiastic, and responsible; in e-mail categorization, a new incoming email may simultaneously belong to two labels business and important; in real-time traffic management of scene classification, the overpass label usually coexists with the labels road and traffic, etc. In a data stream, the underlying data distribution may dynamically change upon new incoming samples, resulting in deterioration on model accuracy. This issue is called concept drift. The concept drifts in multilabel streams are more challenging and complicated than the traditional ones in single-label streams because there is a special type of concept drift called changes in data distribution with labels (CDDL) in multilabel streams, in which the data distributions are dynamically changed with the labels of new incoming samples. To our best knowledge, CDDL is first identified and considered in this paper. Since most multilabel datasets contain single-label samples as well, CDDL becomes even more complicated. For instance, the OMLL classifier is first trained using both multiple-label and single-label samples but the new incoming samples are all single-label or multiple-label, that is, mixed training, single-label or multiple-label update; and the OMLL classifier may be trained using single-label or multiple-label samples only and then other samples gradually appear afterward, that is, single-label or multiple-label training, mixed update, etc. There are many combinations called CDDL cases of such training and update. Among all CDDL cases, there are two extreme cases: 1) multiple-label training, single-label update, and 2) single-label training, multiple-label update. Generally, if the OMLL classifier is robust to the two extreme CDDL cases, it is also robust to the ordinary ones. Hence, only the two extreme CDDL cases are considered in this paper. The two extreme cases of CDDL will significantly influence the OMLL classifier performance in two aspects. Sequential Model Update: Under CDDL, if the training loss over the new incoming samples is significantly larger than that over the old ones, the total loss for all data will also be seriously affected. This leads to the result that the sequential model update moves the estimation apart from the old samples in order to reduce the large loss on the new ones. Although this move enables the final updated model to be more suitable for the new samples, it unfortunately drifts the final updated model apart from the old ones. Note that under CDDL, the multiple-label or single-label instances in the old samples and the single-label or multiple-label instances in the new incoming samples are equally important for estimating the true data distribution of the multilabel dataset. Hence, if large loss over the new incoming samples under CDDL is observed, a deterioration on model accuracy shall occur. Multilabel Thresholding: Since the output of OMLL classifiers is primarily a score vector for sample xi, a label threshold is required to output bipartition, that is, the associated label set and unassociated one. In most existing OMLL classifiers, an appropriate threshold is selected or adjusted using an independent validation set in post-processing manner. However, the data distribution of the independent validation set may be totally different from that of the training dataset and the new incoming stream, leading to deteriorated model accuracy over time. The situation becomes even worse and more complicated if CDDL is considered in the incoming stream. Although some methods adjust the label thresholds via training data rather than the validation set for higher accuracy, they are only suitable for offline learning. It is because, in online learning, the learned data will always be discarded for space issues and only the last chunk of data is available for thresholding. As a result, multilabel thresholding is a big challenge for OMLL. Consequently, CDDL significantly influences both the sequential model update and the multilabel thresholding, and if either is influenced, the classification performance shall deteriorate. For this reason, the OMLL classifier must be robust to CDDL in these two aspects. There are several popular state-of-the-art OMLL classifiers in the literature: streaming multilabel random trees, ensembles of the multilabel Hoeffding tree using the pruned set, ensembles of the multilabel Hoeffding tree using the class incremental learning strategy, and the online sequential multilabel extreme learning machine. SMART, EaHTps, and EaHTcl are tree-based OMLL methods which focus on traditional concept drift and adopt a fading factor to gradually reduce the influence of old samples. This is because the old samples under traditional concept drift are always carrying an out-of-date concept which is totally different from the case of CDDL. Hence, these methods are obviously not robust to CDDL. OSML-ELM is a neural-network-based OMLL method, which can deal with multilabel data streams in real time due to the extreme learning machine mechanism. Remark: ELM is a single-layer feedforward neural network, which is highly effective and efficient but designed for single-label and batch classification only. However, OSML-ELM is not robust to CDDL too. Compared to tree-based methods, the ELM strategy is of higher effectiveness and efficiency in OMLL. Therefore, ELM is adopted as the base classifier in this paper to construct a robust OMLL classifier over CDDL. In this paper, a new ELM-based OMLL method (ELM-OMLL) for multilabel data streams is proposed, which has the following contributions. A new objective function for multilabel classification is defined in ELM-OMLL, that is based on the correlations among different labels. In other words, the labels associated with a sample are ranked higher than those unassociated, as detailed in Section III-A. An efficient closed-form solution for the new objective function in ELM-OMLL is derived for real-time online multilabel applications. Under the closed-form objective function, a new sequential update rule is derived in Section III-B, that can preserve the label correlations learned from all old but discarded samples while updating the model only based on new incoming samples. A constant label threshold is fixed in the objective function of ELM-OMLL. Without the problematic adjustment of multilabel thresholds over the training dataset or validation set, the estimated labels of all old and new incoming samples with or without changed data distribution simply follow the bipartition of the fixed thresholds, as detailed in Section III-A2. Under contributions 1) and 2), ELM-OMLL is effective and efficient for multilabel data streams. Under contribution 3), the training loss on the new incoming samples will not be significantly increased so that the model keeps the estimation from the old ones while correctly learning the new incoming samples. In other words, ELM-OMLL is highly robust to CDDL in its sequential model update. Under contribution 4), the fixed threshold will not be influenced by any changes in data distribution including CDDL, that is, ELM-OMLL is highly robust to CDDL in its multilabel thresholding. This paper is organized as follows. Section II provides a short review on some state-of-the-art OMLL classifiers and prequential error used for robustness analyses. Section III details the proposed ELM-OMLL including the newly defined objective function, the new sequential update rule for online learning, and its robustness to CDDL. Section IV shows the experimental results with analysis. Finally, a conclusion is drawn in Section V.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2869476", "references": [608314793, 841144446, 1495819482, 1565746575, 2012663856, 2026131661, 2027900490, 2032028573, 2038624061, 2038857347, 2042184006, 2052972204, 2057050184, 2073256825, 2083097051, 2088187739, 2099419573, 2111072639, 2114315281, 2119466907, 2133042373, 2137107481, 2139327121, 2158054309, 2167803572, 2182722412, 2343538158, 2344849981, 2413281424, 2480541962, 2512288612, 2527827097]}
{"paragraph": "As a tool for remote sensing and manipulation, a bilateral teleoperator strives to synchronize its master and slave robots tightly, and to provide its human operator with useful haptic cues about the slave-environment interactions. Therefore, the bilateral teleoperation feedback loop includes the human who operates the master, the environment which interacts with the slave and the master-communications-slave system. Because operators vary their dynamics according to their volition, and environments are typically unknown, neither are predictable or trivial to model. Additionally, the master and slave exchange information distorted by time-varying communication delays. Thus, bilateral teleoperation is a nonlinear, time-varying and interconnected system with communication delays and uncertain user and environment dynamics. The physical interaction between the robotic master-communications-slave system aka the teleoperator and the user and the environment aka its external terminators involve exchange of energy. As a key theory for modeling and controlling the exchange of energy among interconnected systems, passivity is often pivotal to the rigorous treatment of closed-loop teleoperation without user and environment models. Existing research exploits the stability of the feedback interconnection of passive systems by assuming passive operator and environment and by employing Lyapunov-like analysis or energy monitoring to offer controllers which maintain teleoperators with time delays passive. Scattering-based, damping injection and adaptive strategies can all be designed to provably stabilize bilateral teleoperation with a unified Lyapunov-like energy function. Scattering or wave-based control can render the time-delay communication channel passive, as well as reduce wave reflections and improve trajectory tracking and transparency. Damping injection control, Proportional-Derivative plus damping PD+d or Proportional plus damping P+d, and extensions to position-force architectures, output feedback, and bounded actuation implement a virtual spring between, and local dampers at, the master and slave sites. Joint-space and task-space adaptive strategies can synchronize master and slave robots with uncertain parameters and constant delays. Energy monitoring-based control can render the teleoperator passive by dynamic damping injection, dynamic modulation of the nominal control force, or both. Time-domain passivity control and extensions to eliminate position drift inject sufficient damping to dissipate the delay-induced energy at each step. The energy bounding and passive set-position modulation PSPM strategies regulate the nominal control inputs to ensure that the teleoperator generates less energy than its physical and control damping dissipates. The two-layer approach modulates the forces computed in the transparency layer and adds damping in the passivity layer to limit energy accumulation in the system. Input–output stability provides another path to rigorous stability of time-delay systems with uncertain dynamics. In particular, input-to-state stability has diminished the conservatism of passive strategies in haptic rendering, and has offered robust position tracking for time-delay bilateral shared control of an aerial vehicle. This paper introduces a novel strategy for input-to-state stable ISS time-delay bilateral teleoperation. The new strategy combines a dynamic master–slave interconnection with dynamic damping injections to each robot. Although force-reflection and force-reproduction architectures can improve transparency, force and acceleration measurements are unavailable or noisy for many commercial robots. Therefore, the proposed strategy relies on a position–position structure. Its controllers require only the position and velocity of the local robot, plus the delayed position of the remote robot. Compared to conventional controllers based on Lyapunov-like analysis or energy-monitoring, which stably connect the master and slave but cannot quantify their error a priori, the proposed strategy can limit the impact of the user and environment on it. A key design step to bring about this property is the transformation of the system dynamics into a first-order passive form through properly designed sliding surfaces. Compared to conventional feedback passivation, the proposed strategy suppresses the Coriolis and centrifugal effects of the Euler–Lagrange dynamics without compensation, by simply modulating the proportional and damping gains according to the local velocities. To the authors’ best knowledge, the dynamic interconnections and damping injection strategy in this paper is the first to make time-delay teleoperation exponentially ISS based on P+d control. The P+d, PSPM, and two-layer approaches are most closely related to the dynamic strategy introduced in this paper. Whereas the P+d and PSPM algorithms synchronize the master and slave exactly in the absence of user and environment forces but overinject constant damping compared to the two-layer method, the dynamic strategy in this paper offers a unique property for robust position tracking in time-delay bilateral teleoperation: given full unlimited actuation, it can confine the master–slave position error to a prescribed invariant set, and can drive it exponentially to a globally attractive set, which includes the origin. The invariant set quantifies the maximum master–slave position error during teleoperation. The globally attractive set measures their position error at steady state. The rate of exponential convergence from the invariant set to the attractive set quantifies the transient response of the teleoperator. More importantly, Lyapunov stability analysis shows that upper bounds on the Lebesgue measures of the invariant and globally attractive sets, and the rate of convergence to the latter, depend on the control gains. This unique property of the proposed dynamic interconnection and damping injection strategy can benefit precision telemanipulation tasks: because constraints on the master–slave position error map to a feasible set for the ISS bilateral teleoperation, proper selection of the control gains can make the feasible set positively invariant. Thus, the control gains of the proposed strategy can be selected to tighten the master–slave coupling, and to indirectly convey the slave-environment interactions to the operator. Experiments with a pair of Geomagic Touch haptic robots compare the controller proposed in this paper to state-of-the-art controllers.", "section": "Methodology", "doi": "10.1109/TIE.2019.2907448", "references": [1965361141, 1967390389, 1974513221, 1977088290, 1982267787, 1990577054, 2008841828, 2010340595, 2017450428, 2040555760, 2050694847, 2099914424, 2100656120, 2105369176, 2109465348, 2113436535, 2116822311, 2118166401, 2135178826, 2135464393, 2142161411, 2142380406, 2153681681, 2164773029, 2167625326, 2172036414, 2290903912, 2484531584, 2600314761, 2698548017, 2783006199, 2785572474]}
{"paragraph": "Multicast is an efficient way to disseminate the same content from a single source node to multiple destination nodes. It has been widely used to conserve the network bandwidth and reduce the load on the source node. Multicast has a lot of successful applications in IPTV networks, enterprise networks, and datacenter networks. To support a multicast session, we usually construct a multicast tree, along which the source node can efficiently deliver data to each destination node. Compared with separately delivering data along a set of independent unicast paths to those destinations, multicast delivers data towards all destinations along a general multicast tree and avoids unnecessary duplicated transmissions. To minimize the total cost of used links in a multicast tree, researchers have designed many approximation mathods for the Steiner minimal tree, an NP-hard problem. Such existing methods, however, are mostly designed for the traditional multicast with only one fixed source node. This type of multicast is also called deterministic multicast. Unfortunately, in many cases, the source node of a multicast is unnecessary to be in a specific location. That is, a multicast may have more than one source node, due to the content replica design. For example, to improve the robustness and efficiency, the content distribution networks and the datacenter networks usually deploy multiple replicas for each file at different locations. If a multicast transfer aims to deliver one file to a set of destinations, those replica nodes of that file can indeed serve as the potential source nodes. Thus, each destination node can be served by any source node as long as certain constraints are satisfied. Such kind of multicast with flexible source nodes is referred to as uncertain multicast. As mentioned in our prior work, the routing structure for an uncertain multicast is usually a forest, consisting of multiple isolated trees. In such a forest, each destination node is served by one source node but it is unnecessary that all source nodes appear in the resultant forest. Thus, the uncertain multicast problem cannot be simply reduced as the deterministic multicast problem. Although efficient methods have been proposed to build a minimal cost forest for any uncertain multicast, the serious impact of the dynamic behaviors on the uncertain multicast still lacks attention. Many multicast applications require the networks to support dynamic multicast sessions, where the membership of the multicast group often changes. For example, in the process of a live broadcast, viewers may join and leave the multicast transmission frequently. Additionally, the set of content nodes which can serve as source nodes is also dynamic. The existing minimal cost forest should be updated when any group member changes in an uncertain multicast. An intrinsic method is to completely rebuild a minimal cost forest after each change so as to achieve the minimal cost. This mechanism, however, causes huge computation overhead, and may cause unaccepted disruptions to the flow transmitting along the existing minimal cost forest. Thus, an efficient and practical method requires the ability to just incrementally update the existing forest to tackle those new joined or leaved nodes in an uncertain multicast. Consider that an uncertain multicast usually faces frequent dynamic behaviors. Moreover, it is necessary to find the balance between minimizing the forest cost and minimizing modifications to the existing forest. In this paper, we study the building and maintaining problem of a minimal cost forest for any uncertain multicast with dynamic behaviors. It is defined as the dynamic minimal cost forest problem, an NP-hard problem. We propose an approximate method, a-MCF method, to build the initial minimal cost forest. To avoid re-compute the entire minimal cost forest for each change in multicast group, we propose d-MCF method to alter minimal cost forest with local modifications. Note that, a number of local modifications may lead to the lost of global optimality. So we also design a rearrangement method, r-MCF method, to monitor the accumulated performance degradation and trigger two types of rearrangement when necessary. The major contributions of this paper are summarized as follows: We define the dynamic minimal cost forest problem, and formulate it as a mixed integer linear programming. The sequence of dynamic behaviors is represented as a request vector, where each element is a modification request to add or delete a single source node or destination node. We propose an approximation method, a-MCF, to completely rebuild a minimal cost forest for each request. It aims at maintaining the minimal total cost and is applicable to the situations where dynamic behaviors is not frequent. We propose d-MCF method to balance the the minimal cost and minimal modifications. Instead of rebuilding the entire minimal cost forest, the d-MCF method responds to each request with local modifications, which significantly improves the responding speed. We propose r-MCF method to monitor the accumulated degradation caused by the local modifications. A rearrangement will be triggered if necessary. That is, to rebuild the entire or partial minimal cost forest with global algorithm. It is a supplement to the local d-MCF method. The reminder of this paper is organized as follows. Section 2 summarizes the related multicast methods. In Section 3, we formulate the problem of dynamic uncertain multicast and present its model. An approximation method is proposed to build minimal cost forest. Section 4 presents a method to modify the existing minimal cost forest for each request. Section 5 discusses the rearrangement method. Section 6 evaluates the performance of proposed methods and Section 7 concludes this paper.", "section": "Related Work", "doi": "10.1007/s11704-018-7429-x", "references": [1914921536, 1987575016, 2016234215, 2039298646, 2069853034, 2087304736, 2095788461, 2103992755, 2104954161, 2105808107, 2119565742, 2121041130, 2138336874, 2149288610, 2152950844, 2179603265, 2399307553]}
{"paragraph": "In many practical tasks the system under control is required to operate along periodic motions, i.e., walking and running robots, path following, rotating electromechanical systems, AC or resonant power converters, and oscillation mechanisms in biology. As clearly explained in Khalil the stability analysis of these behaviors can be recast as a standard equilibrium stabilization problem, but this leads to very conservative results. It is more convenient, instead, to invoke the notion of stability of an invariant set, where the latter is the closed orbit associated to the periodic solution. This approach leads to the important notion of orbital stability. A large number of papers and books have been devoted to analysis of orbital stability of a given dynamical system. However, there are only a few constructive tools available to solve the task of orbital stabilization of a controlled system. A popular approach to address this question is the virtual holonomic constraints (VHC) method, which has been tailored for mechanical systems of co-dimension one. In the VHC method a certain subspace of the state-space is rendered attractive and invariant, leading to a projected dynamics that behaves as oscillators. This is a particular case of the framework adopted in the immersion and invariance (I&I) technique, first reported for equilibrium stabilization and later extended for observer design and adaptive control. It has recently been shown that I&I can also be adapted for orbital stabilization, leading to a procedure that contains, as particular case, the VHC designs. The only modification done to the standard I&I technique is in the definition of the target dynamics that now should be chosen possessing periodic orbits, instead of an equilibrium at the desired point. A main drawback in both the VHC and I&I methods is that the steady-state behavior cannot be fixed a priori, but depends on the initial states. An alternative approach to generate oscillations is reported where it is proposed to construct passive oscillators for Lure dynamical systems using “sign-indefinite” feedback static mappings, which is a mechanism similar to the pumping-and-damping injection discussed below. Unfortunately, since the analysis is carried out applying the center manifold theory – that is a local notion – the obtained oscillators are assumed to have small amplitudes. Orbital stabilization designs, for some particular controlled plants, have also been reported. The aim of this paper is to show that the widely popular interconnection and damping assignment passivity based control (IDA-PBC), originally proposed for stabilization of equilibria, can be easily be adapted to address the problem of orbital stabilization of general nonlinear systems. This leads to two new constructive solutions for this problem that – as usual in PBC – have a clear interpretations from the energy viewpoint. First, the assignment of an energy function that has a minimum in a closed curve, i.e., with the shape of a Mexican sombrero. Second, the use of a damping matrix that changes “sign” according to the position of the state trajectory relative to the desired orbit, that is, pumping or dissipating energy. As usual in all constructive nonlinear controller designs, the success of the proposed methods hinges upon our ability to solve a partial differential equation (PDE). The remainder of the paper is organized as follows. Section 2 revisits the standard IDA-PBC. Section 3 introduces the problem formulation of orbital stabilization, followed by the constructive main results in Section 4. The application to the induction motor (IM) is reported in Section 5. Interestingly, we prove that the resulting controller exactly coincides with the industry standard direct field-oriented control (FOC) first proposed in Blaschke. In Section 6 the orbital stabilization of pendula is studied. The paper is wrapped-up with conclusions and future work in Section 7.", "section": "Introduction", "doi": "10.1016/j.automatica.2019.108661", "references": [1990988364, 2010398082, 2028593157, 2045666702, 2048253740, 2048270103, 2110216011, 2122513851, 2125098927, 2125968092, 2132347660, 2133582863, 2142428281, 2507943388, 2892919453, 2962880166]}
{"paragraph": "In the last few decades, there has been a drastic increase in the demand for renewable power sources such as photovoltaic and fuel cells. Accordingly, these resources imply the need for high voltage gain dc–dc converters to meet the dc voltage requirements, hence high voltage gain converters are at the center of attraction in recent years. Conventional Boost converter is one of the most common step-up converters with the advantages of simple structure, grounded switch, and low cost. However, obtaining high voltage gain is limited by the poor efficiency operation caused due to the large conduction losses and reverse recovery losses of diodes. Therefore, when the voltage gain is higher than 8, a Boost converter is no longer adaptive. To develop high-voltage gain dc–dc converters and avoid extreme duty cycle, various methods have been proposed in the literature in the last decades. Cascaded converter is one of the direct methods as the voltage gain is multiplied by every stage. But, this kind of converter always suffers from high-voltage stress, low efficiency, and large volume. Switched-capacitors and switched-inductors are also widely used in high voltage gain converters, with the advantages of avoiding extremely high duty cycle and presenting higher voltage gain than the conventional Boost converter. However, switched capacitors will cause high-current transients, which may decrease both power density and efficiency, limiting their application to some extent. Switched capacitor in resonant mode has been developed to solve this problem, and switching loss is reduced at the same time. Classical resonant-switched capacitor topology is introduced in the work presented where the output/input voltage ratio equals a factor of n + 2. In order to achieve 8 voltage gain, n should be 6, which means 12 diodes, six capacitors, and two power switches. The work presents a family of automatic interleaved Dickson switched-capacitor converters with the advantages of reduced output voltage ripple and zero voltage switching (ZVS). In order to achieve 8 voltage gain, there should be six diodes, six capacitors, and two power switches. In general, although these topologies can obtain higher voltage gain than the conventional Boost circuit, extra switched capacitor or inductor stages are unavoidable, which results in higher costs and more complex circuits. The work presented proposed a modified single-ended primary industry converter (SEPIC) circuit integrated with a voltage multiplier, which was composed of a diode and a capacitor, obtaining high-voltage gain and low-voltage stress with simple structure. But, the switch worked in hard switching mode and the switching loss will be quite large when applied to high-frequency applications. The increment of switching frequency can greatly improve the power density of power converters as well as the dynamic response speed, besides, the parasitic parameters of the system can be used as passive components and electrolytic capacitors can also be eliminated. As a consequence, high frequency and high-power density have become the mainstream development of power electronic converters these years. But, the corresponding challenge is to deal with the high-switching loss and the magnetic loss resulting from the increased switching frequency. Resonant converters with good soft switching characteristics are preferred for high-frequency applications. And for those converters without soft switching characteristic innately, quasi-resonant and active-clamp may be good choices. Soft switching characteristic has become one of the most important properties to evaluate the performance of converters. In terms of components, the third-generation semiconductor devices and planar magnetic components show satisfactory characteristics in high-frequency applications and are widely used. SiC and GaN devices have the advantages of fast switching speed and low-conduction loss, which are more suitable for high-power density and high-frequency applications. The planar magnetic component can effectively increase the heat dissipation area and its volume is only 20% of that of the conventional winding magnetic component under the same power. At present, many research works on planar transformers focus on how to reduce their parasitic parameters, for example leakage inductance. Up to now, the general method to reduce leakage inductance is to adopt interleaved structure. Although the interleaved structure can effectively reduce the leakage inductance of the transformer, it also increases the parasitic capacitance simultaneously. Excessive parasitic capacitance will cause leakage current and aggravate electromagnetic interference. Therefore, it is necessary to take leakage inductance and parasitic capacitance simultaneously into consideration in the design process. Based on the aforementioned analysis, a modified SEPIC converter based on partly interleaved transformer is proposed in this paper with the advantages of high-voltage gain, low-voltage stress, and ZVS characteristic. Compared with the previous work, the contribution of this paper is the ZVS characteristic of the proposed converter, which makes the converter work under high frequency as well as maintain high efficiency. Consequently, the working modes of the converter are changed and better performance is obtained. Moreover, as there are two magnetic components in the converter, in order to further reduce core volume, we adopt integrated scheme to integrate the transformer and the inductor into one magnetic core, and they are just physically coupled on the same core but magnetically decoupled. In the aspects of optimization of transformer structure, partly interleaved structure is applied to obtain small parasitic capacitance and leakage inductance at the same time. Besides, in order to reduce conduction loss, GaN device was chosen. Compared with traditional SEPIC and other high voltage gain converters, the devices added in this paper are all passive components, ZVS is achieved without auxiliary switch, therefore no additional drive and control circuits are required, so high-voltage gain is achieved with simple structure. This paper is organized as follows. In Section II, the operational principle of the proposed converter is introduced in detail, the parameter design of the converter is described briefly in Section III, while Section IV analyzes the optimization process of the magnetic components, Section V shows the experimental results, and the conclusion is given in Section VI.", "section": "Related Work", "doi": "10.1109/TIE.2019.2910044", "references": [1674332857, 2020005495, 2041804496, 2086568061, 2111434465, 2114647642, 2129189272, 2148532820, 2156779651, 2315644406, 2592432976, 2626278689, 2746847331, 2789786380, 2799643719, 2885662602]}
{"paragraph": "In recent years, implementation vulnerabilities rather than mathematical weaknesses have become significant threats to cryptographic devices. These vulnerabilities enable the adversary to observe the side-channel information leakage of the devices, perturb their normal behaviors and infer the secret intermediates of the cipher. Among the existing implementation attacks, fault attacks (FA) are powerful ones. By injecting faults into cryptographic devices with clock glitches, electromagnetic radiation or laser radiation, the attacks corrupt the normal encryption in the intermediate variable or flow sequence. Then, they acquire additional information from the faulty ciphertexts. So far, fault attacks have been successfully applied to various cryptosystems including AES, ECC, RC4 and so on. Though the values of faulty ciphertexts are helpful for key retrieval, there still exist attacks that do not use them. For instance, ineffective fault attack (IFA) exploits the non-uniform fault models to reveal the value (distribution) of injected intermediates. Fault sensitivity analysis (FSA) exploits the critical intensity of the effective injection to identify the value of the processed data. These attacks are also threatening to cryptographic devices. Up to now, there are mainly two kinds of countermeasures against fault attacks: detection and infection. Both countermeasures can apply to the attacks that rely on the intermediate-oriented injection and the faulty ciphertexts exploitation. In the detection countermeasure, the cipher computation is first performed redundantly in the form of space, time or information, followed by a consistency check between the redundant and original cipher computation results. If the results are different, the fault will be detected. Then, the countermeasure will stop its output to avoid the faulty ciphertext exploitation in fault attacks. However, if the adversary can bypass the consistency check with an additional fault injection, the detection countermeasure will be broken. To avoid the risk, Joye et al. proposed a new defensive strategy called infection. By doubling the algorithm and scrambling the data paths of the two encryptions, the countermeasure corrupts the relationship between the secret key and the faulty output. Finally, it makes the faulty output useless in the original fault attack. As a result, there is no need for any consistency check. The ciphertext can always be returned. Infection function is the core component of the infection countermeasure. It decides how the data paths are scrambled. The early infection functions are constructed with deterministic operations. They were soon proved insecure because their scramble effects can be easily removed when the construction details are known. To address the problem, the later proposed countermeasures have tried to induce randomness into infection functions in various ways, including multiplicative masking, dummy computation, system configuration parameters and so on. Although random infection countermeasures overcome the weakness of detection countermeasures, their security still cannot be evaluated in a general and provable way. Most of the infection countermeasures are first evaluated internally by their designers. These evaluations only focus on some particular fault injection scenarios and base on some specific analytical strategies. Yet they ignore the diversity of fault attacks. As a result, many seemly secure countermeasures are found vulnerable under other attack scenarios in later researches. Besides, the evaluation based on analytical strategies can hardly lead to a fair comparison between different countermeasures. Based on these facts, a natural question is: given an infection countermeasure, whether there exists a general security evaluation framework that is applicable to various fault injection scenarios and independent of the specific analytical strategy. Further, if the answer is yes, can the security be evaluated quantitatively? Patranabis et al. have tried to make the evaluation more general from the perspective of information theory. They took the mutual information between the secret key and the difference of the output ciphertexts as the security evaluation metric. Then, they quantified the security of an AES infection implementation formally under three different fault injection scenarios. However, their works only focus on this specific implementation and its specific infection function. They did not discuss how to generalize their methods to other infection countermeasures, especially for those with complicated infection functions. Therefore, gaps still exist in the infection countermeasure analysis and mutual information quantification. Besides, the evaluation targeted at the integrated cipher implementation. Once the unprotected cipher or the fault model changes, it always requires a new security quantification from scratch. Therefore, their method may not be the best choice for numerous evaluations under different attack scenarios. Furthermore, their quantification results reflect the combined effect of the infection together with the other parts of this implementation, including the unprotected cipher algorithm and some other defensive measures. Therefore, the security comparison between different infection countermeasures is still a problem. Ghosh et al. have tried to separate the security analysis of the infection function from the analysis of the integrated cipher algorithm. They have proved that if the infected faulty ciphertexts are totally random, then no secret key can be retrieved from them. This work provides an identification criterion for the perfect secure infection countermeasure against all the faulty ciphertexts exploitation attacks. However, the criterion is too strict for the evaluation under each specific attack scenario. It is likely to result in false negative security judgments. Besides, this work did not mention how to quantify the randomness of the infected faulty ciphertexts. In this paper, we propose the first comprehensive security evaluation framework for infection countermeasures. The framework covers a large portion of fault attacks that rely on the exploitation of faulty ciphertexts. It is feasible for both the existing and future infection functions. It is also suitable for security comparison between different infections. The framework is constructed from the point of information theory just as previous works, but with the idea to separate the security requirement of the infection function from the integrated cipher implementation. The main principle is to formalize the fault injection effects on the unprotected cipher into the attack scenarios faced by the infection function, and take them into account in the evaluation. Another principle is to decompose the complicated analysis into multiple simple ones, and reuse their results for the efficient evaluation. Firstly, we construct a security judging criterion for infection functions based on the security requirement of the integrated cipher implementation and the security loss of unprotected cipher in the given attack scenario. In this case, the security boundaries are set more precise and personalized under different attack scenarios. Secondly, we put forward a universal method of security quantitative analysis with two important steps: the prior knowledge collection and the simple infection operation decomposition analysis. The analysis takes not only the infection output but also the attack-scenario-related information of the infection input as the prior knowledge. Therefore, it enables a thorough exploration of infection vulnerabilities. On the other hand, the analysis decomposes the infection function into a series of repetitive and simple infection operations that are related to the random bits. Therefore, the security quantification result of the infection function can be easily obtained through the information-theoretical analysis of the simple infection operations. Finally, we make security judgment on the infection function under the given attack scenario, according to the corresponding judging criterion and quantification result. To verify the feasibility of the framework, we take the intermediate-oriented fault injections on AES-128 as the sample attack scenarios and make evaluations on the existing infection countermeasures. The result shows that most existing countermeasures are not as secure as expected. Through the analysis based on all possible prior knowledge, we find not only their well-known vulnerabilities, but also new flaws that have never been taken as threats. We summarize all these vulnerabilities in Table I and highlight the new ones in bold. This work confirms the comprehensiveness of our framework. On the other hand, the quantitative evaluation result gives us a new insight into the application of infection countermeasures. It is instructive for the countermeasure selection when the implementation costs are close. The paper is organized as follows. Section II provides the necessary preliminaries on fault attacks and infection countermeasures. Section III presents the proposed evaluation framework. Section IV takes the fault injections on AES-128 as the sample attack scenarios to demonstrate the security judging criteria for infection functions. Then, the security quantification analyses on RIMBEN, ICDR, and LDRNM are detailed in Section V to VII, respectively. Section VIII generalizes the evaluation and makes comparison between different countermeasures. Section IX concludes the paper and highlights the future work.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2903653", "references": [1483831648, 1493213353, 1493755219, 1537875328, 1556992967, 1935672779, 2058413145, 2060618055, 2066465211, 2082988283, 2104003712, 2110162979, 2110941647, 2112284176, 2112571059, 2132533351, 2135632173, 2147694198, 2151136913, 2170489924, 2230383587, 2295549364, 2475554842, 2521172080, 2889402643]}
{"paragraph": "Advancement in computational technology has enabled simulation of large power system networks in real time. Potential of a real-time simulator (RTS) can be extended remarkably by carrying out a hardware-in-loop (HIL) simulation, where the mathematical model of a part of the system being simulated is replaced by actual hardware. Power-hardware-in-loop (PHIL) simulation is a special case of HIL, where the subsystem in actual hardware is a power hardware (e.g., a power electronic converter and an electric machine). This PHIL simulation serves as a testing methodology for the power hardware under close-to-realistic condition. The PHIL simulation is being used extensively over the past few years in setting up electronic testbeds for drive systems, replacing the conventional mechanical testbeds. This is also used for laboratory education, validating dynamic models of renewable energy sources such as photovoltaic panels and wind turbines, studying the interactions between renewable sources and the power grid, system-level analysis and design of renewable energy technologies, comparative study on different configurations of energy storage systems in terms of storage capacity, emission of pollutants, and economic aspects in hybrid electric vehicles, and an initial experimental validation of a reduced power prototype of a subway. The PHIL simulation requires interfacing the RTS and the power hardware through a power amplifier (PA), as indicated in Fig. 1. The response of the PHIL simulation can be significantly different from the actual system response due to numerous factors. While the transfer function of the PA used should ideally be a unity gain, a practical PA does introduce its own dynamics into the system. Again, while the RTS should ideally perform computation with zero time lag, a practical RTS introduces a finite time delay in the simulation loop. Accuracy also depends on the numerical method utilized to arrive at discrete-time (DT) equations of the subsystem to be simulated in real time. Nonlinearity introduced by the sampler at the input of the RTS and the zero-order hold at its output is another factor, which is often neglected, as has been pointed out recently. Hence, the emulated system could show fast transients, which are significantly different from those of the actual system. The above factors not only impact the accuracy of the PHIL simulation, but also its stability. Consequently, it is possible that the PHIL simulation of a system could be unstable even as the actual system being simulated is stable. This paper addresses factors concerning accuracy, modeling, and stability analysis of the PHIL simulation. Advancement in computational technology has enabled simulation of large power system networks in real time. Potential of a real-time simulator (RTS) can be extended remarkably by carrying out a hardware-in-loop (HIL) simulation, where the mathematical model of a part of the system being simulated is replaced by actual hardware. Power-hardware-in-loop (PHIL) simulation is a special case of HIL, where the subsystem in actual hardware is a power hardware (e.g., a power electronic converter and an electric machine). This PHIL simulation serves as a testing methodology for the power hardware under close-to-realistic condition. The PHIL simulation is being used extensively over the past few years in setting up electronic testbeds for drive systems, replacing the conventional mechanical testbeds. This is also used for laboratory education, validating dynamic models of renewable energy sources such as photovoltaic panels and wind turbines, studying the interactions between renewable sources and the power grid, system-level analysis and design of renewable energy technologies, comparative study on different configurations of energy storage systems in terms of storage capacity, emission of pollutants, and economic aspects in hybrid electric vehicles, and an initial experimental validation of a reduced power prototype of a subway. The PHIL simulation requires interfacing the RTS and the power hardware through a power amplifier (PA), as indicated in Fig. 1. The response of the PHIL simulation can be significantly different from the actual system response due to numerous factors. While the transfer function of the PA used should ideally be a unity gain, a practical PA does introduce its own dynamics into the system. Again, while the RTS should ideally perform computation with zero time lag, a practical RTS introduces a finite time delay in the simulation loop. Accuracy also depends on the numerical method utilized to arrive at discrete-time (DT) equations of the subsystem to be simulated in real time. Nonlinearity introduced by the sampler at the input of the RTS and the zero-order hold at its output is another factor, which is often neglected, as has been pointed out recently. Hence, the emulated system could show fast transients, which are significantly different from those of the actual system. The above factors not only impact the accuracy of the PHIL simulation, but also its stability. Consequently, it is possible that the PHIL simulation of a system could be unstable even as the actual system being simulated is stable. This paper addresses factors concerning accuracy, modeling, and stability analysis of the PHIL simulation. Switched-mode power supply (SMPS)-based PAs are the most common choice for medium- and high-power applications on account of high efficiency, compactness, and relatively lower cost. The actual PA used is typically a closed-loop VSI with passive output filtering to mitigate switching frequency harmonics, as shown in Fig. 2(a). The closed-loop PAs typically have long response times. When the load on the PA is inductive, an open-loop VSI itself could be a good option as a PA, as demonstrated in this paper, since the open-loop PA is capable of replicating the fast dynamics in the system. This paper explores improving the accuracy further by eliminating the effective time delay associated with computation while utilizing an open-loop PA. A PHIL simulation is widely modeled using the s-domain transfer function of the system being simulated. While some works assume the PA to be ideal, ignoring its dynamics, a few others consider the dynamic model of the PA along with that of the power hardware. Although the RTS is a DT system, it is also approximated by its equivalent continuous-time (CT) model in the existing approach, as shown in Fig. 3(a). The total delay in the simulation loop Td, including RTS computation time delay, is modeled by a delay element in the CT domain, as shown in Fig. 3(a). This existing approach does not take into account the effects of sampling, zero-order hold, and the numerical method used for discretizing the CT model. The effects of sampler and zero-order hold on PHIL simulations have been recognized and modeled in the CT domain. It has been demonstrated that PHIL simulations cannot be modeled entirely in the CT domain for the purpose of stability analysis. The CT representation of the DT RTS part is valid only when the input to the sampler varies at a rate much lower than the sampling frequency. In other words, this is valid only when the PHIL simulation loop response time is much higher than the sampling interval. This is true in situations where the response time of the PA and or the computational time delay is high. With the evolution of high-bandwidth amplifiers and fast digital processors, this is no longer valid for the modern and evolving PHIL simulation setups. Hence, discretization effects need to be incorporated in the PHIL simulation model. Furthermore, with an open-loop PA as being proposed, the response time is quite low; also, the elimination of the computational delay is being explored. Hence, this paper proposes a DT-domain model, which takes into account the DT nature of the RTS adequately. Here, the real-time simulated network is represented by the DT equations used in the RTS, while the CT-domain part of the system is represented by its exact discretized equations, as indicated by Fig. 3(b). This model is demonstrated to be more accurate than the conventional model in representing the transients of the PHIL simulation, as well as in predicting stability limits of the same. The stability limits predicted based on the proposed modeling approach are verified through simulations and experiments, considering a simple network, which is widely used as a test case. Furthermore, the open-loop PA is utilized for a more practical case of power-electronic-based emulation of a synchronous generator with frequency-droop and excitation controllers. This is found to be accurate in emulating the various dynamics in the generator, including the fast transients in the excitation system, due to a sudden change in the load.", "section": "Related Work", "doi": "10.1109/TIE.2019.2896093", "references": [1983667108, 2061550804, 2077004742, 2080146761, 2103231471, 2116991566, 2124570265, 2138952522, 2147505830, 2153450951, 2159616085, 2216305900, 2548030707, 2556836235, 2595978989, 2745181186, 2794022823]}
{"paragraph": "E-mail becomes a necessary means of communication because of its convenience and high efficiency. But the number of spam is increasing since it can make big profits with a small spending by spreading advertisement or other disgust news to mail users. Some lawbreakers even send computer virus with an e-mail which results in a huge threat of computer. Spam, usually considered as unsolicited bulk e-mail or unsolicited commercial e-mail, has brought many troubles to our normal communication by e-mail. It was indicated that the number of spam was so large that a majority of network bandwidth and mailbox server’s storage are unable to be used in other important applications. The huge amount of spam also brought much interference to users and had very severe influences for people to work effectively. Moreover, the spam always had threats once it carrying malicious codes secretly which would affected the safety of computer and personal information. It can be seen that there are nearly 60% of e-mails are spam in 2014 and another report revealed a more serious statistical result with the spam rate more than 68% in the third quarter of 2014. In a word, spam detection is still a severe challenge. In order to reduce economical losses caused by spam and improve working efficiency, people from different fields had proposed many anti-spam methods in diversiform perspective, including changing the protocol of e-mail sending and simple keywords filtering, address protection, and so on. With the rapid improvement of artificial intelligence, more and more intelligent classification methods are adopted to cope with them, the most popular approach is with supervised learning methods. In addition, with its robustness and flexibility, automatic intelligent detection methods are widely used in spam e-mail filtering. Similar to other classification tasks, intelligent spam detection can be decomposed into three important research steps, commonly called feature selection, feature construction, and classifier design. The purpose of feature selection lies in selecting features which are much more important in the further processed steps and resulting in a lower dimensionality which is useful to save computation resource and improve accuracy of the classification model. Feature construction methods discover the inner relationship among all existing features and transform them into a new set first, then use this set of features to construct sample vectors. Supervised machine learning methods are very useful in pattern recognition and have been proved to be effective in spam detection domain. The prevalent and commonly used approaches and techniques are introduced in Section II. In this paper, an ensemble decision method for spam detection is proposed by utilizing both global features and local features of an e-mail in the process of decision making. Corresponding features are constructed by further exploiting and improving the term space partition-based feature construction approach. Sliding window technique with different length of windows is introduced to extract local features of the e-mail, as well as position correlated information. Global and local classifiers are constructed, respectively, based on corresponding feature vector sets and make ensemble decision with voting techniques. Five different benchmark corpora named PU1, PU2, PU3, PUA, and Enron-Spam are employed in our experiments to evaluate the performance of our novel spam detection method. As in standard classification performance analysis, we adopted accuracy and F1 measure as the main criteria to compare our results with others. The organization of other contents in this paper are as follows. Section II describes details of some widely used measurements of feature selection and some methods for constructing feature vectors. In addition, machine learning algorithms-based spam detection models are also introduced. The term space partition-based feature construction approach is described in Section III. The proposed ensemble decision method is presented in Section IV. Section V shows the results and comparison of experiments. Finally, the conclusion of this paper is presented in Section VI.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868794", "references": [327991062, 1482320265, 1504008138, 1535723536, 1552665700, 1563132617, 1754050374, 1964752234, 1965272602, 1970024574, 1970974276, 1972222614, 1985523146, 1986678144, 1990016010, 1996802155, 2003924941, 2014179354, 2023670764, 2025500887, 2028772504, 2033485922, 2040919914, 2061989622, 2067334282, 2069350935, 2084538762, 2089923329, 2095195675, 2103539910, 2109343507, 2112837588, 2114748216, 2119303463, 2119479037, 2120070835, 2120149881, 2128233170, 2133564696, 2133990480, 2135069544, 2141337627, 2145094598, 2153635508, 2155806188, 2160536005, 2169384781, 2251925957, 2271887370, 2346875348, 2398905593, 2409795150, 2435251607, 2548118777, 2565439473, 2577888896, 2579280245, 2592516822, 2765520544, 2793652052, 2963858765]}
{"paragraph": "Social network analysis is the process of studying social structures using networks and graph theory. It is used to characterize network or graph structures in terms of nodes and ties, edges, or links that connect them. These nodes can be people, actors, groups, organizations, nations, and products of human activity or cognition such as web sites or semantic concepts. Signed graphs are frequently used to represent the relationships between nodes and signed edges can represent more relationships than unsigned edges, such as like or dislike, trust or distrust or for or against. Signed graphs are often used in social network analysis, so partitioning signed graphs is an important task in social network analysis. It can be used to understand group structures and to analyze group behavior with its social structure. Clustering is an important tool for social network analysis. Clustering methods are used to partition nodes in signed graphs into different clusters using various criteria. Clustering on social networks that is measured using empirical observations usually exhibits homophily, whereby people tend to have most interactions with homogeneous individuals. This tendency has observable characteristics, such as race, gender, age, income, education and other demographics. There is a long history of clustering methods for social network analysis, such as the algorithm CONCOR that was used to discover the block model in human relationship networks. Clustering for signed graphs was first defined as partitioning the point set into subsets such that each positive line joins two points in the same subset, and each negative line joins two points from different subsets. A signed graph is balanced in social network analysis if the point set can be exactly divided into two mutually hostile subsets. It is clusterable if the point set can be divided into subsets, where any two subsets are mutually hostile. Using the balance theorem, an algorithm was proposed that takes the amount of imbalance as criterion by using a locally optimal partitioning procedure to partition the structure of signed graphs in as balanced a method as possible. Both balanced and clusterable are termed ‘k-balanced’ in social network analysis. Moreover, the balance theory had been extended to blockmodeling and generalized blockmodeling that were used in clustering signed graphs in social network analysis. Blockmodeling methods transform networks into blockmodels that represent the structure of networks. Various clustering methods for partitioning networks that use blockmodels have been proposed in the literature. As well as structural balance theory, blockmodeling and generalized blockmodeling, there are various ways for clustering signed graphs, such as correlation clustering, community detection and spectral clustering. Correlation clustering, called CC, divides graphs by minimizing the inconsistent amount that is similar to the amount of imbalance or by maximizing the consistent amount that is similar to the amount of balance. In CC methods, KwickCluster is applicable to signed networks. Fusion moves have been used for the CC method, called FusionCC, in which these points are fused with positive edges into new points and the general CC algorithms are run. A so-called FEC method has been proposed for community mining of signed networks. A criterion function similar to previous methods was proposed and then a spectral approach was used to achieve final partitioning. In social network analysis, we expect to get clusters that have as few imbalances as possible. Using CC methods for signed graphs, the term “inconsistence” is used rather than “imbalance.” However, the two terms have the same meaning. The goal of CC is to achieve clusters that have a minimum value of inconsistence. The objective of community detection is to find clusters that have dense positive links within, but which have dense negative links with other clusters. This is an indirect way to achieve balanced clusters. This study uses the terms “balance” and “imbalance” and develops algorithms that produce clustering results as few amount of imbalances as possible. In this paper, we propose a novel approach for exploring cluster structure of signed graphs that deals quickly with large-scale signed graphs. A new clustering algorithm is designed to deal with large-scale signed graphs, named fast clustering for signed graphs. This uses a random walk gap. A new random walk gap mechanism is constructed. The random walk gap with a gap is performed by making random walks on two graphs, one of which is only on positive edges and the other of which is on both positive and negative edges of signed graphs. The proposed random walk gap can extract cluster structure information from the gap between the two constructed random walk graphs. We then propose the fast clustering for signed graphs algorithm. Some experiments of synthetic and real data are used to demonstrate that the proposed fast clustering for signed graphs has good performance according to the criteria of imbalance and modularity. The next section details the proposed fast clustering for signed graphs algorithm.", "section": "Introduction", "doi": "10.1016/j.socnet.2018.08.008", "references": [131619556, 1411692141, 1502572743, 1937315027, 1967343101, 2033590892, 2069782692, 2087407617, 2091858563, 2094013877, 2097216034, 2102664282, 2144434012, 2232472329, 2469279958, 2605255209, 2914959486]}
{"paragraph": "Clustering is an unsupervised learning technique to find natural groups that are implicated in data. Its main task is to partition unlabeled patterns into subgroups such that the patterns in the same cluster are expected to have the highest similarities and the patterns between different clusters are expected to have the highest dissimilarities. In this manner, the data structure that is revealed by a clustering method is expected to reflect the natural geometry of the data as much as possible. Hard C-means is one of the foremost objective function-based clustering methods in which the degree of each pattern belonging to each cluster is 0 or 1. The validity of HCM will degenerate when dealing with patterns with overlapping areas. Fuzzy clustering, especially fuzzy C-means, as one extension of HCM, utilizes a partition matrix to evaluate the degree of each pattern belonging to each cluster, so that the overlapping partitions can be described effectively. The main challenge of FCM is the sensitivities to noisy patterns which may contaminate the calculations of the corresponding prototypes and membership degrees. Based on rough set theory, which aims at analyzing data involving uncertain, imprecise or incomplete information, a rough C-means clustering method was proposed. Each cluster is described not only by a prototype, but also with a pair of lower and upper approximations. Meanwhile, the boundary region is defined as the difference between the lower and upper approximations. The uncertainty and vagueness arising in the boundary region of each cluster can be captured well in RCM. Since no membership degrees are involved, the closeness of patterns to clusters cannot be detected. Rough sets and fuzzy sets, as two important paradigms of granular computing, are strongly complementary to each other. Incorporating with fuzzy membership degrees, a rough-fuzzy C-means clustering method was presented which integrated the merits of both fuzzy sets and rough sets. The lower and upper approximations are determined according to the membership degrees, rather than the individual absolute distances between a pattern and its neighbors. A robust rough-fuzzy C-means algorithm was further proposed that integrated both probabilistic and possibilistic memberships of fuzzy sets, which could handle overlapping clusters in noisy environments as well as the uncertainty and vagueness in cluster definitions due to involving rough sets. No matter which rough-fuzzy partitive clustering methods are used, some model parameters are involved: the weighted values that evaluate the contributions of lower approximations and boundary regions when calculating new prototypes; the threshold that determines the lower approximation and boundary region of each cluster; the value of fuzzifier parameter m that controls the shape of memberships. Since the contributions of lower approximations are considered more important than the contributions of boundary regions as computing the prototypes, the weighted value for lower approximations is much higher, and its complementarity is applied for the boundary regions. Meanwhile, through the combinational adjustments of the threshold and the fuzzifier, the influence caused by the weighted values can be reduced. In this case, these parameters will take over the effect of weighted values. The threshold that determines the approximation regions of each cluster is often selected depending on subjective tuning in the available research. Different strategies have chosen this value as the average value or the median of the difference between the highest and second highest fuzzy memberships of all the patterns. However, the same threshold is employed for all clusters though the sizes and the densities of the clusters may be significantly diverse. Additionally, the approximation regions are partitioned based on the absolute distances or membership degrees of individual patterns, not the global observation on data for a specific cluster, then the topology of data cannot be detected well with respect to this cluster. Shadowed sets, as a bridge between rough sets and fuzzy sets, provide a new scheme to granular computing. It is an example of three-way, three-valued, or three-region approximations of a fuzzy set according to the framework of three-way decisions which is another new paradigm of granular computing and can provide favorable semantical interpretation and generalized optimization methodology for determining partition threshold values. A shadowed set-based rough-fuzzy C-means method was introduced which gave a technique of automatic selection of the partition threshold. No matter which selection methods are used, an unreasonable threshold will result in undesired approximation region partitions, and then the prototype calculations may be spoiled. The value of fuzzifier parameter m is very important for the updating of prototypes and the corresponding partition matrix. A predefined value of m is often used in the rough set and fuzzy set-based clustering methods. However, it is difficult to express the uncertain notion of fuzziness in a given data set using a single fuzzifier value. To manage the uncertainty generated by the fuzzifier parameter m, an interval type-2 fuzzy C-means was proposed which extended a pattern set to interval type-2 fuzzy sets using a pair of fuzzifier values that created a footprint of uncertainty for the fuzzifier parameter. A general type-2 fuzzy C-means algorithm was further revealed via an alpha-plane representation theorem. However, the values of these fuzzifiers mainly depend on subjective selection or enumeration in the available studies and the results need more interpretations. Recently, the notion of multigranulation in granular computing is developed for solving human-centric problems and interpreting the obtained results from the perspective of multiple levels of granularity. This methodology provides a new insight into analyzing the uncertainties generated by the model parameters. The purpose of this paper is mainly to tackle the uncertainties associated with the two key parameters in rough set and fuzzy set-based clustering approaches, namely the threshold related to the approximation region partitions and the fuzzification coefficient m. The main objectives of this paper are to optimize the partition threshold for each cluster based on shadowed sets, which is obtained from the perspective of the global observation on data and will be used as a cornerstone for establishing the multi-levels of granularity for approximation regions; to capture the uncertainty generated by the fuzzifier parameter m in rough-fuzzy clustering methods via the variations in multigranulation approximation regions formed under multiple values of fuzzifier parameter with a partially ordered relation, rather than at a single level of granularity under a specific fuzzifier value; to update the prototypes by combining the intermediate results obtained at different levels of granularity. In this way, the prototypes calculated at a single level can be modified and then tend to their natural positions; to develop a multilevel degranulation mechanism according to granulation-degranulation philosophy based on which the quality of the clustering model can be evaluated. By integrating various granular computing technologies, i.e., fuzzy sets, rough sets, shadowed sets and the notion of multigranulation, the uncertain information in data, including overlapping partitions, the vagueness arising in boundary regions and the uncertainty produced by fuzzification coefficient, can be handled sufficiently. Experimental results with the use of synthetic and real-world data illustrate the improved performance of the proposed notion in terms of several validity indices, such as relative separation index, Davies–Bouldin index, Dunns index and PBM-index as well as the granulation-degranulation index. The rest of paper is organized as follows. Some rough set-based partitive clustering methods are reviewed in Section 2. The uncertainty generated by the fuzzifier parameter m is revealed in Section 3. Meanwhile, multigranulation approximate regions are formed based on which a new rough-fuzzy C-means method is introduced. In Section 4, the proposed granulation-degranulation mechanisms based on multiple levels of granularity are explained. Comparative experiments are presented in Section 5. Some conclusions are given in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.05.053", "references": [101345952, 1843766148, 1976670813, 1983753875, 1987149779, 1988695218, 1989577218, 1989907351, 1996747841, 1998965536, 2001692054, 2006873874, 2026489439, 2039742983, 2048404808, 2051224630, 2052608046, 2062696711, 2070813883, 2078757499, 2083449452, 2089923511, 2103535990, 2114832876, 2143451122, 2154437129, 2170755382, 2171975443, 2259316796, 2297889545, 2340020088, 2461653276, 2565881538, 2605693713, 2620114837]}
{"paragraph": "Constraint-based techniques, such as Integer Linear Program and SAT modulo theory solvers, play a key role in state-of-the-art approaches for solving challenging problems across a wide range of applications. In this work, we demonstrate how virtual data center allocation, a prominent and increasingly important problem arising in the operation of modern data centers, can be tackled using a pair of high-performance constraints solvers: Gurobi and MonoSAT. We obtain substantial improvements in performance and functionality over previous virtual data center allocation techniques. Central to our results is the formalization of virtual data center allocation in terms of multi-commodity flows, allowing us to exploit the efficient handling of network-commodity flow problems in Gurobi and MonoSAT. We are the first to demonstrate that constraint solvers can be successfully applied to this setting at full data center scales, while also improving on the state-of-the-art. A virtual data center consists of multiple communicating virtual machines, each with individual server resource requirements such as CPU or RAM, along with a virtual network of pair-wise bandwidth requirements between the virtual machines. The virtual data center allocation problem is to find a valid allocation of virtual machines to servers and links in the virtual network to links in the physical network. A valid allocation satisfies the compute, memory, and network bandwidth requirements of each virtual machine across the entire data center infrastructure, including servers, top-of-rack switches, and aggregation switches. The allocation is indicated with dashed lines. For example, the virtual machines for s1 and m are mapped to the same physical server and the virtual link s2 to m is allocated a multi-path route, in which each sub-path provides 1 Gbps. Support for end-to-end and multi-path are two characteristics that distinguish capabilities of our tool, Netsolver, from prior tools. In this work, we introduce Netsolver, a constraint-based virtual data center allocation procedure that is scalable, sound, and complete, with support for end-to-end, multi-path bandwidth guarantees across all the layers of the networking infrastructure, from servers to top-of-racks to aggregation switches to access routers. Netsolver efficiently allocates virtual data centers with a dozen or more virtual machines to full-size physical data centers with over a thousand servers, typically in seconds per allocation. Across a wide variety of data center topologies, Netsolver can allocate as many total virtual data centers to the same physical data center as state-of-the-art heuristic methods, such as SecondNet's allocation algorithm. Furthermore, Netsolver offers the flexibility and extensibility characteristics of a constraint-based approach. In real-world applications, data center operators often need to support additional constraints or optimization goals while allocating virtual machines, such as ensuring that certain virtual machines are placed together. We demonstrate that Netsolver can be easily extended to support additional virtual data center allocation constraints: virtual machine affinity constraints, minimization of the total number of utilized servers, and constraints to load-balance allocations and avoid hotspots. This paper extends results previously published with significant new capabilities, a new Integer Linear Program-based solver back end, and broadened experimental results. Additionally, we have improved the overall runtime performance of Netsolver through the use of automatic algorithm configuration, as well as upgrading the SMT solver from version 1.4 to version 1.6. We use Gurobi version 8.1.0 for all experiments.", "section": "Related Work", "doi": "10.1016/j.artint.2019.103196", "references": [60686164, 1480909796, 1572977974, 1710734607, 1770696206, 1857623778, 1968457703, 2001859357, 2002227246, 2006146932, 2010727402, 2017953449, 2022678927, 2027588582, 2074814670, 2077107611, 2077661716, 2094410510, 2096125134, 2097882016, 2107342126, 2112486185, 2114298221, 2116758077, 2121574037, 2123138012, 2126210439, 2130267070, 2132238781, 2134656724, 2137229314, 2142480021, 2142972529, 2152415706, 2154203494, 2157614013, 2157990152, 2161965229, 2257125174, 2295272781, 2803222079, 2949910966]}
{"paragraph": "Data envelopment analysis is a nonparametric method for measuring the efficiency of the decision-making units by a set of inputs and a set of outputs. DEA requires no assumption for the functional relationships between inputs and outputs and allows individual decision-making units to evaluate their efficiencies by choosing the most favorable input and output weights for themselves. It has been applied in many areas, such as resource reallocation, hospital assessment, and assessment of public finance. However, such an individual evaluation causes more than one unit to be efficient, leading to them being unable to be fully discriminated. In addition, the flexibility in weighting multiple inputs and outputs sometimes produces unrealistic weight schemes. To overcome these disadvantages, the cross-efficiency evaluation method has been developed to rank units based on cross-efficiency scores, which are linked to all units. Unfortunately, the cross-efficiency evaluations obtained from the original DEA are generally not unique because the optimal solution to the DEA linear programs is not unique. For this reason, secondary goals such as the aggressive and benevolent formulations have been proposed to address this issue. Additional models have introduced different secondary objective functions, which include minimizing the total deviation from the ideal point, minimizing the maximum efficiency score, and minimizing the mean absolute deviation. A neutral DEA model for cross-efficiency evaluation has also been proposed, which determines one set of input and output weights for each unit from its own point of view without being aggressive or benevolent toward others. Another approach generalized the original DEA cross-efficiency concept to game cross-efficiency, in which each unit seeks to maximize its own efficiency under the condition that the cross-efficiency of others does not deteriorate. A data DEA cross-efficiency aggregation method based on Shannon entropy has also been proposed. As a large number of zero weights may still exist among inputs and outputs, other models were developed to reduce the number of zero weights. Similar approaches estimate efficiency confidence intervals by using the bootstrap or other sub-sampling techniques. The weights for inputs and outputs of units are estimated from a random sample of input–output combinations, which are obtained from existing production units operating in the studied activity sector. Ratio-based efficiencies such as ranking intervals, dominance relations, and efficiency bounds have also been developed to compare the relative efficiencies of units for all feasible input–output weights. Some methods considered all possible weight sets in the weight space when computing the cross-efficiency, which produces an interval cross-efficiency for each unit. Other procedures were developed to carry out the cross-efficiency evaluation without the need to make any specific choice of weights. The proposed procedure takes into consideration all the possible choices of weights that all the units can make, and yields a range of the possible rankings for each unit instead of a single ranking. It is necessary to note that the interval efficiencies are different from the efficiency confidence intervals. The interval efficiencies contain all the possible efficiency scores of units, including the best and worst efficiency scores from all possible weights in the weight space. The efficiency confidence intervals are defined from a probability point of view, according to which of the most efficient units lie within an interval with a certain confidence level. Many classical decision analysis methods, including DEA, are based on assumptions of rationality and certainty. However, numerous studies have demonstrated that these classical methods cannot explain many phenomena in real applications. Therefore, researchers have explored decision-making theories based on behavior, and many behavioral decision theories have been developed, such as prospect theory, regret theory, and fairness theory. It has been noted that traditional decision and game theories rest on a fundamental assumption that players seek to maximize their individual utilities, but in some interactive decisions, it is observed and seems intuitively reasonable to maximize the utility of the group of players as a whole. Such phenomenon or thinking can be called team reasoning. Experimental evidence suggests that team reasoning predicts strategic choices more powerfully than orthodox game theory in some games. Some researchers commented on such an approach with varying mixtures of agreement and disagreement. In reply, it has been emphasized that decision makers sometimes act to maximize the collective payoff for a group, which cannot be explained in terms of standard social value orientations. Therefore, team reasoning is a distinctive and important mode of reasoning, which should be acknowledged in cognitive psychology and be added to the set of social value orientations used in social psychology. Furthermore, based on the experiments on how players use focal points to select equilibria in one-shot coordination games, two alternative explanations, namely, the cognitive hierarchy theory and team reasoning, were tested and strong support was found for team reasoning. In addition, some researchers considered all the individuals forming a team, and developed consensus models based on the minimum cost of the team. From the above literature review, we find that all the existing DEA approaches focus on the individual interest but ignore the team interest. Motivated by the idea of team reasoning, this paper investigates DEA models that put the team interest ahead of the individual interest. In DEA, all the decision-making units can be considered as a team, and the interest of the team can be reflected by the team's indexes, such as the overall efficiency of all the units, deviations of units, boundaries of units, and relationship between units in the team. The first three can be considered the external performance indexes of the team and expressed by interval values, whereas the fourth can be considered the internal performance index of the team, in which the relationship between units is expressed by the interval pairwise comparison matrix, where one value is the relative efficiency value between each pair of units. Based on such an idea, the following approach is proposed: First, the models are developed to estimate the values of the team indexes. Based on the obtained indexes, the decision makers can express their preferences about the indexes. Then, the individual models are developed to estimate the interval efficiencies of individual units under the condition that the team indexes of units are satisfied. Following the approach proposed above, the structure of the paper is as follows. Section 2 gives some preliminaries; Section 3 establishes the models based on the overall efficiency of all the units; Section 4 develops the models based on the variance of all the units; Section 5 focuses on the boundaries of all the units; in Section 6, the models based on the relationship between units are investigated; and Section 7 provides the conclusions.", "section": "Methodology", "doi": "10.1111/itor.12447", "references": [2006002090, 2011399838, 2022373133, 2051914789, 2061637245, 2078792730, 2116080612, 2136228439, 2141532588, 2145964516, 2153400179, 2163774646, 2179629619, 2298726707, 2474592240, 2554997024]}
{"paragraph": "Fuzzy implications play a key role in fuzzy logic and various applications, including approximate reasoning, fuzzy control, fuzzy relational equations, fuzzy mathematical morphology, image processing, and so on. The overview of many families of fuzzy implications considered with respect to their theoretical or practical aspects are described in the literature. Many authors investigated various methods of constructing fuzzy implications, for instance, S-implications, R-implications and QL-implications. One of the directions of constructing fuzzy implications is to consider an ordinal sum of fuzzy implications after the pattern of the ordinal sum of t-norms. Mesiar and Mesiarová obtained a special class of ordinal sum implications, by residual implications of ordinal sum of t-norms. Su, Xie and Liu introduced a new class of fuzzy implications, by means of the ordinal sum of a family of given implications, which is similar to ordinal sum of t-norms or t-conorms. Moreover, they discussed basic properties of this new class of fuzzy implications, such as neutrality property, consequent boundary, exchange property, and others. Later, other constructions of ordinal sums of fuzzy implications were described. Recently, Baczyński, Drygaś, Król and Mesiar introduced two possibilities of defining ordinal sums of fuzzy implications, based on the constructing of ordinal sum of overlap functions. More precisely, let there be a family of fuzzy implications and a family of pairwise disjoint subintervals of the unit interval with conditions for all indices, where the index set is nonempty finite or countably infinite. Two operations were defined by formulas. Compared with other ordinal sums of fuzzy implications, their biggest merit is that they are fuzzy implications without any additional assumptions on summands. In this paper, we mainly pursue to investigate some properties of the class of ordinal sum implications constructed using the first method. Distributivity between two operations is a property that was already posed many years ago and that is especially important in the framework of logical connection. In classical logic, distributivity of a binary operator over another determines the underlying structure of algebra imposed by these operators. Usually, there are four basic distributivity equations involving implications. Note that the above equivalences are tautologies in classical logic. A new direction of investigations is mainly concerned with distributivity between t-norms and t-conorms. For example, Combs and Andrews attempt to exploit such equations toward eliminating combinatorial rule explosion in fuzzy systems. They refer to one side of the equation as an intersection rule configuration and to its other side as a union rule configuration. Moreover, some discussions appeared and most of them believed that it is necessary to theoretically investigate this tautology before employing it. The general forms of these equations have a role in lossless rule reduction for control and expert systems in fuzzy logic. Recently, Trillas and Alsina investigated the conditions under which such distributivity holds for the R-, S-, and QL-implications, respectively. Some authors consider other equations hold for R-, S-, and QL-implications, respectively. Baczyński and Jayaram studied the distributivity of Yager's f-implications over t-norms and t-conorms. Liu studied the distributivity of S-implications over t-norms and t-conorms. So far, it has attracted many authors to investigate various distributive equations. In this paper, we shall study the distributivity of the class of ordinal sum implications constructed using the first method over t-norms and t-conorms. The paper is organized as follows. Basic concepts and results are recalled, which will be used in the paper. Relations between the class of ordinal sum implications constructed using the first method and other known classes of implications are discussed. The distributivity equations are investigated under this new class of ordinal sum implications. The final section is the conclusions.", "section": "Introduction", "doi": "10.1016/j.fss.2019.01.002", "references": [1914321120, 2011601021, 2025021415, 2025596819, 2033348131, 2041486812, 2068775019, 2084440709, 2099647936, 2101036899, 2109032341, 2113289097, 2114568192, 2155485759, 2159870859, 2162450741, 2162676707, 2163737009, 2181799696, 2238357761, 2344918565, 2476082904, 2491118651, 2749788382, 2753199002]}
{"paragraph": "Iron and steel making is the core of basic industries in a country, supplying the major components for constructions, shipbuildings, and automobiles. It plays an important role in fuel and material cost, making up 10% of the total energy consumption around the world. Ironmaking is the most critical procedure in steel production process, in which blast furnace (BF) with a visualization in Fig. 1 accounts for 65% of comprehensive energy consumption. Therefore, little improvement on raw material performance and operation level of BF can contribute much to energy saving and environmental protection. The quality of coke and ore, together with many process parameters, has a significant impact on the BF. Currently, most of the ironmaking plants still depend on experience control that is of low operating efficiency, which not only consumes extra costs, but also affects the follow-up procedures. Without proper detection or measurement methods, it is impossible to establish an accurate model to capture BF internal status based on ironmaking mechanism. Data-driven modeling, without focusing on detailed physical or chemical transformation, exhibits great potential in modeling the ironmaking process. Machine learning methods such as support vector machine, random forest, and neural network have been successfully applied to BF modeling, mostly estimating the iron quality or internal temperature field distribution. Even though deep learning is deeply entrenched in computer vision, speech recognition, and natural language processing, it has seldom been used in process industry due to the complexity in robustness and time lag. Among the variety of deep neural networks, the RNN exhibits outstanding performance in language recognition due to its advantage in storing history information, which corresponds to the characteristics of continuity and large inertia in BF. To avoid either gradient vanishing or explosion due to long-term dependency in RNNs, an intermediate cell called long short-term memory (LSTM) is added in the hidden layer to filter residual information. By condensing three gates of LSTM into two and mixing the cell state with the hidden layer, GRU gets more applications due to its simple structure and high efficiency. Aside from modeling methods, a direct search method is also crucial in solving optimization problems. Traditional optimization algorithms, like Powell's conjugate direction method, Monte Carlo tree search, and linear programming theory, mainly focus on the convex optimization issues that exist at a certain global-optimum. Modern intelligent optimization algorithm, as a supplement or extension, is always used to deal with unstructured cases such as multimodal functions. In a variety of intelligent algorithms, the genetic algorithm (GA) is a kind of evolutionary algorithm inspired by the process of natural selection that consists of population initialization, fitness evaluation, and operation process. Relying on the elitist preservation and three operators including selection, mutation, and crossover, the GAs possess the ability to generate high-quality solutions to search for optimum state under different conditions. In this paper, we introduce a hybrid model for objects optimization based on an improved deep learning method and a derived self-adaptive GA. The hybrid model allows automatic searching for optimal production indices with its corresponding requirements of raw materials and operation parameters. Remarkably, the unique contributions included in this manuscript can be briefly summarized in the following outline. Through modifying the unit in RNN and realizing self-adaptive population in GA, the proposed dGRU-RNN and SAPGA are more computationally efficient with less number of parameters and are able to meet the real-time requirement of the industry system. It is shown that the modification of two typical algorithms leads to substantially better performance, compared to their original prototypes and other variants on numerical tests as well as actual data validation. The conjunction of GAs and RNNs is innovative in BF optimization, leading to a great convenience in modeling and optimizing without focusing on the practical manufacturing process. Conventional optimization approaches such as mathematical programming, mechanism analysis, and artificial experience are limited in BF due to the lack of effective detection methods and clear ironmaking mechanism. The BF production indices can be optimized by one hybrid model throughout various different working conditions, which only needs to retrain the neural networks when operating mode switches. The incumbent optimization techniques, on the contrary, have to switch models under different kinds of working conditions. After a brief introduction, the rest of this paper is organized as follows. Section II consists of explanation of the proposed model and comparison with their original prototypes. Section III illustrates the experiments and results corresponding to the proposed techniques. Ultimately, Section IV concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2903770", "references": [179875071, 1499332833, 1847088711, 1924770834, 1965969360, 1990771923, 2043382734, 2064675550, 2105593825, 2152551290, 2169920722, 2339071283, 2540224322, 2557283755, 2599886073, 2612688942, 2753713840, 2767358759, 2794370109, 2963047498, 2964060510]}
{"paragraph": "Recent years have seen a growing interest in text mining applications aimed at uncovering public opinions and social trends. This is partially driven by the fact that the Web now holds a large number of opinionated documents, such as opinion pieces and product reviews, to name a few. An additional driver is that the language one uses to express opinion indicates one’s subjective viewpoints; this language can be used to understand and cluster people’s opinion based on belief, experience or emotion, rather than facts. Text mining methods are therefore desired for facilitating automatic discovery of subjective viewpoints present in such large amounts of opinionated documents. We define contrastive opinion mining as the discovery of opinion perspectives held by different individuals or groups, which are related to a given topic but opposite in terms of sentiments. The usefulness of contrastive opinion mining spans across many applications such as discovering the public’s stand on major socio-political events, observing heated debates over controversial issues where different sides defend their viewpoints with contrasting statements, as well as mining issues from product review sites that can serve as an important source of feedback to businesses. For example, there were heated discussions on the web about whether one should install the Mac OS X El Capitan soon after it was released to the public. People express highly controversial opinions after upgrading to the system, i.e., some experienced pleasant performance improvements while others witnessed a significant drop in speed. Considering the huge number of reviews available, it is highly desirable to acquire an overview of the major viewpoints from large amounts of text data automatically, allowing one to convert data into actionable knowledge and then make decisions in a timely manner. Recently, mining contrastive opinions has been applied to a variety of tasks, including analysing editorial differences between multiple media sources, extracting contrastive viewpoints from political debates, as well as examining cross-cultural differences with respect to language use on social media. However, these existing studies on contrastive opinion mining rely on an assumption that input data containing different opinion perspectives are separated into different collections beforehand. While this assumption might hold for some practical scenarios, quite often one needs to analyse contrastive opinion contained in a single collection such as text of streaming social media data. In addition, it is natural that debates on some topics are more prominent or controversial than others, which indicates the importance of the topic. Therefore, being able to understand the prominence of a topic and the levels of contrastiveness of sentiment will enable one to quickly identify information that needs immediate attention. Finally, existing models generally interpret contrastive opinions solely in terms of the extracted topic words, which are not adequate to help us accurately understand the opinions presented in the corpus since the topic words only express shallow semantics. Therefore, it would be illuminating to consider the dependency between the sentences in the corpus and the topic of discussion in order to better understand and interpret contrastive opinion. The representative sentences also help to clarify the coherence of the extracted topics. In this paper, we address the aforementioned issues by proposing a novel unified latent variable model for mining contrastive opinion from text collections. The proposed model makes several distinctive contributions, for it: can be trained flexibly under weakly-supervised or fully-supervised settings, depending on the type of supervision information available; automatically discovers contrastive opinion from both single and multiple text collections; quantifies the strength of opinion contrastiveness towards the topic of interest, which could allow one to swiftly flag issues that require immediate attention; and extracts sentences relevant to topics by adopting a strategy that makes sentiment-bearing topics clearer to users. Extensive experimental results show that our model outperforms several baseline models in terms of extracting coherent and distinctive sentiment-bearing topics which express contrastive opinions. The top sentences extracted by our approach further help us effectively understand and interpret sentiment-bearing topics. Lastly, we evaluate the performance of our model in the supervised sentiment and topic classification task, in which our model outperforms or gives comparable performance to five strong supervised baselines. The rest of the paper is organised as follows. We first review the related work in Section 2, followed by detailed discussion of our model in Section 3. Section 4 and Section 5 present the experimental setup and results, respectively. Finally we conclude the paper in Section 6.", "section": "Methodology", "doi": "10.1007/s11704-018-7073-5", "references": [137786571, 1054465322, 1536494821, 1569041844, 1570776870, 1969486090, 2018650704, 2023907142, 2045818565, 2061806977, 2062589306, 2098062695, 2102498033, 2103587173, 2106224734, 2108420397, 2112247328, 2123751690, 2126200466, 2130339025, 2147946282, 2159426623, 2250879034, 2251582277, 2329121264, 2463177446, 2573269214, 2620851716, 2774886230, 2949541494]}
{"paragraph": "Recent years have witnessed an increasing research interest in the networked control systems (NCSs) whose components are interconnected via the communication network. Compared with traditional control systems, the NCSs exhibit the advantages in lower installation/maintenance cost, higher flexibility, and easier interoperability. Therefore, it is not surprising that the NCSs have been widely applied in a number of areas including, but are not limited to, power grids, process control, industrial monitoring systems, and intelligent transportation systems. Notice that, for large-scale NCSs, it is difficult or even impossible to have central coordination from a certain central point. In reality, it is often the case that the controllers only pursue their own interest by operating in a noncooperative manner, that is, each controller aims at optimizing its individual cost function. As a result, instead of the commonly used social/pareto optimality, the Nash equilibrium (NE) has to be employed in the noncooperative control to quantify the control performance. As such, the study of noncooperative optimal control (OC) problem has drawn a great deal of research attention in large-scale NCSs when certain cost functions are of a concern. In practice, as the scale of the NCSs grows, the amount of data transmitted through the networks tends to be larger and the transmission becomes more frequent by consuming more network bandwidth. As a result, if a large number of nodes obtain the access to the transmission network simultaneously, the network might suffer from insupportable burden, which gives rise to undesirable network-induced phenomena such as the time-delay, packet disorder, and packet dropout. Very recently, to alleviate the transmission burden, a variety of communication protocols have been implemented in the NCSs, among which the Round-Robin protocol (RRP) has been widely used in industry. Under the RRP scheduling, only one node is allowed to get access to the network at each time instant and all nodes occupy the shared network according to a fixed circular order. Nonetheless, to the best of our knowledge, the RRP has not been taken into account in the context of noncooperative OC problem, and this constitutes the first motivation of our paper. On another research frontier, as compared with traditional time-triggered (clock driven) mechanisms, the event-triggered mechanism has been well-recognized to be a much more effective strategy in easing the transmission burden of the networks by reducing unnecessary information exchange. More specifically, the transmission is executed only when the error between the present signal and the one at the last moment exceeds a preset threshold (i.e., the triggered condition is satisfied). As such, the transmission frequency of the redundant information can be effectively reduced while preserving the control performance. Therefore, the event-triggered mechanism has been adopted in the networked control/filtering problems. In spite of their distinct merits, the event-triggered mechanism does complicate the controller design to a great degree. It is, therefore, another motivation of this paper to tackle such event-triggering-induced complexity in the noncooperative OC problems. In the LFC system, the communication networks have been embedded so as to increase their intelligence and scalability. However, the inherent bandwidth limitations of the networks also induce undesirable phenomena into the LFC system such as overlong time-delays, packet dropout, and packet disorder. These unwanted network-induced phenomena should be reflected in the design of the LFC scheme. Otherwise, it would lead to unpredictable catastrophic consequences. As a positive/active way to schedule the communication network in the LFC system, the event-triggered-based LFC schemes have also been dealt with. It is also worth noting that the modern large-scale LFC systems are difficult to coordinate and hence the controllers exhibit noncooperative behaviors. Therefore, the study of the noncooperative LFC schemes with consideration of advanced network scheduling mechanisms becomes significant. Summarizing the previous discussions, in this paper, we target at designing the noncooperative LFC strategy for discrete LFC systems subject to the RRP and event-triggered mechanism. The main contributions are summarized as follows: 1) the underlying LFC system model accounts for three kinds of engineering-oriented complexities comprising the event-triggered mechanism, the RRP scheduling, and the exogenous nonlinearities; 2) a set of noncooperative OC strategies is designed to minimize a certain upper bound of the individual cost function of each controller; and 3) easy-to-check sufficient conditions are established to guarantee the asymptotic boundedness of the derived upper bound of the cost function over the infinite horizon. The rest of this paper is organized as follows. In Section II, the studied system model and the design goals are provided, respectively. In Section III, the noncooperative OC strategies are derived. Then, based on the obtained control strategies, the sufficient condition is given so as to guarantee the boundedness of the upper bound of the cost function over the infinite horizon. In Section IV, a numerical example on LFC systems is provided to verify the validity of the proposed methodology. Finally, Section V conclude this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2903772", "references": [96178029, 1908712309, 1968571787, 1979038365, 2014590430, 2116400466, 2117588008, 2312316488, 2315448583, 2343705368, 2400682472, 2515370279, 2588899480, 2598078237, 2619461977, 2738765409, 2768627897, 2807762526, 2811065464, 2811086794, 2811101845]}
{"paragraph": "In the past years, a fast algorithm called Extreme Learning Machine (ELM) was proposed by Huang, Zhu, Siew, 2004, Huang, Zhu, Siew, 2006b. It is used to train Single Layer Feedforward Networks (SLFN), as shown in Fig. 1, where part of the network parameters (ai and νi) are randomly generated, and the remaining (βi) are found using labeled data and a closed-form solution. Due to its simplicity and speed, ELM gained popularity and has been applied in a wide range of applications such as computer vision and time series analysis, achieving good performances, generally better than classifiers such as Support Vector Machines (SVMs). Variants of ELM were also proposed, turning possible to deal with sequential data (where new samples arrive over time) using Online Sequential ELM (OS-ELM), or increasing the SLFN hidden node number, using Incremental ELM (I-ELM). In ELM and OS-ELM, the number of nodes in the hidden layer needs to be well-chosen to avoid overfitting and underfitting. To overcome this problem, an ELM-based algorithm using ridge regression was proposed by Deng et al.. Although it achieves good results, the resulting network is dense and might suffer from memory and computing limitations. Martínez-Martínez et al. extended Deng et al. work, proposing an algorithm named Regularized ELM (R-ELM), which can select an architecture based on the problem. By using the Elastic Net theory, R-ELM can prune some hidden nodes when dealing with a one-dimensional output. The aforementioned methods were defined considering only one output node, with optimization problems minimizing vector norms, but they can be adapted to multi-dimensional tasks by considering each output separately. To deal with these tasks, Generalized Regularized ELM (GR-ELM) was proposed by Inaba, Salles, Perron, and Caporossi, which generalizes R-ELM, using matrix norms in its objective function, replacing ℓ2 and ℓ1 vector norms by Frobenius and ℓ2,1 matrix norms, respectively. One common characteristic of these methods is the use of ℓ2 or Frobenius norm of the prediction error in each objective function. As pointed by Zhang and Luo, these approaches have some drawbacks when the application suffers from outliers, which is common in real-world tasks, since the model can be biased towards them. To deal with outliers, Outlier Robust ELM (OR-ELM) was proposed by Zhang and Luo, which considers the ℓ1 norm of the prediction error in its objective function, achieving better results in the presence of outliers in regression tasks, when compared with other algorithms. However, OR-ELM was also defined only to one-dimensional outputs. In this paper, we generalize the OR-ELM to multi-target regression problems. We use the same model considered in GR-ELM, replacing Frobenius norm by ℓ2,1 norm of the prediction error, which can be interpreted as an extension of the ℓ1 vector norm. When considering outputs with only one dimension and using only the ridge penalty, our method is the same as OR-ELM. We use a three-block Alternating Direction Method of Multipliers (ADMM) to solve our optimization problem, which is a simple but powerful algorithm. We also propose an incremental version of this method, which can increase the number of nodes efficiently if the desired error was not obtained. Our methods were tested in 15 public real-world datasets, which were contaminated with outliers to verify the robustness of them, using the average relative root mean square error (aRRMSE) as metric. We compared our methods with similar algorithms based on ELM. The paper is organized as follows. In Section 2, we review some ELM-based algorithms. We describe the proposed methods in Section 3. Experiments and results are presented in Sections 4, and 5 concludes the paper.", "section": "Introduction", "doi": "10.1016/j.eswa.2019.112877", "references": [55768394, 1038736503, 1437416296, 1493968464, 1520795727, 1565746575, 1858022548, 1986096622, 1992841740, 2019797988, 2026131661, 2063333832, 2096987757, 2099579348, 2111072639, 2137130182, 2141695047, 2144807460, 2154852616, 2158054309, 2164278908, 2283039974, 2319794630, 2566928557, 2597701578, 2623079200, 2753528158, 2761517195]}
{"paragraph": "Capacities, also named fuzzy measures, are valuable tools for multi-criteria decision making. Among the current interests in fuzzy measure theory are problems concerning the integrals with respect to capacities. These integrals have mostly been defining for non-negative functions and they have also non-negative values. They have applications in economics, statistics, cooperative game theory, machine learning etc. The capacities which have decomposability or σ-decomposability property with respect to pseudo-operation have been observed by numerous researchers. The ⊕-measure pseudo-additive measure is a special type of non-additive measure and with a couple of operations, pseudo-addition and pseudo-multiplication it is used in the construction of the pseudo-integral. In the concept of bi-capacity was introduced and observed as a generalization of capacity. The bi-capacity is also known under names: bipolar capacity and monotone bi-cooperative game. Based on a bi-capacity the bipolar Choquet integral, the bipolar Shilkret integral and the bipolar Sugeno integral were proposed. The discrete bipolar universal integral, which covers all three mentioned bipolar integrals, was introduced. In terms of the difference of two pseudo-integrals the discrete bipolar pseudo-integral was first introduced and then observed. One of the most studied and used inequality concerning Lebesgue integral is the Jensen's inequality. This inequality has applications in numerous disciplines, especially in probability theory, decision-making, information sciences, optimization theory, mathematical economics, etc. Due to the integrals based on capacities are used extensively, there is a need for the research of this type inequality for those integrals. In the framework of the generalized measure theory the Jensen type inequality related to the Sugeno integral, the Choquet integral, the pseudo-integral and other integrals based on non-additive measure was studied by various authors. The Jensen's inequality for ordered weighted averages, which involves real-valued weights, appears in the literature under the name Jensen-Steffensen's inequality. The focus of this paper is on extensions of the Jensen's integral inequalities to the discrete bipolar pseudo-integral, i.e. we will consider the Jensen-Steffensen-type inequality for the discrete bipolar pseudo-integral. In our knowledge, this paper is the first one dealing with the integral inequalities for the bipolar pseudo-integral and the real-valued functions. The observations concern three cases of the discrete pseudo-integrals based on ⊕-decomposable bi-capacities and involve two symmetric operations: the first case is when a pair of operations is generated by an appropriate generating function, while in the last two cases a symmetric-addition is the symmetric maximum. The remainder of this paper consists of four sections. In Section 2 an overview of symmetric pseudo-addition, symmetric pseudo-multiplication and bi-capacities is given. Section 3 consists of the definition of the discrete pseudo-integral, its properties and some new results used in the rest of this paper. The Jensen type inequality for this integral is considered in Section 4. Finely, some illustrative examples are given. Section 5 contains some concluding remarks.", "section": "Introduction", "doi": "10.1016/j.fss.2019.04.015", "references": [36172825, 132598052, 988900036, 1607161872, 1874567253, 1983059862, 2003452701, 2018696312, 2034770973, 2039561821, 2047241205, 2049286184, 2051231971, 2068984276, 2087171145, 2090414707, 2098367511, 2135933771, 2159326516, 2528613903, 2539429473, 2550301631, 2810261982, 2887095029, 2890380371, 2899323569, 2946905971]}
{"paragraph": "Clean drinking water is a critical resource for the health and well-being of all humans. However, water quality is easily deteriorated because of a malicious attack or accidental incident. For example, in 2000, an outbreak of waterborne disease epidemic in Walkerton, Ontario, Canada affected 2300 people because of exposure to contaminated drinking water. The poisoning of water supply affected 71 people in Ruyang city, Henan Province, China in 2007. According to the Global Risks Report, water crises, i.e., water shortage and contamination, have consistently featured among the top-ranked global risks. Deploying water quality sensor networks is considered as a promising approach for detecting contamination incidents in a drinking water distribution system. Owing to the high cost of quality sensors and the high cost of installation and maintenance, optimization methods have been developed to help utility companies choose optimal locations for the placement of water quality sensors aiming at effective detection. Thus far, the problem of determining the placement of water quality sensors and identifying the contaminant sources within WDS for enhancing the monitoring and security capabilities has been widely investigated over the last decade by utility companies and research community in the field of WDSs. The mainstream method is to establish an optimization model of water quality sensor placement, and subsequently apply single-objective and multi-objective optimization algorithms to obtain the optimal layout of sensors. The overall goal of deploying water quality sensors is to monitor potential pollution incidents in order to minimize the risk of contamination intrusion. At present, there are several competing design objectives for sensor placement. The widely accepted objective is the minimization of public health impact, which includes the number of individuals exposed to a contaminant, extent of contamination in a water supply network, detection time, and percentage of contamination incidents not detected. Although a few significant optimization objectives have been proposed, to the best of our knowledge, a many-objective optimization algorithm for sensor placement in WDS has not been proposed yet. Herein, for the first time, we attempt to propose a modified NSGA-III for the deployment of water quality sensors. By employing two typical water distribution networks from the battle of the water sensor networks, comprehensive experiments are conducted and sensitivity analyses with different sizes of WDN are explored. Specifically, the contributions of our work are as follows: We investigate the optimization model of sensor placement in WDS, theoretically analyze this problem, and prove that it is NP-hard. Inspired by the connectivity degree in graphics, we propose a modified NSGA-III for many-objective sensor placement optimization. By employing two typical WDNs from the BWSN, the benefits of the proposed algorithm are illustrated, and some critical factors are also addressed. The rest of the paper is organized as follows. Section 2 reviews the state-of-art works from the aspects of single- and multi-objective optimization. Section 3 focuses on the problem formulation and theoretical analysis. Section 4 presents the modified algorithm. Section 5 discusses the simulation results. Section 6 concludes the paper and presents some issues that should be further investigated.", "section": "Introduction", "doi": "10.1016/j.ins.2018.06.055", "references": [61339854, 1418108976, 1968219458, 1991356461, 1996228877, 2022485595, 2152045847, 2344916885, 2501190883, 2521918431, 2546584549, 2558150887, 2560305685, 2570235502, 2589278106, 2598797402, 2617298469, 2648950690, 2739365735, 2740758288, 2757662287, 2769746177, 2790368141, 2802576758]}
{"paragraph": "Gestures are a ubiquitous means of human-to-human communication and have the potential to play a similarly vital role in human–computer interaction. Hand gestures can act as a bridge to connect human intention to smart hardware and software systems for a variety of real-life applications. For example, amputees can utilize gesture intention to improve prosthesis control and the hearing impaired can potentially use automated gesture recognition to enable communication with the unimpaired who do not know sign language. Beyond medical applications, gesture recognition has also been used to enable in-vehicle gestural interfaces in increasingly intelligent cars and for robot and quadrotor teleoperation. Virtual and augmented reality could also be significantly enhanced through robust and effective hand gesture recognition. The most widely used approach to wearable hand gesture recognition is by sensing dynamic muscle characteristics in the forearm and mapping them to hand and finger postures. Surface electromyography is commonly used to estimate muscle activation levels and classify hand gestures through a commercial device like the Myo armband or various research prototypes. sEMG can also be combined with other sensing modalities to improve classification accuracy. For example, the spatial resolution for classification can be improved by combining sEMG with near-infrared spectroscopy which monitors muscle oxygenation and perfusion or with mechanomyography which detects low-frequency muscle mechanical vibrations. sEMG has been combined with accelerometry to identify kinematic information in the hand and arm for classification. Ultrasound imaging has also been used to detect forearm muscle morphology for hand gesture recognition. The wrist is another location targeted for wearable hand gesture recognition, though it is challenging to use traditional sEMG approaches because there is significantly less muscle tissue than the forearm and is instead composed mainly of tendons and bones. Nevertheless, sEMG combined with accelerometry at the wrist has been used to effectively classify hand and surface pressing gestures. Other approaches include employing barometric sensors or force sensing resistors to estimate pressure and morphology profile changes around the wrist for various hand gestures. Capacitance sensing has been proposed to capture small skin deformations on the user's wrist with deformable and battery-free wristbands. Electrical contact resistance change due to the deformation of the wrist has been measured. Capacitance changes via capacitance sensors at the wrist for different hand gestures have also been estimated. Although forearm- and wrist-based approaches can provide a portable and intuitive interface for gesture recognition, because of the complex relationship between finger movements and forearm muscle or wrist tendon activation, the accuracy of hand gestures involving higher resolution finger angles is inherently limited. Thus, sensors placed directly on the fingers can significantly improve hand gesture recognition accuracy. Commercial data gloves are often used, such as DataGlove family and CyberGlove, to measure finger flexion angles. However, data gloves typically cover almost the entire hand, thus, restricting mechanoreceptive sensations along the fingers and in the fingertips and can be large and bulky making them difficult or impossible for some patients to comfortably wear. Epidermal electronics and artificial skin sensors provide another approach for estimating finger postures. Conductive bond has been utilized and an efficient digital fabrication approach to create a custom skin sensor to recognize various gestures has been proposed. Liquid metal has been used to create wearable soft skin to classify hand gestures and carbon grease has been used for soft sensing to recognize American Sign Language 0–9 gestures. While placing sensors directly on the fingers can improve classification accuracy, it can also be uncomfortable and hinder finger movement, prohibiting widespread adoption. Skin deformation on the back of the hand has also been captured for hand gesture recognition. Photo-reflective sensors have been employed to measure the distance between the device and skin, though thickness and skin color can affect performance. Strain sensors have been utilized on the back of the hand to directly measure skin strains; however, previous studies were limited in that they only investigated limited local sensor placement and the strain sensor was used for a rigid body and was thus, not suitable for soft skin strain measurement. Currently, precise strains on the back of the hand have not been systematically investigated and there is yet to be a completely soft, stretchable sensing system on the back of the hand for hand gesture classification. The back of the hand is well suited for sensing hand gestures. Although the back of the hand is not directly used during finger manipulation, the strain patterns of the skin on the back of the hand dynamically change as the fingers move. Physiologically, the hand is often used for grasping and sensing during which the fingers and palm work in concert for grasping while fingertip mechanoreceptors sense a variety of tactile stimulations including texture, temperature, pressure, vibration, and pain. In contrast, the back of the hand plays a lesser role in grasping and functional sensing. Therefore, with little interference in sensing and grasping, placing sensors on the back of the hand may be more readily acceptable in practical applications than placing sensors directly on the fingers or fingertips and solves the problem of patient's general inability to wear bulky data gloves. Also, an e-skin patch with soft sensors on the back of the hand can potentially be more easily generalized than other approaches, such as a data glove, to account for different hand sizes and to distinguish between left and right hands. A sensor patch could potentially be made irrespective of hand size and the left and right sides could be distinguished via software algorithms. To achieve hand gesture recognition via sensing on the back of the hand, two key challenges must be overcome. Skin strain complexities. In contrast with skin covering the fingers and finger joints in which skin strains are isolated to the movement of each given finger, skin strains across the back of the hand are highly coupled among several different finger movements. Thus, it is necessary to characterize back-of-the-hand skin strain patterns related to various finger movements to optimize sensor placement. Sensor system design and manufacturing. Skin strains on the back of the hand are relatively small as compared with skin strains across the finger joints, and thus, the sensor system must be highly sensitive to the relatively small strains. In addition, the system must be designed to be small, light, and comfortable to maximize user comfort and increase the possibility of long-term compliance. The purpose of this paper is to introduce a novel back-of-the-hand gesture recognition approach for hand gesture classification. In contrast with previous approaches that cover the fingers and fingertips, affixing sensors to the skin on the back of hand avoids inhibiting tactile sensations. This paper addresses the inherent design and implementation challenges with an exploratory back-of-the-hand skin strain characterization experiment, a detailed prototype design, detailed algorithm development, and experimental validation. The primary contributions of this work are: detailed skin strain characterization across the skin on the back of the hand for various finger movements and the corresponding optimal soft sensor locations to maximize hand gesture classification accuracy. A novel back-of-the-hand stretchable e-skin patch prototype for hand gesture recognition including soft sensor design, manufacturing, and experimental validation. To the best of our knowledge, this is the first approach combining multiple soft sensors into a stretchable e-skin patch on the back of the hand for gesture recognition.", "section": "Methodology", "doi": "10.1109/TIE.2019.2914621", "references": [1493357981, 1523513542, 1993666393, 2000781869, 2008834805, 2016071976, 2017337590, 2051268319, 2101234009, 2107969104, 2115466366, 2118561568, 2123676049, 2124820153, 2143426320, 2154053567, 2165619603, 2167101736, 2180884865, 2263270445, 2270568955, 2282752563, 2520477879, 2552886112, 2570810937, 2611233583, 2767053645, 2771247593, 2795900623, 2810696633, 2898581654]}
{"paragraph": "When data originate from different but related tasks, it is possible to improve generalization performance by discovering intrinsic relationships among tasks, and multitask learning is applicable in this setting. The multitask learning framework has been successfully applied in many real-world applications: bioinformatics, computer vision, and speech recognition. Tasks can be related in different ways, and various multitask learning techniques have been developed based on different assumptions. One important assumption of multitask learning is that all tasks have shared common structures, and several algorithms have been developed based on this assumption. However, this may be too restrictive in real-world applications due to the reason that some tasks may be less correlated with others. To overcome this limitation, a number of methods have been proposed by adopting different assumptions. For example, some methods assume all tasks are clustered into different disjoint groups, and model parameters for the tasks in the same group should be shared. The approach assumes that the task within each group should lie in a low dimensional subspace. These methods are suitable for the case in which tasks can be easily clustered. On the other hand, imposing a hard partitioning of tasks may lead to some weakly relevant tasks being assigned to the same group, which may lead to unsatisfactory performance. In addition, feature selection is not taken into consideration. Feature selection techniques are an effective way to avoid the limitation due to high-dimensional data. Most feature selection methods are based on the assumption that each task is learned individually. Unlike conventional model learning, multitask feature selection algorithms choose relevant features shared by related tasks. Many multitask feature selection methods are developed based on the assumption that models for all tasks can be jointly learned in the same feature subspace. Note that the objective of this paper is to identify relevant shared features for the related tasks during model learning, which is also called multitask feature learning. For instance, one method proposed a regularization-based multitask feature selection method to select a common set of features for all tasks. Another model assumes that all tasks share some common features and a few task-specific features. In order to identify the task outliers, the robust multitask feature learning model has been developed. However, these assumptions are still too restrictive. For example, in a document classification task, each topic represents a task and each keyword is a feature. Some methods select a subset of keywords for representing all topics. This is not effective since there may be some overlap between related topics, and different topics should be associated with different discriminative keywords. In this paper, we offer an effective solution to the above problem by analyzing the relevance among tasks, and selecting the appropriate features for related tasks. As shown in a figure, the three topics, education, computer, and sciences, are related, and similar keywords are selected for representing them. In this paper, we consider a more flexible multitask feature selection or learning setting: tasks are jointly learned based on a similarity graph. Different from existing clustering methods in which a hard partitioning is imposed on the tasks, a graph is more suitable for representing the pairwise relevance among tasks. Moreover, the proposed model is able to identify task-specific features for each task. Inspired by previous models, the model matrix is decomposed into two components, and different regularization terms are applied to these components. Specifically, a graph-guided regularization term and a sparsity-inducing regularization term are used to constrain the sparsity of the solution at both the task level and the feature level. A variant of the smooth proximal gradient method is proposed to solve the corresponding optimization problem. The proposed approach is able to simultaneously capture the structure of tasks, and select discriminative features for related tasks. As a result, related tasks share similar features. On the other hand, a number of task-specific features can also be selected for each task. As a result, the proposed method can be applied to high-dimensional data for exploring feature-task relationships. The main contributions of this paper include the following. A flexible multitask feature selection approach which is able to simultaneously capture the task grouping structure, and perform feature selection by adopting a graph-guided regularization framework. A variant of the smooth proximal gradient method which was developed to effectively solve the related optimization problem. A theoretical analysis of the proposed model is also provided. An evaluation of the proposed method on several benchmark multitask data sets has been performed, and experimental results verify the effectiveness and demonstrate the superiority of the proposed model when compared with the standard multitask learning methods. The rest of this paper is organized as follows. In Section II, we review related multitask learning approaches. We introduce the proposed robust multitask feature selection with graph-clustered feature sharing in Section III and develop smooth proximal gradient optimization method in Section IV. We provide a theoretical analysis in Section V. Experimental results on synthetic and real-world datasets are described in Section VI. In Section VII, we draw the conclusion of this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864107", "references": [123755266, 577139289, 1939941161, 1942758450, 1970304186, 1977617632, 1999140058, 2005292390, 2018096278, 2031250362, 2032612424, 2065180801, 2080995863, 2097451239, 2106115875, 2107077531, 2110332320, 2110994494, 2118099552, 2119387367, 2119479037, 2135069544, 2137401668, 2143104527, 2144567071, 2151386286, 2153086202, 2156200251, 2166721725, 2186054958, 2188379175, 2244253939, 2322020277, 2561587488, 2914746235]}
{"paragraph": "Recently, the multiagent systems have drawn much attention due to its wide application. A general multiagent system consists of many agents, where each agent can represent different individual with its own dynamics. Despite the different behaviors of each agent, agents can work together efficiently, which yields the consensus problem in numerous issues such as flocking, formation, cooperation, and so on. Moreover, the consensus problem of multiagent systems is one of the fundamental and significant topics. In multiagent systems, the consensus is achieved by a process that multiple agents share information with their own neighbors iteratively and gradually reach an agreement. Existing researches mainly focused on the consensus problem based on integrator, first-order and second-order dynamics, where the centralized and distributed consensus problems were consider in both undirected and directed network topologies. In addition to that, because nonlinear phenomena is everywhere in real-world, many studies were done with the nonlinear dynamics of multiagent systems. Moreover, impulsive control systems had been paid more attention in recent years in that an evolution of a system may suddenly change in real circumstances. To get closed to reality, impulsive control method is introduced because of its high efficiency with high-robustness and low-cost. Impulsive control had been widely applied to synchronization and consensus problems of kinds of complex networks over last decade. Consensus problem of multiagent systems with each agent described by impulsive dynamics in directed communication networks topology was investigated. By impulsive control method, several conditions were built for synchronization and consensus with switching topology. Consensus criteria of second-order multiagent systems were delivered based on so-called pulse-modulated intermittent control with nonlinear velocity dynamics, which can be reduced to the consensus problem under sampled control. Some papers also proposed the impulsive control method applied to discrete-time systems. A framework of discrete-time system under discrete-time impulsive control with time-delays was introduced. The definition of the jumping operator is different from continuous time system. The state impulsively changes at the next step of every impulsive instant. Moreover, recently such method had been applied to synchronization of complex networks and consensus of multiagent systems. Consider the constriction of communication among agents, the information required to be delivered cannot always be real-valued data that demands higher accuracy and more resources than quantized data. In such situation, communication of networks using quantized data seems advanced. The quantized consensus was proposed and asymptotic convergence of the multiagent systems can be achieved with integer-valued states. The consensus was guaranteed by the proposed static uniform quantizers with an infinite number of quantization levels. For more application to real circumstance, the quantized consensus of multiagent systems with nonlinear dynamics based on leader–follower case was also investigated. Furthermore, the impulsive control method was also applied to both static consensus and dynamical consensus problems, where the impulsive interval is the key to achieve quantized consensus. However, the bandwidth of the communication channel is sometimes limited, which may lead to much loss of important information. To avoid this, previous researches considered that the quantized data needs to be encoded to guarantee that no information will be distorted badly or totally lost. The consensus of multiagent system was achieved by a quantized-observer-based encoding–decoding with limited bandwidth communication as well as the case with fixed quantization level. The leaderless and leader–follower consensus of a special nth-order integrator systems with an exponential convergence rate were further discussed. A high efficient event-triggered scheme with limited communication bandwidth was considered for the first-order discrete-time multiagent system in directed digital networks, where the average-consensus can be achieved as same as general protocols. Seeing from previous researches, the quantized consensus problem of multiagent systems with limited bandwidth communication seems promising due to its wide applications. Many cases from different angels such as dynamics including linear and nonlinear cases, control methods including general protocol, event-triggered schemes, and impulsive protocol as well as the cases with unlimited and limited bandwidth communication were investigated, respectively. But to our best knowledge, there is no results on the consensus problem considered from all the above aspects simultaneously for now. So in this paper, we investigate such problem of nonlinear discrete-time dynamics with limited bandwidth communication via impulsive protocol based on encoding–decoding in directed networks topology. Furthermore, different from previous researches, the encoder and decoder only demand the information at impulsive instants. This paper aims at an investigation into the impulsive consensus of nonlinear multiagent systems with limited bandwidth communication by designing suitable quantizers, encoders, decoders, and impulsive protocol. An encoding–decoding scheme for such problem is proposed for dealing with quantized data. Based on nonlinear control system theory and impulsive control theory, several fundamental consensus conditions and criteria are obtained. Furthermore, the exponential convergence rate is also investigated. The organization of this paper is describe as follows. Fundamental preliminaries of graph theory are introduced in Section I. The design of encoders and decoders as well as the error systems and the according equivalent systems via impulsive protocol are presented in Section III. The impulsive consensus of multiagent systems with limited band width communication are analyzed and the sufficient conditions are obtained in Section IV. Furthermore, the exponential convergence rate is also discussed in Section IV. Numerical examples that strongly support the effectiveness of the theoretical results are presented in Section V. The conclusion of this paper is presented in Section VI.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2863108", "references": [1997505876, 2009220211, 2020274357, 2029274891, 2039993570, 2049119205, 2058766213, 2064991733, 2085172627, 2085213458, 2088919874, 2098127225, 2098760195, 2113584844, 2128928412, 2135529569, 2165744313, 2179502718, 2279241005, 2321207147, 2465203691, 2501046130, 2585867354, 2625546361, 2790395807, 2792338060, 2912291331]}
{"paragraph": "Graphs are among the most powerful and expressive data structures. They have been used to model complex systems of objects and their relationships in many real-world applications including the world wide web, social networks, transportation networks, biological networks and systems. Graph clustering is an important research topic in graph mining and analysis. It consists of partitioning the nodes of an input graph into several groups satisfying some predefined fitness criteria. Typical applications of graph clustering include the identification of functional complexes in protein-protein interaction networks and the detection of communities in social networks. The abundance of data generation in many real-world applications has led to many kinds of information becoming available. This information could be used to annotate graph nodes and edges. The annotation of multiple attributes over the graph elements makes it possible to consider multiple aspects during graph clustering, rendering this approach more robust. Biological networks are among the most important domains of application in which various types of data are available in massive amounts in online databases. Protein–Protein Interaction networks present an example of such biological networks for which huge amounts of heterogeneous data are freely available online, providing a wide range of information about the proteins making up the network nodes including their structures, the genes that encode them and the DNA sequences of these genes. This information may also relate to gene ontology, gene expression levels or genetic mutations. The consideration of all attributes together in a single fitness function results in all the attributes being given equal weights in the clustering which is not appropriate in many real-world applications. Furthermore, some attributes may be outweighed by others with a higher variance, biasing the clustering towards a particular aspect of the data, particularly in cases in which multiple attributes are correlated. This problem can be formalized more appropriately by defining multiple fitness functions separately over different subsets of attributes, such that only those describing the same aspects of the data are considered together in the same function. During the clustering, all the fitness functions are optimized, and the output solution is the one that best fits all the functions simultaneously. However, different fitness functions may conflict, making it impossible to improve one without degrading another. In this scenario, multiple optimal solutions are provided with respect to the set of considered fitness functions. Each solution is undominated and the set of all the solutions form the skyline solutions. Approaches for optimizing graph clustering with respect to predefined fitness functions are usually combinatorial and involve evaluating a very large number of candidate solutions. This task is, thus, computationally very expensive, and may even be infeasible for large graphs. Evolutionary computation methods have proved their efficiency in many applications in which it is hard to achieve an optimal solution. They can estimate near-optimal solutions rapidly even for large data, by restraining the combinatorial search space. An ideal graph clustering should generate clusters with a cohesive intra-cluster structure and nodes with homogeneous properties, by balancing structural and attribute similarities. In this paper, we propose a clustering approach for attributed graphs in which each subset of attributes is derived from a particular data source. Our approach looks for skyline graph clustering solutions based on the dominance relationship over a set of fitness functions. Each of these functions is defined over the graph topology or over a particular set of attributes. Our approach does not require the number of clusters in advance, and each skyline clustering solution is optimized with respect to all the fitness functions simultaneously. This makes the graph clustering more robust. As the skyline operator contains multiple optimal solutions, we defined a ranking procedure to provide an order over the undominated graph clustering solutions. We propose a genetic algorithm-based implementation to facilitate the use of our approach, even for large graphs. At a glance our approach can be summed up as follows: first, disjoint powerset systems are constructed from possible combinations of nodes to form potential candidate graph clusters. A set of chromosomes is then constructed with a personalized initialization algorithm that favors the density and connectivity of the initial candidate clustering solutions. Personalized genetic operators are then used on the first population to generate new optimized chromosomes iteratively. These new chromosomes are then evaluated, using multiple objective functions defined over the graph topology and the node and/or edge attributes. Encoding and decoding functions are also defined to map the contents of the chromosomes to their respective sets in the corresponding powerset system. After repeating this procedure for a predefined number of iterations, a set of approximate undominated graph clustering solutions based on the skyline operator is obtained. We show, through a real-world example of a protein-protein interaction network, that our graph clustering approach is efficient, and we demonstrate how the integration of attributes from multiple data sources makes it possible to obtain a more robust clustering than by considering the graph topology alone. The paper will be organized as follows. Section 2 presents the related works. Sections 3 and 4 define the preliminary concepts, the problem formalization and the proposed approach for evolutionary mining of skyline graph clusters of attributed graphs. In Section 5, we present the case study of the clustering of a large human protein-protein interaction network enriched with large sets of heterogeneous cancer associated attributes. In Section 6, we draw our conclusions.", "section": "Related Work", "doi": "10.1016/j.ins.2018.09.053", "references": [82500914, 1673310716, 1975680103, 1993530334, 2005180870, 2011587419, 2030006812, 2033590892, 2042851051, 2046491030, 2049153008, 2049864887, 2108814382, 2116798163, 2121443461, 2124738517, 2127048411, 2151554678, 2165891030, 2167115106, 2167159964, 2169706457, 2170188482, 2176433868, 2560757461, 2781583846, 2783806899]}
{"paragraph": "Products of stochastic matrices is closely related to the ergodicity of Markov chains as well as the convergence properties of the averaging dynamics driven by them such as distributed optimization, distributed control of robotic networks, and study of opinion dynamics in social networks, etc. The notable works in the first domain can be traced back to those of Hajnal et al. and Wolfowitz more than half a century ago. Hajnal and Bartlett investigated the weak ergodicity of nonhomogeneous Markov chains and proved an inequality named after him which relates the convergence of the products to the scramblingness of the involved stochastic matrices. Wolfowitz investigated the products of stochastic, indecomposable, and aperiodic matrices and proved a sufficient condition for their convergence. The second domain has been started by earlier work and the seminal work of Tsitsiklis which can be formulated under the framework of consensus problem in networks of multiagent systems. Motivated by the broad applications of multiagent systems and the extensive investigation of consensus algorithms, products of stochastic matrices has been an active research area for many years. Various conditions for the convergence of the products have been proposed and proved. In particular, many important results on products of random stochastic matrices have been obtained in the past decade. A necessary and sufficient condition for almost sure convergence of products of independently and identically distributed stochastic matrices, namely, a directed spanning tree in the expectation of the communication graphs, has been derived. This result is generalized to stationary and ergodic sequences and further generalized to more general adapted sequences. From a different perspective, others characterized the convergence properties of independent random stochastic matrices in terms of infinite flow properties in a series of works. These works are further generalized based on a recent extension of the classical Kolmogorov–Doeblin decomposition-separation theorem to inhomogeneous Markov chains. Despite all these important progress, a fundamental limitation in the product theory of stochastic matrices is that it only admits non-negative entries, thus confining its applications in consensus analysis to networks with only non-negative coupling coefficients. Yet there are a variety of situations in which arbitrary coupling weights may arise. For example, when there are two opposite kinds of interactions between the agents, such as the friendly/hostile relationship between individuals in social networks and the activation/inhibition effect between genes in biological systems, negative weights can be used to model the disagreement tendency between the interacting agents. Besides, negative coupling weights may also occur in some auxiliary systems which arise in the process of theoretical analysis. However, due to the technicalities involved, to the best of our knowledge, only very few works have been done on networks with general coupling coefficients. Some work proved a sufficient condition for almost sure consensus among agents over independent identically distributed periodically switching communication graphs. Other work proved some sufficient conditions for both discrete-time and continuous-time consensus networks with general stochastic switching topologies and arbitrary coupling weights. Recently, convergence rates have been estimated for discrete-time consensus in multiagent systems with time-varying delays and general coupling weights. In these works, the coupling structures are described by matrices more general than stochastic ones which admit negative entries or even nonunit but equal row sums. Motivated by these more general matrix classes introduced in these works and the extensive applications of product theory of stochastic matrices, we consider developing new product theories for matrices that are general enough to admit negative entries. This will not only extend the existing results for the products of stochastic matrices but also facilitate the investigation of consensus in networks with arbitrary coupling weights. In this paper, we investigated the products of a class of matrices which can be seen as a generalization of that of stochastic matrices by admitting negative entries while keeping the unit row sum. We established some new results that generalize those from the product theory of stochastic matrices. Particularly, we obtained a generalized version of the classical Hajnal’s inequality. The main contribution of this paper can be summarized as follows: we investigated the products of generalized stochastic matrices and obtained an extended version of the classic Hajnal inequality; based on the general theoretical results, we established a new convergence result for a class of discrete-time consensus algorithms with general coupling coefficients and time-varying delays; and based on the results obtained, we analyzed the convergence properties of a class of continuous-time consensus algorithms with discrete-time communications and controller updates. The rest of this paper is organized as follows. Some basic definitions and results from matrix and graph theories are presented in Section II. Some results for the products of a class of generalized stochastic matrices are obtained in Section III. Based on these results, some convergence results are obtained for a class of discrete-time consensus algorithms with general coupling coefficients and time-varying delays in Section IV. These results are then applied to the analysis of a class of consensus algorithms in networks with continuous-time dynamics but discrete-time communications and controller updates in Section V. This paper is concluded in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868994", "references": [1547358136, 1964647857, 1984259283, 1985343525, 1998059521, 2050592771, 2054856671, 2068866314, 2075175322, 2076041173, 2092697398, 2103554135, 2107396783, 2114403255, 2125668987, 2144408773, 2148995214, 2160413708, 2165744313, 2166132612, 2167183308, 2171153629, 2215764026, 2417709453, 2525747818, 2763354547, 2781512642, 2783384406, 2792799644, 2800777482, 2804514960, 2963490876]}
{"paragraph": "In a Cognitive Radio Network, the accurate spectrum sensing is a challenging task as the Primary Users use different modulation schemes, transmission powers and data rates. This is also because of the interference of the other Secondary Users, variable propagation losses and fading channels. To address these challenges, in a practical wireless channel, the Secondary User can improve the performance of the spectrum sensing by using collaborative or multiple antenna sensing techniques. In addition to wireless channel deficiencies, open access nature of the Cognitive Radio Networks opens up a challenging problem in the CRN policies. Although authors provide a practical, cost-effective solution for enforcing spectrum laws by leveraging the ubiquity and ever-increasing computation resources of mobile devices, one of the main challenges for the reliable spectrum sensing in the CRNs is security and activity of potential malicious users. With this regard, recently, there has been a spur of research work on security issues related to Cognitive Radio Network and spectrum sensing. Among many proposed attack scenarios, two of them are important and recent studies mostly focus on these two cases: Spectrum Sensing Data Falsification attacks and Primary User Emulation attacks. In SSDF case, the incorrect and false local sensing data is shared with other Secondary Users or transmitted to decision center, to deceive them in making the final collaborative decision. In the PUE scenarios which is the interest of this paper, the malicious user prevents other Secondary Users from using the spectrum by emulating the signal characteristics of the rightful Primary User. The Primary User Emulation Attacker problem was first addressed in which two schemes were proposed to detect a PUEA namely Distance Ratio Test and Distance Difference Test. The problem of detection of spectrum misuse behaviors has been investigated by using a multi-hypothesis test to model the spectrum sensing problem with the existence of PUEA. This problem has been studied for practical case against PUEA for Internet of Things applications. In this context, a comprehensive introduction to PUE attacks for IoT, from the attack rationale, detection and defense approaches has been provided. A game theoretical framework has been used to model the PUEA existence in the spectrum and improve the network security by using game-theoretic approaches. However, in these works, the PUEA traffic has not been considered. It is worth noting that the performance of spectrum sensing in a Cognitive Radio Network, considering the existence of the PUEA and its traffic has not been well addressed and studied in previous related works. Actually for PUEA scenario, based on many attack models which so far have been studied, the attackers can be categorized as either stupid or smart attackers. Stupid attackers in order to deceive the Secondary User or decision center, emulate Primary User signal and send such a signal at all times, even when the spectrum is busy. These types of attackers can be caught by the Secondary User or the decision center using some methods such as reputation-based, trusted node assistance or collaborative clustered sensing approaches. Other studies investigate the catching of the PUEA, especially stupid attacker, when it is assumed that the location of primary transmitter is known and employ other schemes such as Time of Arrival, Time Difference of Arrival, Angle of Arrival and Received Signal Strength to discover the location of transmitter. On the other hand as the newer PUEAs, the smarter attackers do not attack necessarily at all the times. They attack to occupy the spectrum in some of the times and in particular as an efficient and intelligent approach, by considering the Primary User traffic. With this regard, only arrival and departure of the Primary User during the Secondary User sensing period has been considered and for this model, the related analytical expressions for both of the detection and false alarm probabilities are obtained. The sensing and throughput trade-off is studied for a target probability of detection. However, it is assumed that Primary User remains constantly either present or absent in the channel during the entire frame duration of the Secondary User. Therefore, the study of sensing and throughput under primary and attacker traffic is of great interest. This is studied only for a Primary User traffic model for an Additive White Gaussian Noise channel. However, the wireless channel fading effect and attacker traffic have not been taken into account in these works. In fact, the random nature of the fading channel may add up to the random arrival and departure of Primary User and attacker, which not only makes the spectrum sensing more difficult, but also significantly reduces the achievable throughput of the secondary link especially if the channel is in deep fade. This trade-off is investigated for fading channels, but none of these works are taking into consideration the Primary User and potential attacker traffic simultaneously. In most similar works, it has been assumed that the attacker is absent or present within the entire sensing period of Secondary User. This conventional model has been investigated, but in this paper, the conventional model, the ability of the smart attacking in Rayleigh flat fading channel is studied and thus when the Primary User is active, the attacker is not present. To the best of our knowledge, the attacker traffic and its impact on the Cognitive Radio Networks in the wireless fading channels, which is important for practical deployment of CRNs, have not been studied in the previous works related to security of the CRNs. In this paper, we consider a Smart Primary User Emulation Attacker which does not make the channel busy all the times by taking into account the attacker and Primary User activity parameters and traffics. It is notable that for an intelligent attacker, it is important to know when to attack and hence the attack time would be a useful knowledge for the PUEA. In practice, an attacker may not misbehave all the times, because if the attackers occupy spectrum or report falsified results in all the times, the attackers will be caught easily by using the mentioned methods. An easy solution to solve this issue, is attacking in the specific time by considering the Primary User activity traffic. So, we consider a scenario where the Secondary User performs detection and transmission over Rayleigh flat fading channels for random arrival and departure of the Primary User and Smart Primary User Emulation Attacker which modeled by a joint and dependent traffic model. For such a scenario, we derive closed form expressions for the average probability of detection and false alarm. Furthermore, by calculating the performance of the Cognitive Radio Network and throughput of Secondary User’s network in the presence of the Smart Primary User Emulation Attacker, the impact of the Primary User and Smart Primary User Emulation Attacker traffic parameters are examined on these indices. The remaining of the paper is organized as follows: Section II introduces the problem setup, the system model and problem formulation. In Section III the Primary User and Smart Primary User Emulation Attacker activity in the spectrum is modeled by a dependent Markov chain and corresponding probabilities are calculated analytically. In Section IV the exact closed-form expressions for the false alarm and detection probabilities for spectrum sensing considering the existence of the Smart Primary User Emulation Attacker is obtained. In the following, the impact of the existence of the Smart Primary User Emulation Attacker on the throughput of the Secondary User’s network is investigated and the effect of the Primary User and Smart Primary User Emulation Attacker traffic parameters are examined. Also launching PUE attacks in the conventional and also smart proposed ways are studied in this section. In Section V, the test rule is proposed based on the Generalized Likelihood Ratio Test to detect legitimate user from smart illegitimate user. In Section VI, we discuss numerical results to show the impacts of the various factors on the performance of the detector and throughput of the Secondary User’s network. Finally, conclusions are drawn in Section VII.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2911168", "references": [1558067883, 1948074959, 1982558500, 1985718260, 1998126734, 2015513948, 2019917261, 2031211320, 2078258085, 2084436032, 2115991477, 2120636900, 2122274174, 2123856000, 2127040151, 2159238968, 2162958263, 2166877981, 2168077425, 2171402290, 2248608259, 2290284287, 2576299952, 2726126858, 2783612791, 2802338095, 2921176247, 2964052684]}
{"paragraph": "Importance measures providing information about the importance of a component or a group of components on the system performance, such as reliability or availability, productivity, safety, or any performance metrics of interest, can help to identify design weaknesses or operation bottlenecks and to suggest optimal modifications for system upgrades and maintenances. In the literature, a large number of importance measures have been developed and successfully applied for various purposes. In risk analyses, importance measures are used in risk-informed decision-making. In reliability engineering, importance measures are used to prioritize components in a system for reliability improvement. Recently, importance measures have been applied for maintenance optimization and spare parts management. More specifically, Birnbaum structural importance measure is used to build a decision indicator for maintenance optimization of multi-component systems with complex structure. Differential importance measure is proposed to use in inventory management. More recently, the link between component importance and preventive maintenance policy has been discussed. In the framework of condition-based maintenance optimization, the current condition of components, such as failure state, working state, or deterioration level, is an important issue and needs to be taken into account in decision-making. However, very few existing importance measures allow incorporating the actual condition of components over time. Moreover, in practice, positive economic dependence, which implies that joint maintenance of several components is cheaper than performing maintenance on components separately, often exists and should be integrated in maintenance decision-making in the framework of maintenance optimization. To the best of our knowledge, no existing importance measure allows taking into account this kind of interaction between components. To face this issue, in this paper, a novel time-dependent importance measure based on the conditional reliability evaluation of the system, namely RIM measure, is proposed. At a given time and given the real condition of the components of a system, the proposed importance measure can be used to rank the components or groups of components according to their ability to improve the system reliability for a given mission. The proposed RIM measure is then extended to take into account the maintenance cost and the economic dependence between components. Indeed, the extended RIM measure is defined as the ratio of the cost benefit given by the maintenance of a component or group of components to its total maintenance cost. This indicator can help to identify the most cost-effective components or group of components for preventive maintenance before a given mission. This paper is organized as follows. Section 2 is devoted to the description of general assumptions and different reliability metrics. Different kinds of information on components at a given time are also discussed and integrated in the evaluation of reliability metrics. Section 3 focuses on the definition of the proposed time-dependent importance measure, namely RIM. The influence of information level on the RIM measure and RIM-based importance ranking is also investigated. An extension of RIM measure is developed in Section 4. Maintenance cost structures and economic dependence between components are also formulated and discussed. To illustrate the uses of RIM measure and its extension, a numerical of a 5-component system is introduced in Section 5. In addition, some numerical results are herein discussed. Finally, the last section presents the conclusions drawn from this work.", "section": "Introduction", "doi": "10.1016/j.ress.2019.106633", "references": [261935476, 1550064664, 1807378533, 1999069087, 2002699192, 2019006256, 2053885255, 2084648532, 2126484023, 2126967364, 2147664181, 2155326466, 2202654822, 2290696661, 2340663414, 2612742145, 2620514866]}
