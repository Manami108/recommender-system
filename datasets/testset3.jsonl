{"paragraph": "The cooperative teleoperation technique is applicable in many areas, such as unmanned aerial vehicles control, repair and assembly tasks in space, and nuclear and toxic environment. The robots vehicles act cooperatively and finish the tasks under orders from operators, avoiding harm to the operators. The multiple robots teleoperation system has been extensively studied in recent decades. According to previous studies, the teleoperation system with multiple robots can be divided into two categories: the single-master multiple slave systems and the multiple-master multiple-slave MMMS systems. The bimanual teleoperation system is a typical MMMS system. In 2005, the model of haptic MMMS systems was built by Sirouspour. Based on the assumption that a rigid tool object was firmly grasped by several slave robots, all the slave robots and the unique object formed a close chain. The system was centrally teleoperated through all sensors and actuators. This model was replicated by Chen et al., Li et al., and Sun et al. Chen et al. considered the arbitrary communication time delay and used integrated adaptive robust control for this model. Li et al. studied this issue using random time delays and an adaptive fuzzy force position control method. Sun et al. used the wave-variable-based passivity control method for MMMS systems. Other researchers also considered the limits of input saturation constraints or sensorless control. The assumption that the object is rigidly contacted by several robots is far from real because even a small position error will cause large internal forces between the two robot ends, and the internal force affects the robot grip changes on the object. Nonrigid contact is more applicable but is seldom considered for teleoperation. Nonrigid contact and soft environment influence system transparency and stability. Willaert et al. and Reynaerts et al. analyzed the transparency and stability in bilateral teleoperation in a bounded environment. They stated that a priori knowledge on the environment reduces conservativeness with respect to classical approaches. For an object manipulated by multiple robots, the transparency for each robot is determined not only by the object itself but also by the motions of other robots. The pushing or pulling actions make each robot's hand feel the hardening or softening of the object. Thus, a fine internal force tracking control method is crucial for bimanual teleoperation. In addition, the large uncertainty dynamics and time-varying delays usually influence the system stability and degrade the position tracking effect and transparency performance. Various reported methods to improve the position tracking effect of the bilateral teleoperation were designed using fuzzy, adaptive laws, or switching-based control methods to ensure the system steady-state or even transient-state performance. Some variables in the controllers such as joints and joint velocities are continuous and time related, but the joint accelerations and disturbance forces are discontinuous and nonlinear. However, the difference between the two types of signals is seldom considered in the teleoperation system. In this paper, we consider the internal forces caused by object deformation and changes of the contact positions and express them by relative impedance. Based on the performance analysis for cooperative actions, the desired slave positions for internal force control are deduced. Then, two approaches are adopted for motion synchronization and minimization of internal force tracking errors. One is designing a hybrid position error item for position tracking errors and the ones caused by force differences. An adaptive adjusting factor, determined by the relative impedance, is designed to balance the control authorities of the position control and force control. The other is designing a torque control module to the coupled item of uncertain dynamics, disturbance, and internal forces. Different parts of the item are divided and solved separately by the nonlinear characters. An integral sliding surface containing the relative impedance item is designed to minimize the estimation errors. The rest of this paper is organized as follows. Section II proposes the desired positions for internal force tracking based on the analysis of system performance in coordinating operation tasks. In Section III, we design the master and slave controllers. Section IV presents the system stability conditions by linear matrix inequalities LMIs. In Section V, an experiment in which a bimanual robot grasps a soft ball is performed to demonstrate the robustness of the proposed method. Section VI concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2907446", "references": [1965555825, 2007775256, 2008418625, 2014760095, 2017280345, 2035384793, 2062674159, 2110237472, 2113706371, 2120489973, 2124324456, 2160001738, 2166405763, 2169440744, 2183314124, 2342926333, 2575014985, 2698548017, 2749412222, 2795436790]}
{"paragraph": "The huge volume of check-in data from various location-based social networks enables studies on human mobility behavior on a large scale. Next POI recommendations is the task to predict the next POI a user will visit at a specific time point given her historical check-in data. This task has been studied extensively in recent years. Next POI recommendations is different from typical recommendation tasks such as movies, songs, or books because a wide range of contextual factors are related to the user's spatial behaviors. These auxiliary factors include the temporal context, sequential relations, geographical influence, and auxiliary meta-data information such as textual description and user friendship. However, these factors are heterogeneous in nature. While some relevant aspects are continuous values such as geographical distance and the time interval, others are in the form of discrete values such as friendship, textual words, or day of the week. Harnessing useful signals from all these heterogeneous factors to predict a user’s next move is not an easy task. Existing solutions based on matrix factorization and embedding learning techniques have delivered encouraging performances. These solutions project users and POIs and the associated context factors into a shared hidden space with dense vector representations. The preference score is then calculated directly based on these vectors through the inner product operation. However, shallow factor or embedding learning is too limited to express the complex knowledge underlying user spatial behaviors with multiple context factors. In existing methods, different context factors are often modeled separately. Then a simple combination is applied to derive the final recommendation score. That is, we need to devise an individual model for each context factor. This modeling methodology is complicated and the resultant solution would be inferior, since the different factors carry varying degrees of useful knowledge and their interactions could be much more complex. Some other methods incorporated multiple context factors as additional constraints to guide the learning process. For example, check-ins made at a specific time period are grouped together for dynamic feature learning. However, these constraints may not always be useful to match user-POI interactions. A single factor or embedding learning could inevitably incur information loss through a joint optimization of both preserving the constraints and matching user-POI interactions. One plausible solution is to enlarge the dimension number. However, given the sparsity nature of user-POI interaction data, it would easily result in data overfitting. The neural network with dense vector representation based techniques provide a new way of modeling these factors in a unified manner. This offers two benefits: By adding nonlinear transformations on top of the embeddings of users, POIs and their associated factors, we can separate the embedding learning and high-level spatial intent learning to better understand user spatial behavior, leading to a better recommendation accuracy. Specifically, we encode the semantic relatedness or constraints among POIs or users into the corresponding embeddings. For example, users at Golden Gate Overlook are likely to visit Baker Beach in San Francisco, and vice versa. Therefore, Golden Gate Overlook and Baker Beach are projected closer in the embedding space. Without the need to match user-POI interactions, the embedding learning would capture the latent features for users, POIs and the associated constraints to its fullness. Also, both new users and POIs may be covered partially by the associated auxiliary meta-data such as textual description or friendship. With dense vector representations, we can easily estimate the spatial intent from the associated meta-data for these cold-start cases. Since not all constraint information or latent features are useful for all user-POI interactions, the nonlinear transformation operation is adopted to learn how to extract high-level spatial intent for next POI recommendations. We can also devise factor-based nonlinear extractions to accommodate some specific context factors that are strongly relevant to spatial intent extraction. Through a unified framework with neural treatment and dense representations, the complex interactions among the context factors, users and POIs can be learnt smoothly without handcrafted modelling for each factor alone.", "section": "Related Work", "doi": "10.1007/s11704-018-8011-2", "references": [154472438, 179875071, 1500188831, 1546409232, 1614298861, 1924770834, 1971129545, 2009779426, 2044672016, 2047604680, 2059512502, 2059512573, 2064675550, 2070915285, 2073013176, 2084677224, 2087692915, 2094286023, 2101409192, 2110485445, 2110953678, 2118463056, 2137245235, 2137983211, 2146232090, 2154851992, 2156387975, 2205235818, 2242161203, 2250739653, 2408538552, 2471486255, 2511929605, 2512971201, 2515144511, 2531384334, 2534727297, 2536880093, 2539781657, 2567312369, 2572589325, 2575006718, 2605350416, 2762735242, 2788114581, 2949274928, 2950635152, 2962995178]}
{"paragraph": "Neighborhood Systems were proposed through extending the strategies of the nearest neighbors. In a neighborhood system, an object is associated with its neighborhood rather than its nearest neighbors. The classifications based on neighborhoods were proven to be more efficient than the classifications based on nearest-neighbor search. The space of neighborhoods was also investigated to approximate global data distribution. From the view of topology, it has been demonstrated that the neighborhood spaces are more general than the data-level spaces. This indicates that transforming original data into neighborhood systems will facilitate the data generalization. Through extending Rough Sets with neighborhoods, Neighborhood Rough Sets were proposed to construct approximations of data space. Different from the equivalence classes defined by symbols in the classic rough sets, the basic granules in neighborhood rough sets are the neighborhoods in numerical or nominal data spaces, which makes the model represent the mixed-type data well. Formulating data space with neighborhood rough sets, data distributions can be approximated by Neighborhood Covering, which consists of a group of homogeneous neighborhoods, i.e. all the data samples in a neighborhood belonging to the same class. Neighborhood covering provides us an effective way to represent data distributions on neighborhood level. Moreover, to obtain the concise representation of data distribution, Neighborhood Covering Reduction methods were used to remove the redundant neighborhoods from the initial neighborhood coverings. Based on the neighborhood coverings of data distributions, the learning methods can be implemented for classification and feature selection. Comparing with other kinds of learning methods, NC-based methods require no parameter setting and are robust to complex data. For the classification with neighborhood covering, the existing methods directly classify an unknown sample into a class according to its nearest neighborhood. However, this certain classification strategy strictly classifies the uncertain data and may lead to serious classification mistakes. Because of the unavoidable inconsistency between the training data and the unknown world, there generally exist uncertain cases in data classification. Thus it is required to design a cautious NC-based classification for uncertain data to reduce the classification risk. To implement the uncertain classification with neighborhood covering, we expect to construct a possibilistic measure of the belongingness of neighborhood coverings and thereby design a three-way classification strategy. This solution originates from the methodologies of Three-Way Decisions. In the process of three-way decision making, decision rules are extracted from the data with uncertainty through tri-partitioning data space into Positive, Negative and Boundary regions. From the view of classification, the three regions correspond to the cases of certainly belonging to a class, certainly beyond a class and non-commitment, i.e. uncertain case. In the light of the superiority of fuzzy sets for the learning tasks on uncertain data, we adopt fuzzy membership functions to measure the possibilities of data samples belonging to neighborhoods. Based on the neighborhood memberships of data samples, we apply tri-partitioning methodology to reformulate the neighborhood-based classification and propose a Three-Way Classification method with Fuzzy Neighborhood Covering. The proposed method involves two parts: fuzzy extension of neighborhood coverings and three-way classification with fuzzy neighborhood coverings. Different from the traditional covering model formed by the union of neighborhoods, the fuzzy neighborhood covering consists of a group of neighborhood membership functions which are integrated to form the membership distribution of neighborhood coverings. The membership distribution of neighborhood coverings of different classes induces a soft partition of data space. According to the memberships of neighborhood coverings, data samples are classified into certain classes and uncertain case. The contributions of this paper are summarized as follows. Extend neighborhood covering to Fuzzy Neighborhood Covering. Fuzzy neighborhood covering consists of a group of neighborhood membership functions and forms an uncertain measure of neighborhood covering belongingness. In contrast to the set-level approximation of neighborhood coverings, the fuzzy neighborhood covering provides a membership-level approximation of data distributions. Propose Three-Way Classification with Fuzzy Neighborhood Covering. Based on the fuzzy neighborhood covering of a class, data samples are classified into Positive, Negative and Uncertain cases according to their memberships. The three-way strategy separates uncertain cases to reduce the classification risk. The remainder of this paper is organized as follows. Section 2 introduces the related work. Section 3 describes the entire workflow of the proposed three-way classification method. Section 4 introduces the strategy of the fuzzy extension of neighborhood coverings and also presents the three-way classification algorithms with fuzzy neighborhood coverings. In Section 5, experimental results validate the effectiveness of the proposed three-way method for uncertain data classification. The work conclusion is given in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.07.065", "references": [167016754, 837378864, 938113424, 1021182246, 1533530351, 1585743408, 1597852916, 1834402812, 1973892392, 1989393769, 1990116832, 2006873874, 2015932775, 2024638676, 2024646871, 2025542264, 2033857698, 2041182276, 2053036794, 2059429344, 2070813883, 2088779313, 2089923511, 2093329961, 2110183025, 2134866037, 2158633287, 2163952039, 2200382070, 2217596628, 2340020088, 2551757246, 2609279172, 2616721392, 2699869760, 2912707296]}
{"paragraph": "Motivation. Nowadays, automata-based modeling and verification methods are mainly used in two different ways: for designing digital systems based on specifications expressed by the end-users of these systems or from the knowledge designers have of their environment; and in order to abstract existing systems that are too complex to comprehend in their entirety. In both cases the complexity of the systems being designed calls for increasingly expressive abstraction artifacts such as time and probabilities. Timed automata are a widely recognized modeling formalism for reasoning about real-time systems. This modeling formalism, based on finite control automata equipped with clocks, which are real-valued variables which increase uniformly at the same rate, has been extended to the probabilistic framework. In this context, discrete actions are replaced with probabilistic discrete distributions over discrete actions, allowing to model uncertainties in the system's behavior. This formalism has been applied to a number of case studies. Unfortunately, building a system model based either on imprecise specifications or on imprecise observations often requires to fix arbitrarily a number of constants in the model, which are then calibrated by a fastidious comparison of the model behavior and the expected behavior. This is the case for instance for timing constants or transition probability values. In order to incorporate these uncertainties in the model and to develop automatic calibration, more abstract formalisms have been introduced separately in the timed setting and in the probabilistic setting. In the timed setting, parametric timed automata allow using parameter variables in the guards of timed transitions in order to account for the uncertainty on their values. The reachability emptiness problem, i.e., the emptiness of the set of valuations for which a given discrete state is reachable, is undecidable for parametric timed automata, even for bounded parameters, for a single integer-valued parameter, or only when strict inequalities are used. Decidable subclasses were exhibited. Parametric probabilistic timed automata were proposed to answer the following question: given a timing parameter valuation, what are other valuations preserving the same minimum and maximum probabilities for reachability properties as the reference valuation? Parametric probabilistic timed automata were then given a symbolic semantics; a method has been proposed to synthesize optimal parameter valuations to maximize or minimize the probability of reaching a discrete location. In the purely probabilistic setting, Interval Markov Chains have been introduced to take into account imprecision in the transition probabilities. IMCs extend Markov Chains by allowing to specify intervals of possible probabilities on transitions instead of exact values. Methods have then been developed to decide whether there exist Markov Chains with concrete probability values that match the intervals specified in a given IMC. Contribution. In this paper, we propose to combine both abstraction approaches into a single specification theory: Parametric Interval Probabilistic Timed Automata. In this setting, parameters can be used in order to abstract timed constants on transition guards while intervals can be used to abstract imprecise transition probabilities. Allowing this higher level of freedom allow for incremental design, where one can first give large sets of values for which the system may be defined, and then further refine them. This refinement will take the form of an instance of a probabilistic interval, or the concrete instance of a timing parameter. As for IMCs, it is important to be able to decide whether the probability intervals that are specified in a model allow defining consistent probability distributions. This is called the consistency problem. First, in the context of Interval Probabilistic Timed Automata with no timing parameters, we propose an algorithm that solves this problem. Second, in the parametric setting, since the behavior of the system is conditioned by the calibration of parameter values, it is necessary to decide whether there exist parameter values that ensure consistency of the resulting model and synthesize these values when this is possible. We show that the existence of such parameter valuations is undecidable in the general context of PITAs. Still, we exhibit a sufficient syntactic condition on the use of the parameters to ensure decidability, when parameters are partitioned into lower-bound parameters and upper-bound parameters in their comparisons with clocks. In addition, we propose a construction that characterizes, whenever the parametric probabilistic zone graph is finite, the set of parameter values that ensure consistency of the resulting ITA. We finally address the problem of parametric consistent reachability, i.e., of synthesizing valuations for which a given state is reachable and the model is consistent. Example 1 The Root Contention Protocol, used for the election of a leader in the physical layer of the IEEE 1394 standard, consists in first drawing a random number, then waiting for some time according to the result drawn, followed by the sending of a message to the contending neighbor. This is repeated by both nodes until one of them receives a message before sending one, at which point the root is appointed. This protocol was modeled using parametric timed automata, with probabilistic timed automata, and using parametric probabilistic timed automata, i.e., parametric timed automata extended with probabilistic distributions. The goal of the protocol is that each node reaches either the child status, or the root status. In addition, observe that we use probabilistic interval distributions; they can be seen as an additional design freedom, allowing for incremental design. The one going out from ROOT_IDLE clearly admits no implementation, as no instance of the two intervals can be such that their sum is equal to 1. This probabilistic interval distribution could be either disabled by setting other probabilities to 0 so that location ROOT_IDLE becomes unreachable; or by tuning the values of the four parameters or the parameters in the other PITAs in parallel so that the guard going out from ROOT_IDLE becomes unsatisfiable. The rest of this manuscript is dedicated to this problem. Outline. We start Section 2 with preliminary definitions and then introduce the concepts of ITAs and PITAs. In Section 3, we study the consistency problem for ITAs and propose a constructive algorithm based on the zone-graph construction that decides whether an ITA is consistent and produces an implementation if one exists. In Section 4, we move to the general problem of consistency of PITAs. We first show that this problem is undecidable in general and then exhibit a decidable subclass. We then propose a construction that characterizes, whenever the parametric probabilistic zone graph is finite, the set of parameter values ensuring consistency of the resulting ITA. We also consider the problem of parametric consistent reachability. Finally, Section 5 concludes the paper.", "section": "Related Work", "doi": "10.1016/j.jlamp.2019.04.007", "references": [1550659912, 1655603041, 1824805445, 1946664948, 1972539585, 1976659775, 1992147928, 2007490191, 2009280542, 2025782239, 2028827800, 2044499152, 2057873236, 2077321406, 2098166444, 2101508170, 2125500714, 2150220388, 2155953212, 2163161023, 2290009222, 2294999689, 2296504218, 2530593621, 2585211837, 2607981089, 2900618080, 2950889910, 2951158023]}
{"paragraph": "Many firms have attempted to effectively utilize big data analytics to improve their decision quality. Big data analytics refers to the use of a new generation of analytical tools to effectively analyze and get insight from data that is high in terms of variety, volume, and velocity. The advent of modern technology, new types of data, and advanced analytical tools provides firms both opportunities and challenges. A recent survey found that the number of firms that intend to invest in using big data analytics during the next two years dropped from 31% to 25% in 2016. This could be due to the fact that for many firms it is still not clear if the benefits of the use of big data outweigh its challenges and costs. Therefore, although in the last few years big data has become a key factor in transforming the way in which firms do their business, many of them have still not started to utilize such data. Hence, the main objective of this study is to investigate the role of positive and negative valence factors on the impact of bigness of data on big data analytics usage within firms. This study responds to the call for focusing not only on big data benefits, but also on its risks and challenges. According to a recent report, only 15% of firms invested in the use of big data analytics successfully deployed their big data analytics projects to production. One of the key challenges that firms face in utilizing big data is security issues involved with big data aggregation and analysis. With the advent of advanced technologies, firms can collect and process huge amounts of sensitive information regarding their employees and customers, as well as trade secrets, intellectual property, and financial information, which could be a valuable target for attackers. Thus, understanding the role of big data security concern as a negative valence factor on the impact of big data on big data analytics usage is critical. Another main challenge facing firms seeking to use big data analytics is the complexity of processing and analyzing big data. Due to the complexity of analyzing big data, many employees may avoid or postpone the use of big data analytics. Therefore, although advanced technologies enable firms to develop Information Technology infrastructure to access and obtain big data, because of the data security concern and task complexity issues the use of big data analytics may be reduced considerably. Thus, the first objective of this study is to investigate the role of data security concern and task complexity as negative valence factors on the relationship between bigness of data and big data analytics use. One of the main benefits of obtaining big data from different sources is to improve data diagnosticity. Data diagnosticity refers to obtaining valuable information from data. The most successful firms are obtaining big data to transform data into insight and then action. Improving data diagnosticity could be one of the main factors that impact the use of big data analytics within firms. Another main benefit in obtaining big data is the increase in data accessibility. The increase in volume, variety, and velocity enhances the availability of data for firms to make more informed and faster decisions. Therefore, the second objective of this study is to investigate the role of data diagnosticity and data accessibility as positive valence factors on the relationship between bigness of data and big data analytics use. The existing literature mainly considers anecdotal evidence regarding the benefits and challenges of obtaining big data; empirical evidence about the impact of bigness of data on big data analytics use and the role of positive and negative valence factors on this association is lacking. Our study draws on the valence theory perspective to address the following research questions: Do data security concern and task complexity mediate the impact of bigness of data on the use of big data analytics? And do data diagnosticity and data accessibility mediate the impact of bigness of data on the use of big data analytics? Both of these questions examine novel aspects not previously studied in the Information Systems literature. Towards the above objectives and leveraging the above theory, we propose and empirically validate a research model using survey data collected from 140 IT managers and data analysts. The findings of this research provide a theory-based understanding of the impact of data security concern, task complexity, data accessibility, and data diagnosticity on the impact of bigness of data on the use of big data analytics within firms, while also providing guidance for managers to successfully utilize big data analytics within their firms.", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2018.12.011", "references": [374575654, 1496783311, 1511720087, 1567491469, 1966998110, 1974689173, 1978795070, 2003813760, 2008420823, 2017604730, 2022987632, 2040263621, 2050091520, 2072808001, 2093548531, 2100379340, 2107913771, 2109574129, 2109730248, 2118023920, 2132747867, 2141975087, 2171515724, 2209900197, 2258542398, 2284916459, 2321624443, 2339446221, 2407961458, 2431181726, 2498362352, 2519706319, 2529936865, 2548753368, 2563642040, 2606568264, 2613879460, 2727729132, 2733392713, 2763473974, 2766977116]}
{"paragraph": "Goal-oriented a posteriori error estimates are a powerful tool in numerical approximations of many engineering problems since they provide relevant information about the error in a quantity of interest rather than about the error measured in some norm. The quantity of interest is expressed in terms of a functional. The technique of the estimates is based on the solution of an auxiliary (dual) problem, adjoint to the original (primal) problem. The dual weighted residual (DWR) method promoted by Becker and Rannacher, see also the general framework by Prudhomme and Oden, the approach of Maday and Patera, multi-objective error estimation, enhanced least-squares finite element methods by Chaudhry et al., or the constitutive relation error (CRE) approach of Ladevèze et al. and Rey et al., are very popular approaches to goal-oriented error estimation; this can also be built in the discretization scheme as in Kergrene et al. The obtained bounds are, however, often not guaranteed in the sense of yielding a fully computable number that is rigorously greater than or equal to the goal error. Obtaining rigorous guaranteed bounds is possible upon introducing the equilibrated flux approach closely related to the CRE method. In particular, Ainsworth and Rankin follow this path and develop and compare a number of alternative approaches, in the context of a linear second-order elliptic problem, also focusing on general inhomogeneous Dirichlet boundary conditions. Their idea is to split the error into two components where the first error is bounded by a computable dual-weighted residual and the second one, claimed small, is estimated via equilibrated fluxes. An important focus whose rigorous investigation has been started only recently is the theory for nonconforming, discontinuous Galerkin, and mixed methods: let us cite in particular Mozolevski and Prudhomme and Dolejší et al. Finally, to the best of our knowledge, all the above-cited results rely on the assumption that both the primal and the dual discrete problems are solved exactly. This may not be satisfied in practical large-scale simulations and, actually, developing the theory not relying on such an assumption is a basis of full adaptivity including all meshes, polynomial degrees, and solvers. The present article develops a unified framework for goal-oriented a posteriori error estimation for a model linear second-order elliptic equation. We consider inhomogeneous Dirichlet and Neumann boundary conditions and a goal functional including the most demanding engineering application of the normal flux passing through some surface, unlike in many papers which concern the primal variable evaluation around a region or point evaluation of derivatives of the solution. We next apply this framework to various finite element methods (conforming, nonconforming, and discontinuous Galerkin) as well as to the finite volume method. We derive guaranteed goal-oriented a posteriori error estimates based on equilibrated flux and potential reconstruction and treat the tricky remainder term in the goal error expression following. Our approach is significantly different from the well-known dual-weighted residual method and extends upon also bounding the essentially higher-order terms and including the case where the continuous and discrete solutions lead to the same goal. The discretization of the primal and dual problems yields two linear algebraic systems. We do not suppose here their exact resolution. This inexact solution of the linear systems influences the goal error as well as its estimates. Following these ideas, we decompose the primal and dual estimates into estimates on discretization and algebraic error components. We can then prescribe efficient stopping criteria for iterative solvers applied to both primal and dual algebraic systems, balancing the two components. This paper is organized as follows: Section 2 introduces setting and useful notation. Section 3 describes the model problem and the goal functional. Section 4 then establishes a posteriori error estimates for the goal functional in an abstract framework. In Section 5, we then show how to apply the framework to various discretization schemes. Finally, Section 6 proposes a fully adaptive algorithm with a posteriori stopping criteria for the primal and dual solvers and illustrates the theory by numerical experiments.", "section": "Related Work", "doi": "10.1016/j.cam.2019.112367", "references": [145228056, 624819622, 1980377066, 1991960225, 2007289139, 2010435932, 2011247691, 2023874953, 2051889472, 2076725930, 2082627325, 2083027300, 2090453131, 2120378605, 2131660612, 2155216327, 2167521757, 2596343147, 2604616037, 2952118045]}
{"paragraph": "With the widespread use of integrated automation systems and rapid development of information technologies, modern large-scale processes, such as steel-making and paper-making, are developing toward intelligence, complexity, and integration. Take the hot strip mill process as an example, it is a long process of product processing from raw materials to the final products, which has obvious levels, mainly including equipment level, real-time control level, process control level, manufacturing execution level, business management level, and business strategy level, as shown in Fig. 1. They work together to keep high-efficiency of operation and ensure stability of product quality. Since there are too many coupled process variables and control loops, once two or more faults occur simultaneously or sequentially in a control unit or a system, they may function together to affect the final product quality. Thus, as the first protective layer of modern large-scale process, detection and isolation of quality-related multiple faults are of great importance, which have recently become one of the hotspots both in academia and engineering areas. Of all the system levels, the real-time control level plays a vital role in quality monitoring, which can collect and store vast amounts of production data, monitoring data, and maintenance records. However, production and product quality are always not online measurable. An intuitive solution is to build the relationship between process and quality data, and then by using the online available process measurements to monitor the product quality and detect the quality-related faults. Associated with these trends, some efficient quality-related fault detection approaches, such as partial least squares, canonical variable analysis, and their extensions, have been deeply studied in the process monitoring field. However, they may not function well in quality-related multiple faults. The most important reason is that most of the monitoring statistics only consider the magnitudes of the latest, known, or large process shifts, while they may not be suitable for detecting the varying and unknown quality-related multiple faults in large-scale processes. In consideration of the efficiency in dealing with large-scale processes mainly characterized by complex operating mechanism, multiple operation modes, and multilevel cooperative associations, the distributed and multiblock monitoring approaches, such as multiblock kernel PLS and multiblock concurrent PLS, have been developed. These methods can preferably reduce the modeling complexity and describe the global and local operating systems. Moreover, by means of dividing multilocks, the abnormal operations can be detected in a decentralized way, and the most responsible faulty control loop or block can be located easily as well, which will provide practical technologies or solutions for hierarchical detection and isolation of quality-related multiple faults in large-scale processes. Comparing with the achievements in fault detection, very few research efforts have been focused on the more challenging issues on fault isolation in large-scale processes. Although some common methods, such as contribution plots, reconstruction analysis, and branch-and-bound search, have been introduced, issues on smearing effects and heavy computational burden still exist for large-scale processes with a large number of variables. In comparison, linear discriminant analysis, due to its simple forms for dimensionality reduction and supervised classification, has also been used to identify variables responsible for the detected process abnormalities. Nevertheless, the problems of small sample size and nonrobustness always exist, which limit the popularizations and applications in engineering practices. To tackle the small sample size problem, various extensions of LDA have been developed, of which two commonly used approaches are penalized discriminant analysis and exponential discriminant analysis. The PDA algorithm makes the within-class covariance matrix be nonsingular by adding a nonnegative penalty matrix, while the penalty matrix is difficult to choose and the solutions may not be the optimal discriminant directions. By contrast, the EDA algorithm solves the singularity problem by mapping the covariance matrices into their exponential forms, in which the choice of penalty matrix has been avoided. However, potential concurrency and coupling make the multifault data present complex multimode characteristics, the EDA algorithm uses l2 norm to calculate the scatter matrixes that leads it to be sensitive to outliers. Although some sparse constraint techniques, such as sparse discriminant analysis and sparse EDA, have been developed for classification, the performance is still sensitive to the selection of the number of reduced dimensions. Motivated by the above considerations, based on the process data, quality data, and maintenance records of real-time control level, this paper concerns the main characteristics of quality-related multiple faults for large-scale processes, i.e., varying, unknown, and multimode, the main contributions are as follows. To propose a hierarchical detection and isolation scheme for quality-related multiple faults in large-scale processes. To put forward a real-time and hierarchical detection approach for quality-related multiple faults based on adaptive kernel canonical variable analysis and Bayesian fusion, which can fully mine the local information inside the process and detect the quality-related multiple faults from plant-wide-level to subprocess-level. To develop an accurate isolation method for quality-related multiple faults based on robust sparse EDA, which cannot only help operators to take appropriate actions in a short response time, but also reduce the downtime and save production loss. The rest of this paper is organized as follows. In Section II, the preliminaries and the problems to be addressed are given. Sections III and IV focus on the hierarchical detection and isolation methods, respectively. Then, the verification results are presented in Section V. In Section VI concludes this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2898576", "references": [1984672166, 1990916425, 1991519797, 2027717478, 2063823978, 2098815387, 2111346607, 2112545207, 2123180320, 2137922462, 2146842127, 2164278908, 2341771485, 2560143868, 2612496192, 2741385306, 2754813553, 2775523681, 2791052411, 2888609085, 2897534289]}
{"paragraph": "With the burgeoning development of portable sensing techniques, pedestrian dead reckoning system consisting of wearable sensors has been attracting increasing attention. Such technologies have resulted in wide applications, ranging from emergency and safety to healthcare monitoring, military defense, intelligent environment, etc. Multiple types of techniques have been introduced to implement pedestrian navigation. A traditional practice is to deploy a global positioning system; however, low satellite-signal reception in urban canyons and indoor environments renders GPS-based solutions ineffective in such scenarios. Alternative technologies and approaches have been proposed, such as infrared light, Bluetooth, ultrasound, wireless local area networks, etc. These technologies are widely used for indoor positioning beacons, as most of current smartphones have support for the WiFi and Bluetooth. Other RF technologies like radio frequency identification or ultrawideband radio are also adopted for smartphone-based pedestrian dead reckoning. Nonetheless, these methods have as the drawback the need for the creation and maintenance of a network, and weak geometry would degrade the signal strengths and even lead to ambiguity problems. Alternatively, microelectromechanical system magnetic, angular rate, and gravity sensors offer a beacon-free solution. MEMS sensors are small, lightweight, low-cost, and self-contained and hence are often employed for the pedestrian dead reckoning system. However, the output of the MEMS sensors is characterized by bias, bias instability, and other errors, which are integrated through the navigation equation to produce position errors that grow with time. To reduce the errors due to this integral drift, a workable solution is to attach the MARG sensors on foot and to estimate the position by the zero-velocity-update algorithm. The principle of the zero-velocity-update algorithm comes from the fact that when a pedestrian walks, a stance phase occurs, and at that moment, the velocity of the foot is practically zero. At this point, an acceleration integral could be avoided. Nevertheless, there are two issues necessary to be taken into account when improving the pedestrian dead reckoning system using MARG sensors. The position accuracy is characterized by the accuracy of foot orientation. Once a constant heading error occurs, it would lead the position error to grow linearly with time. Moreover, attitude errors affect the calculation of the specific force, thereby inducing a worse estimation of position. Zero-velocity-update detection is also a significant factor concerning the step length estimation. On the one hand, it must have a low false detection rate; otherwise, the erroneous step would increase the position error. On the other hand, estimating the start and end of each step accurately is conducive to improve accuracy. Thus, accurate tracking of the foot orientation is critical for pedestrian dead reckoning. An extended Kalman filter is one of the most widely applied methods in the pedestrian dead reckoning system because it linearizes the model about the current estimated point and can cope with the nonlinear attitude estimation system. Nevertheless, when the nonlinear characteristic is very strong, the extended Kalman filter dramatically degrades the accuracy, and it would even cause divergence of the filter. Nonlinearity is common in attitude estimation of pedestrian dead reckoning, for example, the attitude dynamics, environmental magnetic variation, acceleration variation from human motion, etc. Poor performance or even divergence arising from the linearization implicit in the extended Kalman filter has led to the development of other attitude estimators, like unscented Kalman filter. In addition, considering the tradeoff between the computational efficiency and performance, a complementary filter was developed using the gradient descent algorithm. Other studies have attempted to make the zero-velocity-update algorithm more sophisticated to enhance the performance of pedestrian dead reckoning. Generally, most of the zero-velocity-update detectors utilize acceleration and angular rate to define a flag of stance phase. However, these sorts of threshold-based methods might work inefficiently when short-term fluctuations occur. Thus, a clustering algorithm was proposed to accurately distinguish stance phases. Hidden Markov chain has also been introduced to classify human motions, whereas some are not specialized for real-time pedestrian dead reckoning, and the hierarchically structured approach needs more computational cost. Additional use of external sensors has also been explored. For instance, with a permanent magnet attached on one foot and an inertial measurement unit on the other foot, the walking pattern was measured as a magnetic field changes very rapidly when the foot is in the midstance phase. Thereupon, to address these issues, we develop a MEMS MARG-sensor-based pedestrian dead reckoning approach for indoor localization, including a double-step unscented Kalman filter and the hidden Markov models. The contributions of the present paper are twofold. As set forth, linear attitude estimators and the extended Kalman filter might dysfunction when the nonlinear characteristic is very strong, so the double-step unscented Kalman filter is designed to estimate attitude more accurately. The implementation of the unscented Kalman filter is extremely simple, as it does not require derivative information, which is prerequisite in the extended Kalman filter and the complementary filter. Yet, the computational burden of the unscented Kalman filter is relatively limited and is comparable to that of the extended Kalman filter. Furthermore, as the double-step suggests, we separate the measurement updates of the gravity and magnetic field and eliminate the unwanted correction for the Euler angle error. It is because the gravity vector is invalid for yaw estimation, and the magnetometer is usually less reliable for roll/pitch estimation than the accelerometer. Unlike the aforementioned approaches, we model the zero-velocity-update detection as an assignment of pattern recognition. The principle is that a true swing phase can be accepted only when the detection statistics continue for a period of time. The hidden Markov model is an effective tool for identifying the continuous swing phase from the sequential signals. The proposed hidden Markov model recognizer estimates the parameters of models from the training data and then judges the phase on the probabilistic models. Since the phase is based on the sequential signals, the false zero-velocity-update detection could be eliminated. This paper is organized as follows. Section II expounds the proposed double-step unscented Kalman filter. Section III introduces the hidden Markov model-based zero-velocity-update detector and the dead-reckoning method for indoor location. The experimental results are presented in Section IV. Finally, Sections V and VI conclude this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2897550", "references": [1998358366, 2003410899, 2019976143, 2025904705, 2038785536, 2053947927, 2056201856, 2056427752, 2076186907, 2085535099, 2100989187, 2113325920, 2121514940, 2123487311, 2124647600, 2148874154, 2150893603, 2171720331, 2214650344, 2363969975, 2417291942, 2751613606, 2767797175]}
{"paragraph": "Recent advances in video and audio editing tools allow creation of high quality tampered video content, which could easily change the semantics of original video content. Therefore, in order to ascertain the authenticity and integrity of digital videos, great efforts were made in the area of video forensics in the past decade. Since video recompression appears in the tampered videos, detection of video double compression has become one of the most important video forensics techniques. In the past few years, many efficient methods have been proposed to detect double compression when the coding parameters of the primary and the secondary compression are different. Existing methods adopt temporal analysis of detection features, which seek the periodical spikes caused by the transformation from I-frames to P-frames to detect GOP structures non-aligned video double compression. In the case of different compression qualities, researchers usually rely on properties like the statistical features of the distributions of DCT coefficients, the first digit distribution of non-zero quantized AC coefficients and Markov statistics to detect double compression. While in the case of altered codecs, some methods based on the idempotency property of lossy coding are proposed and proved promising. However, with the aid of video-editing softwares, attackers can easily obtain information like the coding parameters. They are more likely to recompress the tampered video with the same coding parameters in order to avoid leaving obvious recompression fingerprints, thus disabling all methods mentioned above. Therefore, a series of forensics algorithms have emerged to detect double compression with the same coding parameters. For double JPEG compression, several methods were proposed utilizing the DCT or blocks convergence in repeated transform coding (degradation mechanism). After that, the degradation mechanism was applied in double MPEG-x (i.e., MPEG-2, MPEG-4) and H.264/AVC compression detection. In our previous work, we proposed double compression detection schemes for MPEG-x and H.264/AVC videos with same primary and secondary coding parameters based on degradation mechanism analysis. To sum up, all video double compression detection methods mentioned above are designed for MPEG-x and H.264/AVC standard. The rise of HD/Ultra HD videos throws down a challenge to H.264/AVC since it no longer manages to encode such videos at a satisfactory bit rate. Contrarily, H.265/HEVC video compression standard gains upper hand over H.264 as HEVC introduces many new features like quadtree-based block partitioning, more flexible intra prediction angular modes, Advanced Motion Vector Prediction (AMVP) and merge mode. Due to the advanced techniques introduced in HEVC, all methods mentioned for earlier standards are not suitable for double HEVC compression detection. Meanwhile, there is no experimental study on resolution above 720p in the current literatures. Thus, the detection of double compressed HEVC videos (720p or above) with the same coding parameters is a new and challenging issue. To conquer this problem, quality degradation model and a detection scheme is proposed to expose double HEVC compression with same coding parameters. And it is a new research for HD videos. In this paper, models of double compression with the same quantization parameter (QP) and our prior work are introduced firstly. Secondly, by modeling the intra coding process and analyzing different effects caused by filtering, rounding and truncation operations, the degradation mechanism in multi-compressed I-frames is mathematically and experimentally analyzed in section III. Thirdly, for the detection of double compression in HEVC, the Intra Prediction Unit Prediction Mode (IPUPM) is adopted after a comprehensive analysis of the degradation mechanism and statistical feature is extracted from repeatedly compressed videos with the same coding parameters. Finally, experiments are conducted on HD (720p and 1080p) YUV sequences instead of CIF (352×288) or QCIF (176×144) YUV sequences. According to the experimental results, the proposed algorithm has been demonstrated to be effective in detecting HEVC double compression with the same coding parameters which performs better than existing methods, besides, the proposed method’s robustness is proven by being applicable to a wide range of QPs.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2918085", "references": [1509496202, 1598549602, 1969665649, 1980486363, 1991569626, 2039760575, 2051670588, 2059907896, 2076389153, 2088263286, 2099711786, 2106609510, 2127355996, 2131145525, 2171810915, 2402580812, 2412226701, 2462361453, 2547212236, 2588728105, 2737588607, 2753052222, 2785873985]}
{"paragraph": "Economists commonly use iterated strategy elimination procedures as solution concepts in games. Such procedures thus constitute one of the cornerstones for modeling agents' behavior in economic theory. The predictive power of iterated elimination procedures is in general lower than that of equilibrium-related notions; however, since the latter requires players to correctly forecast their opponents' behavior, the former seems more appropriate in situations of multiple equilibria wherein either the players or the economic analyst lack accurate data about past play or such data appears uninformative about future behavior. For instance, this is the case in many application of auction theory, e.g., wireless spectrum, carbon emission rights and online advertising. Consequently, thorough understanding of the forces behind iterated elimination is relevant from both a purely theoretical perspective and a more applied point of view, and is key to effective mechanism design and correct identification in empirical analyses. The conceptual appeal of iterated elimination procedures is that they carry the intuitive game-theoretic appeal of strategic reasoning: if a player is certain that some of her opponent's strategies are not going to be played, then she might deem some of her own strategies to be unreasonable. However, as discussed in Samuelson's classic analysis, strategic reasoning is in conflict with the criterion of cautiousness, which dictates that players favor strategies that, ceteris paribus, hedge against unexpected behavior. If players are modeled as expected utility maximizers, the clash seems inescapable: Strategic reasoning requires each player's beliefs to assign zero probability to some of the strategies, while cautiousness requires player's decision to be sensitive to those strategies that receive zero probability and are therefore of negligible importance for the maximization problem. Given that economic modeling often invokes the avoidance of weakly dominated strategies as a criterion for equilibrium selection, the seemingly mutually exclusive nature of strategic reasoning and cautiousness requires clarification. Such an understanding is desirable in particular in scenarios where behavior is likely to be reasoning-based and cautiousness plays a role. This paper proposes a new take on this longstanding problem by suggesting a novel theoretical foundation for the interplay between strategic reasoning and cautiousness. The analysis by Samuelson clearly shows that two ingredients are necessary to overcome this tension: First, multiple beliefs are needed to account for epistemic conditions that would be mutually excluding if required to be satisfied by a single belief. Second, the best-reply needs to be sensitive to all these beliefs. We achieve this within our framework by augmenting the underlying standard decision-theoretic foundation for each player by allowing for incomplete preferences where each player's strategic uncertainty is represented by a possibly non-singleton set of beliefs thus allowing for ambiguity, and a rational player chooses a strategy that is a best-reply to every belief in her set, so that the resulting choice is robust to the possible ambiguity faced by the player. Under this set-up, and inspired by Brandenburger et al., we say that a player assumes certain behavior by her opponents if at least one of the beliefs in her set has full-support on the collection of states representing such behavior. Consequently, the introduction of ambiguity and the requirement of robustness give great flexibility: It is possible for a player to assume certain behavior and, simultaneously, assume certain more restrictive behavior. If the player is also rational, her choice needs to be a best-reply to both of these beliefs. Hence, in particular, the tension between strategic reasoning and cautiousness is solved: A player can be strategically sophisticated by having one belief that assigns zero probability to her opponents playing dominated strategies, and at the same time cautious by having another belief that assigns positive probability to every strategy of her opponents. Thus, our model overcomes the problem as identified by Samuelson since it allows precisely for the two necessary ingredients. Based on the above, we build a framework that provides reasoning-based foundations for iterated admissibility—the iterated elimination of weakly dominated strategies. In Theorem 1 we show that, when type spaces are belief-complete, iterated admissibility characterizes the behavioral implications of rationality, cautiousness, and common assumption thereof. From our characterization, it is easy to see that the foundations of iterated admissibility necessarily require the presence of ambiguity whenever strategic reasoning has any bite. If the elimination procedure consists of multiple rounds, the set of ambiguous beliefs needs to contain a specific belief with full-support on the set of opponents' strategies that survive each round. Theorem 2 provides the analysis for the relaxation of belief-completeness and shows that, in this case, it is self-admissible sets which characterize the behavioral implications of rationality, cautiousness and common assumption thereof. Although the main approach in the paper is conceptual and focused on the link between cautiousness in reasoning-based processes and robustness to ambiguity, the results provide a methodological contribution for the use of incomplete preferences in game theory, which is a subject of interest in itself aside from its interpretation as a reflection of ambiguity. The literature studying the conflict between strategic reasoning and cautiousness is epitomized by the seminal paper by Brandenburger et al., who shed light on the question by building upon the lexicographic probability system approach. Lexicographic probability systems represent the uncertainty faced by a decision maker whose preferences depart from standard Bayesian preferences by allowing violations of the continuity axiom. In this setting, Brandenburger et al. provide reasoning-based foundations for finitely many iterations of weakly dominated strategy elimination based on rationality and finite-order assumption of rationality, but also present a celebrated impossibility result: under some standard technical conditions and generically in all games, common assumption of rationality cannot be satisfied. This negative result has spurred a line of research concerned with obtaining sound epistemic foundations for iterated admissibility. Keisler and Lee and Yang propose answers by tweaking topological properties of the modeling of higher-order beliefs and the notion of assumption, respectively, while Lee obtains foundations by proposing a modification in the definition of coherence. Catonini and De Vito also provide foundations by introducing a weaker notion of the likeliness-ordering of events that characterizes the lexicographic probability system, and via an alternative definition of cautiousness that restricts attention to the payoff-relevant component of the states. In a slightly different direction, Heifetz et al. propose a new solution concept, comprehensive rationalizability, that coincides with iterated admissibility in many settings and admits epistemic foundations. Within a standard Bayesian decision-theoretic model, Barelli and Galanis provide a characterization for iterated admissibility by introducing an exogenous tie breaking criterion. Robustness to ambiguity is studied by Stauber with a different interpretation from ours. Our paper can be regarded as complementary to the lexicographic probability system approach as standard Bayesian preferences are also abandoned by dropping completeness instead of continuity. Both these relaxations allow for multiple beliefs, but while the former requires a specific order, our model drops the order altogether and allows for multiplicity directly. However, apart from the transparent link between cautiousness and robustness to ambiguity that our framework allows for, the nice structure of the sets of ambiguous beliefs representing incomplete preferences has some additional advantages. First, it is easy to show that rationality and common assumption of rationality is a non-empty event and thus, that iterated admissibility is properly founded for all games. Second, the definitions and formalism involved do not require departures from the canonical definition of the objects involved: The modeling of higher-order beliefs, including the definition of coherence, and the version of assumption that we rely on are natural extensions of their counterparts in the realm of standard Bayesian preferences; and the notion of cautiousness invoked in our theorems is not necessarily restricted to environments where the sets of states have a specific structure. Finally, the presence of ambiguity via incomplete preferences has been shown to be empirically testable by recent work by Cettolin and Riedl. The rest of the paper is structured as follows. First, Section 2 provides an informal, non-technical overview of the effect of robustness to ambiguity on predictions in games, and specifically, on iterated admissibility as a solution concept. Section 3 reviews both the game-theoretic and the decision-theoretic preliminaries and Section 4 introduces the epistemic framework and the interpretation of strategic cautiousness as a manifestation of robustness of ambiguity. Section 5 the presents the epistemic characterization results. Section 6 concludes. All proofs and purely technical digressions are relegated to the appendices.", "section": "Related Work", "doi": "10.1016/j.geb.2019.10.001", "references": [1966539360, 1972794255, 1975677767, 1990567021, 2038925917, 2056835800, 2059624968, 2102964299, 2103840301, 2139774323, 2157343758, 2281517513, 2293184373, 2592552726, 2922214939]}
{"paragraph": "Many optimization problems can be modeled in practice as multi-objective optimization problems (MOPs) that involve multiple conflicting objectives that need to be optimized simultaneously. MOPs with more than three objectives are usually referred to as many-objective optimization problems (MaOPs). MaOPs have recently garnered greater research interest in the field of evolutionary multi-objective optimization (EMO) due to the challenges proposed by them to EMO algorithms. With the increase in the number of objectives, the performances of Pareto dominance-based EMO algorithms, like the NSGAII, deteriorates dramatically because most solutions become non-dominant in relation to one another. These Pareto-dominance-based EMO algorithms fail to provide satisfactory selection pressure to drive the population towards the target Pareto front (PF) of the given problem. In indicator-based EMO algorithms like hyper-volume estimation (HypE), the computational cost of calculating the hyper-volume (HV) indicator increases exponentially with the number of objectives. On the contrary, indicators for evaluating the quality of non-dominant solution sets for MOPs fail to provide satisfactory accuracy for MaOPs, which also degenerates the performance of indicator-based EMO algorithms on MaOPs. However, decomposition based EMO algorithms like MOEA/D have emerged as a promising algorithmic framework of solving MaOPs because they are less prone to the explosion of the objective space. In MOEA/D, the target MOP is first decomposed into a set of scalar optimization sub-problems by means of scalarizing methods, and these sub-problems are optimized by using evolutionary algorithms in a collaborative manner. Therefore, the employed scalarizing method plays an important role in the performance of the MOEA/D algorithm. In implementations of MOEA/D, weighted sum (WS), weighted Tchebycheff (TCH), and penalty-based boundary intersection (PBI) are three widely used scalarizing methods. Using different scalarizing methods can cause implementations of MOEA/D to yield drastically different performance. The WS method outperforms the TCH method in terms of convergence but fails to handle MOPs with concave PFs. The TCH method is robust against PF shapes of the target MOPs but fails to provide enough selection pressure when dealing with MaOPs. To deal with the shortcomings of the scalarizing methods mentioned above, several proposals have been made to enhance the robustness and improve the performance of MOEA/D for different MOPs with varying numbers of objectives. Ishibuchi et al. developed variations of MOEA/D with adaptively employed scalarizing methods during a search procedure. Wang et al. proposed an Lp scalarizing method in which the shape of the contour line can be adaptively adjusted to provide a balance between population diversity and convergence. In contrast to the Lp scalarizing method, PBI is a commonly used scalarizing method, whose contours can be specified by a parameter θ to fit with different shape of the PF. Because the no-free-lunch theorem is well documented in the field of the population-based optimization algorithms, an appropriate configuration of the parameter θ is necessary to effectively solve a given target MaOP. This work investigates the relationship between shapes of the contour line and parameter θ, to develop a scheme to adaptively choose θ for a diversity of problems. The PBI method with an appropriate penalty parameter value has been shown to be superior to the WS and TCH methods on MOPs, especially on MaOPs, yet its performance is highly sensitive to the setting of the penalty factor, which can have a significant effect on the convergence and uniformity of the solution set. However, few studies have sought to provide guidelines on choosing an appropriate penalty factor, especially when dealing with MaOPs. Some studies have shown the impact of choosing different penalty factors on the PBI’s scalarizing function, but do not provide a guideline on values that work well on different problems in different objectives. Yang et al. and Ming et al. argued that the determination of a suitable penalty factor depends on the PF shape of the target problem and developed enhanced PBI methods with adaptive penalty strategies for solving MOPs with irregularly shaped PFs. Ishibuchi et al. investigated the influence of the penalty factor on the PBI method for MaOPs. They concluded that a good parameter setting of the penalty factor is highly correlated to special characteristics of the target problems. Due to the complexity of appropriately setting the penalty factor in the PBI method for MaOPs, few studies have been devoted to this topic. The specification of the parameter θ in the PBI method can be influenced by many factors, such as population size, neighbourhood size, PF shapes, and the number of objectives. Instead of considering all these factors simultaneously, this work focuses on the relationship between the parameter setting of θ and the number of objectives. Unlike prevalent work on parameter adaptation in the PBI method that has focused on the impact of the PF shapes of the target MOPs on the best performance of θ, the key aim of this research is to statistically identify patterns in selecting the best-performing θ for MaOPs. The main contributions of this work can be summarized as follows: A comprehensive experimental study on various benchmark problems in different dimensions is conducted to highlight the influence of the penalty factor θ on the performance of the PBI based MOEA/D algorithm. The experimental results were used for interesting observations and analysis of specifications of the penalty factor θ in the PBI scalarizing method. To consider a wide range of candidate values of θ, a two-stage adaptive penalty scheme is proposed to identify an appropriate θ adaptively during the runs of PBI-based MOEA/D algorithm. The robustness and effectiveness of the proposed Ada-PBI are validated. The remainder of this paper is organised as follows: Section 2 introduces the algorithmic framework of the MOEA/D and the PBI method. Section 3 describes observations concerning the best performing values of θ based on the experimental results in Appendix A. Section 4 analyses the variation in the best-performing θ during an optimization run. Section 5 develops a two-stage adaptive penalty scheme for PBI and proposes the Ada-PBI algorithm, and Section 6 verifies its robustness and effectiveness. Section 7 offers the conclusions of this paper.", "section": "Introduction", "doi": "10.1016/j.ins.2019.03.040", "references": [69831977, 124905757, 1461232806, 1519778441, 1525375343, 1605922079, 1662894842, 1990057974, 1997053037, 2015321644, 2022485595, 2040622444, 2126105956, 2127459836, 2139196352, 2143381319, 2278034838, 2312339748, 2344916885, 2521918431, 2558939852, 2619137665, 2626804954, 2757662287, 2794503197, 2892369032]}
{"paragraph": "Supercapacitors are becoming increasingly popular as energy storage devices in wireless sensor network applications. With advantages of fast charging capacity, high power density, and longer lifespan, supercapacitors can extend the lifespan of sensor nodes up to 20 years. However, the application of supercapacitors in a WSN entails certain modeling and control-related challenges. Consideration of the leakage effect is one of them, which is critically important for effective online monitoring of supercapacitors. Motivated by this issue, we focus on estimating the state-of-charge of supercapacitors considering the leakage effect. Supercapacitor modeling approaches can be categorized into two groups: electrochemical models with high accuracy, which involve high computational complexity, and equivalent circuit models, different variants of which provide a tradeoff between accuracy and computational complexity. In this paper, we mainly focus on ECMs due to their computational effectiveness in online monitoring and control. Several variants of ECMs have been reported in the literature. Some of the existing ECMs consider the leakage effect; however, they lack complete observability when the SOC is considered as a state variable. On the other hand, the other group of ECMs do not have such an observability issue, but they ignore the leakage effect. From the online health monitoring and energy management viewpoint, continuous monitoring of the SOC in supercapacitors is of critical importance. In recent years, many techniques have been presented for the SOC estimation of supercapacitors, for example, Kalman-filter-based approaches, an H∞ observer, a sliding-mode observer, the Luenberger observer, an artificial neural network, and a practical model-based approach. However, none of these aforementioned approaches consider the leakage effect while estimating the SOC. Although some schemes consider leakage and indirectly track the SOC or energy by estimating the internal voltage states, they lack an explicit estimation of the SOC, which is essential for online monitoring. Consideration of the supercapacitor leakage effect is essential for online monitoring in low-duty-cycle applications, such as WSN. A typical case study has shown a 24% variation in the expected lifetime of a supercapacitor-powered MICAz sensor node when the supercapacitor is considered to be ideal and has leakage. In light of the aforementioned review, we have found the following research gap in the existing approaches: none of these works explicitly estimates the SOC of the supercapacitor considering the leakage effect, based on a fully locally observable model, to the best of our knowledge. Note that the explicit estimation of the SOC is more beneficial, as it directly provides the amount of charge remaining in the supercapacitor, as opposed to drawing indirect inference about the SOC from internal voltage states. Observability of the model is an important property for designing and quality performance of an estimation scheme. In light of the aforementioned research gap, the main contribution of this paper lies in the following: proposing a fully locally observable supercapacitor ECM that considers the SOC as an explicit state, as well as captures the leakage effect; and designing an SOC estimation scheme on this specific ECM considering nonlinearities and uncertainties. Specifically, we have the following. The proposed ECM captures the leakage effect, charge redistribution phenomenon, as well as the supercapacitor open-circuit voltage component. We have designed an unscented Kalman filter-based SOC estimation algorithm on this specific ECM. We have illustrated our approach considering a commercially available 5 F supercapacitor on a realistic WSN application. We have studied the robustness of the proposed algorithm in the presence of parametric and measurement uncertainties. The remainder of this paper is organized as follows. Section II describes the development of the supercapacitor ECM. Section III details the UKF-based SOC estimation algorithm. Section IV presents the experimental identification of the ECM. Section V discusses the simulation and experimental studies. Finally, Section VI concludes this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2897506", "references": [1655437105, 1911337059, 1970375531, 1991094976, 1996160942, 1999403921, 1999888009, 2000340713, 2013619261, 2112112990, 2161202291, 2282236521, 2756879798, 2775981176, 2794390055]}
{"paragraph": "The concept of three-way decisions was originally introduced by Yao to describe the three regions of rough sets. As a matter of fact, the theory of three-way decisions goes beyond rough sets and is related to three-valued sets, three-valued logics, shadowed sets, and orthopairs. Yao formalized a more general framework of three-way decisions called the trisecting-and-acting model, which divides a universal set into three pair-wise disjoint parts and performs effective strategies on some or all of the parts. The ideas of three-way decisions have inspired many three-way approaches and applications, for example, three-way classification, three-way analysis, three-way clustering, three-way recommendation, three-way fuzzy matroids, and three-way approximations. In three-way classification, the three regions are known as the positive, boundary, and negative regions, and can be interpreted in terms of three types of decision rules, namely rules for acceptance, rules for non-commitment, and rules for rejection, respectively. Objects satisfying acceptance rules are put into the positive region, objects satisfying non-commitment rules are put into the boundary region, and objects satisfying rejection rules are put into the negative region. The decision-theoretic rough set model, proposed by Yao, is one possible way to construct the three regions of three-way decisions. The decision-theoretic rough set model systematically calculates thresholds from loss functions using the well-known Bayesian decision procedure. Researchers have studied three-way decisions and decision-theoretic rough set models in various types of situations. For example, some generalized loss functions with triangular numbers, interval numbers, point operators, dual hesitant fuzzy elements, and established the triangular fuzzy decision-theoretic rough set model, interval-valued decision-theoretic rough set model, intuitionistic fuzzy decision-theoretic rough set model, and dual hesitant fuzzy decision-theoretic rough set model, respectively. Within the framework of fuzzy probabilistic approximation space and interval-valued fuzzy probabilistic approximation space, some generalized classical relations to fuzzy relations and interval-valued fuzzy relations, and proposed the fuzzy decision-theoretic rough set model and interval-valued fuzzy decision-theoretic rough set model. Other researchers studied three-way decisions in the framework of three-way decision spaces in an attempt to provide a solid mathematical foundation. The results of three-way decisions and decision-theoretic rough set models have been successfully applied to many fields, such as software defect prediction, cluster analysis, face recognition, government decisions, decision-making, attribute reduction, pattern discovery, credit scoring, etc. A multiset is a collection of elements in which elements may occur more than once. The idea of multiple instances of the same element has existed throughout the development of mathematics. For example, the prime factorization of an integer greater than zero is a multiset whose elements are primes, for example the number 360 has the prime factorization which gives the multiset 2, 2, 2, 3, 3, 5. Every monic polynomial over the complex numbers corresponds in a natural way to a multiset of its roots, for example the roots of the polynomial constitute the multiset 1, 1, 3, 3, 3. Multisets are not only of interest in mathematics, computer science, physics, but are also very common in our daily life. For example, decision results for a paper from a group of reviewers, grades for interviewees from a group of interviewers, and evaluation results from a group of customers are all multisets. Suppose that four doctors make their decisions of whether a patient has a disease. The probabilities of having the disease, as given by four doctors, are 0.9, 0.9, 0.9, and 0.1, respectively. We have two means of representing the results: 0.9, 0.1 and 0.9, 0.9, 0.9, 0.1. The former is a classic set which deletes all duplicate elements, while the latter is a multiset. The former cannot provide full information about the result, while the latter causes a person to believe that the patient has the disease. In this paper, we generalize the decision-theoretic rough set model to deal with multiset-valued data. We develop two generalized models known as the multiset-decision-theoretic rough set model and multiset-fuzzy-decision-theoretic rough set model. The remainder of this paper is organised as follows. Section 2 reviews basic concepts of a multiset, three-way decisions, and the decision-theoretic rough set model. Several new operations of multisets are introduced and their corresponding properties are discussed. Section 3 investigates three-way decisions in multiset-valued information tables. The multiset-decision-theoretic rough set model is established, which generalizes loss functions using multiset values. By integrating the multiset-decision-theoretic rough set model with the fuzzy decision-theoretic rough set model, the multiset-fuzzy-decision-theoretic rough set model is created for multiset-valued information tables. The conclusion follows in the last section.", "section": "Methodology", "doi": "10.1016/j.ins.2018.08.024", "references": [101345952, 147108263, 837378864, 881110586, 1041128124, 1756509722, 1883715000, 1969463949, 1969535228, 1977880445, 2023750115, 2048472139, 2052715156, 2070416356, 2070813883, 2074056315, 2089923511, 2114832876, 2176795090, 2199918042, 2217596628, 2272142993, 2297889545, 2499835732, 2515748156, 2519715111, 2549504134, 2560804083, 2561843635, 2586356223, 2588487610, 2600072788, 2607909519, 2609279172, 2620114837, 2740407498, 2763102103, 2772794611, 2774054418, 2792234535, 2793566013, 2799813871, 2806445482, 2887596728]}
{"paragraph": "Hierarchical classification is to classify a given data belonging to classes that are organized into a class hierarchy. The class hierarchy is usually represented by a tree or directed acyclic graph. In machine learning, the need for hierarchical classification has been raised and attempts have been made to solve. Hierarchical classification has been applied to several domains, including text classification, functional genomics, and image annotation. In the existing studies, a variant of support vector machine, which is one of the well-known algorithms in classification, or a method of classification based on a decision tree, is widely used. Neural networks have also been used for hierarchical classification. Classification is one of the most important problems in machine learning. Especially in human and agent interaction, classification is needed when the agent recognizes the surroundings and chooses a reaction in a certain situation. The interaction takes place in real time and the surrounding situation changes frequently. The agent should be able to learn new knowledge incrementally and reflect it in response to the changing environment. When a new data that belongs to a new class is given, the existing classification methods should be retrained for all data including the new data. It means the new data cannot be classified in real time. Therefore, incremental class learning is required to be able to classify the data to a new class without retraining for all the data. Although several methods have been proposed to enable incremental class learning in flat classification that classifies input data into classes without considering the class hierarchy, they have not been attempted in hierarchical classification. In hierarchical classification, the existing methods should have a predefined class hierarchy, or even if the hierarchy can be built incrementally, the classes that make up the hierarchy should be predetermined. That is, the given data is classified into predefined hierarchical classes under the assumption that the class information is known in advance. If data corresponding to new classes is additionally given, the methods should be retrained for all the existing data and the new data. In this context, a hierarchical classification method capable of incremental class learning is needed. In order to enable incremental class learning in multiclass classification, some existing algorithms form several unit classification modules that make up an ensemble. In contrast, other algorithms, including fuzzy adaptive resonance theory-supervised predictive mapping and class incremental extreme learning machine, are variants of the existing classification algorithms that add the functionality to a single module. Fuzzy ARTMAP is a classification network that is based on a clustering network, fuzzy adaptive resonance theory. Fuzzy ARTMAP performs classification through the map field module that matches the clustered category node from a fuzzy ART network for the input vector and the clustered category node from another fuzzy ART network for the label vector. CIELM is an algorithm that enables incremental class learning in extreme learning machine. It uses the least-squares solution and Moore–Penrose inverse to find the weighted connections between input nodes and category nodes online. Since CIELM includes the multiplication of a matrix with a size proportional to the size of the training dataset, a large amount of memory is required for learning a large dataset. In this sense, in this paper, fuzzy ARTMAP is adopted as a base network. Our proposed ARTMAP for hierarchical classification network is composed of hierarchically stacked modules, and each module incorporates two fuzzy ARTMAPs. The first one determines whether additional classification for a higher level class should be performed or not. The other one classifies the corresponding class for a certain level depending on the given input. ARTMAP-HC is capable of incremental class learning so that it can classify data belonging to a new class for each class hierarchy level. As the level in class hierarchy is increased, a new module is added and stacked on the lower level module hierarchically. In this way, regardless of the level of the class hierarchy and the number of classes for each level, ARTMAP-HC is able to incrementally learn data belonging to new classes. We also apply additional processes of normalization and prior labels appending to overcome the limitations that occur when fuzzy ARTMAP networks are hierarchically built for classification. First, a novel online normalization process is adopted in ARTMAP-HC. In fuzzy ARTMAP networks, continuous inputs with attribute values between 0 and 1 could be used. It means that the value range of all data should be assumed to be known beforehand to normalize the data by the maximum value. Our ARTMAP-HC, however, is able to classify a new input data without prior knowledge of the maximum value of the dataset. We expand the dimension of an input vector and then normalize the vector using its maximum absolute value. In the expanded dimension, the scale component is preserved even after normalization. Second, the process of prior labels appending is adopted to reflect the class dependency between levels. When a module is added for the next level, the elements of the prior input and the label of the previous level are concatenated and input to the additional module. The prior label information is included in the appended input, thereby the class consistency can be maintained by selecting the label that is a child node of the previously classified parent node. ARTMAP-HC is compared to the existing hierarchical classification methods, which are not capable of incremental class learning, for three different tasks: text categorization; image annotation; and protein function prediction. Our ARTMAP-HC shows comparable performance and even outperforms other methods. The incremental class learning ability is also validated through experiments by training the network for training data instances sequentially one by one and testing after each training. ARTMAP-HC shows improved performance even when an input data instance belonging to a new class is added for training. After analyzing the performance of the proposed ARTMAP-HC, ARTMAP-HC is applied to a multimedia recommendation system for digital storytelling. The system provides appropriate media during a conversation between a user and an agent that is embedded in a smartphone. By providing media related to the dialogue, the meaning of the dialogue can be conveyed more effectively. Since media can be classified by its type and then its specific genre hierarchically, ARTMAP-HC can be applied. When the keywords of the dialogue and context information of the user are given, ARTMAP-HC can select the related type of media. Then, the selected media is recommended and provided in the smartphone during the dialogue. The multimedia recommendation system verifies the applicability of ARTMAP-HC. The rest of this paper is organized as follows. Section II briefly describes hierarchical classification and fuzzy ARTMAP which is the basic network of the proposed ARTMAP-HC. Section III proposes the ARTMAP-HC. Section IV presents the experimental results and the multimedia recommendation system, respectively, to validate and demonstrate the effectiveness and applicability of ARTMAP-HC. Finally, concluding remarks follow in Section V.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2866869", "references": [190228827, 1563942368, 1620204465, 1768675166, 1968259359, 1980179845, 1991020953, 1999954155, 2012611887, 2026401830, 2038906069, 2061453990, 2087347434, 2101234009, 2119821739, 2121526711, 2123096872, 2150766729, 2155440340, 2156465113, 2157438458, 2158054309, 2166280719, 2171066191, 2495774670, 2520368209, 2556889907, 2677572839, 2735629075, 2743276272]}
{"paragraph": "Predictive maintenance helps factories reduce up to 40% maintenance cost, which is about $630 billion per year. As a critical component of predictive maintenance, machine fault diagnosis aims to determine the machine health condition from sensor data collected by billions of sensors. For example, vibration data collected by accelerometers placed on motors are used to determine motor health condition. Machine-learning techniques are widely used to construct fault diagnosis models based on a large amount of data. Machine-learning methods are affected by redundant information contained in sensor data. Hence, researchers extract features from sensor data based on domain knowledge to remove the redundant information and retain the relevant information. The commonly used manual feature extraction methods in machine fault diagnosis include: time domain statistical methods, such as mean and standard deviation; wavelet transform; and empirical model decomposition. Manual feature extraction can be labor-intensive and time-consuming. To overcome the weakness of manual feature extraction, deep learning DL-based representational learning methods learn features automatically from raw data. For example, restricted Boltzmann machine RBM learns representations by approximating the probability distribution of the original inputs. Hierarchical representations can be obtained by stacking RBMs to form a deep structure, which is known as deep belief networks DBN. Autoencoder AE learns representations by forcing the reconstructed outputs equal to the original inputs. Similar to DBN, stacking AEs forms stacked AE SAE that learns hierarchical representations. Convolutional neural network CNN extracts features by applying a set of learnable filters on the data points. However, the existing DL methods do not consider any relationship among data points. An important property of representations is to preserve the geometry of data points, i.e., the local and global geometries. Local geometry is preserved by retaining the same relationship between a data point and its neighbors before and after representation learning. For example, local linear embedding first reconstructs each data point from a linear combination of its neighbors and then learns representations that have the same linear relationship among data points. Laplacian eigenmaps is a graph-based method that preserves the distances between data points and their neighbors while mapping them from the input data space to the representation space. Although these methods can be seen as a nonlinear shallow neural network, they fail to discover deep representations. In manifold regularization was proposed to preserve local geometry of input data. The manifold regularization can be integrated into DL methods to learn deep representations with the local geometry preserved. Global geometry is preserved by retaining the same relationship among all data points before and after the representation learning. For example, linear discriminant analysis rotates the axes to maximize the between-class variance and minimize the within-class variance. Isometric feature mapping retains the geodesic distances among all data points during the learning process. In this paper, we propose a novel representation learning method, which is named fast AE with the local and global penalties FAE-LG, with the following properties. FAE-LG learns representations directly from the un-normalized raw input data. The learned representations preserve the local geometry of the data. The learned representations preserve the global geometry of the data. FAE-LG learns representations with discriminative capability efficiently. Specifically, the first property is achieved by minimizing the error between the reconstructed data and the original input data. The second property is achieved by minimizing the distance between each data point and its nearest neighbor. The third property is achieved by minimizing the difference between the representations and the random projected input data points. The last property is achieved by minimizing the error between the predicted labels and the ground truth. Hierarchical representations are obtained by directly stacking multiple FAE-LGs, denoted as Stacked FAE-LGs SFAE-LG, without any additional tuning step. In summary, the contributions of this paper are as follows. We propose a novel representation learning method, FAE-LG, that can learn discriminative representations with local and global geometries of input data preserved. Moreover, we propose stacked FAE-LGs that can learn hierarchical representations. The representations learned by SFAE-LG can be used by a classifier, e.g., linear regression, to detect machine fault. SFAE-LG with linear regression demonstrates better classification performance than the competitive methods on two real-world benchmark datasets. We theoretically prove the global geometry of input data can be preserved by minimizing the difference between the representations, which is mapped by a linear activation function, and the random projected input data points. Also, the experiment results show the nonlinear activation can approximately preserve the global geometry of input data. We experimentally demonstrate that it is important to preserve both local and global geometry for representation learning in machine fault diagnosis. SFAE-LG requires fewer hidden neurons in each layer and thus has less computational complexity compared with SAEs. Furthermore, SFAE-LG can be trained more efficiently because it does not require the additional fine-tuning step as existing SAEs. The rest of this paper is organized as follows. In Section II, AE and SAE are described briefly. Section III presents the details of the proposed methods. The experiment setup and results are discussed in Section IV. Finally, Section V concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2905830", "references": [1622676895, 1930624869, 2002106843, 2025768430, 2026131661, 2062083887, 2072128103, 2101491865, 2104290444, 2119821739, 2121647436, 2140336071, 2140833774, 2156718197, 2170505850, 2218318129, 2317595875, 2412781046, 2461729787, 2480364715, 2618530766, 2731659850, 2746106790]}
{"paragraph": "The static synchronous compensator (STATCOM) as a member of the flexible alternating current transmission system plays an important role in modern power systems in supporting the grid voltage and improving the power factor. The cascaded H-bridge (CHB) multilevel converters have been widely used in medium-voltage high-power STATCOM systems given the following advantages: transformer less, low switching frequency, low dv/dt, and linearly increased number of components with the number of stages. Although industrial grade products are already available, various control strategies and modulation methods are being proposed to improve the performance of the CHB-STATCOM. Recently, model predictive control (MPC) as a promising control method in power electronics has received growing attention. Unlike conventional control strategies focusing on exploring the closed-loop control law, the MPC transforms the control problem into an online open-loop optimization problem by building discrete mathematical models and cost functions. By applying MPC for the CHB STATCOM, the dynamic response can be improved and the multiple objectives can be controlled flexibly. Despite these superiorities, some issues like the steady-state precision, the robust performance, and the high computation complexity still limit the practical application of the MPC. Several methods have been reported to improve the control performance of the dc-link voltage and ac. This paper investigates the high time complexity issue in the MPC for the CHB-STATCOM. The underlying optimization of the MPC is a standard linear or quadratic integer programming problem. And its time complexity increases exponentially with the number of cascaded stages in a CHB converter. Considering the very short control time and relatively low-profile processors in power electronics, the online implementation of the MPC for a large-scale CHB converter is quite challenging. Many works have been reported to deal with the abovementioned issues. Some approaches like the adjacent vectors, the cell by cell method, the preselection algorithm, and the heuristic MPC have been proposed to reduce the computation time. However, those methods may cause suboptimal results. A sphere-decoding algorithm is adopted to reduce the computation time of the multistep finite control set (FCS)-MPC of the CHB converter. However, the performance of the sphere-decoding algorithm depends on the selection of initial radius. Specifically, a small initial radius may lose optimal solution and a large initial radius still has a long execution time. Explicit MPC has shown very high efficiency by solving parts of the algorithm offline with parameter programming tools, but it is only suitable for the linear predictive model. Recently, a partially stratified optimization method has been proposed to convert the MPC into multiple optimization problems, resulting in a simpler method for each optimization problem. Similar methods have been called as the dual stage MPC and the priority sorting approach for other multilevel converters. With the stratified method, the MPC of CHB STATCOM can be rebuilt as the current MPC and the capacitor voltage balancing (CVB)-MPC, where the former can be solved within linear time by the simplified branch and bound algorithm proposed and the latter can be solved within polynomial time by the mixed optimization algorithm. Therefore, the overall time complexity has been reduced to the polynomial level. Considering the mainstream control platform in multilevel converters systems is the digital signal processor (DSP) + field-programmable gate array (FPGA) architectures, where a low-cost DSP is used for executing algorithm and a small-size FPGA is used for the pulsewidth modulation (PWM) modulation and various logic functions like sampling or fault protection. If directly implementing the MPC on the abovementioned architecture, the algorithm execution time will be very long for large-scale systems, limiting the practical application of MPC. In view of the parallel operation nature in both the FPGA and the MPC, the FPGA is expected to significantly reduce the execution time of MPC. In one implementation, the double-step MPC for a variable speed drive system with a three-level converter has been implemented on a small-size FPGA within 17.26 μs. The FPGA implementation method has also been utilized on other topologies like two/three-level back to back converter, three-phase four-leg grid-tied inverters, and direct matrix converters. Although FPGA has shown excellent performance on implementing MPC for the abovementioned topologies, in fact, the computation burden of the optimization problem in abovementioned topologies is not very high. If the computation burden is further increased, the FPGA implementation method still suffers from high space complexity. For example, the computation power of FPGA has been evaluated on a multistep MPC for a three-level flying capacitor converter. When the predictive step exceeds five, the FPGA implementation will be also very challenging. The computation burden in multistep MPC is quite like the multilevel MPC. A high level graphical programming language is applied to implement the MPC algorithm. That method can simplify the programming in FPGA, but it will further intensify the high space complexity issue especially for small-size FPGAs. Several works have been done for the parallel implementation of MPC for multilevel CHB converter. By allowing one leg to be switched in every switching period, the M3PC for seven-level CHB solid transformer has been implemented on a DSP+FPGA platform. However, it may not solve the global optimal result, limiting the dynamic control performance. Diophantine equations are utilized to simplify current-control MPC of a multilevel CHB-STATCOM in wind farm application. Although the computation time has been reduced to a relatively small value, it does not use MPC strategy in balancing the dc-link voltages. Based on the abovementioned analysis, the conventional parallel implementation method is effective when the optimization problem is not very complex. However, for the MPC of large-scale CHB STATCOM, its online implementation needs further optimization. Implementing MPC for multilevel converters suffers from long execution time (on DSP) or excessive resource consumption (on FPGA). The contribution of this paper is proposing an implementation method to reduce the time and space complexity of the MPC at the same time for CHB multilevel converters to a linear level. Then the MPC of a large-scale CHB-STATCOM can be implemented online with a low-cost DSP and a small-size FPGA. Despite the implementations of the MPC on FPGA or DSP+FPGA are well-known. But if directly implementing the optimization algorithms on the DSP+FPGA platform, the resource consummation will be very high, requiring high-power FPGA for large-scale CHB-STATCOM. In case of the abovementioned problem, the algorithm architecture and execution timing have been carefully designed to trade off the execution time and the resources utilization. The FPGA is not only used for logic functions but also for executing the six high-complexity sorting tasks in CVB-MPC. And the DSP is configured to execute the simplified branch and bound algorithm in current MPC and other low-complexity calculation tasks in CVB-MPC. By running the DSP and FPGA in parallel, the total algorithm execution time is only determined by that of the DSP, which is a linear level. Then, a parallel comparison-based algorithm with multiple comparator accumulators (CACs) is proposed to minimize the resource utilization of the sorting algorithm in FPGA under a limited time range. The global optimization of the MPC for medium-voltage large-scale CHB STATCOM can be implemented online with the existing DSP + FPGA architecture.", "section": "Introduction", "doi": "10.1109/TIE.2019.2901647", "references": [1980542474, 2112169395, 2144038908, 2207874715, 2334725632, 2344920143, 2407893810, 2407934800, 2514670737, 2548618581, 2587229405, 2591461235, 2594512086, 2605820745, 2606297888, 2607655799, 2800412091]}
{"paragraph": "Integration technologies are driving industrial automation. The continuous integration of sensors, controllers, robots, tools, etc. has brought the concepts of self-aware equipment, intelligent factory, and cyber-physical system, among others. Recent studies introduced the development of integration technologies. In industrial automation, the programmable logic controller (PLC) system, motion control system, and visual system have become increasingly important and inseparable, as can be seen in various domains these days. Recent advances in image processing and pattern recognition have contributed to the growth of the visual system, which is being applied in various fields. The visual system could obtain parameters to replace the human visual system and address many tasks by extracting features from the image, especially the ever-growing requirements in industrial scenarios, work that is finished in dangerous environments, tasks that are challenging for capabilities of human vision, and applications to large-scale industrial production, wherein numerous visual systems are needed. After visual processing, the motion control system, as the power of automation, is normally needed to drive some actuators to finish severe tasks, which remarkably benefits labor force replacement. Furthermore, PLCs have become a base of automation because of their high reliability and easy programming. Many studies are focusing on PLCs to extremely extend their applied fields. For instance, Jiang et al. guaranteed reliability by verifying the programs of the PLCs, Dominic et al. improved their system performance using advanced algorithms, and Wu et al. simplified the development complexity of their system with a special software structure. Using the aforementioned visual, motion control, and PLC systems, Chen et al. implemented a typical case, as described below. The visual system analyzes the context and obtains the error inserted into the motion control system. Simultaneously, the PLC analyzes information, such as the position limitation of every axis, to make some logical judgments accordingly. Regarding the normal development of these applications, the problems are met as follows: The visual, motion control, and PLC systems are individually developed and combined using the communication protocol. However, this method should always be redesigned with the programs in these systems because of the difference of the visual and motion control algorithms and their organized logic programs. Moreover, many communication protocols such as EtherCat, Modbus, CAN are used in these systems. The visual, logic, and motion control programs are always mixed together and developed within numerous visual systems, PLCs, and motion controllers, which increases the unnecessary complexity. The requirements from customers are also increasing. Combined with the aforementioned two points, the development of corresponding applications has become a cumbersome task for developers. Hence, how to pose a flexible structure for the integration of the visual, motion control, and PLC systems, which will ease the complexity, has attracted our interest. The contributions of our study are as follows: We propose herein a vision control algorithm (VCA) protocol for generic data interaction between ePLC and visual system, which benefits the reuse of algorithms and the uniform use of communication protocols. A multilevel flexible architecture is posed for individual programming, which includes the flexible, control, and algorithm layers. Correspondingly, the customized hardware, memory allocation, and Petri-net-based multithreading structure are described to support the proposed flexible architecture. The VCA protocol-based multilevel flexible architecture implements further integration of the visual, motion control, and PLC systems. The two implemented cases indicate that the proposed architecture could reduce the complexity of one application and from one to another. The rest of this paper is organized as follows. Section II introduces the related works. Section III presents the hardware and software structure and memory allocation. Section IV discusses the mechanism of the VCA protocol in detail. Section V illustrates the execution process of the proposed system. Section VI presents the two implemented cases, binocular catching robot and winding machine with a visual system. Section VII concludes this paper. In addition, abbreviations are listed in Table I for readability, which does not include the ones in Section III C and Sections IV A and B.", "section": "Introduction", "doi": "10.1109/TIE.2019.2902783", "references": [250665209, 1908361146, 1964645131, 2000757515, 2031643315, 2045388415, 2064143234, 2074638806, 2079144454, 2116258318, 2313767655, 2341523100, 2463984612, 2523469470, 2525538692, 2765445970, 2769685271, 2801320097, 2805077076]}
{"paragraph": "The clustered vehicle-routing problem is a variant of the classical capacitated vehicle-routing problem in which the customers are grouped into disjoint clusters. A feasible CluVRP route must serve each cluster integrally, that is, all customers of a cluster must be served by the same vehicle and in consecutive visits. This problem has been approached by exact optimization algorithms as well as metaheuristics. We consider a relaxation of the CluVRP, the soft-clustered vehicle-routing problem, in which the only additional constraint compared to the CVRP is that all customers of a cluster must be served by the same vehicle. In contrast to the CluVRP, visits to customers of the same cluster may or may not be interrupted by visits to other customers. The SoftCluVRP has been introduced previously. CluVRP and SoftCluVRP arise in applications where the routing decision must take into account already taken clustering decisions. For example, parcel or small-package delivery in courier companies is an application field. In a first step, parcels are sorted according to given districting. Typically, the sorting policy, i.e., the sorting of parcels by regional zones and or ZIP codes, is altered only once in a while. Note that the sorting policy must always be fixed before the actual demand distribution over the zones is known. In a second step, the batches of parcels, as they result from the sorting, are delivered to the recipients. An instance of the SoftCluVRP is defined over a complete undirected graph with the vertex set and the edge set E. The vertex set comprises the depot vertex 0 and the customer vertices. The vertices are partitioned into clusters where we define the depot cluster for convenience. The customer clusters are indexed and a positive demand is associated with each cluster, while the depot cluster has zero demand. For each cluster, the cardinality of a cluster is defined and refers to the index of the cluster to which each vertex belongs. A homogeneous fleet of m vehicles with capacity Q is hosted at the depot 0. Non-negative routing costs are associated with every edge. Note that for the CluVRP the constraints must be replaced by the following conditions: for all clusters, there exists an index such that the hard-cluster constraints are satisfied. The contribution of this paper is the design and computational analysis of different branch-and-price algorithms for the exact solution of the SoftCluVRP. Branch-and-price is the leading solution approach for many variants of the VRP and can be summarized as follows: A route or path-based extended formulation of the VRP variant, the so-called master program, is solved via column generation. The starting point is always a restricted master program that comprises a small subset of routes and relaxes the integrality constraints on the route variables, with integrality later enforced via branching. Missing routes for the solution of the linear relaxation of the master program are dynamically and iteratively generated with the help of a pricing subproblem. This pricing problem can be formulated as a shortest-path problem with resource constraints. For almost all VRP variants, the associated SPPRCs are best solved with dynamic-programming labeling algorithms. To our surprise, the SPPRCs that result from the SoftCluVRP are extremely difficult to solve even for instances of rather moderate size. The paper at hand will show that even with all the recent acceleration techniques available for the SPPRC dynamic-programming labeling algorithms, the situation hardly improves. For example, bidirectional labeling has proven very effective for many VRP variants but the soft-cluster constraints are so loose that the combinatorial explosion is often not effectively suppressed. A similar phenomenon is reported for some loosely-constrained VRPs with pickup-and-delivery structure. Our most important finding is, therefore, that a relatively simple integer programming formulation solved with a standard IP-solver is highly effective for the SoftCluVRP pricing problems. This result is rather remarkable because IP-based approaches such as branch-and-cut have almost never reached the performance of labeling-based pricing algorithms. The remainder of this paper is structured as follows. Section 2 provides a compact three-index formulation for the SoftCluVRP that we use to derive the path-based reformulation and the pricing subproblem. Section 3 presents the two exact solution approaches for the pricing subproblem that are based on dynamic-programming labeling and branch-and-cut. Moreover, a primal heuristic pricing algorithm tailored to the SoftCluVRP is presented. The overall pricing strategies as well as branching and its impact on the subproblem is discussed in Section 4. Section 5 summarizes the comprehensive computational studies conducted on the new branch-and-price algorithms. Final conclusions are drawn in Section 6.", "section": "Methodology", "doi": "10.1016/j.ejor.2019.07.019", "references": [1591939288, 1965032790, 1970355999, 1984779002, 1986056047, 1986261983, 1990372046, 1991149726, 2018269848, 2029341065, 2059816321, 2093698911, 2093795574, 2096910555, 2099391941, 2102076751, 2113137767, 2119097848, 2121666077, 2138121296, 2148767145, 2155597893, 2156117830, 2187883154, 2336259629, 2586188487, 2593783647, 2597991082, 2601282084, 2732379263, 2768260271, 2899944127]}
{"paragraph": "In this work we describe the application of combinations of symbolic computation methods in various computer algebra systems to a key problem from computational biology. The work serves to demonstrate how recent advances in such algorithms, and crucially their effective combination, allows for their application on problem instances previously thought beyond reach. In this introduction we start by describing the biological networks that are our topic of study, and highlight previous relevant work. We then outline the remainder of the paper and clarify the relationship of this article to prior work. The mathematical modelling of intra-cellular biological processes has been using nonlinear ordinary differential equations since the early ages of mathematical biophysics in the 1940s and 50s. A standard modelling choice for cellular circuitry is to use chemical reactions with mass action law kinetics, leading to polynomial differential equations. Rational functions kinetics, for instance the Michaelis-Menten kinetics, can generally be decomposed into several mass action steps. An important property of biological systems is their multistationarity by which we mean their having multiple stable steady states. It is instrumental to cellular memory and cell differentiation during development or regeneration of multicellular organisms and is also used by micro-organisms in survival strategies. It is thus important to determine the parameter values for which a biochemical model is multistationary. As demonstrated in the next section, with mass action reactions, testing for multiple steady states boils down to counting real positive solutions of algebraic systems and so is suitable for study with symbolic computation and computer algebra systems. The models studied in this paper concern intracellular signalling pathways. These pathways transmit information about the cell environment by inducing cascades of protein modifications all the way from the plasma membrane via the cytosol to genes in the cell nucleus. Multistationarity of signalling usually occurs as a result of activation of upstream signalling proteins by downstream components. A different mechanism for producing multistationarity in signalling pathways was proposed in this mechanism the cause of multistationarity are multiple phosphorylation/dephosphorylation cycles that share enzymes. A simple, two-step phosphorylation/dephosphorylation cycle is capable of ultrasensitivity, a form of all or nothing response with no multiple steady states. In multiple phosphorylation/dephosphorylation cycles, enzyme sharing provides competitive interactions and positive feedback that ultimately leads to multistationarity. Multistationarity has important consequences on the capacity of signalling pathways to process biological signals, even in its elementary form of two stable steady states. This is known as bistability and is present in our case study problems. Bistable switches can act as memory circuits storing the information needed for later stages of processing. The response of bistable signalling pathways shows hysteresis, namely dynamic and static lags between input and output. Because of hysteresis one can have, at the same time, a sharp binary response and protection against chatter noise. Our study is complementary to works applying numerical methods to ordinary differential equations models used for biology applications. Algorithmically the task will be to count the positive real solutions of a parameterised system of polynomial or rational systems, making symbolic methods a possible tool. Due to the high computational complexity of this task considerable work has been done to use specific properties of networks and to investigate the potential of multistationarity of a biological network out of the network structure. However, prior work only determines whether or not there exist rate constants allowing multiple steady states, instead of coming up with a semi-algebraic description of the range of parameters yielding this property. These approaches can be traced back to the origins of Chemical Reaction Network Theory whose main result is that networks of deficiency 0 have a unique positive steady state for all rate constants. Given a bistable mechanism it is also important to compute the bistability domains in parameter space: the parameter values for which there is more than one stable steady state. The size of bistability domains gives the spread of the hysteresis and quantifies the robustness of the switches. The work is relevant here: they used symbolic tools, including cylindrical algebraic decomposition as we do, to determine the number of steady states and their stability for several systems. They reported results up to a 5-dimensional system using specified parameter values, but their method is extensible to parametric questions. Higher-dimensional systems were studied using sign conditions on the coefficients of the characteristic polynomial of the Jacobian. In some cases these guarantee uniqueness of the steady state. In Section 2 we outline the particular biological model and symbolic problem that we aim to solve: BioModel 26 of the MAPK network. In Sections 3 and 4 we describe two independent symbolic attempts to solve the problem. The first in Section 3 is able to identify symbolically the multistationarity regions of a 1-dimensional parameter space with a combination of Virtual Substitution and Cylindrical Algebraic Decomposition in the Redlog package for Reduce. The second in Section 4 goes on to give full semi-algebraic solution formulae with a combination of Real Triangularization and Cylindrical Algebraic Decomposition using the Regular Chains Library for Maple. The solutions were obtained in different computer algebra systems using different fundamental algorithms, but all from the family of methods for real quantifier elimination. We move on in Section 5 to describe a new pre-processing method for the problems inspired by graph theory and Gaussian elimination. Then in Section 6 we describe how a combination of ideas from all three preceding sections can be combined to provide solutions over a 2-dimensional parameter space. In Section 7 we discuss testing the stability of fixed points. Then in Section 8 we consider an alternative larger model from the MAPK network. In Section 9 we compare the models and detour to describe a symbolic grid sampling approach to this problem, including a comparison of this to a leading numerical solver. We consider how further progress could be achieved in Section 10, identifying a conjecture for determining where multistationarity for MAPK may occur without the costly calculations described. Finally we summarise and give final thoughts in Section 11. This journal article follows conference papers at ISSAC 2017 and CASC 2017. The present article reproduces this material clarifying, correcting and extending in places. In particular, Sections 3 and 4 were largely described in the ISSAC 2017 paper and Sections 5 and 9 in the CASC 2017 paper. The most notable new contributions are in Section 6, where we describe for the first time semi-algebraic solutions with two free parameters; and in Section 10, where we identify a promising conjecture for investigation.", "section": "Methodology", "doi": "10.1016/j.jsc.2019.07.008", "references": [126540604, 1788620677, 1916563250, 1966161069, 1974021382, 1975442866, 1983787096, 1986264093, 1988741929, 2007600823, 2013702670, 2022125816, 2026612342, 2026774968, 2040951317, 2051499723, 2051595244, 2055000959, 2065776042, 2067705970, 2082387287, 2104715607, 2114124738, 2114649252, 2120467997, 2126528701, 2149652522, 2150928761, 2370198512, 2510395247, 2516653421, 2589479599, 2608023254, 2611332476, 2724130349, 2963039959, 2963980012]}
{"paragraph": "Compared to induction motor, the PMSM presents several advantages, such as high efficiency, high reliability, high power density and high torque to current ratio. Therefore, PMSM is used in wide applications, such as electric and hybrid vehicles, high-end white goods (refrigerators, washing machines, dishwashers, etc.), high-end pumps, fans and in other appliances. Despite its advantages, the PMSM remains complicated and difficult to control when good transient performance under all operating conditions is desired. This is due to the fact that the PMSM is a nonlinear, multivariable, time varying system subjected to unknown disturbances and variable parameters. Over the past decades, various control techniques have been developed in order to improve the PMSM performance. However, the widely used approach consists in using linear field oriented control theory. Another popular control technique, the DTC has a relatively simple control structure yet performs at least as good as the FOC technique. However, DTC involves hysteresis type, for both stator flux and torque magnitudes control, which introduces variable and uncontrollable switching frequency; which in turn produces large torque and flux ripples and high current distortion. Many artificial intelligence techniques and random search methods have been employed to improve the controller performances. Neural network, fuzzy system, and genetic algorithm have been widely applied to proper tuning of PID controller parameters. But all have some shortages. GA has a big computational complexity. Fuzzy system itself has many parameters to be optimized, the results of these experiments showed that fuzzy controllers perform better, or at least well as, adaptive controllers. Moreover, this technique offers the advantage of requiring only a simple mathematical model to formulate the algorithm, which can easily be implemented by a digital computer. These features are appreciated for nonlinear processes for which there is no reliable model and complex systems where the model is useless due to the large number of equations involved. Nevertheless, the main problem with fuzzy logic is that there is no systematic procedure for the design of fuzzy controller. The superiority of fuzzy controller, it can adapt his structure, acting on number of factors which constitute the internal configuration of this type of controller, such as: Fuzzification blocks, Fuzzy rules, block defuzzification and input, output gains. Additionally, it is possible to use the fuzzy logic to adjust or supervise the parameters of traditional PI regulator. Particle swarm optimization is a stochastic global optimization technique. The PSO technique can generate a high-quality solution within shorter calculation time and stable convergence characteristic than other stochastic methods. Because the PSO method is an excellent optimization methodology and a promising approach for solving the optimal PI controller parameters problem. In this paper, a direct torque controlled PMSM based on the AFLC and the PSO is presented. A comparison study between these techniques is also presented. To show the performances of the DTC of the PMSM based on the AFLC and the PSO, simulation results are presented. To validate the simulation results, these algorithms are implemented on a test bench around a DSPACE 1104. These techniques show high performances compared to the conventional DTC. Comparing these techniques, we see that the DTC based on the PSO is more efficient than the DTC based on the AFLC.", "section": "Related Work", "doi": "10.1016/j.matcom.2018.04.010", "references": [1854387882, 1968093301, 1969900902, 2008059784, 2016150415, 2030282996, 2032649063, 2032775349, 2045059098, 2049768223, 2051736125, 2062238863, 2064981932, 2072662494, 2081696533, 2086068525, 2099829287, 2115521895, 2153587688, 2180382463]}
{"paragraph": "With the rapid development of artificial intelligence technology and network technology, the internet of things has gradually become the mainstream of social development in the future. Under this background, the trade retail industry needs to establish its customer relationship network in combination with artificial intelligence technology. At the same time, it needs to conduct law mining in combination with customer selection behavior in network, and carry out personalized excavation of customers under the support of data mining technology to help customers make decisions. On this basis, it can effectively enhance the customer experience. The research on intelligent customer network has entered a climax since 2010, and related research also provides the basis for the creation of this article. The intelligent customer relationship network usually uses the customer's equipment's movement trajectory data, customer platform operating data, customer network base stations, and other content as customer behavior data. Using this data, researchers started relevant research. Some achieved a goal of recommending to the taxi driver the passenger sequence with the greatest revenue through in-depth analysis and excavation of GPS trajectory data in taxis. Others designed and implemented a time-awareness system that can be used to personalize the taxi drivers travel route with the greatest benefit per unit of time. Based on the different advertising platforms, an advertisement delivery system suitable for mobile web pages and mobile phone apps was proposed by analyzing customer location and related situational information and fully exploiting the mobility of customers in the mobile commerce system. A personalized travel package recommendation system based on tourist interest preferences was designed, which can recommend a set of personalized and best-suited attraction collections for tourists. Mobile customer's check-in data was studied and analyzed to obtain various features of the location social network; based on this, a location-based recommendation algorithm was designed and implemented. With the progress of research, many personalized recommendation systems for mobile clients have also been successfully launched, such as the Facebook mobile application of personalized push ads, the personalized Bizzy recommended by local shops, and the personalized reading system Zite. A detection method for mobile app ranking fraud was proposed by exploring personalized preferences mining method for mobile customers based on context awareness. Security privacy issues under personalized recommendation technology were discussed, and a mobile app recommendation algorithm was proposed to protect customer information security. Based on statistical analysis of a large number of microblog customer data, a method for personalizing popular micro topics was proposed by calculating similarities between microblog customers and micro topic. In addition, in terms of data sparsity and cold-start problems faced by collaborative filtering, the use of the K-nearest neighbor method was proposed to map attribute-feature and calculate the feature vectors of new customers and new projects. A combination of data migration and data clustering was proposed to solve the system cold start problem. To solve the problem of sparseness in collaborative filtering algorithms, a clustering method based on the attributes of the project and the use of the mean of the project categories to fill in the null values in the original scoring data was proposed. At present, major e-commerce platforms at home and abroad have developed their own mobile terminals. However, the research and application of personalized recommendation systems for mobile platforms is still in its infancy, and there is still room for improvement in its recommendation quality and operating efficiency. The idea of considering the trust between customers in the recommendation process was first proposed. The trust between customers is established through the displayed customer trust evaluation and debilitating spread. The trust is divided into reliability trust and decision trust. The reliability trust is the subjective probability that entity A acts according to entity B's expectations, and decision trust refers to the subjective degree of relative security feeling obtained by an individual trusting a certain entity in a certain environment. One approach uses the ratio of the number of customer recommendations to the total number of recommendations as the degree of trust between customers and applies this calculation method to the recommendation system, where the confidence value ranges from 0 to 1. A trust model based on fuzzy logic representation was also proposed based on the fuzzy nature of trust relationships. Benefiting from the development of Internet of Things technology and data mining technology, the spread of consumer trust has become multi-directional. It has been mentioned that all the characteristics of s-commerce (except for economic feasibility) had significant effects on trust and that trust had significant effects on purchase intentions. Hence, the characteristics of consumer trust communication and behavior decision-making under the Internet of Things are necessary to study. Based on the above analysis, we can see that the current decision model based on the Internet of Things to build a customer relationship network is less researched, and most of them are recommending unilateral information to customers based on personalized recommendations. Therefore, based on the Internet of Things technology, this study builds a more complete customer relationship network based on personalized recommendations, and adopts an improved collaborative filtering recommendation algorithm as a basis for decision models to extract contextual features that characterize customer trust. At the same time, this research uses the analytic hierarchy process to complete the model building process, helps customer relationship network service objects to provide decision support, completes product information recommendation, solves new customer cold start problems, and improves existing scoring prediction formulas. Therefore, this study fully considers the impact of customer trust relationships on the recommendation results in the scoring prediction process. The remainder of this article is organized as follows. Section 2 is the literature review and methodology. The experiment results are showed in Section 3. Section 4 presents discussion in the experiment results. The conclusion is presented in Section 5.", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2018.11.013", "references": [63326921, 165056234, 1727274110, 1874148276, 1943579973, 1994223401, 2015177282, 2042874218, 2043472477, 2056660873, 2061222800, 2103281268, 2108142795, 2128991589, 2228664849, 2310994956, 2773841654, 2788552370, 2789731659, 2790855116, 2893840913]}
{"paragraph": "The standard definition of fuzzy sets identifies them with pairs, where X is a crisp domain and a membership function is defined on X and valued in a suitable structure L of membership degrees—often the real unit interval, which will be our choice throughout this paper. We then speak of a fuzzy subset of the crisp domain X or a fuzzy set on X. It can be noticed that even though the domain X can in general vary, and so fuzzy sets involved in a given construction can generally have different domains, the usual fuzzy set-theoretical operations (such as the unions or intersections of fuzzy sets) and relations (such as inclusion or equality) are traditionally defined only for fuzzy sets on the same domain X. One may then wonder if and how unions and intersections, or any other fuzzy set-theoretical relations and operations, can be defined for fuzzy sets with different domains. The common—and often implicit—method of circumventing the issue of different domains is to choose a sufficiently large reference set X so that the domains of all fuzzy sets under consideration are subsets of X, and extend each membership function to the whole domain X by setting its value to zero for all elements outside its original domain. Then all of the fuzzy sets become defined on the same domain, and the standard, common-domain definitions of fuzzy set-theoretical operations can be applied. We will call this method the unification of domains. It can be claimed that if supplemented with the unification of domains, the theory of fuzzy sets on a fixed domain is sufficiently general for all common purposes, and so the variability of domains need not be considered. In Section 2 of this paper we are, nevertheless, going to argue a case that the unification of domains is not always harmless; and that, therefore, the actual domains of fuzzy sets need sometimes be tracked and taken into account when applying fuzzy set-theoretic operations. The resulting problem of how to define suitable operations for fuzzy sets with different domains is examined in Section 3. In Section 4 we discuss several ways of representing fuzzy sets with variable domains and opt for a modified vertical representation as the most flexible option. The resulting technical apparatus of variable-domain fuzzy set theory will be described in the separate second part of this two-part paper. The capability of the variable-domain approach to resolve the issues raised in Section 2 is assessed consecutively in the closing sections of both parts of the paper.", "section": "Related Work", "doi": "10.1016/j.fss.2018.11.002", "references": [1898259180, 1966322836, 1980724971, 1989393769, 1991673847, 2009738227, 2015943153, 2025256895, 2054789666, 2066915024, 2083449452, 2087435110, 2118819262, 2912565176, 2943118972]}
{"paragraph": "The Shapley value and the prenucleolus are two well-known values for cooperative games. The Shapley value is an average of the contributions of an agent, while the prenucleolus is the value that minimizes the dissatisfaction of the worst-off coalitions. The nucleolus differs from the prenucleolus by only taking into account individually rational imputations. Coincidence between the Shapley value and the prenucleolus is uncommon and, in general, difficult to check without computing both values. A sufficient and necessary condition for this coincidence to hold has been provided, but it requires the computation of both the Shapley value and of a parametric family of sets, for which the computation mimics that of the prenucleolus. This characterization can be applied in order to identify the coincidence in some particular classes of games, such as airport games, bidder collusion games, and polluted river games. Coincidence is also found in some three-agent games based on bankruptcy problems. For general coalitional form games, we have coincidence if the game only has two agents or if all agents are symmetric within the normalized game. Some other games have also been proposed, all having in common that the value of a coalition is equal to the sum of the values created by the pairs composing that coalition. These games are called 2-additive games. The coincidence persists in games that satisfy a certain property. These games are such that the contributions of an agent to any coalition and its complement sum up to an agent-specific constant. A particular instance of such games has also been studied. Some researchers study the coincidence from a geometric point of view. Instead of providing classes of games where both values coincide, they study the properties that lead to this result in some already existing classes, such as certain types of games. A similar, yet different problem, is the invariance of the payoff assigned by an allocation rule to a specific player in two related games. In this paper, we present another family of games, called clique games, in which the Shapley value and the nucleolus coincide. The family can be described as follows: the set of agents is divided into cliques that cover it. A coalition creates value when it contains many agents belonging to the same clique, with the value increasing linearly with the number of agents in the same clique. Agents may belong to more than one clique, but the intersection of two cliques contains at most one agent. Finally, if two agents are not in the same clique, there exists at most one way to connect them through a chain of connected cliques. The family of clique games has a non-empty intersection with some other classes of games, and that intersection consists of 2-additive games. Some clique games are not in these other classes, and some of those games are not clique games. A clique game is convex, and hence its Shapley value is the average of the extreme points in its core. We thus obtain a link between three crucial concepts of cooperative game theory: the nucleolus, the core, and the Shapley value. Naturally, graph-induced games provide a fertile ground to apply our result. We first consider graph-restricted cooperative games. In these games, a coalitional value function is accompanied by a graph that summarizes the cooperation possibilities: a coalition cannot fully cooperate if some of its members have no path between them that uses only the vertices of agents in the coalition. When we consider a symmetric coalitional value function, assigning shares of the value created among agents is akin to defining centrality measures. We show that when the coalitional value function increases linearly with the number of agents in a coalition, we obtain coincidence of the Shapley value and the nucleolus for a family of graphs. Another graph-induced game that we study is the minimum coloring game, in which the graph represents conflicts between pairs of agents. We wish to assign agents to facilities, but cannot assign agents that are in conflict to the same facility. As facilities all have a cost of one, we wish to minimize the number of facilities used. Coincidence between the Shapley value and the nucleolus for a particular family of graphs can be explained by the fact that the graphs induce clique games. Our third example is the minimum cost spanning tree problem. This well-studied problem has agents connecting to a source through a network, with the cost of an edge being a fixed amount that is paid if the edge is used, regardless of the number of users of the edge. Any such problem has a non-empty core even though it may not be convex. Moreover, its Shapley value is not always in its core. When we consider elementary versions of this problem in which all edges have a cost of zero or one, the subset of cycle-complete problems generates clique games. Cycle-complete problems are such that if there exist multiple distinct free paths between a pair of agents the edge connecting them directly must also be free. Our result on clique games then applies, yielding that the nucleolus coincides with the Shapley value and the permutation-weighted average of extreme core allocations. The paper is divided as follows: preliminary definitions are in Section 2. Section 3 describes and illustrates clique games. Section 4 contains the coincidence results. Applications to graph-induced games are discussed in Section 5.", "section": "Introduction", "doi": "10.1016/j.mathsocsci.2019.10.002", "references": [1964733860, 1975617599, 1983125930, 1995443028, 1995559809, 2008131782, 2022418489, 2032441230, 2046116913, 2073225716, 2081334386, 2087121928, 2103267790, 2106417017, 2113370239, 2192499632, 2292459874, 2515623706, 2625101186, 2963187969]}
{"paragraph": "In most real-world applications, uncertainties cannot be avoided. Many tools or methods have been proposed to deal with different kinds of uncertainties. For example, statistical methods are used to cope with randomness in many applications, whilst fuzzy sets are utilized to handle cognitive uncertainties. Since the appearance of the fuzzy set, it has been applied in many areas including the modelling, prediction and control domains. However, fuzzy membership functions are of a quantitative nature, which makes them inappropriate for applications that have a qualitative nature. It is practically important to approximate a fuzzy set by using several levels of grades to achieve an acceptable trade-off between accuracy and cost. Three-way approximations provide us with an important means to achieve this objective. Three-way approximations of fuzzy sets can be made through a pair of thresholds. The concept of shadowed set approximates a fuzzy set by mapping its membership grades to the truth values 0, 1 or to the unit interval. Decision-theoretic three-way approximations of fuzzy sets have been introduced in which the required thresholds are computed by minimizing the decision cost. An optimization-based framework for constructing three-way approximations of fuzzy sets has also been proposed. The three-way approximations can be interpreted in terms of the positive, negative and boundary regions within the theory of three-way decisions and have been studied by various researchers. As a special case of the three-way approximations of fuzzy sets, the shadowed set does not have precise numeric membership values. Its distribution is partitioned into three distinct zones, which are the core zone, the shadowed zone, and the exclusion zone. Core and exclusion zones have truth values, whilst the shadowed zone is an entire unit interval perceived as a zone of uncertainty. In recent years, various studies on shadowed sets have addressed the construction of shadowed set models. Generally, there exist two ways to generate shadowed set models. The first aims to induce shadowed sets from existing fuzzy sets. Extensive discussions have been made on the calculation of the optimized threshold for inducing shadowed sets from triangular, Gaussian and parabolic fuzzy sets. To remedy the deficiencies in existing methods and retain the total amount of fuzziness and the localized redistribution throughout a universe of discourse, a new algorithm for the induction of shadowed sets based on gradual grades of fuzziness was introduced along with analytical formulas to calculate the threshold values. Shadowed fuzzy sets have also been defined and their construction demonstrated. Shadowed set approximations of fuzzy numbers have been presented through a newly defined approximation measure, showing that the shadowed set approximation preserved the expected interval and width of a fuzzy number. Error-based three-way approximations of shadowed sets for dynamic fuzzy sets have been constructed and an alternative decision-theoretic formulation for calculating the thresholds has been presented. A principle of trade-off in games has also been applied to determine the thresholds of three-way approximations in the shadowed set context. The other way to generate shadowed set models is by shadowed clustering algorithms. The shadowed C-means clustering algorithm was developed which can automatically choose the threshold parameters and was applied to satellite image segmentation. A shadowed set-based rough-fuzzy clustering method was proposed, which automatically decides the threshold parameter and then determines the lower bound and boundary region of each cluster. A spatial shadowed C-means clustering algorithm was proposed by implanting local spatial information in the estimation procedure of membership values, and was successfully applied to image segmentation. A new technique of rough-fuzzy clustering based on multi-granulation approximation regions was developed to deal with the uncertainty associated with the fuzzifier parameter, and an active three-way clustering method via low-rank matrices was investigated to satisfactorily address the uncertain relationship between an object and a cluster. In the paradigm of Computing with Words, the first step is to construct models of the linguistic words. Different methods have been proposed to model linguistic words by fuzzy sets or membership functions. The fuzzy Delphi method has been used for membership function fitting and applied to a human resources assignment problem. The maximum entropy principle has been adopted to automatically determine the membership functions for linguistic words. A fuzzy measure method has been used for automatic membership function estimation. An interactive fuzzy multi-objective approach has been used to generate modified S-curve membership functions and applied to a transportation planning problem. These methods are all based on the collected crisp data. On the other hand, for a specific word, if one person is asked to provide a value for it, usually one interval will be given owing to the vagueness of the word and the uncertainty of the person surveyed. In addition, different people usually have different understandings of the same word. As a result, if different people are surveyed, different intervals will be obtained. Researchers have discussed how to construct interval type-2 fuzzy set models for linguistic words using this type of interval data. A type-2 fuzzistics method, which represents the interplay between fuzzy sets and statistics, has been proposed for constructing symmetric models based on the collected interval data. Conversion of interval data to Gaussian fuzzy sets via fuzzy statistics and membership function fitting has also been discussed, and cloud models for words have been generated. The shadowed set theory provides us with another tool to model the uncertainties in the collected interval data. However, shadowed sets are usually induced from the existing fuzzy sets or are established by a shadowed clustering algorithm. To the best of the authors’ knowledge, there is no research focusing on the interval data driven construction of shadowed set models. Therefore, how to utilize such interval data to construct the shadowed set models for linguistic words remains to be studied. On the other hand, how to construct the shadowed set models directly from the interval data and which types of shadowed sets are appropriate to be chosen as word models must be addressed before deciding on the data driven modelling algorithm. Another important point is how to evaluate the constructed shadowed set models. This paper attempts to solve these issues and provide an effective interval data driven modelling algorithm for constructing the shadowed sets for linguistic words. Our study, which directly utilizes the interval data to construct the shadowed set models, provides a new perspective in the research areas of shadowed sets. The main contributions of this study are listed in detail as follows. Corresponding to the popularly used fuzzy sets for linguistic word modelling, four kinds of shadowed sets are introduced according to their shapes. These shadowed sets may include the most widely used ones in real-world applications, especially in linguistic word modelling applications. The normal, left-shoulder and right-shoulder shadowed sets correspond to the most widely used trapezoidal or triangular shapes in modelling and control applications. An interval data driven approach is proposed to achieve the modelling of shadowed sets for linguistic words. The proposed approach includes three main steps, which are the interval data pre-processing, statistics computation of the remaining interval data, and the shape and parameter determinations of the shadowed sets. Especially, in the statistics computation step, two different methods – the tolerance limit method and the percentile method – are presented. To test the reasonability of the constructed shadowed set models, both the uncertainty-capture capability and accuracy are taken into account, and three novel indices are proposed. The first index computes the proportion of the left and/or right-end points being included in the shadowed zone, whilst the second index considers the ratio of the core zone to the shadowed zone. The third index is a hybrid index, which is comprised of the first two indices to balance the uncertainty-capture capability and the accuracy. To verify the proposed method, it is applied to two applications which are the 32-word codebook example and the thermal feeling words modelling problem. In addition, comprehensive comparisons with other algorithms and analyses of the results are provided. The organization of this paper is as follows. In Section 2, introductions and definitions of shadowed sets will be given. In Section 3, an interval data driven method for constructing shadowed set models will be proposed. In Section 4, the proposed method will be applied to the 32-word codebook example and the thermal feeling word modelling problem. In this section, detailed comparisons and comprehensive analysis will also be given. Finally, in Section 5, conclusions will be made.", "section": "Introduction", "doi": "10.1016/j.ins.2018.11.018", "references": [101345952, 1516346799, 1549841259, 1570082825, 1843415386, 1981231764, 1984472447, 1988695218, 1998965536, 2002278745, 2006873874, 2010360766, 2020694587, 2023843216, 2039742983, 2052722125, 2061430293, 2070813883, 2078757499, 2089923511, 2091336043, 2093400038, 2114832876, 2117501337, 2149017759, 2153406165, 2154437129, 2171527822, 2238283829, 2297889545, 2398851267, 2435864146, 2475810011, 2620114837, 2792234535, 2806445482, 2884189343, 2884615103, 2887732625, 2912565176]}
{"paragraph": "Three-way decision model 3WD is a trisecting-and-acting model for complex problem solving. The basic idea of this model is to divide a universal set of objects into the three pair-disjoint regions and to devise the different effective strategies to act upon those objects of each region. In recent years, there are several successful application results in many fields such as investment management, cluster analysis, face recognition, recommendation and the others. However, how to act upon three probabilistic regions of 3WD is still under an active research. By considering the costs of decision results and decision processes, a typical model of 3WD, sequential three-way decision S3WD, has emerged and become a cost-effective decision-making method. This model aims at achieving a required level of accuracy with a minimal cost in obtaining evidence especially for a problem under multiple levels of granularity. Classical sequential three-way decision models mainly implement a sequential, multistep three-way decision-making using a multilevel granular structure of the universe with respect to a sequence of sets of attributes. In some cases, a more reasonable decision-making for complex problem solving is made under multiview granular structures. Therefore, the extended sequential three-way decision models still consider multiple independent granular structures of the universe for decision-making. As we all know, Qian et al. introduced multigranulation rough set model, where the lower and upper approximations of a concept are characterized under multiple granular structures. The multigranulation rough set models using different multiple binary relations for different information systems have been paid widespread attentions. The researches on multigranulation rough set models mainly focus on the following three aspects. The first aspect is the extended model researches on multigranulation rough set models using multiple different binary relations. Using multiple tolerance relations, Qian et al. and Yang et al. discussed incomplete multigranulation rough sets, and Xu et al. constructed and discussed two types of multigranulation tolerance rough set models. Furthermore, Xu et al. constructed multigranulation fuzzy rough sets based on the fuzzy approximation space using multiple fuzzy relations. Lin et al. constructed multigranulation covering fuzzy rough sets in a covering approximation space. Huang et al. discussed an intuitionistic fuzzy multigranulation rough set model. In addition, Qian et al., Xu, Yang and Guo, Liu et al., Sun et al. and Feng and Mi studied different multigranulation decision-theoretic rough set models. The second aspect is knowledge discovery researches on multigranulation rough set model. Liang et al. proposed an efficient feature selection algorithm using a multigranulation view. Lin et al. presented a feature selection method using neighborhood multi-granulation fusion. Li et al. compared multigranulation rough sets with concept lattices via rule acquisition, and further constructed a three-way cognitive concept learning model via multi-granularity. Tan et al. constructed belief structures and characterize knowledge reduction in terms of evidence theory for the multigranulation spaces with decisions. The third aspect is topology analysis researches on multigranulation rough set model. Yang et al. studied the hierarchical structures of multigranulation rough sets. She and He discussed the topological and lattice-theoretic properties of multigranulation rough sets. Yao proposed a unified framework to classify and compare two basic models based on the construction of a family of equivalence relations and a combination of the family of approximations, respectively. With the insightful gain from above multigranulation discussions, one can find a more reasonable decisions for all objects should be made via sequential three-way decisions and multiview granular structures. To this end, we combine with sequential three-way decisions and multigranulation rough sets to build a generalied sequential three-way decision model via multi-granularity. More specifically, we adopt the different aggregation strategies to aggregate the lower approximations and the upper approximations, and construct the weighted arithmetic mean multigranulation sequential three-way decisions WAMMS3WD, the optimistic multigranultion sequential three-way decisions OMS3WD, the pessimistic multigranultion sequential three-way decisions PMS3WD, the pessimistic-optimistic multigranultion sequential three-way decisions POMS3WD and the optimistic-pessimistic multigranulation sequential three-way decisions OPMS3WD. Furthermore, we discuss the rightness and rationality of these models and analyze the corresponding relationships and differences. Finally, real-life experiments are employed to demonstrate their feasibility and effectiveness. The remainder of this paper is organized as follows. The next section deals with some preliminary concepts and properties regarding the Pawlak’s rough sets, multigranulation rough sets and decision-theoretic rough sets. In Section 3, we introduce the five types of multigranulation sequential three-way decisions MS3WD and investigate the corresponding properties. In Section 4, we analyze the relationships and rationality among the first four types of MS3WD models. Finally, Section 5 concludes the paper.", "section": "Related Work", "doi": "10.1016/j.ins.2019.03.052", "references": [95749022, 1021271351, 1131999718, 1606022329, 1704544815, 1751558718, 1843766148, 1966546225, 1969463949, 1973688644, 1976555061, 2013267216, 2029753718, 2037053120, 2048472139, 2050237791, 2050515685, 2057608478, 2067179717, 2069125985, 2070813883, 2074145995, 2078032741, 2083449452, 2145020281, 2160396543, 2162755671, 2167293341, 2170755382, 2172368975, 2208856346, 2217596628, 2252128398, 2272554295, 2305735113, 2310435784, 2340020088, 2345465422, 2350506101, 2549504134, 2550409005, 2600420962, 2617679434, 2727405836, 2737801118, 2892294644]}
{"paragraph": "The Task of background subtraction is to divide an observed image into two complementary sets of pixels, i.e., the foreground object and the background that does not contain any object of interest. For analyzing an image or a video, the computational efficiency can be improved significantly by dealing with only the foreground object detected via background subtraction. In most background-estimation methods, the static part is generally assumed to be the background, and the rest is considered as the foreground object. Nevertheless, background subtraction remains a challenging research topic nowadays, because of many unfavorable factors in real applications, such as illumination variations, noise, ghost artifacts, etc. So far, many effective works have been reported for background estimation. A complete survey was provided for both the traditional and recent approaches. In that survey, the various algorithms were categorized in terms of the mathematical models and the critical situations to handle. The Gaussian model is a relatively simple and widely used statistical model. In the Gaussian model, the pixel intensity values of the consecutive frames can be modeled by a Gaussian distribution. As the most common approach, Gaussian mixture model was proposed to deal with the dynamic backgrounds. Subsequently, many improvements to the GMM were developed to make it more robust and adaptive to the critical situations. Nevertheless, for the background and foreground, the probability density functions are likely to vary from image to image, and will not have a known parametric form. By estimating the pdf directly from the data, a nonparametric kernel density estimation approach was proposed to build the statistical representations of the background and the foreground. Instead of using an explicit pixel model, each background pixel is modeled with a set of closest samples in the Visual Background Extractor. In another approach, each background pixel is assigned with a series of key color values stored in a codebook. A multiscale spatial-temporal background model was proposed to detect motion in low contrast dynamic scenes. A robust background model for object detection, namely co-occurrence probability-based pixel pairs, was proposed to deal with the illumination variation and the burst motion. A thorough review was made for the recent developments of a class of approaches, which can be represented by a decomposition framework of low-rank plus additive matrices. Compared to those approaches based on mono or trichromatic images, the multispectral images can provide a more elaborate spectral-spatial and temporal model for a more accurate segmentation. In order to alleviate the heavy computation cost and complexity, an online stochastic tensor decomposition approach was proposed for background subtraction in multispectral video sequences. To ensure continuity on the spatial and temporal manifolds, a background–foreground model was proposed by incorporating the spatial and temporal sparse subspace clustering into the robust principal component analysis framework. Instead of using low level or handcrafted features, a background subtraction algorithm was proposed based on spatial features learned with convolutional neural networks. A detailed analysis was performed for a deep neural network-based background subtraction approach with respect to the feature maps, the important filters for the detection accuracy, and the operations to suppress false positives from dynamic backgrounds. By using the output features from different layers of the network, a multiscale fully convolutional network architecture was proposed for infrared foreground object detection. In some real-time applications, the computational efficiency is a major concern for background-subtraction algorithms. In this paper, an effective background subtraction approach is proposed, based on a subsuperpixel model. Instead of a whole superpixel, each initialized superpixel is divided into K smaller units, i.e., subsuperpixels, via the k-means clustering algorithm. A multidimensional feature vector is then used to represent each superpixel. In order to deal with the ghost artifact, a background model updating strategy is devised, based on the number of pixels represented by each cluster center. As the superpixel is refined and its representation is simplified, the proposed approach can achieve a competitive accuracy and require less computation time for background subtraction. The remainder of the paper is organized as follows. A brief review of the various superpixel-based approaches is given in Section II. In Section III, we present our proposed background-subtraction algorithm. Experimental results and related discussions are given in Section IV, and the concluding remarks are presented in Section V.", "section": "Introduction", "doi": "10.1109/TIE.2019.2893824", "references": [653074768, 1943880421, 1970529618, 1988061476, 2003830783, 2029535991, 2031708389, 2052524720, 2102625004, 2108811167, 2111861511, 2127070222, 2158604775, 2161086211, 2241814552, 2417256080, 2538254898, 2713774261, 2782892077, 2793865950, 2806294667, 2808104678, 2810497654, 2896303665]}
{"paragraph": "Large-scale systems have been found in many applications, such as autonomous vehicles; electric power systems; satellite formations; and robotics. In large-scale systems, subsystems usually exchange information through a communication network. The communication network used in the large-scale systems offers numerous advantages like simple installation, easy maintenance, and low cost. However, the information transmitted through the network may suffer from communication delay. As a result, the information available to each subsystem is incomplete at each time step. To achieve the best system performance with the incomplete information, decentralized control has been proved to be a useful control technique. However, the design of the optimal decentralized control strategy is a challenging task, because it is computationally intractable in general. For example, the optimal decentralized control policies may be nonlinear even for the linear system. Decentralized control with communication delay has attracted a lot of research attentions since 1970s. The optimal decentralized control with one step delay sharing pattern was studied by the matrix minimum principle and by the second-guessing technique. For the multiple step delays sharing pattern, two structural results to the optimal decentralized control design were established. The decentralized stochastic control with symmetric delay and asymmetric delay has been studied by a common information approach. A common feature of the results is that each subsystem estimates not only its own subsystem state but also the others. This implies that the subsystem state is estimated more than one time. For the case that each subsystem only estimates its own subsystem state, Wang et al. studied the decentralized output feedback control for a two-player system with one step communication delay. For delay patterns arising from a communication graph, the optimal decentralized LQG control problems were investigated by the information hierarchy graph and by the independence decomposition technique. The results are for the state-feedback controller design. The output-feedback case was considered, but the result is only suitable for a three-player system with a chain structure. The sufficient statistics of linear control strategies was studied for large-scale systems with delay pattern defined over a communication graph. In those studies, the communication delay is assumed to be determinate. In practice, the network environment is affected by random factors, and the communication delay is naturally random. The LQG problem with varying communication delay has been studied. The results proposed are sound, but are only for two-player system with state feedback case. The framework of the randomized information pattern can be used to model state-feedback LQG problem with random communication delay, but the realization of the optimal controller design is not derived. Thus, the optimal output feedback LQG control with random communication delay is not fully studied. This paper focuses on the optimal decentralized output-feedback control of a large-scale system. The information is transmitted from one subsystem to other subsystems through a network with random delays. The random delay satisfies Bernoulli distributions. Under this setup, we derive a linear matrix equation used to design the optimal controller for global estimation case. Also, the optimal value of the cost function is obtained, and is shown to strictly increase as the delay probability increases. The optimal LQG problem under local estimation case is also studied. An iterative algorithm is exploited to design the gains of the optimal controller for local estimation case. It is shown that the algorithm converges to person-by-person optimum. The optimal value of the cost function has the same monotonicity as the one of global estimation case. Finally, two numerical examples are given to illustrate the effectiveness of the theoretical results. The contribution of this work is summarized as follows. Compared to prior work, the local estimation case is also studied in this paper, while others only studied the global estimation case. The local estimation is important for large-scale systems. One advantage of local estimation is that it consumes less computational resources. Compared to the global estimation case, the main challenge of the local estimation case is that the available estimated state is incomplete, thus, is not a sufficient statistic for optimal decision. Then, the corresponding optimization problem is nonconvex. Our contribution is to establish the framework of the optimal decentralized controller under local estimation, and exploit an iterative algorithm to compute the gains of the controller in the sense of person-by-person optimum. We study the random delay case instead of the determinate delay case. The main challenge of the random delay case is to deal with the random sparse structure constraints induced by the random communication delay. Our contribution is to propose the method of Hadamard product derivative to design the optimal decentralized controller under the random sparse structure constraints. We investigate the property of the cost function with respect to the delay probability. Our contribution is to prove that the optimal value of the cost function is monotonically increasing with the increase of the delay probability. Based on this result, we can find the critical delay probability effectively by the binary search method for a given value of the cost function, such that the optimal value of the cost function is smaller than the given one when the delay probability is below the critical delay probability. To the best of our knowledge, the similar results do not appear in the related literature. Our contribution is to design the optimal decentralized output-feedback controller for more general large-scale systems. However, the results established earlier are only for two-player systems under state feedback and uncorrelated subsystem noises. In other words, the methods proposed previously are unsuitable for our problem.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868968", "references": [1932883063, 1979094997, 1979177727, 1991888757, 1997144258, 2047691814, 2052705342, 2056313725, 2092691181, 2105876604, 2110240006, 2113789941, 2115679890, 2119099274, 2126230455, 2146890818, 2151756741, 2237353654, 2326020541, 2344639242, 2551093915, 2578362634, 2587622940, 2618231873, 2759084312, 2784185058, 2963254670, 2964182829]}
{"paragraph": "Many optimization problems in the real world contain multiple incommensurable and conflicting objectives. Such problems are named multi-objective optimization problems (MOPs), which have earned considerable attention in the practical applications such as recommendation system, shop scheduling. There are three main goals in a MOP: to improve the convergence property so that obtained solutions can get as close as possible to the true Pareto front (PF); to maintain the diversity of the generated solutions, in order that all solutions can spread evenly throughout the PF; to make use of the least computational complexity on condition that the performance of convergence and diversity is ensured. Over the last two decades, MOEAs have become increasingly popular for solving MOPs. Most MOEAs approximate the entire PF, however, the ultimate goal of a MOP is to help DMs to identify several or one final Pareto optimal solution that satisfy his/her most in practical application. In multiple criteria decision making (MCDM), DMs usually give preferences to identify the most preferred solution among multiple conflicting objectives. The preferred solution may be a single preferred solution or an approximation of the region of interest. The DM’s preference has received a great of attention on MOEAs, including the reference points, reference directions, utility function, fuzzy logic etc. The existing preference-based optimization methods can be classified into three categories according to the time when preferences are incorporated, i.e. before optimization (a priori), after optimization (a posteriori) and iteratively during optimization (interactive approach). Next, we give a simple review of these three categories. A priori methods: DMs need to input their preferences before the beginning of the optimization. Some typical priori methods are listed as follows: one method first combined a MOEA with the goal information as an additional standard for group membership allocation. Another proposed a guided MOEA, which modified the definition of dominance based on the DM’s preference information. A different approach proposed to combine the reference information of the reference point with the non-dominate sorting operator of the elitist non-dominated sorting genetic algorithm. Another proposed a preference based evolutionary approach which directly uses reference point information in fitness evaluation. Recently, a method incorporated the DM’s preference information into the multi-objective evolutionary algorithm based on decomposition, which modelled the DM’s preference information as an aspiration level vector. The disadvantages of these methods are that DMs may have limited knowledge about the problem and their preferences may be inaccurate or even misleading. A posteriori methods: DMs need to select a small number of solutions according to their preferences from the entire PF obtained by a MOEA. In fact, many well-known MOEAs belong to it. In addition, scholars have proposed some other novel algorithms to efficiently solve MOPs, such as Two-Archive, improved Two-Archive and the reference vector guided evolutionary algorithm. Compared with the a priori methods, DMs are able to better understand the trade-off relationships between the objectives. It should be noted, however, that it becomes increasingly hard to obtain a representative solution set as the number of objectives increases. Interactive methods: DMs need to articulate their preferences during evolution. They are allowed to modify their preferences, typically based on the domain knowledge acquired during the optimization. With a deeper understanding of the problem as the optimization proceeds, DMs are able to fine tune their preferences according to the obtained solutions in each iteration. Several interactive MOEAs have been proposed in the literature over the last 10 years. One proposed an interactive method that is able to convert different forms of preferences into one single format and then uses that to guide the search towards preferred solution(s). Another put forward a new preference based interactive simple indicator-based evolutionary algorithm, it extended the previous version by taking the preference information of the DM into account, a set of non-dominated solutions are iteratively shown to the DM and then DMs are required to provide his/her preferences by classifying this set into preferred and/or non-preferred solutions. A different approach presented an interactive preference-inspired co-evolutionary algorithm, similar to another, in which candidate solutions and goal vectors are co-evolved to focus on a region of interest, which is defined by user-provided reference point, weight and search range. It also allows the DM to brush the region in the objective space, alleviating the burden of setting parameters. One other proposed an interactive MOEA/D, named i-MOEA/D, during periodical interaction, a set of current solutions are offered to the DM, and the search is then guided by renewing the preferred weight region towards the neighborhood of the solution which is most preferred. Based on this, we can conclude that the key of the interactive algorithm is how the DM’s preference and the algorithm during evolution. In summary, eliciting the preference information in an interactive manner seems to be more interesting. This enables the DM to progressively learn and understand the characteristics of the MOP at hand and adjust their elicited preference information, consequently, solutions are effectively driven toward the region of interest. To the best of our knowledge, there does not exist an interactive MOEA that can make the region reduce gradually, until finding the closest solution to the DM’s preference information. Furthermore, there is no guarantee to find the preferred solutions when tackling many-objective problems. In this paper, we propose a new multi-layer interaction preference based multi-objective evolutionary algorithm through decomposition. MOEA/D is a recent MOEA framework with many successful applications. In the proposed algorithm, we use a new version of MOEA/D with a Maximin operator called MOEA/D-Maximin which can utilize the Maximin function to evaluate the fitness value of individual, it can get the non-dominated solutions easily and ensure the diversity of the population simultaneously. The developed multi-layer interaction strategy consists of two layer interaction, in the first-layer interaction, the DM will provide a reference vector and an initial radius to determine a rough preference range, and then update the solutions in this range. After the first-layer interaction, the DM will check whether he/she is satisfied with the result, the process will stop if the result is satisfactory, otherwise it will go on to the next interaction. In the second-layer interaction, the most preferred solution from the result of first-layer interaction will be chosen as the new preference direction, and the weight vector is redefined by an angle-based preference method. During the interactive process, the DM can control the search process and get more involved in the procedure. The main contributions of our work are summarized as follows: MOEA/D-Maximin is applied to find the DM’s preference, which is an effective approach. A required number of solutions can be gained by finding the same number of weight vectors around the reference direction and find the closest solution to each chosen weight vector. A novel multi-layer interaction strategy for searching the accurate solutions is proposed. During evolution, the preferred region is reduced gradually, making it easier for the DM to get the final preferred solution(s). A fast way to get preference region is established, during the selection process, the preference weight vector is redefined using the angle-based method. Besides ZDT problems with two objectives, the proposed algorithm shows its potentiality for solving many-objective problems, i.e., DTLZ problems with 3, 5, 8, 10 objectives. In the remainder of this paper, we first review some background knowledge and related works in Section 2. Section 3 introduces the main process of our proposed algorithm. Experimental designs and findings are presented in Section 4. Section 5 concludes this paper and discusses the future research.", "section": "Related Work", "doi": "10.1016/j.ins.2018.09.069", "references": [93031587, 114258432, 146421413, 240075219, 626466576, 1509866077, 1564638059, 1612556207, 1662894842, 1968219458, 2011048370, 2022485595, 2055092083, 2056654250, 2074921660, 2077433479, 2080858591, 2083457599, 2106884655, 2110828487, 2125899728, 2126105956, 2127950596, 2136386226, 2143381319, 2151879717, 2152551290, 2156194072, 2171996959, 2343601797, 2502932208, 2515223602, 2558359969, 2588157598]}
{"paragraph": "Type-2 fuzzy logic, which is a generalization of ordinary type-1 fuzzy logic, has made a significant breakthrough in the area of computational intelligence. It has been shown in several studies that type-2 fuzzy logic systems, especially interval type-2 fuzzy logic systems, have the capability to handle or represent uncertainties in a better way due to their extra degree of freedom provided by the footprint of uncertainty in comparison to their type-1 counterparts. Generally, the footprint of uncertainty parameters have been handled as design parameters and, thus, they are tuned in order to reduce the effect of uncertainties and nonlinearities on system performance. In this context, theoretical studies have been presented on how the size or shape of the footprint of uncertainty affects the interval type-2 fuzzy mappings. Researchers have also focused on defining interpretable relationships between the footprint of uncertainty sizes and the output of the interval type-2 fuzzy logic systems to use the benefits of the linguistic meaning of type-2 fuzzy sets and fuzzy logic systems. As a result of these studies, the deployment of interval type-2 fuzzy logic systems has become popular in real-world applications where uncertainties and complexities are high such as control system design, outlet temperature control, flight control, computing with words, decision making, and games. Games are gaining more attention as they are challenging testbeds for computational and artificial intelligence methods. One of the most pervasive real-world game types is the pursuit-evasion game. The pursuit-evasion games have been handled in many studies since many other robotic problems can be seen as a variant of pursuit-evasion games such as obstacle avoidance, leader-following, and path-planning. In literature, there are different game scenarios developed for pursuit-evasion games depending on the game environment, number of the agents, limitation of the moving capabilities of the agents, and definition of the capture. Various methods such as game theory, dynamic programming, and fuzzy logic have been successfully employed to solve this challenging problem. In this paper, we will present a systematic and interpretable design approach to generate pursuing strategies with type-2 fuzzy logic and their deployment to a real-world pursuit-evasion game. First, we will present a novel type-2 fuzzy strategy planner that is composed of two single input interval type-2 fuzzy logic systems. Then, through detailed theoretical investigations, we will show that it is possible to design a linguistic strategy which defines both the pursuer’s approaching behavior and approaching side to the evader by simply tuning the footprint of uncertainty sizes of the interval type-2 fuzzy sets. We will not only present a theoretical way to tune the footprint of uncertainty sizes such that to generate pursuing strategies but also provide an interpretable relationship between the footprint of uncertainty sizes and the linguistic pursuing strategies. Furthermore, we will present a type-1 fuzzy decision making mechanism to tune the footprint of uncertainty sizes of the type-2 fuzzy strategy planner in an online manner. Thus, the resulting pursuing strategy, named as intelligent pursuing strategy, can be seen as an online-generated type-2 fuzzy logic-based pursuing strategy that is capable of transition between various pursuing strategies. This paper will also present a real-world pursuit-evasion game which will act as a platform to evaluate the developed type-2 fuzzy logic-based pursuing strategies and type-1 fuzzy decision making mechanism. Several experimental results will be presented to first validate that the type-2 fuzzy strategy planner has an interpretable relationship between its footprint of uncertainty sizes and output. Then, the performances of the proposed pursuing strategies will be examined against a human controlled evader to show that the resulting performances are satisfactory in the dynamic pursuit-evasion game. The results will demonstrate that the type-1 fuzzy decision making tuned type-2 fuzzy strategy planner, which generates the intelligent pursuing strategy, has improved the pursuing performance as it has the capability to adjust its pursuing strategy in real-time. Section II will briefly introduce the pursuit-evasion games. In Section III, the type-2 fuzzy strategy planner and the novel type-2 fuzzy logic-based pursuing strategies will be presented. In Section IV, the type-1 fuzzy decision making mechanism will be given. Section V will provide the experimental results and Section VI will present the conclusions and future works. The widely used abbreviations are given in a table for the convenience of the reader.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2868405", "references": [40650588, 1961697688, 1964569793, 1970974975, 1978838873, 1989434135, 2063615461, 2092965146, 2093051821, 2121235060, 2125223024, 2125631815, 2145416734, 2150879558, 2223927396, 2308507486, 2410041922, 2413454671, 2466197628, 2521651672, 2550759882, 2551462992, 2553405792, 2559820313, 2588293788, 2748314524, 2759567813, 2765770996, 2771998355]}
{"paragraph": "Financial crisis prediction (FCP) is highly essential for financial firms, which aims to reduce the upcoming losses by calculating possible risks and avoids new credit proposals when the default risk is higher than a predefined acceptance level. This process is also entitled as a credit default classification process, which refers a customer as “non-default” when he pays back the loan, or the customer is referred as “default”. The accuracy of the FCP plays an important role to determine the financial firm’s productivity and profitability. For instance, a small positive adjustment in the accuracy level of a potential user with default credit will minimize a huge future loss of an organization. FCP can be considered as a data classification problem, which refers a customer as “non-default” when he pays back the loan, or the customer is referred as “default”. Numerous researches have been done on the classification of FCP, started from the year of 1960′s. At the earlier days, traditional methods employed mathematical functions to predict financial crisis, which differentiates the financial institution between stronger and weaker ones. In the year 1990′s, the concentration has moved towards artificial intelligence (AI) and machine learning (ML) based expert models like neural network (NN) and support vector machine (SVM). Recently, AI techniques are adopted to refine the conventional classification models. However, the presence of many features in the high-dimensional financial data leads to several issues like overfitting, high computational complexity and low interoperability. This is because of the curse of dimensionality, which occurs based on the ratio of number of features and number of instances. The easiest way to solve this issue is reducing the number of available features using feature selection methodologies. Feature selection process intends to identify appropriate subset of features and has significant implications for issues like reduced noise by removing noisy features, save cost and computation time required to implement an appropriate model, simplifying resultant models and facilitating easy use and updation of models. The chosen subset of features are useful to represent a classification function, which affects several dimensions of classification like learning time, accuracy of the classification algorithm and cost integrated with the features. The feature selection methodology is used in various applications like data mining, ML and pattern recognition, to reduce the dimensionality of a feature space and to enhance the predictive accuracy of a classification algorithm. On the basis of evaluation criteria, feature selection techniques are divided into wrapper, embedded and filter based methods. Wrapper method uses a learning algorithm as a part of evaluation, to assess the goodness of the selected feature subset. Though wrapper methods are widely used, it has some limitations like high computational complexity, finding user-specified parameter of the learner and inherent learner limitations. Embedded methods are computationally less complex than wrapper methods, but the chosen feature subset is not independent on the learning algorithm. Because of these limitations, filter method is employed in this study. Filter approaches evaluate feature subset using predefined metrics rather than the learning models and chosen features. The process of feature selection is considered as an optimization problem, with a performance measure for every subset of features, which denotes expected classification performance of the resultant model. The problem is to search the feature subset space to find the optimal or near-optimal one on the basis of performance measure. Various methods have been presented to determine the suboptimal solutions in comparably smaller amount of time. Stochastic algorithms like simulated annealing, scatter search, ant colony optimization (ACO) and genetic algorithms are popular because they produce high precision results at faster rate. ACO is one of the popular swarm intelligence technique, widely applicable for approximate optimizations. It is a popular metaheuristic algorithm capable of achieving well enough solutions to solve NP hard problems in a reasonable time period. It has been adopted to hold several real world problems like scheduling, vehicle routing, industrial problem and so on. ACO is used to solve tedious combinatorial problem in 1990′s. It is based on the foraging nature of real ant colonies. The ACO algorithm has been chosen for FCP because of the following reasons: ACO leads to better exploration which makes it suitable for appropriate feature selection and the selected features resulted to efficient classification performance. Moreover, the increased number of features in the financial dataset makes the ACO algorithm suitable for the feature selection. The mapping of any optimization problem to ACO algorithm requires a sequence of steps include graph representation, heuristic desirability, positive feedback process and constraint satisfaction method. The contribution of the paper is summarized as follows. This paper designs an effective ACO-FCP model by the incorporation of ACO based feature selection (ACO-FS) and ACO based data classification (ACO-DC) algorithms. The inclusion of ACO-FS method identifies the optimal feature subset and helps to improve the classification performance of ACO-DC method, which in turn improves the overall performance of the ACO-FCP model. The proposed method is applied to a set of five benchmark dataset which includes qualitative bankruptcy dataset, Analcat data bankruptcy dataset, Australian Credit dataset, German Credit dataset and Polish dataset. For comparative analysis of feature selection results, namely genetic algorithm (GA), Particle Swarm Optimization (PSO) algorithm and Grey Wolf Optimization (GWO) algorithm based feature selection methodologies are used. To test the classifier results, ACO-DC method is compared with well-known nine classifiers. The experimental results reported that the proposed ACO-FCP model shows better performance when compared to the state of art methods. The remaining part of this paper is structured as follows: Section 2 summarizes the state of art methods of feature selection and data classification approaches on FCP. Section 3 explains the proposed ACO-FCP model in a clear and classified manner. Section 4 presents the performance evaluation of proposed method against different dataset and the paper is concluded in Section 5.", "section": "Methodology", "doi": "10.1016/j.ijinfomgt.2018.12.001", "references": [1862312035, 1969054362, 1985716252, 1996413276, 2004473119, 2014915963, 2017337590, 2029765676, 2031239249, 2039355518, 2071193822, 2072461903, 2080614264, 2089939190, 2093195672, 2093980846, 2109508799, 2116045745, 2118573797, 2122684851, 2125965138, 2130759652, 2148633389, 2149237332, 2154929945, 2162523902, 2167101736, 2167435760, 2172238468, 2180466864, 2307376191, 2336505047, 2341338744, 2343420905, 2508160469, 2744516260, 2887289205]}
{"paragraph": "Recent years have witnessed a considerable increase in the diffusion of, and interest in, new forms of peer production based on decentralized interaction within communities of independent participants. One of the main questions motivating the current interest in peer production concerns the emergence of order in the almost complete absence of hierarchical organizational structures and centralized coordination mechanisms. This issue is at the heart of what Padgett and Powell identify as the problem of “emergence” – or how organizational and social structures arise out of vortexes in the flow of social life rather than being buildings of stone. Addressing questions about the emergence of organized order becomes particularly important – and difficult – when the absence of hierarchical conflict resolution mechanisms makes controversies among participants difficult to settle and potentially detrimental to the peer production process and its outcomes. How can order be achieved and maintained – and how can the production of anything collectively valuable be possible – under conditions of extreme decentralization and latent conflict that characterize peer production? Because conflict increases the private cost of producing public goods, these questions are at the heart of the current debate about the sustainability of peer production, and the survival of the open source production movement. Conflict in peer production organizations cannot just be settled by fiat, or by relying on hierarchical authority, but only by building collective consensus. In the near-absence of formal organizational structure, peer production projects are mostly regulated through informal networks arising from task-oriented interaction within communities of participants. Our study is guided by two non mutually exclusive hypotheses on the emergence of organizational order from decentralized text production and editing activities in Wikipedia – the free encyclopedia that anyone can edit, and the peer production organization of interest in this paper. Wikipedia may be considered as broadly representative of open peer-production projects where voluntary participants contribute and edit content that is made collectively and freely available. The hypotheses we formulate allow us to express fundamental theoretical principles of social organization in terms of hypotheses on the evolutionary dynamics of signed event networks. According to the first hypothesis, reputation hypothesis, positive and negative interaction (i.e., agreement and disagreement) are explained by the reputation of the target actor. The reputation hypothesis predicts that more reputable actors are more likely to receive agreement, while disagreement flows toward less reputable actors. According to the second hypothesis, balance hypothesis, expressions of agreement and disagreement are organized according to membership in latent communities of friends and enemies. The balance hypothesis predicts that agreement is expressed mainly towards friends and disagreement mainly towards enemies – regardless of their reputation. Balance theory explains the formation of signed networks, but empirical evidence for it has been mixed. Some suggest status theory as an alternative which can explain the structure and evolution of signed networks. Status theory predicts that negative relations tend to point away from actors with high status and toward actors with low status, while positive relations tend to flow from lower to higher status actors. The predictions derived from the reputation hypothesis are a subset of the predictions that can be derived from status theory. More precisely, the reputation hypothesis makes only predictions related to in-coming relational events, such that actors that received many positive events in the past are more likely to receive positive events and less likely to receive negative events in the future, and actors that received many negative events in the past are less likely to receive positive events and more likely to receive negative events in the future. The reputation hypothesis reflects an alter-centric conception of status as a social position conferred to ego by alters through acts of deference. This view of status is considered alter-centric because deference cannot be seized by an actor but rather is something that is awarded by others. The predictions of the reputation hypothesis are consistent with results produced by studies of dominance hierarchies in animal societies. These alternative perspectives on the micro-mechanisms of network formation imply different network macro-structures. According to the reputation hypothesis, actors are assigned reputation values from a one-dimensional scale that influences probabilities to receive agreement or disagreement, regardless of the sending actor. The ratio of incoming positive ties over incoming negative ties would increase with higher reputation. In contrast, the balance hypothesis posits the emergence and progressive crystallization of a polarized network in which two groups, factions, mutually fight each other. Membership in these groups explains the probabilities to receive agreement or disagreement – but only if we take into account the group membership of the sending actor. According to balance theory, actors would be more likely to agree with members of their own group but more likely to disagree with members of the other group. Thus, members from opposing factions would assess contributions of the same third user differently. We test these hypotheses in an analysis of networks of signed relational events among the contributing users of the 1206 Wikipedia articles that are labeled as controversial. Controversial articles are those which regularly become biased and are likely to suffer future disputes, as defined by Wikipedia. We extend currently available relational event models and analyze patterns of agreement, positive relations, and disagreement, negative relations, among contributing users of Wikipedia given the full history of their previous interaction. The models we specify and estimate include explanatory mechanisms encoding effects consistent with both balance and reputation hypotheses. We focus on the subset of controversial Wikipedia articles because we expect this context to be uniquely useful for identifying and illuminating the coordination mechanisms underlying the hypotheses that we have outlined and because controversial articles involve a high level of interaction among contributors almost by definition. Because our study covers the complete lifetime of Wikipedia, this feature of controversial articles gives rise to a very large sample of relational events. The bipartite structure directly connecting contributing users to text in Wikipedia, dually connects contributors. For this reason, observable expressions of agreement or disagreement connecting users to text through acts of editing may be interpreted as agreement or disagreement between users. Thus, contributors interact through their joint involvement in the production of text – the raw input of Wikipedia articles. We are interested in identifying and interpreting patterns in the emergent social order in Wikipedia resulting from this signed event network. Signed network data have been collected and analyzed throughout the history of social network analysis, and the analysis of signed network relations is currently experiencing a surge of renewed interest. Linking the dynamics of production relations in Wikipedia to explicit models for signed networks allows us to illuminate fundamental general issues in the analysis of peer production. To foreshadow the results of our analysis, we find strong support for balance theory: the micro-dynamics of positive and negative events seem to support a balanced, and hence polarized, macro-structural social order. An additional contribution of our study is to show that the alter-centric status implications of the reputation hypothesis, namely the predictions related to in-coming relations, receive strong empirical support; but, on the other hand, the predictions of status theory related to out-going relations are not supported. Specifically, we show that actors initiating many negative events do not necessarily have high status, and actors initiating many positive events do not necessarily have low status. Likewise, we find no empirical evidence for the anti-reciprocity of positive and negative relations predicted by status theory. Thus the empirical predictions of status theory, which received strong support in the context of signed networks of relations such as like/dislike or high/low esteem, have to be restricted in our setting to the alter-centric components of status that are captured by our reputation hypothesis. Above and beyond these substantive insights, our paper also makes a clear methodological contribution by extending current relational event models for signed, weighted, and directed social interaction data. Building on previous models, we make several methodological improvements that are necessary to deal with large networks of relational events and propose an indicator of user reputation that proves to be one of the strongest and most reliable predictors for future positive or negative events. After presenting the theoretical background in more detail in Section 2, we describe the empirical setting in Section 3. The construction of edit networks and relational event models for these is detailed in Section 4. Results are presented and discussed in Section 5. We close by discussing implications of our work and outline possibility for future work in Section 6.", "section": "Introduction", "doi": "10.1016/j.socnet.2018.12.003", "references": [575879543, 1595890742, 1930424165, 1967343101, 1968058338, 1991430386, 2003959894, 2012701861, 2024962845, 2043026073, 2043253351, 2054703469, 2067509321, 2081085399, 2086038483, 2089973895, 2093753056, 2102064986, 2102271161, 2102664282, 2111122424, 2120476697, 2125815607, 2132531582, 2132781340, 2136387422, 2137818934, 2142517301, 2148540666, 2148785382, 2158139315, 2164181247, 2170340032, 2232472329, 2285889764, 2741466057, 2950048827]}
{"paragraph": "With a burgeoning number of IoT devices penetrating into all aspects of our lives, privacy-related issues are attracting increasing interest. Users of these devices worry about being watched, listened to, or tracked by wearable devices and smart home appliances. Sensitive personal data like lifestyle preferences and location information may be abused for unwanted advertisement purposes or for more nefarious objectives like unauthorized surveillance. In China, for example, over 20 million surveillance cameras equipped with artificial intelligence have been installed. The new surveillance systems have raised fears among citizens that the technology is being used to monitor their daily lives. To put consumers at ease and to encourage adoption of IoT technologies, privacy consideration should be incorporated into the core design of IoT solutions, with users being given more control over what information can be shared with the service providers. We model an IoT network consisting of multiple devices using the decentralized detection framework. In this model, IoT devices or sensors make observations, where each is the local observation of sensor. Each sensor then maps its local observation to a new value and sends that to a fusion center or service provider. The fusion center uses these mapped values to infer a hypothesis of interest, which is also called a public hypothesis. However, since the sensor information may also be used by the fusion center to infer other hypotheses that the user may not have given authorization for (which are known as private hypotheses), sending unsanitized sensor information to the fusion center may result in privacy leakage. For example, on-body wearables containing accelerometers and gyroscopes can be used to measure a person’s movements and pose in order to detect falls. However, the sensor information may also be used to detect other activities performed by the person, leading to loss of privacy. In this paper, we investigate the case where the fusion center is authorized to infer a public hypothesis based on sensor information that is sanitized in such a way that makes it difficult for the fusion center to infer an infinite set of private hypotheses. To define the set of private hypotheses, we consider an uncertainty set defined around a nominal private hypothesis. This is because in most applications, one can define a specific public hypothesis, but it may be difficult for a user to specify multiple private hypotheses that she wants to protect. Our goal is to design local privacy mappings at each sensor to protect the information privacy of private hypotheses that are “close” in a specific sense to the nominal hypothesis. We can interpret this framework as providing robust privacy for the nominal private hypothesis, in the same spirit as robust hypothesis testing.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2916650", "references": [1601684460, 1958009659, 2009816033, 2017613761, 2031533839, 2037759417, 2038194220, 2053801139, 2054365984, 2082639035, 2085472312, 2092214308, 2096870293, 2098145033, 2102931639, 2109426455, 2115915204, 2119874464, 2130118845, 2152926062, 2162408301, 2182112800, 2394738186, 2398203045, 2399072287, 2435473771, 2520571093, 2556397940, 2556626427, 2566050141, 2576765166, 2583220279, 2592694684, 2610910029, 2744172881, 2749457063, 2769615563, 2776458542, 2783793345, 2802013709, 2808012592, 2810524107, 2896658168, 2963446403, 2963535017, 2963634943]}
{"paragraph": "Cyber-physical systems (CPSs), whose most notable feature is tight integration and cooperation between cyber and physical components, have attracted attention from many researchers in the past decades. Typically, CPSs consist of processing units monitor, control physical processes by means of sensors and actuators networks, such as transportation networks, future power systems and smart grids, and high speed train systems. Thus, they are prone to failures especially cyber attacks on the data and communication channels, thus causing damages or breakdown, for example, the Stuxnet storm which damaged the Iran’s nuclear program, breakout accident in nuclear plant, and power blackouts in Brazil. Many works devoting to the attack detection, secure estimation, and control of CPSs have recently appeared in the literature, which can be classified in three classes. The first class addresses attacks. Teixeira et al. presented a novel attack space based on the adversary’s model knowledge, disclosure, and disruption resources, and it illustrates the attack effects by implementing experiments on a wireless network control system, whereas the attack detection and secure control is not discussed. In second class, detection of attacks to CPSs is investigated. For example, the detectability of an attack to CPSs is explored with structured system theory and a Luenberger like observer is proposed to detect the attack. Whereas, if only an upper bound on the cardinality of the attacked sensors is available, the number of needed monitors is combinatorial in the size of the attacked sensors. A sliding mode observer-based method is presented to estimate the attack to system dynamics and sensors separately, in addition to the state estimation. Nevertheless, the results are only applicable to the systems where the so called observer matched conditions are valid. A sufficient and necessary detectability condition of an attack is proposed in terms of the system dynamics eigenvectors by exploring the strong observability of the system. A sufficient and necessary condition of undetectable attacks in the presence of side initial state information is presented, and a detector is proposed to detect attacks in a finite steps. It is noted that, secure state estimation is not involved. Even though the strong observability helps to describe the undetectable attacks, as many CPS are distributed, we feel that, a more explicit characterization is needed and distributed finite time attack detection is needed. The third class focuses on secure state estimation or secure control design in the presence of attacks. A specific computationally feasible decoding algorithm is proposed to estimate states of CPS when some sensors are corrupted and it also gives a characterization on the maximum number of attacked sensors allowed for this decoder to correctly estimate the states. By showing how to design a secure local control loop to improve the resilience of the system, and presenting L1/Lr decoder for secure state estimation, these results are extended. By utilizing the sparse observability of a system, it shows that the state can be estimated securely under an s-sparse attack if and only if the system is 2s-sparse observable, and an event triggered observer is proposed to estimate the state securely. These results are extended with a novel multimodal Luenberger observer based on efficient satisfiability modulo theory to reduce the complexity of the estimation problem. Attack detection and secure estimation are also investigated for linear systems under sensor attacks in the presence of noise, which shows that detectable attacks can be detected in finite, yet sufficiently large number of steps. The complexity of the estimation problem can be efficiently reduced by satisfiability modulo theory. However, the estimation algorithms are in a centralized form. Mo et al. investigated the resilient detection when there exist s attacks to p sensors. A minimax optimization method is proposed to minimize the worst-case probability of error against all possible manipulations by the attackers. These results are extended. Again, it only shows that, asymptotical convergence is obtained and when the number of attacks is more than or equal to half the number of sensors, attackers can render the information provided by the manipulated measurement useless, thus an optimal worst-case detector is proposed by solely using the a-priori information, without utilizing all measurements. Also note that, the results are in centralized form and only admit exponential convergence of estimation errors or a finite, yet sufficiently large steps to obtain the secure state estimation, which implies that it may need quite long time. This calls for a prescribed finite time distributed secure estimation and secure control. Therefore, motivated by these above discussions, we address distributed secure state estimation and control of the CPSs with some sensors being corrupted by malicious attackers, with the block diagram of the considered CPS and the proposed schemes given in Fig. 1. The presented schemes consist of distributed secure preselectors, distributed finite time observers, and a virtual fractional dynamic surface-based distributed secure controllers. Our contributions and methodologies can be outlined as follows. By exploring the distinct properties of unidentifiable attacks for linear CPS, sufficient conditions that secure state estimation can be solvable is established. Also based on the above conditions, we propose distributed observers with secure preselectors to solve the secure state estimation problem. It is shown that under the sufficient conditions, states can be exactly obtained in a given finite time. Then with the obtained secure state estimation, distributed secure controllers based on a virtual fractional dynamic surface are designed, which guarantee that, the state of the CPS can be made track the desired trajectories with finite time containments, and all the signals are continuous and bounded. Also some guidelines on the choice of the design parameters are presented. The remainder of this paper is organized as follows. In Section II, system models are given and some definitions are presented. Also our objectives are formulated as secure state estimation and secure control problems. In Section III, a sufficient condition ensuring that the secure state estimation can be solvable is proposed and established. Then finite time observers with secure preselectors are designed to solve the secure state estimation. Section IV proposes a fractional dynamic surface-based distributed secure controller to solve the secure control problem. Section V presents a numerical simulation of a islanded micro-grid system under sensor attacks to verify our theoretical findings. Finally, Section VI gives some concluding remarks.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868781", "references": [66053595, 1486477872, 1972523481, 1974729410, 2003530067, 2010255981, 2039427951, 2044762091, 2062132646, 2074851925, 2083730444, 2112784437, 2221032419, 2273673096, 2324357600, 2395221178, 2556022294, 2963209376, 2963639842]}
{"paragraph": "Autonomous vehicles are experiencing substantial advances in academia, industry, and military involving many domains, such as engineering, computer science, and information technology. The most important reason is that autonomous vehicles are capable of alleviating the issues, such as road accidents or traffic congestions. Although the idea of autonomous vehicles arose almost a century ago, it did not become true until the 1980s, when the PROMETHEUS project was launched. Advancements in both related hardware and software technologies in the last several decades have further stimulated the interests in autonomous vehicles and many achievements have been made. For instance, as a milestone in autonomous vehicle technology, a series of DARPA Grand Challenge was held from 2004. The challenges were conducted in both off-road (e.g., 2005) and urban settings (e.g., 2007). The teams were from both industry (e.g., General Motors) and academia (e.g., Carnegie Mellon University, Stanford University, and Virginia Tech), all of them with enthusiasms for improving the capabilities of autonomous vehicles. Other competitions or tests have also been carried out as well, such as the Intelligent Vehicle Future Challenges and the Public Road Urban Driverless Car test (PROUD). In terms of the commercial efforts, Tesla's Autopilot system and the Google self-driving car are attracting more attention. According to society of automotive engineers international standard J3016, six driving automation levels are defined from “no automation” to “full automation.” Each level is featured by its exclusive products, such as adaptive cruise control in level 1 and Autopilot in level 2. From level 3, the system or the car itself starts to percept the driving environment and then decides the actions. Obviously, as the “brain” of autonomous vehicles, the decision layer is one of the most important parts. Fig. 1 shows the general structure of the autonomous vehicle taking the four wheel drive electric vehicle as an example, mainly consisting of the perception, data fusion, decision, and control layer. For the detailed explanation of each layer and their connections, please refer. This paper only puts the major emphasis on the decision and control layer. The decision layer is commonly hierarchically structured into global route planning, behavior planning, and local motion planning. At the top level, an autonomous vehicle should decide a route based on passengers’ requirement (e.g., the fastest, free of the toll, etc.), the traffic condition, and road network from the current position to the requested destination. A behavior planner aims to reason a sequence of driving behaviors (e.g., turning, stopping, or going straightly) along the planned route according to the current traffic conditions, such as the traffic participants and signals. Whereas, the local motion planning is intended to provide vehicles with a safe (i.e., collision free and stable), economical, human-like trajectory with a good ride as well as less computational efforts along the predefined route and in accordance with the decided behavior. Given the missions, onboard sensors, and the online sensing information, the global route and the behaviors can be accurately planned. However, more attention and efforts are paid to the local motion planning, and most of them originated from the mobile robotics. According to the literature, the local motion planning approaches perform in two ways. On one hand, the motion planning is separated spatially and temporally, namely local path planning and velocity planning. On the other hand, the motion should be spatiotemporally planned as a whole. As a traditional path-planning method, Dijkstra's algorithm usually does not carry any velocity information used in the static environment tempting to seek for a collision-free path among static obstacles from the current vehicle location to a temporary terminal location. Generally, numerous studies, such as the geometry-based, grid-based, sampling-based, meta-heuristic-based, artificial potential field (APF)-based, and artificial intelligence (AI)-based methods are extensively used for path planning or motion planning. In the geometry-based method, a path generation method using the Voronoi cell algorithm is presented. However, this algorithm only performs well in static rather than dynamic environments and Voronoi edges may be discontinuous and thus unsuitable for the nonholonomic vehicle. In grid-based approaches, an algorithm is proposed generating maneuvers for high-speed autonomous vehicles over large distances, which is based on Anytime Dynamic A*. Although the solving process is fast, the resulting path is not continuous and it is usually not easy to find the heuristic rule. Sampling-based algorithms are popular in motion planning. A trajectory planning method based on the state-space sampling is studied. The produced paths connecting the initial state with the sampling terminal states are smooth and kinematic feasible. However, the method may produce no available path in a dense transportation environment because of very limited state sampling space. A motion planning method for autonomous vehicles based on the rapidly-exploring random tree algorithm is presented. It can explore the space very quickly; however, the resulted trajectory is suboptimal, jerky, and not curvature continuous, which needs further smoothing. Another sampling-based method is the state lattice by discretizing the state space in a deterministic manner. The original version is the nontemporal to deal with the static obstacles, and the spatiotemporal state lattices are extended to plan in the presence of moving ones, whereas, its fidelity and real-time performance depends on the sampling density. It is time consuming when generating too many trajectory candidates and making individual evaluation of each trajectory. Another effective method is based on meta-heuristic. A motion-planning method for mobile robots in uncertain environments based on the genetic algorithm (GA) is provided, showing that the result is highly accurate and the robot can respond fast. However, when using GA in autonomous vehicles, the computation complexity is a big trouble for real-time applications. In addition, the classic APF is widely used too. It has advantages in avoiding obstacles in real time; however, the solutions may easily be trapped in the local minima. Recently, the applications of AI-based methods in motion planning are arising. An end-to-end system according to the convolutional neural network is presented, which transforms the camera's raw pixels information directly to steering commands of the autonomous vehicle. It is much more efficient than traditional motion planning techniques and more effective in some scenarios. However, it is hard to collect and train the data from all possible scenarios and the robustness of the system needs to be improved. The path and speed profile are separately planned, where the speed profile is generated by using temporal optimization to optimize all time stamps for the waypoints along the given path. A combined path and speed planner is proposed, which first discretizes both the spatial and temporal space to search the best trajectory according to a set of cost functions. Authors combine the APF and model predictive control (MPC) to automatically decide its path and velocity in a concise manner. The control layer, namely motion tracking control, is to guide the vehicle along the planned local trajectory while satisfying some requirements, such as the dynamics stability. In literature, the decision and control algorithms are fused together, which makes the formulation more elegant, but takes more computational efforts. Because of the limited onboard computational resources, most of the existing research deals with the planning and tracking problem separately in a hierarchical manner. According to the configuration of the studied vehicle (e.g., the type and number of available actuators), numerous control strategies have been extensively studied. Given the common assumption that the tire is purely rolling instead of taking the slip angle into consideration, a linear matrix inequality method is adopted to control a kinematic vehicle model to study how the tire slip angle impacts the closed-loop performance. An MPC with constraints on both inputs and states is used, which concluded the yaw rate and the sideslip angle should also be considered in MPC formulation. Otherwise, the vehicle could fail to accurately track the prescribed path or even become unstable in some extreme conditions. A hierarchical adaptive control is used to address the unknown and nonuniform parameters related to the road condition, where the adaptive law is developed for each wheel. A sliding mode control based on chained system theory is developed for path tracking to attenuate the unmeasurable sliding effects and lateral disturbances. A robust composite nonlinear feedback control is proposed to investigate the path-following performance when the actuation system failures. The aforementioned approaches for path or trajectory planning and tracking of autonomous vehicles definitely solve the corresponding problems presented in each literature. However, they are still not applicable because of the computational resources needed, complexity, solution optimality, and heavy training/retraining tasks. As a result, this paper presents a novel hierarchical planning framework to achieve a balance between these criteria. This paper for the first time proposes a local motion-planning and tracking framework for autonomous vehicles based on the resistance network and artificial potential functions. Due to the nonholonomic feature of the autonomous ground vehicle, the grid size or shape should be rearranged. In addition, like the classic grid or graph-based path-planning methods, the proposed method based on resistance network originally works well in the environment with static obstacles. In other words, the velocity information is not considered when the path is planned. To address this issue, the path planning is repeated temporally in the prediction horizon with the current states and to check if a feasible local trajectory exists. Otherwise, the velocity should be regulated or adapted. By using the proposed resistance-network algorithm, the whole process is speeded up compared to the current grid-based algorithms. The grid is constructed by considering the kinematic constraints of the nonholonomic vehicle. In addition, by using this method, the driver's styles, such as conservative, moderate, and aggressive, can be included by choosing different grid angle in the kinematic-feasible range. The resulting trajectory is then followed by an experimentally validated Carsim model controlled by an MPC. The feasibility and effectiveness are validated by studying several different cases. More importantly, by adjusting the number of the grid and the prediction horizon length, the methods can be easily used in real time compared to other optimization-based planning methods. This paper is organized as follows. In Section II, the proposed motion-planning and tracking framework is elaborated step by step from five different aspects. Section III develops an MPC-based trajectory tracker using a bicycle model. Case studies to demonstrate the feasibility of the proposed method are presented in the next section, starting with the demonstration of the experimentally validated Carsim model. Section VI concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2898599", "references": [1656390188, 1699327229, 1977085960, 1979841516, 2000140632, 2005354226, 2015431502, 2113029345, 2134765105, 2144293566, 2169261433, 2306644740, 2341848647, 2342840547, 2343568200, 2404985975, 2524886531, 2740607401, 2792969186]}
{"paragraph": "The massive deployment of information and communication technologies in industry is transforming the traditional legacy-electromechanical-based systems into modern industrial cyber-physical systems ICPS which tightly integrate the cyber space with the physical space. ICPSs are expected to significantly promote the manufacturing productivity and realize smart services. However, ICPSs are suffering from cyber-attacks due to their increasing connections to the Internet. As cyber-attacks against ICPSs could cause equipment damage, environmental pollution, or even fatalities, ensuring the security of ICPSs is an issue of great concern. Existing security countermeasures for ICPSs e.g., encryption, access control, and intrusion detection lack a quantitative decision-making mechanism to actively defend against advanced persistent threats. Game theory, as an effective formal tool for strategic-behavior analysis, provides the capability to quantitatively model the interaction between attackers and defenders, which can guide system operators to carry out appropriate attack mitigation strategies and reduce the loss caused by attacks. Recently, some researchers have employed game theory to propose security decision-making approaches for ICPSs. In Hankin proposes a hybrid approach using game theory and classical optimization to produce decision support for the defenders of ICPSs. Feng et al. integrate risk assessment with game model to optimize the allocation of defensive resources in multiple chemical facilities. Chen et al. present a comprehensive game framework to seek reliability strategies for defending power systems. Yuan et al. build a hierarchical Stackelberg game model to address the problem of resilient control of networked control systems under denial of service attacks. Niu and Jagannathan use a zero-sum game to derive the optimal strategy for defending dynamic systems in the presence of both cyber-attacks and physical disturbances. In the authors introduce a games-in-games principle for defending ICPSs. This is a cross-layer solution for resilient defense of ICPSs and it is quite promising for protecting ICPSs against cross-layer attacks where the attackers penetrated from the cyber space to the physical space. Despite these previous efforts, however, the majority of them make too simple assumptions about the cyber layer of ICPSs and they generally model the cyber layer as multiple independent elements or abstract dynamic systems. But in fact ICPSs have many kinds of devices communicating with each other through complex networks in the cyber layer. The lack of adequate cyber layer modeling makes these methods not entirely applicable to real-world ICPSs. Moreover, most previous works assume that game model parameters e.g., gains, losses, and game state transition probabilities can be obtained by security experts. But in reality, ICPSs are usually very complex, so it is difficult, if not impossible, to build a game model with all the parameters accurately assigned by security experts. For example, the payoff net gain parameters in the cyber and physical layer have to be evaluated with totally different metrics in the existing approaches. As for the cyber layer, the payoffs are typically measured by dollar values, while the payoffs in the physical layer are usually quantified in terms of control performance degradation. Different quantification metrics increase the difficulty of building a comprehensive security game framework which contains both the cyber and physical layers. Besides, as indicated in traditional methods usually fail to explore the patterns among system variables while data-driven realization can take full advantage of the abundant running data to build more accurate models. In short, a unified payoff quantification method with data-driven parameter learning ability can help alleviate the difficulty of formulating a cross-layer security game model for ICPSs. Motivated by the discussion above, this paper presents a stochastic game model for cross-layer security decision-making in ICPSs. With consideration to the difficulties in game parameter configuration, we first analyze the probability of successful vulnerability exploitation to obtain the game state transition probability distribution, then develop a time-based unified payoff quantification method to quantify the gains and losses of game players. Furthermore, a Q-learning algorithm is devised to learn the optimal strategy profiles when a portion of the model parameters is still unable to be accurately specified using the proposed method. The main contributions of this paper are as follows. A cross-layer game-theoretic approach to security decision-making in ICPSs is proposed to defend against cross-layer attacks where the attacker penetrates from the cyber space to the physical space. Unlike many previous game-theoretic security approaches which construct two game models for the cyber and physical layers in ICPSs, our approach only needs to build one game model with the help of a time-based unified payoff quantification method. A reinforcement-learning algorithm is devised for optimizing the security game model of ICPSs. This is a neural-based adaptive output-feedback control strategy and it can be applied to many other nonstrict-feedback stochastic nonlinear systems. The rest of this paper is organized as follows. In Section II, we model the attack propagation process in ICPSs. Section III presents the game formulation in detail. Section IV introduces the proposed Q-learning algorithm. Then a case study is described in Section V. Finally, Section VI concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2907451", "references": [2056451850, 2067681173, 2110908300, 2128625988, 2152106854, 2294109163, 2315448583, 2330650066, 2345280790, 2612172766, 2754813553, 2777754251, 2787260237, 2810491061, 2888609085]}
{"paragraph": "A cataract is defined as a lenticular opacity, usually presenting with poor visual acuity. According to a World Health Organization report, the estimated number of people in the world who are blind will exceed 40 million by 2025. More than 50% cases of blindness are caused by cataracts, which are considered as the most common cause of blindness. When a patient delays treatment for longer, the visual impairment will be more severe. It is important to improve the quality of eye care service, particularly pre-detection. Although it is widely accepted that early detection and treatment can reduce the suffering of cataract patients, people in less developed regions still cannot receive timely treatment, owing to a lack of professional ophthalmologists. Slit lamp imaging and Scheimpflug imaging are commonly used techniques for clinical cataract diagnosis. Cataract diagnosis protocols include the lens opacity classification system LOCS III, Oxford clinical classification, and American cooperative cataract research group CCRG method. The LOCS III requires a slit lamp for clinical assessment, while the Wisconsin cataract grading system requires photographic grading, both of which are intricate procedures for most patients and can only be performed by well-experienced ophthalmologists. The Oxford clinical classification and CCRG method exhibit similar problems. The ultrasound backscattering signal is used for cataract assessment based on the animal model. By using probability density features and multiclass classifiers, the accuracy of cataract hardness assessment achieves 95% when using a small training set. Optical coherence tomography and the ultrasound biomicroscope can provide more accurate screening. However, these imaging techniques are expensive and their operation is complicated, meaning that they are hardly widespread in less developed regions. Thus, it is crucial to simplify the process and reduce costs for early cataract screening. A simple method for detecting cataracts based on the fundus camera was proposed by the Beijing Tongren Hospital. The detection is realized by evaluating the blurriness of retinal images. By taking the diagnosis using a slit lamp as the truth value, the sensitivity when using retinal images achieves 100% cortical cataract, 84.2% nuclear cataract, and 76.2% posterior subcapsular cataract. The feasibility of this method has been investigated, and inexperienced graders can be trained with several examples 100 image pairs. Compared to ophthalmologists, the accuracy of minimally trained graders reaches 99%. The fundus camera can easily be operated by technologists, or even patients themselves. Since the fundus camera was invented in 1910, retinal images have been used extensively in the diagnosis of ophthalmological diseases, such as glaucoma, age-related macular degeneration, and diabetic retinopathy. Moreover, this technique has been studied in an attempt to quantify cataract severity automatically. Four typical retinal images, representing non-cataract, and mild, moderate, and severe cataracts, are displayed in Fig. 1. Fig. 1(a) presents a healthy retina, in which the main vessels, optic disc, choroid, and even capillary vessels can be clearly observed. An image with a mild cataract is shown in Fig. 1(b), where the main vessels and optic disk are visible, while the choroid and capillary vessels are only faintly visible. In Fig. 1(c), only the main blood vessels and optic disc are visible. Furthermore, almost no retinal structures can be observed in Fig. 1(d). It can be concluded that less retinal structures can be observed with more severe cataracts. A cataract develops slowly, and may be partial or complete, as well as stationary or progressive. Patients with cataracts of different severities require different treatments. Patients with mild cataracts can be treated by wearing antiglare sunglasses to delay deterioration, while surgical assistance is essential for moderate and severe cataract patients. By extracting appropriate features from retinal images, it is possible to distinguish the different levels of severity. This study attempts to develop an automatic method for grading cataract severity based on retinal images. This approach is simple and requires little knowledge, which is meaningful for improving the medical conditions in less developed regions. The main contributions of this paper can be summarized in terms of three aspects. The Haar wavelet transformation is improved based on the retinal image characteristics. By means of this improvement, the contrast of the one-layer detail components is substantially enhanced, and the distortion of the detail components from multilayer decomposition is avoided. Based on the improved Haar wavelet and back-propagation neural network BP-net, the optimal feature extraction threshold is obtained according to the ergodic searching process. Three sets of two-class classifiers are trained, and a hierarchical classification method is developed on this basis. The remainder of this paper is organized as follows. Section 2 reviews the related work. Section 3 elaborates on the necessity for improving the Haar wavelet, and details the improvement process. Section 4 presents the methodology applied. Section 5 reports on the performance analysis and provides a discussion. Section 6 presents conclusions and future work.", "section": "Introduction", "doi": "10.1016/j.inffus.2019.06.022", "references": [1977880724, 1995023030, 2000289531, 2020112862, 2040584809, 2061997187, 2066454956, 2149430368, 2159764117, 2198235674, 2344060962, 2344251399, 2504300638, 2517822188, 2595212727, 2604229724, 2625621508, 2743461367, 2807570175, 2811429666]}
{"paragraph": "The digital age is already upon us, and we have started reading and writing on digital devices in place of a physical piece of paper. However, the coexistence of printed and digital documents is ensured by factors such as ubiquitousness, ease of use, cost and security of documents. One of the important forensic aspects of a printed document is the knowledge related to the source of that document. The knowledge of printer used to print a document can not only help in criminal investigations but also safeguard the use of paper for legal, administrative and other official records. Source attribution of printed documents using digital techniques has gained significant importance in recent times. Traditional methods use chemical or microscopic techniques which are time-consuming, costly, may even be intrusive and require an expert examiner. On the contrary, working on the scanned image of a printed document converts it into a classical pattern recognition problem involving feature extraction and classification. So, in digital techniques, only an office scanner and computer are required. These techniques involve the use of an extrinsically embedded signature or watermark before or during the printing process as well as detection of source printer based on intrinsic signatures. The early methods based on intrinsic signatures relied on printed documents scanned at high resolutions which would be a costly and time-consuming affair. A relatively new intrinsic approach is based on geometric distortions induced by a printer. However, these either require the original soft copy of the printed document or a way to generate the reference soft copy from the printed document which would be troublesome and/or inaccurate in most real-life applications. There is another category of intrinsic signature which is based on texture analysis. Most of the successful methods in this category extract features from all occurrences of a specific letter type, font type and font size. Then a classifier is trained to predict labels of the test data. Thus, extracting information from all letters require training multiple classifiers, one for each letter differing by letter type, font type, or font size. Also, each of those classifiers would require enough amount of input letters for proper training. Further, extraction process for each of those letters such as use of optical character recognition limits the classification process. An attempt was made recently to extract information from all letters by use of a single classifier. The feature extraction pipeline gave promising results when letters printed using a specific font were used to train as well as test. However, in many practical scenarios, testing data might consist of only a particular font or language, not present in the training data. Building on the approach, this work presents a new feature descriptor for source classification of printed documents with the aim of addressing the cross font problem, a scenario where font of letters in test data is different from font of letters present in the train data. The printer’s signature related to texture is a result of non-uniform and unintentional imperfections in toner developed on a printed page. The local texture features around a pixel in a small neighborhood would be directly dependent on the way this toner appears or spreads in that neighborhood. Thus, we introduce a new printer specific local texture descriptor to capture textures on the scanned image of a printed document. The proposed method makes no assumption about the type of the letter. Following are the major contributions of this paper: Introduction of a novel set of printer specific local binary pattern based features which can capture printer generated local texture patterns. Demonstration that encoding and regrouping strategy based on the orientation of intensity and gradient-based small linear structures makes the method a very powerful discriminative feature. Evaluation of the proposed system comprehensively on the publicly available dataset, showing that, given sufficient amount of train data, it outperforms existing methods. Also, it reduces confusion between printers of the same brand and model. Encouraging results on cross font dataset, outperforming state-of-the-art methods by large margins. The remainder of this paper is organized as follows. Section II briefly describes existing intrinsic signature based techniques for classifying the source printer of printed text documents. The details of our proposed feature descriptor have been mentioned in Section III. The proposed system is described in Section IV. A series of experiments have been conducted to test the efficacy of the proposed method. Their description and results have been discussed in Section V. Finally, we present the conclusions that can be derived from this work along with the pending challenges for future work in Section VI.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2919869", "references": [60318, 1480586119, 1481922366, 1566219896, 1820514791, 1976499261, 1977737593, 1981119885, 1987392461, 2021549453, 2029832855, 2078765864, 2079317077, 2089575713, 2117487104, 2127206948, 2131081720, 2131978991, 2135392152, 2138584058, 2145947562, 2147141800, 2149286313, 2153635508, 2156436243, 2163352848, 2165553585, 2606661584, 2682476753, 2798033509, 2800424369, 2801323926, 2963123309]}
{"paragraph": "There are many new computing systems that incorporate wearable computers or IoT devices that do not have a good means of entering text (e.g., smart glasses). The immersive nature of virtual reality systems with head-mounted displays also makes it difficult for users to enter text in mobile situations. Many new interaction methodologies for these systems have been introduced, such as hand gesture recognition with a sensor-based glove, gesture-based swipeboard techniques, and vision-based hands-free interaction methods. However, the proposed methods are still not elaborate enough to be used extensively. Even when interacting with conventional computers, text entry may not be comfortable or even impossible, for example, in lectures. Figure 4. Palm-worn design offers stable camera views of all fingers in a different wrist poses ((a), (d) for flexion; (b), (e) for neutral and (c), (f) for extension). Hence, we present TapSix, a palm-worn device that detects finger taps on six virtual keys on a tactile surface to enter text. With TapSix, users can type without paying visual attention to the invisible virtual keys and fingers, as shown. Its Bluetooth wireless communication and Human Interface Device (HID) profile enables remote interaction with most computers and IoT devices. Moreover, a wide angle two-dimensional (2D) camera is installed to capture the view of all five fingers. Figure 1. (a) TapSix measures the distance between fingertips and any kind of surface as well as multiple finger taps in real time. (b) Wireless wearable device prototype consisting of a camera on the palm and a processor on the back of the hand. (c) Text entry for VR application. Its palm-worn design is unique and provides a stable view angle for all five fingers even when the upper arm moves unpredictably. As a camera is worn on the wrist in most previous studies, wrist motion could affect the view of five fingers. Because the camera is installed on the palm, the distance from the camera to the fingers is very short. Thus, instead of 3D depth cameras, a low-cost 2D camera that requires a short imaging distance is employed. A variety of approaches to text entry use one-handed chorded keyboards that consist of five keys corresponding to five fingers. In these keyboards, the possible number of chords is 32, or 2N, where N = 5 fingers. This number is greater than 26, the number of letters in the English alphabet, but it is not enough to cover many special keys. To generate a sufficient number of chords, anatomically uncomfortable finger tap combinations may be required, and it could be hard to find a letter-to-tap mapping that is easy to learn and memorize. TapSix provides 48 (i.e., 3 × 24) chords. That is, it consists of six virtual keys with two keys for the thumb (three possible combinations) and another four keys for the other four fingers, as shown. Therefore, it is possible to avoid combinations that are anatomically uncomfortable or difficult to detect accurately. We also introduce a tap recognition algorithm to detect the tapping of multiple fingers on six virtual keys on different surfaces such as the chest, the arm, a desk, or a virtual surface which is a non-physical two-dimensional surface in the air and could be recognized to be touched if a fingertip meet or pass through the surface. Because the thumb can touch two different virtual keys, the algorithm robustly differentiates between the two possible touches of the thumb. Our algorithm uses only geometric features to minimize the computation in the wearable application and, unlike previous studies, it also measures the distance between each finger and the tactile surface, which can be used for additional interactions such as volume control or zoom in/out. A new letter-to-tap mapping protocol is proposed to fully utilize the six-key feature of TapSix. It is possible to systematically optimize the letter-to-tap mapping with respect to multiple objectives, but it is impossible to exclude some uncomfortable taps if only five keys are used. In our proposed mapping, an alphabet is divided into five groups, one for each vowel, and each group is assigned a one-finger, two-finger, or three-finger tap set – meaning that no tap with more than three fingers is used in a letter-to-tap mapping. While some finger combinations are anatomically uncomfortable even with two or three fingers, it is possible to exclude these combinations by using more comfortable combinations. TapSix hence enables users to enter text in a variety of environments, as shown in. Figure 2. A variety of environments in which TapSix can be used for text entry.", "section": "Introduction", "doi": "10.1080/10447318.2019.1597573", "references": [68101676, 1528201305, 1971990626, 1980513660, 1989391659, 2033694835, 2035550706, 2063944808, 2071799443, 2099287431, 2099800354, 2111765895, 2124870725, 2130267005, 2167040800, 2171287854, 2404051271, 2408252211, 2423984454, 2470040761, 2581706031, 2609211631, 2793527415, 2896353548, 2896779713]}
{"paragraph": "The field of efficiency and productivity analysis using frontier estimation methodologies has been developing very rapidly in the last four decades. Since the seminal articles of Charnes et al., Banker et al., and Färe et al., the literature developing both methodological and empirical contributions to the nonparametric frontier literature (often identified by the name DEA data envelopment analysis) has been literally booming. Equally so, since the seminal articles of Aigner et al. and Meeusen and van den Broeck, stochastic frontier analysis (SFA) has almost equally flourished along both methodological and empirical lines. Further methodological developments have led to new and somewhat separate streams of literature. Common to this broad efficiency and productivity literature using some form of frontier estimation is the enormous and booming empirical literature that has emerged. A wide diversity of sectors has been studied using cross section, time series, or panel data covering a wide variety of geographical areas (from municipalities and counties to regions, and from countries to continents). On the one hand, this efficiency and productivity literature has led to an abundance of surveys aimed at summarizing general or specialized methodological advancements. On the other hand, this flood of empirical frontier applications has also lead to a multitude of empirical surveys. The latter empirical surveys are the main topic of this contribution. As a matter of fact, there are a lot of empirical surveys available in the literature focusing on specific sectors of application. But, to the best of our knowledge, none of the existing studies have looked at what are the most surveyed fields of empirical applications and what are instead those in which there are no or few surveys, and how this situation evolved over time. To the best of our knowledge, there is no survey on surveys in the field of frontier methods (SFA, FDH free disposal hull, DEA, and their extensions) proposed to evaluate the many facets of the efficiency literature in the different areas of the economic activities. The main real difference from our proposal to other surveys is to use a bibliometric methodology to assess the size and importance of the applications in those areas: in addition to the number of surveys, the co-occurrence of the concepts, methods, and areas is used to define a degree of generality that allows the visualization of gaps and overlaps in the field. The topic of this paper is exactly to fill up this gap. The basic objective of this contribution is to provide a state of the art survey of empirical surveys of frontier estimation applications as applied to different economic sectors. By lack of a better concept, we label this a meta-survey. This amounts to asking the basic question: in which sectors and fields do empirical surveys exist? And if such empirical survey exists for a sector, we want to determine how many such surveys exist for this field and how recent these surveys are? Furthermore, we look at the connections among different sectors and fields of application through co-citation analysis. This should allow us to identify the gaps in the existing sectors and fields and offer some interpretations of the currently available literatures. To develop such a meta-survey of empirical surveys of frontier estimation applications, we encounter the following series of methodological problems. We obviously need to have a full description of all the possible economic sectors and fields, so as to be able to identify existing gaps in the literature. To identify a rather universal taxonomy of economic activity, we adopt the United Nations' International Standard Industrial Classification (ISIC) taxonomy. We allocate all of the empirical surveys we encounter to one of the available taxonomic classes. This is done manually here, but our work could also provide useful suggestions on how one could standardize this activity in the future. Finally, we attribute all empirical surveys also to the Journal of Economic Literature (JEL) classification, which is often used to classify research papers mainly in the economics literature. We discuss about the importance of these classifications to standardize, improve, and facilitate further analysis and updates of this study. The main methodological tool applied in our meta-survey is the systematic review. In addition, we employ advanced clustering and mapping techniques. Finally, a co-citation analysis is performed to investigate the evolution of the interconnections among economic sectors and fields of application. This contribution is structured as follows. We first put the notion of research review in context to clearly delineate what we do different from other existing reviews. In the next section of methodological notes, we introduce the notion of a systematic review in some detail. In the following sections, we specify in detail the methodology used and in particular, the queries that have been run in the systematic search, the main outcomes of the survey, and the bibliometric and mapping exercises done on the keywords. Section 7 offers some final comments and outlines future research.", "section": "Related Work", "doi": "10.1111/itor.12649", "references": [117860167, 1532851129, 1983429456, 2019180267, 2061834261, 2062593961, 2073305391, 2081489108, 2094462749, 2129029143, 2150220236, 2400069463, 2515647668, 2555546862, 2561843306, 2625383600, 2754610342, 2766067217, 2804156693]}
{"paragraph": "With the increasing demand for distributed systems and the growth in the scale of the applications, a traditional networked control system using wired communication networks to transfer the information between communicating units in the closed-loop control system introduces limitations. The use of wireless communication networks offers significant advantages over their wired counterparts, which results in low cost and flexible network architectures by decreasing the cost of the installation, modification, and upgrade of the system components compared to their wired equivalent. This control system architecture is referred to as a wireless networked control system (WNCS) in which the sensor and actuator communicate with the controller through a wireless network. WNCS has been attracting increasing research interest and has been finding various applications in smart grid, intelligent monitoring, and building automation. With its inherent advantages, WNCS facilitates prospects in the manufacturing industry. Although the WNCS provides many benefits compared with the traditional wired NCS, it is also predictable that the WNCS is more vulnerable than wired NCS due to its inherent openness nature from the wireless network. The communication system design for a WNCS requires guaranteeing the performance and stability of the control system, with the limited battery resources of sensor nodes, despite the unreliability of wireless transmissions and the shared wireless medium. As such, a deal of literature has been found on securing the WNCS in both the control and communication communities. Recently, a number of security incidents of NCS have been reported. For instance, the industrial control systems in Iran have been infected by the advanced computer worm named “Stuxnet”. The frequent occurrence and severe damage of such incidents motivate extensive investigations on the secure issues of WNCS. In the reported literature, two types of cyber attacks have been investigated, namely: the deception attack corrupting the data integrity and the denial-of-service (DoS) attack corrupting the data availability. To be more specific, the deception attack aims at modifying the information in the transmitted data packet and requires comprehensive information of systems a priori. In comparison, the DoS attack aiming to congest the information transmission is easier to launch because prior knowledge of the system is not required. Furthermore, it is worth mentioning that the information loss caused by the DoS attack could degrade the system performance or even render the whole systems unstable. Due to the potential threat associated with a DoS attack, it is significant to investigate the security issue of the WNCS subjected to a DoS attack. A literature survey shows that a number of methodologies have been reported to alleviate or cancel the DoS-attack-induced impact, such as packet dropout or time delays. The methodologies concerning the security of the control system under a DoS attack could be classified as an attack-tolerant method and attack-mitigated method. Specifically, the attack-tolerant method has been proposed such that the controlled system is capable of tolerating the undesired phenomena caused by DoS attackers while the attack-mitigated method has been employed in order to directly compensate the undesirable effect induced by DoS attacks. It is worth mentioning that for the attack-mitigated method, the cross-layer design methods have been employed which not only design the controller to adapt to the attack-induced phenomena, but they also develop security strategies to protect the communication network of the NCS. However, the investigation of the cross-layer design method for the control problem of WNCS has been really scattered, which formulates the main motivation of writing this paper. Summarizing the above discussions, although the cross-layer design approach has stirred some initial attention from the researchers, the co-design method to secure the WNCS has not been adequately addressed. In this paper, the WNCS is modeled hierarchically, where in the cyber-layer, a signal-to-interference-plus-noise ratio (SINR)-based security game is employed such that the players make a decision on their transmission power. In the physical layer, the player designs optimal control strategies subject to the network constraints imposed by the upper layer. Based on the obtained strategies, we propose a novel cross-layer design method, that is, the pricing mechanism design approach with which the studied WNCS can remain stable despite DoS attacks. To be specific, the contributions of this paper are highlighted as follows. A novel SINR-based game theoretic model is put forward to describe the interactions between the transmitter and DoS attacker. The sufficient condition is established for the existence and uniqueness of the Nash equilibrium, and the corresponding transmission power strategies are given for the transmitter and attacker, respectively. The impact from the attack and defense model on the controlled system is described in terms of packet dropout, which is assumed to be correlated according to the Markovian rule. In the presence of such packet loss, the optimal control scheme is developed in the delta-domain in order to better adapt to the fast-sampling protocol. Furthermore, a novel sufficient condition is established such that the control performance remains within the security region over the infinite horizon. An extended-state-observer-based dynamic feedback pricing mechanism is designed such that the studied WNCS remains within a desirable security region by strategically tuning the pricing parameters.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2863689", "references": [32864775, 1934866980, 1992456875, 1996056443, 2009257627, 2021959953, 2031578370, 2047399268, 2063688797, 2135216999, 2139197149, 2144114773, 2150113962, 2206930994, 2230652337, 2315448583, 2324627801, 2337446187, 2340384825, 2346026831, 2469287823, 2505598063, 2595324233, 2619461977, 2740518031]}
{"paragraph": "Due to the wide applications in engineering, distributed optimization for multiagent system is a hot topic in the optimization community. The objective is to minimize a sum of convex functions, which need to design an algorithm to makes each agent can find the optimal solution by exchanging local information with neighboring agents over the network. Recently, the algorithms of distributed optimization have been developed in discrete-time or continuous-time. In the following, we will briefly review the literature about these algorithms. In the past ten years, discrete-time multiagent optimization and their applications have been widely investigated. A discrete-time subgradient method was applied to multiagent optimization, where each agent minimize its own objective function while exchanging information locally with other agents in a time-varying topology. Based on saddle points of the Lagrangian and penalty functions, a distributed Lagrangian primal–dual subgradient algorithm was designed for solving objective function with a global inequality constraint. By removing a projection step in most discrete-time multiagent algorithm, a class of distributed stochastic gradient algorithms was developed to solve the problem by using only local computation and communication. Moreover, there have been various types of distributed algorithms proposed for solving different kinds of optimization problems. Recently, some researchers have started to focus on the distributed continuous-time optimization. An alternative distributed dynamical method was designed which converges to the optimal solution set of a sum of convex differentiable functions with globally Lipschitz gradients. Using a proportional–integral protocol with agents’ output information, a continuous-time multiagent system was developed for distributed constrained optimization. Nonuniform gradient gains and finite-time convergence in distributed continuous-time optimization were discussed. An extremum-seeking controller was designed for distributed optimization over sensor network. Moreover, some recent works on the application of distributed continuous-time optimization are discussed. A distributed Laplacian-gradient dynamics algorithm was developed that guarantees to find the optimal solution to the distributed economic dispatch with or without capacities constraints. Under any initial conditions, a novel distributed algorithm for optimal resource allocation with feasibility constraints and economic dispatch of power systems was proposed. Based on the theory of switched systems, the scaled consensus problem for switched multiagent systems composed of continuous-time and discrete-time subsystems was investigated. Based on multiple interconnected recurrent neural networks, a collective neurodynamic method for distributed optimization was designed. Considering the existence of time delay in signal processing and signal transmission process, some multiagent time delays consensus problems were investigated. Above these multiagent algorithms in distributed optimization, the continuous-time communications with neighboring agents are necessary. In reality, the frequent communications are not desirable for neighboring agents due to high energy cost. Therefore, how to design the distributed algorithm which can reduce communication cost is very meaningful. For example, some researchers proposed the event-trigger schemes which means that neighbors’ information is only available at discrete sampling time instants. In addition, if communications among agents occur only at some discrete-time instants, impulsive scheme can reduce the cost of communication. Up to now, many results about impulsive stability, synchronization, and consensus have been reported from the point of view of control theory. Using the switched Lyapunov function method, the exponential stability of nonlinear impulsive switched systems was discussed. The stability of nonlinear differential system with state-dependent delayed impulses was investigated. Considering the distributed impulsive controller, leader–follower synchronization in heterogeneous dynamic networks was studied. However, to the best of the authors’ knowledge, few results for distributed optimization algorithm using impulsive schemes have been reported in the literature. In this paper, based on continuous-time algorithms, we provide a framework of impulsive quasi-consensus for distributed constrained optimization. First, adopting the interior point method, the constrained optimization problem can be transformed into an unconstrained problem. Using logarithmic barrier penalty, the approximation error is mathematically analyzed. It shows that the approximation error is smaller when penalty parameter increases. Next, using the continuous-time gradient method and algebraic graph theory, the impulsive average quasi-consensus algorithm is designed, in which each agent can deal with the aforementioned local objective functions, and each agent can only communicate with neighboring agents at the impulsive instants. Under certain conditions, we can prove that the impulsive average quasi-consensus can be achieved. The main advantages of our algorithm are as follows. Compared with the continuous-time algorithm, the impulsive average quasi-consensus algorithms only regulate the state at the fixed-time instants, which can reduce the cost of communication with neighboring agents over the network. Especially, due to the interior point method, our algorithm has simple structure and the least number of state variables rather than extra state variables required in other algorithms. The remainder of this paper is organized as follows. In Section II, the preliminaries on algebraic graph theory and distributed optimization problem are introduced, respectively. The distributed impulsive quasi-consensus algorithm is proposed in Section III. Simulation results on two numerical examples and one sensor network localization problem are presented to verify the effectiveness and the performance of the proposed algorithm in Section IV. Finally, Section V concludes this paper.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2869249", "references": [1923997169, 1968094122, 2022948311, 2027741263, 2040840525, 2063246107, 2092483181, 2120765325, 2130308005, 2151685478, 2164278908, 2164642931, 2191781618, 2215067230, 2273484141, 2341544006, 2342445519, 2342805660, 2375258435, 2406054937, 2409418461, 2421576504, 2464348263, 2517192452, 2521320587, 2526043704, 2530618030, 2539372614, 2553512835, 2584604366, 2593034620, 2611497108, 2612356425, 2744910090, 2761385453, 2766228189, 2767760906, 2791781861, 2796434525, 2806651853, 2963031548]}
{"paragraph": "We measure, store, and share an immense amount of data about ourselves, from our vital signals to our energy consumption profile. We often disclose these data in return of various services, e.g., better health monitoring, a more reliable energy grid, etc. However, with the advances in machine learning techniques, the data we share can be used to infer more accurate and detailed personal information, beyond what we are willing to share. One solution to this problem is to develop privacy-preserving data release mechanisms that can provide a trade-off between the utility we receive and the information we leak. Denoting the data to be released by random variable Y, and the latent private variable as X, we apply a privacy-preserving mapping on Y, whereby a distorted version of Y, denoted by U, is shared instead of Y. Typically, privacy and utility are competing goals: The more distorted version of Y is revealed, the less information can be inferred about X, while the less utility can be obtained. As a result, there is a trade-off between obtaining utility and leaking privacy. Since privacy can be a concern in legal transactions of data, it appears in different areas, where information is transferred from a user to a legitimate receiver of information. For instance, in database privacy, data is published publicly, while preserving the privacy of individuals (identity, attributes, etc.). Another example is privacy in smart grids, where a smart meter measures and reports the power consumption of a user to the electricity provider to improve the reliability and energy efficiency, and from this information, several private features of the user, such as their usage patterns or daily life habits, can be leaked. The statistical view of privacy (information-theoretic, estimation-theoretic, and so on) has gained increasing attention recently. For example, a general statistical inference framework is proposed to capture the loss of privacy in legitimate transactions of data. The privacy-utility trade-off under the log-loss cost function is considered, called the privacy funnel, which is closely related to the information bottleneck introduced earlier. Privacy and utility are expressed in terms of correctly guessing probabilities. A generic privacy model is considered, where the privacy mapping has access to a noisy observation W of the pair (X,Y). Different well-known privacy measures and their characteristics are also investigated. We study the information-theoretic privacy in this paper. For two probability mass function p,q on random variable X, the total variation distance is defined as δ(pX,qX)=12∥p−q∥1, where p and q are the probability vectors corresponding to probability mass functions (pmf) pX and qX, respectively. We measure the privacy-leakage (about the private variable X by revealing U) by the following average total variation distance T(X;U)=EU[δ(pX|U,pX)]=12∑upU(u)∥pX|u−pX∥1. Note that T is not symmetric, and we have T(X;U)=0 iff X and U are independent. First, we characterize the optimal utility-privacy trade-off under this privacy measure for three different utility measures, namely mutual information, minimum mean-square error (MMSE), and probability of error. Then, we motivate the proposed privacy measure by showing that it satisfies both the post-processing and linkage inequalities, and it provides a bound on the leakage measured by mutual information, maximal leakage, or the improvement in an inference attack with an arbitrary bounded cost function as considered earlier.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2903658", "references": [1997176470, 2026875645, 2077145963, 2099471712, 2147080696, 2147324825, 2156740744, 2343097824, 2608598632, 2734457061, 2746431867, 2766599694, 2779622762, 2805329953, 2962793079, 2962986088, 2963115192, 2963278610, 2963412800, 2963535017, 2963634943, 2963660569, 2964003198]}
{"paragraph": "One of the central problems in elimination theory is the construction of determinantal formulae for the resultant. In this context there is also a special emphasis on exploiting the sparsity of the input, or in other words the support, of the involved polynomials. Among the various constructions the best we can hope for is a degree-one formula; that is a matrix whose non-zero entries are coefficients of the input polynomials, and whose determinant is equal to the resultant. The Sylvester-type formulae fall in this category. Unfortunately, such formulae do not always exist for given Newton polytopes. There are also Bézout-type formulae where the entries of the matrix are coefficients of the Bézoutian polynomial and thus they are high degree polynomials in the coefficients of the input polynomials. We call the matrices that have entries that are both Sylvester-type and Bézoutian-type, hybrid. We focus on resultants and discriminants for polynomial systems in two variables. A polynomial system is unmixed if all of its polynomials have the same Newton polytope, and mixed otherwise. Exact resultant formulae are mostly known for certain classes of unmixed systems, and very little is known for the general mixed case. We are interested in optimal degree-one formulae for the mixed resultant. Degree-one formulae are very convenient for both the analysis and the implementation of resultant methods, since their matrix expressions are simple to compute and have bitsize that matches the bitsize of the input. Common degree-one formulae are the Sylvester-type formulae; in this work we present a different one that expresses a second order Koszul map. Khetan presented explicit exact formulae for an arbitrary unmixed sparse bivariate polynomial system. His determinantal formula is a hybrid Sylvester and Bézout type. Also in the unmixed case there are necessary and sufficient conditions for the Dixon resultant formulation to produce the resultant. In the same context, Elkadi and Galligo proposed to use a variable substitution and two iterated resultants to compute the resultant polynomial. Regarding Sylvester-type formulae, matrices expressing optimally the resultant of unmixed bivariate polynomials with corner-cut support are found. Moreover, Sylvester formulae for more general unmixed bivariate systems were depicted. The proof of the main theorem makes use of tools from algebraic geometry, including sheaf cohomology on toric varieties and Weyman's resultant complex. There are also methods for constructing resultant matrices for bivariate polynomial systems that combine Sylvester type blocks with toric Jacobian blocks in the case where the Newton polytopes of the polynomials are scaled copies of a single polygon. The determinant of these matrices is a multiple of the sparse resultant, that is, the formula might not be optimal. Resultants are closely related to discriminants. Discriminants have many applications, ranging from singularity theory of partial differential equations to the computation of the Voronoi diagram of curved objects. Especially for the bivariate case there is work that relates the mixed discriminant with the sparse resultant and the toric Jacobian. Tensor-product systems fall in the general category of multihomogeneous systems. One finds the first expressions of the resultant of such systems as the determinant of a matrix. For unmixed multigraded systems Sturmfels and Zelevinski provided optimal Sylvester-type formulae. These formulae arise as certain choices of a Weyman complex of modules. Many, if not all, classical resultant matrices are instances of such complexes, including the projective resultant. There is a systematic exploration of possible determinantal complexes, and also a software package that produces formulae for unmixed and even scaled resultants. Interestingly, there is a plethora of hybrid resultant matrices that consist of Bézout-type and Sylvester-type blocks. The main contributions of this work are as follows. We present a determinantal degree-one formula for the resultant of arbitrary mixed bivariate tensor-product systems. The formula applies without any restrictions on the bidegree of the polynomials, it expresses a Koszul map and has degree one with respect to the coefficients of the system. Moreover, we prove that the univariate and the bivariate case are the only cases among tensor-product polynomials which admit an unconditional formula of degree one. We provide a constructive method to compute this matrix explicitly, by identifying the dual multiplication maps, therefore making our formula explicit. We call the matrix Koszul resultant matrix. The latter construction allows us to derive formulae for computing the discriminant of one or two bivariate tensor-product polynomials. Compared to the existing literature, we reduce significantly the degree of the extraneous factor which is involved in the computation. Another important aspect is that our formulae provided are free of nonzero multiplicative constant, so that they yield smoothness criteria that are valid in arbitrary characteristic. The rest of the paper is organized as follows. In the next section we present preliminary results that we need for our construction. In Section 3 we present the mixed resultant complex for the bivariate case and we derive the determinantal Koszul formula. Moreover, we show that universal degree-one formulae arise for at most two variables. In Section 4 we provide the algorithmic construction of the Koszul resultant matrix, by identifying the cohomology groups which appear in the complex. Finally, in Section 5 we tackle the problem of computing some related discriminants and mixed discriminants. Throughout the paper, several toy examples accompany the main results. A preliminary version of this paper appeared earlier. In the current final version the Sections 4 and 5 are expanded significantly, and in particular the computation of mixed discriminants is improved by relating them with mixed resultants of lower degree.", "section": "Methodology", "doi": "10.1016/j.jsc.2019.07.007", "references": [1857034947, 1963919699, 1966341842, 1968297701, 1971439379, 2025561788, 2027815348, 2048329955, 2058393311, 2061296216, 2066758283, 2067754443, 2088129367, 2108888104, 2149153582, 2618054456, 2952936986]}
{"paragraph": "With the popularity of mobile Internet and mobile devices, images and videos need to be played in different resolutions. In this way, devices in special resolution need to resize the images to meet the requirements. However, the quality of image retargeting results affect the quality of experience. Currently, many image retargeting method have been presented, all of which are not universal for application situations. Therefore, it is necessary to design a universal retargeting method. In this process, how to evaluate the results of image retargeting becomes a critical issue, which will be the main purpose for this paper. Presently, there have been many research achievements in image-quality evaluation. Wang used structural similarity to measure the error visibility for image-quality evaluation. Sheikh and Bovik made use of the relationship between image information and visual quality. A novel feature similarity method designed for image assessment was proposed by Zhang. Later, the image-quality evaluation was gradually developed for video evaluation. Based on these typical algorithms, we can arrive at a primary conclusion. In addition, some video-quality assessment metrics were proposed. Traditional full reference image-quality assessment often makes use of the subtraction between distortion image and reference image. In early research on image retargeting evaluation, many researchers directly applied the traditional image-quality evaluation algorithms on image retargeting quality assessment. However, there is a big difference between them. The most obvious problem is that image retargeting pays attention to the geometrical shape and contextual matching after resolution changes, which is difficult to solve by traditional image-quality evaluation algorithms. Based on these issues, many special evaluation algorithms for image retargeting were designed. Simakov proposed a principled approach based on optimization of the well-defined similarity measure. The problem it considered is retargeting of image or video data into smaller sizes. Liu presented an objective metric simulating the human vision system. Different from traditional objective assessment methods that work in a bottom-up manner, it used a reverse order that organizes image features from global to local viewpoints. Inspired by Wang, Fang proposed an effective but simple image retargeting quality assessment method which can be called IR-SSIM. In the assessment process, the scale-invariant feature transform flow is used to find the pixel correspondence. And an SSIM map is computed to measure the preserved structure information in the retargeted image. Zhang made another breakthrough by developing an aspect ratio similarity metric. In the computing process, local block-quality changes are used. Rubinstein first set up a database for evaluating the retargeting image named RetargetMe. They present the first comprehensive perceptual study and analysis on image retargeting. Ma put forward another database designed as a diverse independent public database with corresponding subjective scores, and they also give an effective evaluation method. Based on perceptual geometric distortion and information loss, Hsu presented a new objective-quality assessment method for image retargeting. At the same time, they also built another database called NRID. Jiang conducted research on image retargeting quality assessment through learning sparse representation. They focused on finding the potentiality of sparse presentation based on distortion-sensitive features. Ma resorted to the pairwise rank learning approach to discriminate the perceptual quality between the retargeted image pairs. Liang considered five different key factors for image retargeting, such as salient regions, influence of artifacts, the global structure of the image, well-established aesthetics rules, and preservation of symmetry. Liu put forward image retargeting quality assessment based on four quality factors and support vector regression. They accounted quality factors into two categories: shape distortions and visual content changes. Although the above algorithms have achieved some good results, they still have a lot of problems. These problems can be summarized in three aspects. In image retargeting quality assessment, most of the evaluation methods use simple regression methods, which do not correspond with the perception simulation in the human vision system. Based on this consideration, the deep learning method will greatly benefit the accuracy of the evaluation algorithm. Although there are some methods considering geometrical shape or contextual matching separately, the relationship between the features based on the two different parts has not been fully studied. If this problem can be solved, the evaluation result will improve. Most of the image retargeting quality assessment methods only inherit traditional image evaluation framework. And how to design a framework designed especially for image retargeting quality assessment is very important. The cross database experiments are ignored in previous research. In the proposed algorithm, the following contributions of this paper can be summarized to improve the performance of the image retargeting quality assessment algorithm. Segmented stacked autoencoder based on image representations is used to simulate the retargeting image perception process in the human vision system. Especially, we propose a deep quality evaluator for image retargeting based on the connections of two modules: image representations and segmented stacked autoencoder. On the one hand, the image representations are used for image information exaction, which can simulate the first image perception step from eyes to brains in the visual pathway. On the other hand, segmented stacked autoencoder makes use of a greedy training method layer by layer to train each layer of the network sequentially. The process corresponds with the retargeting image perception in human brains in the visual pathway. Based on the above consideration, we choose it to finish the final assessment of the retargeting images. The proposed method overcomes overfitting and finds the complementary image features for the whole framework. Presently, the method of deep learning has promoted the ability of the pattern recognition algorithm, but encounters an overfitting problem in the traditional image-quality evaluation. In order to solve the overfitting problem, we introduce regularization as the solution to make the deep model more accurate in the test stage. In addition, we choose the network input by considering two complementary parts: geometrical shape and contextual matching. And we make a deep study in the relationship between the two categories. Experiments show that the geometrical shape and content matching can actually provide a more reliable feature group for deep learning on image retargeting quality assessment. Cross database experiments have been performed in this paper and it can promote the development in practical applications. In previous research on image retargeting quality assessment, cross database experiments are always ignored during the different building principles. In order to improve the practical value of image retargeting evaluation, we put forward the cross database experiments. On RetargetedMe and NRID, we set virtual differential mean opinion score for quality of every retargeted image based on the preferred number in the original paired comparison methodology, which can correspond with CUHK. Through a comprehensive validation, the proposed metric correlates well with subjective observations and it can be used for a general quality evaluator in image retargeting quality assessment. The rest of this paper is organized as follows. Section II will illustrate the background and motivation based on related work. In Section III, the special algorithm framework will be given. In Sections IV and V, the experimental design and experimental results are shown. Finally, the conclusion will be given in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2864158", "references": [1566135517, 1574818812, 1581984155, 1755382400, 1963884485, 1967046884, 1974647172, 1977246677, 1985949187, 1994197834, 2018377917, 2022164110, 2046119925, 2070322626, 2080008265, 2084363474, 2085431493, 2085889771, 2089947415, 2090518410, 2102285609, 2102591820, 2102817897, 2109506130, 2110528268, 2115273023, 2121809595, 2133665775, 2135306627, 2136211010, 2139144545, 2141983208, 2155175457, 2161907179, 2161927477, 2162692770, 2166554908, 2167322721, 2168397407, 2235636382, 2241147999, 2340803990, 2347050905, 2463319845, 2463322449, 2511113797, 2527269634, 2539033431, 2543770934, 2606880804]}
{"paragraph": "As a novel decision-making approach, three-way decisions are to divide a universal set into three pair-wise disjoint regions by developing an appropriate strategy. During the application, these regions can be interpreted by the corresponding strategies (or semantics), e.g., three-way decisions with shadowed sets, three-way decisions with granular computing, three-way decisions with cognitive computing and three-way decisions with decision-theoretic rough sets (DTRSs). From the general form, it has an acceptance decision region, a deferment decision region and a rejection decision region, which exactly maps the positive region, the boundary region and the negative region of rough sets, respectively. Among the researches of three-way decisions, three-way decisions with DTRSs proposed by Yao are a major topic of research. It has attracted the attention of many researchers. For instance, by using the game theory, Yao and Azam developed a new approach to determine the probabilistic thresholds of rough sets and further discussed the applications of game-theoretic rough sets. Hu deeply studied the measurement on decision conclusion in three-way decision spaces. In allusion to the risk attitudes of the decision maker, Li and Zhou designed three models of three-way decisions. Liang and Liu provided us some different perspectives for the determination of the loss functions of DTRSs. In multiset-valued information tables, Zhao and Hu investigated two generalized decision-theoretic rough set models for three-way decisions. Huang et al. developed multi-granulation DTRS method for acquiring knowledge from multi-scale intuitionistic fuzzy information tables. In the framework of Bayesian decision procedure, DTRSs can successfully explain three corresponding regions by considering the decision risk factors. Nowadays, it has been applied in many fields, such as the risk decision-making, the government decision-making, etc. From the existing research works of three-way decisions with DTRSs, the determination of its two basic ingredients is the essential question, i.e., the loss function and the conditional probability. In particular, the loss function is evaluated by the decision maker, which is related to the risk factors. Fuzzy multisets, as an extension of the multisets, were initially studied by Yager and Miyamoto. Then, Paul and John investigated type-2 fuzzy multisets. Sharma et al. discussed fuzzy multiset regular languages. Riesgo et al. proposed some basic operations of fuzzy multisets. In recent years, as an extension of fuzzy multisets, dual hesitant fuzzy sets (DHFSs) proposed by Zhu et al. provide us a new evaluation format. Compared with hesitant fuzzy sets (HFSs), DHFSs have generalized the concepts of HFSs and fuzzy sets (FSs), which mainly assess the value of the membership degree. As with intuitionistic fuzzy sets (IFSs), DHFSs are made up of the membership degree and the non-membership degree, but its two parts may possess several possible values, i.e., HFSs. It has been applied in many fields, e.g., the blood transshipment judgement, the medical diagnosis problem, the teacher evaluation, the optimal assignment of projects to teams. In the aspect of decision-making procedure, Farhadinia proposed a method for computing the correlation coefficient of DHFSs. Ren and Wei developed a prioritized multi-attribute decision-making method to solve dual hesitant fuzzy decision-making problems. Based on a new similarity measure, Singh designed a method for solving dual hesitant fuzzy assignment problems with restrictions. In a word, DHFSs can be suitable to a more complicated and uncertain environment. Fortunately, Liang et al. introduced the new hesitant format of DHFSs into DTRSs and deeply studied the corresponding three-way decision model. For that work, it mainly discussed the loss functions of DTRSs with dual hesitant fuzzy elements (DHFEs). Most of the existing research works of three-way decisions ignore the risk appetite of the decision maker. However, in our real life, the decision result can be impacted by the risk appetite of the decision maker, e.g., an optimistic decision, a pessimistic decision, and an equable decision. In fact, the decision maker is always of bounded rationality under risk and uncertainty. We introduce two questions to highlight the motivation that leads us to consider the risk appetite, and explain the importance of the risk appetite in different scenarios. Question 1: Imagine that you are richer by 20000 than you are today, and that you face a choice between options: (A) receive 5000; (B) a 50% chance to win 10000 and a 50% chance to win nothing. Which one do you choose? Question 2: Now imagine that you are richer by 30000 than you are today, and that you are compelled to choose one of two options: (C) lose 5000; (D) a 50% chance to lose 10000 and a 50% chance to lose nothing. Which one do you choose? In fact, a fully rational decision-maker would treat the two decision-making problems as identical, because they are identical when formulated in terms of states of wealth. According to the statistical results, you probably choose the gamble in Question 2 and the sure thing in Question 1. When the decision maker faces the certainty and uncertainty, he or she tends to the risk aversion. When the decision maker merely encounters the uncertainty, he or she tends to the risk preference. As a valuable tool to cope with the risk appetite character, TODIM (an acronym in Portuguese for Interactive Multi-Criteria Decision Making) has been widely used in decision making under risk and uncertainty. In accordance with the risk appetite of the decision maker, it can help the decision makers adjust the corresponding parameters and then makes the decision results to conform more to the decision makers’ preferences. For the TODIM method, the reference point is pivotal. Since the reference point is determined by pairwise comparison, we can judge the gain or the loss. Then, the dominance or the perceptive value can be further computed based on the gain and the loss. In this paper, according to the basic model of dual hesitant fuzzy three-way decisions, we introduce the TODIM method into three-way decisions and propose risk appetite dual hesitant fuzzy three-way decisions. Our research work starts from the loss function of DTRSs. With regards to the basic ingredient of three-way decisions, we study dual hesitant fuzzy entropy and cross-entropy measures for determining the conditional probability. In this case, the conditional probability information is objectively deduced from the loss function matrix. Then, considering the different comparison methods of DHFEs, we further design two types of strategies to deduce three-way decisions by utilizing the pairwise comparison of loss functions. One is the score function of DHFEs. The other is the likelihood of DHFEs. Finally, these results are used to support the project investment evaluation of online peer-to-peer (P2P). This study involves the risk appetite of decision maker and designs a series of decision analysis methodologies for three-way decisions. Hence, it can vastly enrich the range of applications. The remainder of this paper is organized as follows: In Section 2, we list some symbols used in this paper in advance. Section 3 provides basic concepts of DHFSs and TODIM. The determination of the basic ingredients of dual hesitant fuzzy three-way decisions model is deeply analyzed in Section 4. By introducing TODIM, the approaches for deriving three-way decisions are further developed in Section 5. Section 6 uses the example of the project investment evaluation of P2P in Internet finance to elaborate and compare the two approaches of Section 5. Section 7 concludes our research work and indicates future studies.", "section": "Related Work", "doi": "10.1016/j.ins.2018.12.017", "references": [95749022, 1606022329, 1747894202, 1883715000, 1979092190, 1979454583, 1990116832, 2008010437, 2014417436, 2037003318, 2041404167, 2068843436, 2070737895, 2070813883, 2071162171, 2080404663, 2083199031, 2084718108, 2086869454, 2092666229, 2114832876, 2116365436, 2157687268, 2297889545, 2340020088, 2504734778, 2588487610, 2591943698, 2594843209, 2594848528, 2605263372, 2831624444, 2887732625, 2889210423, 2898660825]}
{"paragraph": "Fuzzy sets were proposed to describe an uncertain and vague concept by allowing graded membership values. In many practical applications, although the membership grade can discriminate elements on tiny details, a fuzzy membership function is difficult to interpret and understand based on its infinite number of values. In fact, there is no need to discriminate elements that are extremely similar to each other. The important reason is that an extremely accurate membership grade is unlikely to provide much valuable information. In addition, the research indicates that humans cannot deal with much information. Recent research indicates that the actual number of values that we are able to process is about four. Hence, the approximations of fuzzy sets by a few membership grades are considered to simplify complex problems. Because the approximations of fuzzy sets were viewed as a new research field, many scholars have devoted themselves to researching this issue. Initially, Zadeh briefly introduced a pair of thresholds to transform fuzzy membership grades in the unit interval into three regions, namely, complete belongingness, complete exclusion, and uncertainty, which are similar to the three disjoint regions of a rough set, meaning the positive, negative, and boundary regions, respectively. Similarly, the approximations of fuzzy sets have been extensively investigated. For example, a pair of thresholds was introduced to approximate a fuzzy set in formulating interval sets. Based on the conception of rough fuzzy sets, a pair of a lower approximation and an upper approximation of the rough fuzzy set was calculated to form a three-way approximation. However, there is no sufficient theory to interpret the two required thresholds. Then, the theory of shadowed sets, which provides an objective optimization function to calculate the two thresholds by means of searching for a balance point of uncertainty in given three disjoint parts, was proposed. Moreover, the composition of shadowed sets that can characterize the essence of fuzzy sets has two characteristics, which are as follows: effectively reducing the complexity of the calculation, and having a more concise explanation. Intuitively, the composition of shadowed sets obeys the following rules: when the membership grade of an element is near 1, it is elevated to 1 and placed into the positive region; when the membership grade of an element is near 0, it is reduced to 0 and placed into the negative region; when the membership grade of an element is neither near 1 nor near 0, it is placed into a shadowed region. In other words, it is placed into the boundary region. From these three rules, there are elevation and reduction operations that utilize the thresholds that can offer acceptable levels of the membership grade of an element that is near 1 and 0, respectively. Recently, because the idea of shadowed sets is novel and can simplify complex problems, it has attracted many scholars to research from different theoretical and applied fields. However, most applications of shadowed sets are limited by the condition that the sum of the thresholds equals one. To avoid the condition that the sum equals one, a decision-theoretic model that can calculate a pair of thresholds with different significance was proposed, which is mainly composed by the shadowed sets and three-way decisions. Recently, a general framework for studying three-way approximations of fuzzy sets was proposed. In this general framework, the existing methods for calculating optimal thresholds can be concluded according to three aspects: uncertainty invariance, minimum distance, and minimum cost. What is more significant is that the theory promotes the development of shadowed sets and three-way decisions. We shall see that the idea of three-way decisions plays a key role in the process of constructing the models. Because of the idea of three-way decisions, the thresholds can be calculated to divide a domain into three disjoint regions by a loss objective function. From this viewpoint, the following three issues must be considered in the three-way decision model: interpretation of a set of values for acceptance or rejection, construction of an evaluation function, and determination of acceptance and rejection. In the three-way decision model, a pair of thresholds is used to make decisions as follows: accepting the object if an object satisfies the criteria or is above a certain level; rejecting the object if an object does not satisfy the criteria or is below another level; deferring the object if an object cannot be determined to be accepted or rejected. Then, combining the shadowed sets with the three-way decisions, the thresholds can be obtained. Compared with the shadowed sets, this model offers several advantages as follows: it provides a meaningful interpretation of the objective function, which is the overall cost of three-way approximations; based on the value 0.5, a pair of thresholds can be immediately obtained in terms of various decision costs by minimizing the total cost; in addition, the acquired pair of thresholds need not satisfy the condition that their sum is one; the decision-theoretic three-way interpretation provides a new viewpoint to research on the shadowed sets. Although the condition that their sum equals one is not necessary in this model, it still requires that one threshold is greater than 0.5. This is because the shadowed region with the uncertain interval value is transformed into a single value 0.5. As a result, from the principle of the minimum decision cost, there is a difference between the obtained overall cost and the least cost in some special cases. In order to provide a more reasonable explanation, we suppose that both the unit cost of reducing the membership grade of an element to 0 and the unit cost of elevating the membership grade of an element to 1 are infinite. Furthermore, both the unit cost of reducing the membership grade of an element to 0.5 and the unit cost of elevating the membership grade of an element to 0.5 are constant and greater than 0. According to the formula for calculating the pair of thresholds in this model, the pair of thresholds are acquired approaching 1 and 0. Based on the above conditions, the problem is simplified to the nearest neighbor principle. That is, if one considers many points that are adjacent to a given point, the principle suggests that one should choose a point with the minimum distance to the given point. Based on the principle of the nearest neighbor, the optimal intermediate value varies with the data distributions. Therefore, the value 0.5 is not appropriately considered as the optimal point. In a sense, the meaningful value 0.5 lacks a certain adaptation for different data distributions. In order to extend the model, another variable value between 0 and 1, which can not only avoid the condition that one threshold is greater than 0.5 but also minimize the total cost, is proposed to replace the single value 0.5. Therefore, in this paper, a general three-way approximation of a fuzzy set with a variable value is defined. In some special cases, the variable will evolve into the value 0.5. That is, compared with the rough Bayesian model and parameterized rough set model, the proposed model can evolve into the original model and is applicable to different data distributions. Then, according to this new definition, a novel loss objective function is constructed. In addition, the relationship between the required thresholds and the variable is discussed in detail. Finally, the obtained thresholds are taken into the new loss objective function, and the question of searching the optimal membership grade is converted into an optimization problem. In order to address the optimization problem, particle swarm optimization is introduced to establish an effective heuristic algorithm to obtain an optimal value. Compared with other optimization algorithms, the basic idea of the PSO algorithm is inspired by social behavior of bird flocks to initialize a set of random solutions, and an iterative method is used to search for the optimal solution. Each possible solution is expressed as a particle in a group, and the position vector, velocity vector, and adaptation are three essential characteristics of each particle. All particles are flying at a certain speed in the search space, and there will be a global optimal solution by tracking the current optimal solution. Because of the simple concept, easy implementation, and quick convergence, the PSO algorithm has attracted much attention and been widely applied to fuzzy sets. In summary, our contributions in this paper are as follows. Based on the theory of shadowed sets and the decision-theoretic model, the concept of a general three-way approximation of a fuzzy set is proposed to replace the single value 0.5 with a variable value between 0 and 1. Then, by introducing parameters that can characterize the cost of the elevation and reduction operations in shadowed sets, a new loss objective function is established. From the perspective of the decision-theoretic rough set, the relationship between the required thresholds with different significance and the variable is obtained by introducing the idea of three-way decisions. The required pair of thresholds that is calculated by the proposed model in this paper need not satisfy the condition that their sum equals one and also need not satisfy the condition that one threshold is greater than 0.5. The main difference is that the proposed model is just a special model of the general framework. More importantly, the relative properties of the membership grade are discussed more deeply. The PSO as a heuristic algorithm is first introduced to search for the optimal value by minimizing the total cost. In addition, a series of simulation experiments validate that the proposed model can not only evolve into the earlier model but also provides richer insight into data analysis. The remainder of this paper is organized as follows. In Section 2, many preliminary definitions are reviewed briefly. In Section 3, the cost of the elevation and reduction operations in shadowed sets are presented. In Section 4, the relationship between the required thresholds and the variable value is discussed in detail. In Section 5, the related experimental results and analysis are provided. Finally, some conclusions regarding the advantages of the proposed model are drawn in Section 6.", "section": "Methodology", "doi": "10.1016/j.ins.2018.10.051", "references": [101345952, 1577668191, 1964228588, 1969535228, 1977880445, 1988695218, 1997362234, 1998965536, 2006873874, 2007929615, 2012313887, 2037053120, 2039742983, 2050237791, 2050515685, 2052722125, 2067145099, 2070542280, 2070813883, 2078757499, 2080797159, 2084718108, 2089923511, 2114832876, 2136191688, 2142185590, 2154437129, 2294062232, 2340020088, 2613659757, 2620114837, 2752073986, 2770194470, 2912565176, 2990482619]}
{"paragraph": "Ordinal regression (OR) is an especial machine learning paradigm which aims at learning a prediction rule or predictor on the set of ordered categories. OR resembles both regression problems and classification problems while retaining some differences with them. Different from general regression problems, the labels in OR are discrete and finite. Besides, it is also different from the classification problems since there exists a naturally ordinal relationship among the categories of OR. For example, in movie rating, there exists an ordered relationship among the rating grades, such as poor < average < good < very good < excellent. Thus, OR has both the ordinal and discrete properties of the labels. OR can be adopted to address a variety of real world applications such as information retrieval, collaborative filtering and so on. So far, various OR methods have been proposed and they typically can be grouped into three categories: naive approaches, ordinal binary decompositions and threshold models. The naive approaches treat OR problems as standard regression or classification problems. For example, Kramer et al. first transformed the ordinal scales into numerical values and then fit the model using the common regression tree. However, the distance between any two classes may not reflect their natural difference. The second category handles OR problems by first decomposing the original problem into multiple binary classification problems and then combining these binary classifiers to get a final predictor. The third strategy is relatively more popular, the models in this category assume there exists a latent variable that can reflect the ordinal information among different classes, thus, we need to learn a prediction function f (x) that maps the input to a real scalar value, and a set of thresholds that partition the output into several scales. The naive regression approaches also map the input to a real number, however the distances between different classes in the threshold models are not defined and can be learned during training, which may yield more promising results. Some representative threshold methods include SVOREX, SVORIM, KDLOR, and GPOR. Although existing OR methods can yield better results compared with traditional regression or classification approaches, they have not yet considered the latent structures of the given data, particularly the interactions among covariates, thus losing interpretability to some extent. The models with interpretability can identify the important and unimportant features. Linear model is a typical model with interpretability since the scales of the linear term coefficients reflect the importance of the corresponding features. Consider the traditional linear regression model: y = β0 + Σ βi xi where β0 is the bias term, βi are the main effects of the model. Although linear regression model has been widely applied due to its simplicity and good generalization ability, it still might fail to make correct predictions in some scenarios where the inherent structure of input variables are non-linear. Interaction modelling has received lots of attentions in statistics recently, modelling the interactions between variables is a natural extension of the linear approach. The pairwise interaction models typically can be formulated as follows: y = β0 + Σ βi xi + 1/2 ΣΣ ωij xi xj where ωij xi xj is the interaction term with respect to main effects xi and xj, the interaction models can capture the nonlinearity of input data as well as the relation among variables to some extent. Factorization machine (FM) is a recently proposed method that models all interactions among variables using factorized parameters. More specifically, FM with degree of 2 can be formulated as follows: y = β0 + Σ βi xi + 1/2 ΣΣ viT vj xi xj. In FM, ωij = viT vj is the coefficient of the interaction between xi and xj. Instead of using an independent parameter for each interaction, FM models the interaction by factorizing it, so ωij and ωik share the same parameter vi. Rendle showed that this specific structure can learn feature interactions from sparse and high dimensional data more effectively than the ordinary interaction models such as the previous equation. However, the objective function of FM is not convex, thus solving it often gets stuck in poor locally optimal solutions. To overcome the non-convexity of FM, Yamada et al. and Blondel et al. proposed the convex factorization machine (CFM) by imposing low rank constraint on the coefficient matrix. By this way, CFM not only inherits the advantages of FM but also enjoys a convex objective function. However, fitting regression models with interactions of variables is challenging due to the Ckn interactions, where n denotes the number of variables, k denotes the interaction order. As a result, the number of parameters usually exceeds the sample size in the resulting small sample size problems, leading to computational burden and an ineffective model. In order to solve the problem, feature selection is usually adopted to screen out main effects and the interactions of interest. One effective selection approach is the all-pairs lasso, which applies a lasso penalty to all main effects and all interactions. However, the all-pairs lasso treats both the main effects and the interactions equally, consequently, ignores the inherent structure behind the input variables. Recently, hierarchical sparsity in the interaction modelling has become increasingly popular in statistics, a hierarchical structure between main effects and interactions has been shown to be very effective when identifying important features and interactions, the specifically sparse structure can be concluded as the following two patterns: Strong heredity: if an interaction term xi xj is included in the model, then the corresponding main effects xi and xj must also be included in the model. Weak heredity: if an interaction term xi xj is included in the model, then at least one of the corresponding main effects xi and xj is included in the model. There have been a lot of researches about hierarchical interactions, and most of the existing works about hierarchical structure are to impose convex regularizers or constraints on their loss functions. For examples, Bien et al. proposed a lasso-based method (hierNet) that achieves the hierarchical structure by adding several constraints. Zhao et al. introduced the composite absolute penalties (CAP) family by combining different norms including L1 to induce the hierarchical relationships between predictors. Radchenko et al. proposed a new approach named VANISH which is based on the penalized least square formulations. Furthermore, Haris et al. provided a unified framework named FAMILY, it has been shown that several existing approaches can be seen as its special cases, such as the hierNet and the VANISH. In this paper, we consider pairwise interaction for which we first adopt the factorization machine to model the interactions between variables. To investigate the latent structure of given data, a hierarchical structure with strong heredity is introduced by imposing the convex penalties, while weak heredity can also be achieved. Secondly, we add some ordinal constraints to the objective function to respect the ordinal conditions. After that, we present our new OR method which can select variables of great significance and the interactions that are more important to the prediction. Our main contributions are three-fold as follows: 1) We propose a new OR method: ordinal factorization machine with hierarchical sparsity (OFMHS), which combines FM and hierarchical sparsity together to explore the hierarchical structure behind the input variables in OR tasks. In essence, the FM and the hierarchical interaction modelling are special cases of OFMHS. In addition, we also develop the ordinal version of the FM and the ordinal version of hierarchical interaction models as simplified counterparts of OFMHS. 2) The objective function of OFMHS is convex but not smooth, thus relatively hard to be optimized, for which we derive an ADMM based algorithm to solve the problem. Our algorithm can be guaranteed to converge to the global optimum due to its convexity. 3) Experiments on synthetic and several other real world datasets are conducted to show that OFMHS can reveal the latent structure behind the input variables effectively while getting comparable performance with other state-of-the-art OR methods. The rest of this paper is organized as follows. In Section 2, we give the notation and preliminaries. In Section 3, we give an overview of FM and hierarchical interaction modelling. Then in Section 4, we present our new OR method together with two simplified versions, and then optimize it effectively followed by extensive experiments in Section 5 to evaluate our proposed method. Finally, we give our conclusion in Section 6.", "section": "Related Work", "doi": "10.1007/s11704-019-7290-6", "references": [151545113, 1486694121, 1489442429, 1491036874, 1585159768, 1951829412, 1970554427, 1980896222, 2009088607, 2050346719, 2057932305, 2083261714, 2094286023, 2100556411, 2103972604, 2124648328, 2138265962, 2142575165, 2147232804, 2149427297, 2164278908, 2175634846, 2190044943, 2295739661, 2470457291, 2505972586]}
{"paragraph": "Pattern discovery has been widely studied in machine learning tasks such as image recognition and text analysis. A sequential pattern is a totally or partially ordered subsequence of a transactional dataset, symbolic sequence, numeric time series, or other data sequence. Sequential pattern discovery, also called pattern mining, refers to the discovery of all frequent sequential patterns from these data. For transactional datasets such as retailing, market-baskets, and planning, a pattern is a sequence of events, where each event is represented by an itemset. This type of representation has horizontal and vertical scalability, leading to general association rules. For symbolic sequences such as protein and text, a pattern is a subsequence of characters, also called a string. It can easily represent codons or keywords. For numeric time series such as stock prices and petroleum production, where the trend of fluctuation is essential to the stock holders and petroleum cooperation, a pattern is a subsequence of real values. One interesting research direction of symbolic sequential pattern discovery involves the generalization of patterns. A plain pattern is a subsequence that must be exactly matched. A wildcard, which is also called a motif, matches any character in the alphabet. A wildcard gap matches any subsequence within the length constraint. In this way, a pattern with wildcard gaps is able to handle noise or shifts. To enrich the semantics of the pattern, the alphabet is divided into weak and strong parts. A weak-wildcard gap matches a subsequence of weak characters. Consequently, a pattern with weak-wildcard gaps not only ignores weak characters, but also maintains strong ones. In this paper, we introduce a more general and flexible type of pattern called a tri-pattern that is inspired by three-way decisions. The alphabet is partitioned into three parts corresponding to strong, medium, and weak characters, respectively. In this situation, a tri-wildcard gap matches any sequence of medium or weak characters with a length between N and M. A tri-pattern is a sequence of strong or medium characters containing periodic tri-wildcard gaps, where the term periodic indicates that the gaps between any two adjacent characters are identical. The idea of a tri-partition has been widely adopted in many applications. Biologists partition the human amino acid alphabet into essential, conditional, and nonessential amino acids. Petroleum experts partition oil production fluctuation into significant, insignificant, and minor changes. The tri-partition of situations and actions also has attracted a large amount of research interest in data mining, especially three-way decisions. To the best of our knowledge, however, a tri-partition of an alphabet has not been considered in the three-way decisions research community. Tri-patterns are more general than the four existing types of patterns. Let tri-patterns be Type I patterns, plain patterns be Type II patterns, patterns with periodic wildcard gaps be Type III patterns, patterns with periodic weak-wildcard gaps be Type IV patterns, and strong patterns with periodic weak-wildcard gaps be Type V patterns. Types II through V are the special cases of Type I. Similar to existing frequent pattern discovery problems, a new problem for Type I patterns is defined. Using the Apriori property of the new problem, an Apriori algorithm is designed for efficient pattern discovery and tree pruning. We compare these five types of patterns in three application areas to reveal the universality of the new pattern. The first application is human protein sequence mining. Essential, conditional, and nonessential amino acids correspond to strong, medium, and weak characters, respectively. Twenty sequences were concatenated to discover frequent patterns. These patterns and their popularity in the original sequences were analyzed. Tri-patterns are the most meaningful type, while the other types, especially Type II, suffer from too many weak characters. For the pattern popularity, tri-patterns have the best minimal and second-best average performance. The second application is oil well daily production time series analysis. A coding table was designed to convert the numeric time series into a nominal sequence. Significant, insignificant, and minor changes correspond to strong, medium, and weak characters, respectively. We used two years of well data to discover frequent patterns, which are finally matched in the original time series. Compared with distance-based approaches that analyze numeric time series directly, the coding and mining approach handles different levels of fluctuations more easily. Observation on the original time series shows that tri-patterns match some similar subsequences with minor differences. In contrast, the four existing types of patterns either exclude some similar subsequences or include dissimilar ones. The third application is forged Chinese text mining. Notional words, function words, and special characters correspond to strong, medium, and weak characters, respectively. Four sets of text for news, novels, history, and law were collected. Forged Chinese text occurs when some text creators distribute advertisements for illegal items such as forged money or invoices. They may insert some characters into the text that makes the text still readable, but difficult for automated analysis. In this way, the text can avoid spam filters and similar blockers. Our purpose is to extract keywords from the forged text that are also keywords in the original text. This problem is closely related to fuzzy text matching and keyword extraction. With tri-patterns, the algorithm achieves the highest accuracy in mining keywords from forged text. The remainder of this paper is organized as follows. Section 2 reviews some related work. Section 3 defines the new types of pattern, analyzes the relationships among the five types, and presents the new pattern discovery problem. Section 4 demonstrates the Apriori property and proposes the new algorithm. Section 5 explains the experimental settings and results on three kinds of real-world data. Finally, the concluding remarks are discussed in Section 6.", "section": "Introduction", "doi": "10.1016/j.ins.2018.04.013", "references": [1877823, 964455177, 1208197292, 1511669739, 1608194207, 1632950138, 1969463949, 1974461203, 1980120082, 1985716338, 1988757176, 1997362234, 2006873874, 2009418433, 2029677459, 2048472139, 2050385127, 2059429344, 2090509093, 2118376687, 2126452743, 2136634369, 2143507938, 2151077154, 2158454296, 2217596628, 2310435784, 2340020088, 2344564588, 2345465422, 2506225748, 2515543847, 2551757246, 2566882407, 2580172056, 2616721392, 2727405836, 2762194741]}
{"paragraph": "In the area of geometry there are now a large number of computational tools that can be used to perform many different tasks, dynamic geometry systems, computer algebra systems, geometry automatic theorem provers, among others. All these tools are clients of geometric information—information that can be found on repositories of geometric knowledge such as Intergeo, TGTP, GeoGebra Materials, among others. It can be claimed that the usefulness of such servers of geometric information is directly related with the possibility of an easy retrieval of the information a given user is looking for. Therefore the information should be organised in such a way that it will be possible the design of filters adjusted to the user's preferences. The organisation of information through the taxonomy concept allows to allocate, retrieve and communicate information within a system in a logical way, that is, in classes, subclasses, sub-subclasses, and so on. Each of these levels aggregate information about the existing documents in the repository. An advantage of this form of access is the user's guarantee of best selection of searched term, since the classes contain mutually exclusive topics. Different taxonomies would answer to different users' needs. The problems in the servers must be classified in such a way that, in response to a client query, only the problems in the user's level and/or interest and/or language are returned. If the organisation of a large field of knowledge like mathematics are to be considered, taxonomies like Mathematics Subject Classification can be found. In this paper the much narrower scope of constructive geometry is addressed, i.e. geometric constructions made by dynamic geometry systems and geometric problems, eventually with an associated construction, manipulated by geometric automated theorem provers. The term “geometric problem” is here used in a general way, it is used in relation to the case study, i.e. taxonomies for TGTP. There was and there is an important debate on the distinction between problems and theorems, some scholars consider that they should be distinguished, others consider that problems can be reduced to theorems, others consider that theorems can be reduced to problems. This debate is very old and continues today. However, this paper is a case study about taxonomies for TGTP, therefore theorems, their proofs, construction problems and their solutions are considered from a single and uniform taxonomic point of view. For researchers in geometric automated theorem proving it will be interesting to look for: conjectures not yet proved by GATPs; theorems with readable proofs; theorems proved efficiently; etc. If a taxonomy based on those criteria would please the researchers in automatic geometric reasoning, it might not be completely suitable for the educational community. When designing filters for educational purposes, education levels, levels of geometry reasoning and also personal preferences must be considered. Adding to other approaches, a new approach to geometrography is considered, taking into account a very interesting point of view on geometrical constructions classification. Applying geometrography's principles to the dynamic geometry systems, it is possible to redefine the concepts of coefficient of simplicity and a new coefficient of freedom to measure the complexity and dynamics of a DGS construction. With reference to the paper's case study, Thousands of Geometric problems for geometric Theorem Provers (TGTP) is a web-based repository of geometric problems with integrated GATPs. It is being developed to support the testing and evaluation of geometric automated theorem proving systems. The list of problems in TGTP can be explored with some powerful textual and geometric search mechanisms, but, if adapting the search to each user's needs is pursued, it is necessary to introduce a classification for each TGTP problem, stating their characteristics, in face of one or more intended users' expectations. Originally TGTP was aimed to the geometric automatic theorem provers community, as said above to support the testing and evaluation of geometric automated theorem proving systems, so its expected audience is mostly researchers whose background is mathematics and/or computer science and whose research focus is automatic reasoning, formalisation of mathematics, artificial intelligence, among others. The interest in proofs and proving in mathematics education and the application of GATPs in mathematics education opens a new community of potential users of TGTP. The classification of each TGTP problem at a given educational level should be possible and not very difficult. The classification accordingly to a level of geometry reasoning would be more difficult. Nevertheless such classifications are useful in any educational environment, e.g. when linking with educational platforms like the Web Geometry Laboratory. The current search mechanisms in TGTP allow its users to search for a given specific problem, or set of problems, e.g. look for Ceva's theorem, look for all problems with the word “circumcircle” in its description, look for problems containing some given geometric configuration. The introduction of taxonomies in TGTP can add a filtering step that, together with the text and geometric search mechanisms, will allow to tailor TGTP's usefulness to each user's needs. For example, a secondary school teacher preparing a class about circumcenter centre would choose filters: CCS classes, CO.A.1 and C.A.3; construction complexity, simple; proofs in education, verification: good, filtering the TGTP database, or any other geometry knowledge repository, in such a way that a good set of examples could be browsed and choose as teaching materials. An interesting development of this research is the application of the taxonomies presented in this paper to proof assistant area. We do not consider this application here, however we give some suggestions about it. Overview of the paper: The paper is organised as follows: first, taxonomies for GATP research and taxonomies for education will be discussed. Then, the application of taxonomies to TGTP will be analysed providing some examples, also a new approach to geometrography will be presented. Finally, conclusions are drawn and future work will be discussed.", "section": "Methodology", "doi": "10.1016/j.jsc.2018.12.004", "references": [144472580, 158485348, 161392887, 1268136390, 1409110429, 1515021858, 1559406907, 1839505014, 2031433674, 2032405865, 2064030682, 2088823840, 2120243289, 2125565018, 2131408699, 2136058053, 2282070325, 2567325662, 2609408480, 2613478558, 2793599632, 2803506036, 2887162633]}
{"paragraph": "A shadowed set, proposed by Pedrycz, maps the membership grades of objects in the universe to a set {0, [0, 1], 1} based on a pair of thresholds. The membership grade of an object indicates the degree of the concept is applicable to that object. The objects whose membership grades are between the thresholds constitute shadowed areas. That is, a shadowed set uses a set of three values as membership grades. Yao proposed a theory of three-way decisions based on the philosophy of thinking in threes. Three-way decisions have been widely applied in frequent pattern mining, clustering, classifications, social network analysis, and the collective knowledge exploitation, just to name a few. The trisecting-and-acting framework consists of two parts, trisecting and acting. Trisecting or trisection or a tripartition works on dividing a whole into three parts. Acting designs the most effective strategies for processing the three parts. Various theories, such as rough sets, fuzzy sets, shadowed sets, interval sets, have been used to construct three-way decisions. A shadowed set is a three-way approximation of a fuzzy set. A shadowed set based three-way approximation is constructed by choosing a single number in the unit interval as the membership grades of elements in the shadowed areas. The selected single number replaces the unit interval to represent a membership grade of the highest uncertainty. Thus a shadowed set based three-way approximation is defined as a mapping from the universe to the three-value set {0, σ, 1}. The membership grades are elevated or reduced to 1, σ, and 0 by the elevation and reduction operations. These two operations produce the elevation and reduction errors which show the difference between the original membership grades and the corresponding elevated or reduced values. Recently a generalized framework of three-way approximations using a set of three values was proposed. Shadowed set based three-way approximations are viewed as a case of the generalized framework. Three-way approximations can be converted to three-way decisions, and vice versa. One of the fundamental issues of applying three-way approximations is the determination and interpretation of the pair of thresholds. A general optimization-based framework for interpreting and determining the thresholds was introduced. Three principles were summarized: a principle of uncertainty invariance, a principle of minimum distance, and a principle of least cost, used to determine the thresholds. A framework for constructing a shadowed set according to a principle of uncertainty invariance was also proposed. A symmetric model was used to compute thresholds by minimizing the difference between the shadowed area and the sum of the elevated and reduced areas. Analytical formulas to calculate the thresholds based on a measure of fuzziness of a fuzzy set were proposed. Based on the principle of minimum distance, the nearest interval approximation of a fuzzy number using a distance measure was explored. Various distance-based three-way approximations of fuzzy sets have also been examined. A decision-theoretic approach to calculate the thresholds by minimizing decision costs was proposed, which obtains the thresholds according to the principle of least cost. The error-based model was derived by considering a loss function satisfying additional properties. Another shadowed set approximation works in the opposite way, that is, constructing shadowed sets using the α-cuts of a given fuzzy number. We apply a principle of tradeoff with games in this research to determine the thresholds of three-way approximations in the shadowed set context. Let’s examine the elevation and reduction errors produced in shadowed set based three-way approximations. When the thresholds change, the elevation and reduction errors change correspondingly. The increases of thresholds cause the decrease of the elevation errors and the increase of the reduction errors. The decreases of thresholds cause the increase of the elevation errors and the decrease of the reduction errors. We desire a shadowed set model with the minimal elevation errors and the minimal reduction errors. However, the elevation and reduction errors are not able to decrease simultaneously. The decrease of elevation errors results in the increase of reduction errors, and vice versa. The aim of this research is to find a pair of thresholds which define a shadowed set based three-way approximation based on a tradeoff between the elevation and reduction errors. The game-theoretic shadowed sets are proposed. They apply a game mechanism to determine and interpret the thresholds of shadowed sets according to a principle of tradeoff with games. The contradiction between the elevation and reduction errors are formulated as the competitive games in which the increase of one player’s payoff may cause the decrease of the other player’s payoff. The players gradually approach a compromise by repeatedly modifying the thresholds in games and then finding equilibria of games. The resulting thresholds can be induced from the game result. A shadowed set based three-way approximation defined by the resulting thresholds is able to represent a tradeoff between the elevation and reduction errors. The paper is organized as follows. We first analyze the errors produced by the shadowed set based three-way approximations. When the thresholds change, the changes of elevation and reduction errors are investigated. We then discuss game-theoretic shadowed sets, including the game formulation and repetition learning mechanism. The settings of game players, the strategies performed by each player, payoff functions, game equilibrium, repetition of games, and stopping criteria are explained in detail. Finally, we apply game-theoretic shadowed sets in an example to show how they work.", "section": "Methodology", "doi": "10.1016/j.ins.2018.07.058", "references": [101345952, 1021182246, 1549841259, 1963024134, 1988695218, 1998965536, 2001258251, 2006136802, 2006873874, 2010360766, 2030827420, 2052715156, 2052722125, 2059429344, 2066111511, 2078757499, 2080404663, 2091336043, 2111589951, 2114832876, 2120260965, 2154437129, 2284529841, 2297889545, 2407310659, 2551396410, 2560804083, 2568228263, 2588487610, 2620114837, 2752073986, 2792234535, 2796438895, 2799813871, 2804489692, 2806445482]}
