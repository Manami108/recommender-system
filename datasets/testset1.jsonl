{"paragraph": "Object tracking is a hot topic in computer vision. Due to its wide applications in motion analysis, video surveillance, human–computer interface, etc., it has attracted much attention recently. However, because of various complex factors, for example, occlusion, deformation, illumination variation, scale variation, fast motion, and background clutter, tracking is still a challenging task. A typical tracking system often has three modules, that is, the appearance model, the motion model, and the update model. The appearance model, which plays a crucial role, can be divided into two types. The first is the generative model, which only uses the information of the target. The second is the discriminative model, which often formulates tracking as a binary classification problem and is known as the tracking-by-detection method. Since the latter model adopts the information of both the background and the target, it seems to offer more accurate and robust representation. However, there are still some potential issues in existing tracking-by-detection methods. First, it is pointed out that the objectives of the tracker and the classifier are not consistent in many methods, where the objective of the tracker is to find the optimal location of the target precisely while the objective of the classifier is to predict the labels of the samples accurately. Second, the training samples are often undersampled randomly and assigned to equal weights. This sampling strategy does not make full use of the spatial constraints around the target and ignores the different importance of the samples. Besides the above issues, we also find that the high spatial correlations of adjacent image patches do not get enough attention, which increases the complexity of sampling, training, and detection. Furthermore, the size of the tracker is often fixed and the inner structure of the tracker is often neglected, so that the scale variation is not well addressed. These issues may be important factors affecting the tracking performance. Although some of these issues have been pointed out by some researchers, they have not been taken into account in a unified framework. Recently, many correlation filter-based tracking methods have been proposed and achieved great success. These methods formulate tracking as a ridge regression problem, which can make full use of the spatial information and solve the downsampling issue in the binary classification model. Besides, the correlation filter-based methods transfer the ridge regression to a correlation filter problem by the circulant structure assumption, which can be realized by fast Fourier transformation with high efficiency. Although these methods obtain both high tracking accuracy and fast tracking speed, there still exist some issues which degrade the performance of the trackers. For example, the correlation filter is obtained based on the circulant structure assumption, where the samples are represented by rectangle boxes larger than the true target and are implicitly generated by cyclic shift of the original target. However, it should be noted that these samples do not fit the real scene, as the sample with large horizontal translation displays. In addition, in order to make the samples satisfy the circulant structure assumption, a bounding box used to represent the sample should be much larger than the target and a Hanning window must be utilized to deal with the discontinuity of the image margin, which leads to two additional problems. The large bounding box contains much more background information which may not represent the target appearance accurately and brings more challenges to scale adaption and robustness to background changes. Besides, the Hanning window also changes the appearance of the training patch, adding the representation errors. In this paper, we propose a novel tracking method by learning a scale-adaptive tight correlation filter, attempting to address the above issues in tradition tracking-by-detection methods and in conventional correlation filter-based methods. First, we formulate tracking as a correlation filtering problem from the signal detection perspective and learn a tight correlation filter as the tracker, which does not need the prior assumption existing in other correlation filter-based methods. By taking the target as the signal of interest to detect, the objective of filtering is to find the optimal response corresponding to the target, which is consistent with the objective of tracking. Since the filter is set as the same size as the target, it is tight and can effectively reduce the effect of the background. The samples corresponding to the correlation filter are real rather than virtual, which improves the representation accuracy. In addition, we establish the relationship between the proposed correlation filtering formulation and the ridge regression model from the practical physics background, which enriches the tracking-by-detection framework. Different from the binary classification formulation which assigns discrete labels to samples, in our method, we would like to build a ridge regression model to assign continuous values to samples. Specifically, we point out that the correlation filter is a specific realization of the ridge regression, and the correlation filter corresponds to the appearance model of the tracking-by-detection framework. Second, we present a multiscale strategy to handle the scale estimation problem adaptively by using the spatial structure of the correlation filter. In our method, the learned correlation filter has the same size with the target. Since the filter retains the spatial structure in the 2-D plane, the multiscale filter banks can be approximately implemented based on the interpolation technique, which can be used for multiscale filtering. Then, the target scale can be estimated adaptively by exploring the optimal response on multiple scales heuristically. The experimental results in the benchmark datasets demonstrate that our method can achieve desirable tracking performance and outperform many state-of-the-art trackers, especially in the condition of scale variation. Third, we develop a novel location method in the motion model by making use of both the correlation filter and the distance importance weight of the candidate samples. In most traditional tracking-by-detection and correlation filter-based methods, the confidence score is obtained by only the appearance model, for example, the classifier or the correlation filter, and the candidate samples are assigned to the same importance. However, in this condition, the similar distractor, which is far away from the target, may cause the tracking drift. In this paper, we consider the impact of the distance corresponding to each candidate sample as well as the confidence obtained by the correlation filter. In other words, the final confidence map is the combination of the distance importance score and the filtering result, which can effectively reduce the impact of the similar distractors. The main contributions of this paper are three-fold. We formulate tracking as a tight correlation filtering problem instead of the circulant structural assumption-based correlation filtering. In this formulation, the size of the correlation filter is set the same as the target, which can effectively avoid the effect of the virtual samples and reduce the influence of the background information. Specifically, the zero padding and cropping strategy are introduced to efficiently realize the learning of the filter and tracking in the Fourier domain, which does not need the circulant assumption and can effectively address the boundary effect issue. We approximately construct the multiscale filter banks for scale adaption based on the interpolation strategy and dense feature extraction. Since the multiscale filters are directly built in the filter domain, our method may locate the target more precisely than many other scale adaptive methods which realize scale adaption by scaling the images. We present a novel location strategy to determine the tracking result which considers both the filtering result and the distance importance of the candidate samples. On one hand, the filtering results obtained by the multiscale filter banks reflect the basic location information from the appearance model. On the other hand, the distance importance of the candidate samples is taken into account, which can effectively alleviate the impact of the similar distractors. The remainder of this paper is organized as follows. In Section II, we review the tracking-by-detection methods, and talk about the correlation filter-based tracking methods. Section III formulates tracking as a correlation filtering problem and explains it from the perspective of ridge regression. The detailed realization of the proposed method is shown in Section IV and the experimental results are displayed in Section V. Section VI concludes this paper.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868782", "references": [29474918, 134197611, 161114242, 182940129, 791392043, 818325216, 1513768190, 1857884451, 1961006157, 1964846093, 1991156752, 1995903777, 1997121481, 2000326692, 2001785244, 2002406878, 2023640470, 2034938692, 2038943746, 2044986361, 2068094367, 2069351745, 2071551242, 2072528130, 2075742732, 2077931794, 2089961441, 2098854771, 2098941887, 2105403175, 2109579504, 2124211486, 2127575804, 2132103241, 2139047213, 2144992113, 2149829493, 2154889144, 2156137575, 2158592639, 2158827467, 2158917775, 2162383208, 2162919312, 2165037244, 2214352687, 2244252896, 2293873019, 2298605637, 2317058546, 2336929960, 2509359898, 2557641257, 2605145377, 2612225782, 2681067697, 2738318237, 2768634781, 2964334666]}
{"paragraph": "Cyber-physical systems (CPSs) tightly integrating control of physical systems, computation and communication are desired to play a leading role in the development of future civil infrastructures and industrial applications. In CPSs, the open cyber layer interconnected with the physical layer is also vulnerable to the adversaries while providing some advantages in the interaction between these two layers. For example, the Stuxnet malware incident caused substantial damage to Irans nuclear program, and the attack of Maroochy Shire Councils sewage control systems resulted in a flooding with a million liters of sewage. These events have indicated the importance of securing the cyber layer. Compared with the general secure issue focusing on estimation/control for the sensor/actuator fault and control for networked systems under data packet dropouts, the secure control problem for CPSs has some specific characteristics. The attack is implemented with determined objectives to deteriorate the system performance, and it is difficult to use the traditional approaches to describe attacks. Taking the control for networked systems as an example, the general approach to describe the imperfect communication links is to impose some assumptions following the probability distribution on the transmitted data. The authors in have argued that these probability distributions are not easy to justify in the attack scenario. Recently, great efforts have been made to secure the cyber world, which mainly refer to two aspects, that is, the attack detection for remote estimator and secure control under attacks. For the latter, malicious adversaries usually deploy two categories of attack actions to the cyber world, namely, denial-of service (DoS) attacks resulting in the loss of data and deception attacks deteriorating the integrity of data. Some results involving deception attacks can be found in and the references therein. DoS attacks, which can be launched through exploiting some classic approaches, such as flooding-based attacks, teardrop attacks and ping-of-death are more frequently adopted to hijack into the encrypted communication network. Accordingly, in this paper, we will investigate how to establish an effective resilient control scheme for CPSs in the presence of DoS attacks. Many researchers have contributed to this topic. For example, an optimal DoS attack scheme was proposed via maximizing the estimation error in, through which we can acknowledge what methods the adversaries may employ to successfully prevent the communication between the physical process and the cyber realm. As to the secure control problem, one important step is to establish the rule to describe the attacks. Assuming the DoS attacks were deployed periodically, Foroush and Martinez developed an event-trigged resilient controller under the assumption that the attack period could be detected. Befekadu et al. provided an access to find optimal control schemes under DoS attacks, which respectively were described via a Bernoulli stochastic distribution process and a Markov model. As argued previously, the DoS attack model in and, which is similar to the description of data packet dropouts in networked systems cannot exactly characterise the attack nature. Targeting at removing the probability assumption, Peng et al. constrained the attack energy through providing a maximum number of successive DoS attacks and designed a resilient event-triggered controller for multi-area power systems. Note that the attack frequency is not considered in that paper. Persis and Tesi proposed the concepts of attack frequency and attack duration, based on which the resilient control strategy has been derived to guarantee the input-to-state stability property for the physical process in question. Following this paper, some representative results have been reported, see for example and the references therein. For CPSs, the defenders can design some schemes to defend attacks while the malicious adversaries intend to hijack into the cyber realm. Although no assumptions except for the attack frequency and attack duration are imposed on attack actions, how to build up a mechanism to make the attacks satisfy these assumptions has not been investigated yet. In the past literature, an interesting approach to building up the defense mechanism is to introduce the game-theoretical methodology, in which the defender and malicious adversaries are regarded as two players and the mixed defense strategy is derived via computing the saddle-point equilibrium of the game matrix. Ding et al. modeled the sensor and attacker as two players in a stochastic game, presented the optimal defense schemes and developed the remote estimation algorithm under DoS attacks. More applications of game theory to secure cyber world can refer to and the references therein. For the secure control problem, there also exist some results, which are related to the topic of this paper to some extent. Considering the attacks launched by correlated jammers both in the sensor and the actuator (i.e., both the state and the control signals undergo the malicious disturbance) and utilizing the Stackelberg game theory to make the defense decision, a resilient controller has been designed to maintain the system performance in. Yuan et al. exploited the Stackelberg game theory to establish an optimal defense scheme and provided a resilient controller for the delta-domain physical process under DoS attacks. However, an assumption satisfying the Bernoulli distribution has been imposed on the attack. More recently, a zero-sum stochastic game approach has been proposed to design defense strategies in, in which the corresponding secure control policy has been derived under replay attacks and fault data injection attacks. In this paper, the problem of resilient control is investigated for CPSs under DoS attacks. The main contributions of this paper can be summarized as follows: 1) According to the fact that whether the DoS attack is launched successfully or not, the physical process is described as a switched model, which is analyzed based on the concepts of attack frequency and attack duration. 2) A switching resilient sliding mode controller is proposed to maintain the exponential stability of the physical process under DoS attacks and an estimator is integrated into the design process to estimate the lost data resulting from attacks, which can mitigate the detrimental effect on the system performance. 3) Combining the zero-sum game theory with the concepts of attack frequency and attack duration, an active defense policy is established to determine the switching logic for the designed controller. The rest of this paper is organized as follows. Section II describes the system formulation and preliminaries. Section III provides the stability and stabilization results. The active defense-based resilient sliding mode control design process is presented in Section IV, and then we conclude this paper in Section V.", "section": "Related Work", "doi": "10.1109/TIFS.2019.2917373", "references": [1967684385, 1988084557, 2002579908, 2009257627, 2040634421, 2041587044, 2062132646, 2105773843, 2148439597, 2149365358, 2315448583, 2435203810, 2510308676, 2516521254, 2523012541, 2582126286, 2613286333, 2619851883, 2780545917, 2807358671, 2891602998, 2898559182]}
{"paragraph": "Many real-world optimization problems are composed of multiple conflicting objectives which need to be optimized simultaneously. For a multi-objective optimization problem, an improvement of the performance on one objective often leads to the deterioration on other objectives. Therefore, multi-objective optimization algorithms can only search for a set of trade-off solutions to approximate Pareto optimal solutions. In three decades, MOEAs have attracted great attention for being able to solve a class of real-world optimization problems that have multiple criteria or objectives. However, traditional Pareto-based MOEAs can only effectively solve two- or three-dimensional optimization problems. In real world, the number of considered objectives can be larger, and these problems are known as many-objective optimization problems. When facing MaOPs, it’s not easy for traditional Pareto-based MOEAs to converge into the Pareto front. The main reason is that the proportion of non-dominated solution increases rapidly with the number of objectives. Consequently, the density based second selection criterion in Pareto-based algorithms plays a leading role in the selection process of Pareto-based MOEAs. However, studies indicate that a diversity-based selection criterion has a detrimental impact on the population’s convergence. This criterion prefers the dominance resistant solutions which have good diversity over the objective space but are far away from the desired Pareto front. Therefore, balancing the convergence and diversity of the population for MaOPs has become a challenging research topic in the field of many-objective optimization. To solve the problems above, many methods have been proposed, and can be divided into the following categories: Loosening Pareto-dominance approach. With the increase of the number of objectives, the Pareto-dominance relationship is difficult to distinguish between solutions in terms of convergence. By loosening the Pareto dominance relation, the solutions approach the Pareto front more rapidly will be identified. For example, the controlling dominance area approach adjusts the dominance area of solutions by setting an appropriate parameter. The grid dominance adds the selection pressure by adopting an adaptive grid construction. The epsilon-dominance based MOEA can obtain good convergence and uniformity performance by dividing the objective space into hyper-boxes, each of which is assigned at most one solution. Decomposition-based approach. Using a set of uniformly distributed weight vectors, these approaches decompose a MOP into a number of single-objective sub-problems and then uses a search heuristic to optimise these sub-problems simultaneously and cooperatively. In contrast to the Pareto dominance criterion, decomposition-based approaches can rank the entire population and form a total order among the solutions and thus providing sufficient selection pressure in a high-dimensional objective space. Recently, some decomposition-based MOEAs, such as MOEA with decomposition, multiple single-objective Pareto sampling and MOEA/D based on localized weighted sum, have been found to work well on MaOPs. Ranking-based approach. Ranking-based algorithms can distinguish between solutions by defining a new sorting method. Similar to decomposition-based approaches, ranking-based approaches can form a total order among the solutions. The relation favour is to rank the solutions by comparing the numbers of their superior objectives. Therefore, the relation favour will prefer those solutions if most of their objectives are superior to that of others. Additionally, the average ranking is another ranking method. Firstly, on each objective, it ranks all the non-dominated solutions on the basis of their objective values, and the number of ranking values of a solution equals to the number of the objectives of the problem. Then, average ranking sorts the solutions by means of their average ranking values. Density estimation based approach. Recently, some researches have shown that some modification of diversity maintenance in Pareto-based algorithms can also promote the convergence of population, for example, the diversity management operator and shift-based density estimation. DMO uses one indicator to evaluate the population’s convergence. This approach relies on the true Pareto front of the problem. However, in practical applications, the Pareto front of most problems is unknown. SDE, as a diversity estimation, takes two aspects into account: convergence and diversity. In contrast to DMO, SDE has a high usability, and does not need to know the Pareto front of the problem. Experimental results show that it can significantly improve the performance of Pareto-based algorithms. To evaluate the performances of the above MOEAs, the diagnostic assessment framework is another research hotspot in the evolutionary computation community. It contains three important elements. The first one is multiple performance metrics. They mainly evaluate the effectiveness, reliability, efficiency, and controllability of an MOEA. Effectiveness checks whether an MOEA achieves high level of performance. Reliability captures performance changes in the parametric process and random seed testing. Efficiency refers to achieving high levels of performance in the minimum number of function evaluations. Controllability measures the ease-of-use or sensitivity of MOEAs to their parameterizations. Then, an adequate sample of problems is another element. A number of test problem have been developed to benchmark the performance of MOEAs, such as DTLZ, WFG and multiline distance minimization problem. Among them, the most widely-used element are DTLZ and WFG. And the last one is the ability to uncover pertinent parameter controls and dynamic search behavior within the algorithm. Although the above studies clearly enhance the search ability of MOEAs and various methods were proposed to tackle MaOPs, the area of evolution many-objective optimization is far from being mature. Furthermore, the loosening Pareto-dominance approaches inevitably encounter difficulties in determining the degree of slack in the new dominance relation for difference problems, leading to the emergence of dynamic tuning methods. For the decomposition-based approaches, two critical issues need to be considered. One of which is that the specified weights’ distribution needs to be consistent with a given problem’s Pareto front. The other is that the configuration of weight vectors suffers the curse of dimensions in many-objective space. Due to the lack of a diversity maintenance strategy, the ranking-based approaches may lead the evolutionary population to converge into a small part of the Pareto front. As mentioned above, the true Pareto front of the problem affects the performance of the density estimation based approach. Although the SDE is off the hook, parts of the solution near the boundary are easily eliminated by it. In this paper, we focus on the first approach and wish to propose a dominance relationship named as angle dominance that is insensitive to parameters. One interesting property of the angle dominance is its capability of reflecting the convergence and extensity of solutions in the population. This is in contrast to existing dominance criteria, which typically only involve convergence or both convergence and uniformity. The basic idea of the angle dominance is simple. By substituting the objective vector of a solution with an angle vector, the angle dominance enlarges the dominance area of the solution. This not only increases the selection pressure towards the Pareto front but also is able to maintain boundary solutions very well. In addition, the angle dominance can be easily applied to any Pareto dominance based algorithms. The rest of this paper is organized as follows. Section 2 briefly reviews the work related to dominance relationships and angle-based environmental selection. Section 3 is devoted to description of the proposed angle dominance criterion and introduces the framework of the angle dominance based NSGA-II which is denoted as NSAG-II+AD. Section 4 presents the algorithm settings, test functions, and performance metrics used for performance comparison. The experimental results and relevant analyses are presented in Section 5. Finally, Section concludes the paper and gives our study priorities in the future work.", "section": "Related Work", "doi": "10.1016/j.ins.2018.12.078", "references": [42050702, 77146796, 225560312, 1500125712, 1533748389, 1607221389, 1968219458, 1974276217, 2022485595, 2039252507, 2071694551, 2072583763, 2072661909, 2097536098, 2098907614, 2103476212, 2105511939, 2108968575, 2126105956, 2128357515, 2143185749, 2143381319, 2153654820, 2165626989, 2172250608, 2278123559, 2405688792, 2469950730, 2512033555, 2521918431, 2563621803, 2573114156, 2614323825, 2757662287, 2762482744, 2791954669, 2802576758, 2809762112, 2897208679, 2962873766]}
{"paragraph": "Owing to the openness of wireless signals, private information is easy to be wiretapped by illegal users. Thus, the issue of secure communication is very important for wireless networks. Physical layer security communication has been widely investigated for wireless networks since it can achieve perfect security without using an encryption key. The study of physical layer security was Wyner’s seminal work, where the secrecy capacity of a degraded wiretap channel was studied. The degraded wiretap channel model in Wyner’s work was further extended to the nondegraded case by Csiszár and Körner, and to the Gaussian degraded wiretap channel by Leung-Yan-Cheong and Hellman. Recently, the idea of cooperative communications has also been introduced to improve the physical layer security, especially when the relay node has multiple antennas, where beamforming and/or artificial noise (AN)-aided scheme can be employed. As one kind of cooperative communications, cognitive two-way relay (CTWR) networks are recently proposed to improve the spectrum efficiency, where the cognitive transmitter (CT) operates as a relay node and helps to forward the signals of the primary transmitters (PTs) by using two-way relaying scheme. The single-antenna CTWR networks are widely studied. Motivated by the benefits of multiple antennas, the authors consider that the PTs and/or CTs in CTWR networks are equipped with multiple antennas, with the objective of maximizing the information rate at the cognitive receiver (CR) or minimizing the sum mean square error of CRs’ signals or maximizing the minimum signal-to-interference-and-noise ratio (SINR) at two CTs. The above works do not consider the information security problem. For CTWR networks, physical layer security is an important issue and has been investigated. However, all the works consider that the nodes all have single antenna and thus no beamforming scheme is studied. In this paper, we study beamforming design for cooperative secure transmission in CTWR networks, which consist of two PTs each with single antenna, one CT with multiple antennas and one CR with single antenna. An illegal user with single antenna tries to eavesdrop the information of PTs. In order to ensure the information security for PTs, we introduce the CT as the relay node, which helps to forward the signals of two PTs. At the CT, we design the beamforming matrix for the PTs’ signals, the beamforming vector for the CR’s signal, and the AN beamforming matrix, with the objective of maximizing the secrecy sum rate (SSR) for PTs under the quality of service (QoS) constraint at the CR and the transmit power constraint at the CT. To solve the SSR maximization problem, we propose the monotonic optimization (MO) based double-layer iterative algorithm, which can obtain the global optimal solution and has very high complexity. In order to reduce the complexity, we propose a sequential parametric convex approximation (SPCA) based single-layer iterative algorithm, which can converge to a Karush-Kuhn-Tucker (KKT) point. We also propose a zero-forcing (ZF) based closed-form algorithm, which has the lowest complexity. Furthermore, we derive the asymptotic achievable secrecy sum rate when the CT transmit power goes to infinity. There are several related works on beamforming and/or AN-aided design for physical layer security in cooperative communications. In, single-input single-output (SISO) relay networks are investigated, where all the nodes have single antenna and the authors propose distributed beamforming or relay selection for security. In, multi-input single-output (MISO) relay networks are considered, where the relay has multiple antennas and all the other nodes have single antenna, and the authors propose suboptimal secure relay beamforming and AN signal beamforming scheme based on semidefinite relaxation and Gaussian randomization method. In, the authors consider multi-input multi-output (MIMO) relay networks, where all the nodes have multiple antennas. Exploiting singular value decomposition and constrained concave convex procedure, suboptimal beamforming and AN-aided schemes are proposed. We have to emphasize that, as shown, direct optimization of the secrecy rate for the MISO/MIMO relay network is a non-convex problem, which is very difficult and thus only suboptimal algorithms are provided. For CTWR networks, the study of physical layer security is still sparse, where all the nodes are considered to have single antenna and thus beamforming cannot be employed. The authors consider secure communications for CRs while the authors provide information security for primary receivers (PRs). In, a resource allocation algorithm is proposed to maximize the secrecy sum rate for CRs in orthogonal frequency-division multiple access-based relay networks, where the relays employ decode-and-forward (DF) scheme. Reference derives the intercept probability for two CTs over Rayleigh fading relay channels, where the CTs exchange their information via a relay node when the PTs are silent. Reference proposes a sequential second price auction and designs an efficient searching algorithm to maximize the secrecy sum rate for the CRs. Reference aims to maximize the secrecy rate for the PR through applying cooperative relaying and jamming of the CTs, where the DF relaying is used. In, PTs and CTs employ the same relay node to amplify and forward the signals, and the secrecy rate for each PT against the eavesdropper is maximized. Our work is quite different from the previous studies. Firstly, we develop a global optimal algorithm for the SSR maximization problem while only suboptimal algorithms for information security are provided. Secondly, we derive the asymptotic achievable secrecy sum rate which serves as a performance upper bound while others do not provide the asymptotic analysis. Thirdly, we consider a more general system model where the CT has multiple antennas while others assume all the nodes to have single antenna. Lastly, we introduce the CT as a relay and design the signal and AN beamforming matrix/vector at the CT to provide the security for PTs while no AN and no beamforming are designed at the relay in previous works. This paper is organized as follows. In Section II, the model for secure information transmission in a CTWR network is described. In Section III, we develop the MO based global optimal algorithm. In Section IV and Section V, we provide the SPCA based single-layer iterative algorithm and ZF based closed-form algorithm to reduce the complexity, respectively. In Section VI, we derive the asymptotic achievable secrecy sum rate when the CT transmit power goes to infinity. In Section VII, we present the numerical results and in Section VIII we conclude our paper.", "section": "Introduction", "doi": "10.1109/TIFS.2019.2918431", "references": [1985006711, 1994016836, 1997063496, 2043769961, 2047053814, 2048581882, 2051061649, 2065313044, 2066409766, 2069088834, 2100573451, 2103781722, 2112193315, 2122744491, 2144007657, 2268781749, 2293824895, 2329287813, 2339128359, 2343272313, 2416790798, 2418032643, 2464448394, 2516648133, 2518778058, 2519839157, 2530027684, 2547657982, 2549952757, 2617309511, 2793673064, 2794313215, 2806393332, 2806649719]}
{"paragraph": "Hydraulic systems have large force/torque output and small size to power ratio in comparison to servo systems driven by electrical or pneumatic motors. Hence, electro-hydraulic systems have been widely used in various engineering products including vehicle active suspensions, robotic manipulators, and other simulators. However, servo systems driven by hydraulic actuators usually have unavoidable nonlinearities, load variations, and parametric uncertainties (e.g., friction and oil leakage). To retain satisfactory operation performance, numerous control techniques have been developed, such as H∞ control, adaptive control, robust control, and backstepping control. Among these methods, adaptive control can cope with parametric variations due to its online learning ability. To further address unknown nonlinearities that cannot be presented in a linearly parameterized form, function approximators, e.g., neural networks (NNs) and fuzzy systems (FSs) were further adopted. However, these function approximators impose heavy computational costs and require a fairly long time to achieve convergence, which is also influenced by the adopted adaptive laws and learning gains. Hence, the practical application of adaptive control approaches for hydraulic systems is still not a trivial task. Apart from the aforementioned approaches, disturbance observer based methods have also been proposed and used, where various observer structures were developed based on different methodologies, and devoted to estimate and then compensate for the uncertainties. The original disturbance observer (DOB) was proposed by Ohnishi et al. to estimate the unknown load torque for the speed regulation of a dc motor system. Chen et al. designed a nonlinear disturbance observer (NDO) to estimate the lumped disturbances in robotic manipulators. Kim et al. presented a high-order NDO to estimate the biased sinusoidal signal. In Yao et al. employed an extended state observer (ESO) originally proposed by Han to estimate both the unknown system states and the modeling uncertainties in a hydraulic rotary system. However, the majority of classical DOB-based approaches need to construct an observer, where the observer parameters should be carefully selected to make a compromise between the robustness and control response. To simplify the parameter tuning, a new unknown input observer has been proposed, where first-order filters are applied on the measured dynamics and only one tuning parameter needs to be selected. On the other hand, most of the existing control methods for electro-hydraulic systems have been developed by means of the backstepping scheme because these kinds of systems are generally in a strict-feedback form. It is noticed that the derivatives of virtual control actions should be calculated repeatedly in the backstepping design, which leads to the “explosion of complexity” problem. Although dynamic surface control was developed to remedy this issue, the introduced filter operations create extra complexity and increased computational costs. Moreover, full system states have to be known or measurable in these backstepping designs. This assumption is particularly stringent in the practice since the internal cylinder force in the hydraulic systems is generally immeasurable. Nevertheless, some hydraulic parameters (e.g., oil leakage coefficient and effective bulk modulus) cannot be precisely identified, and the dynamics of fluid inside the cylinder and servo valves are also difficult to model. Hence, to avoid the use of these variables in the control designs, the hydraulic actuator was treated as an ideal force generator only in some literature, e.g., the work in. This simplification results in inevitable mismatch compared to realistic systems. In fact, these aforementioned difficulties, to some extent, create the gap between the theoretical studies and practical applications of hydraulic systems. Hence, it deserves further investigation to develop simple control (e.g., without backstepping and function approximation) for electro-hydraulic systems, in which the actuator dynamics are considered explicitly and only the measurable output is used to facilitate the practical application. Motivated by the above discussions, we will present a new output-feedback control design for high-order electro-hydraulic systems in this paper. The method to be presented can not only deal with unknown nonlinearities without using function approximators and backstepping but also have a few tuning parameters and fast convergence. To avoid using backstepping, we first transform the system model into a canonical form, where both the unknown dynamics and disturbances are considered as a lumped term. This derived transformed system allows to use Levant's differentiator to reconstruct the immeasurable system states, which has attractive finite-time convergence property to contribute to enhancing the convergence of both the observer error and control error. Then a simple unknown dynamics estimator (UDE) with only one tuning parameter is presented to estimate the lumped dynamics. Finally, one step, backstepping-free control is constructed by using the estimated system states and uncertainties. Comparative simulations and experiments based on a realistic test rig are given to show the effectiveness of the proposed schemes. In this control, only the system output (motion displacement) is required, leading to an output-feedback control scheme. The complex function approximators (e.g., NNs and FSs) and lengthy backstepping are all avoided. Consequently, fewer transducers and reduced computational costs are required. All these salient features make the proposed control scheme particularly suitable for applications. The main contributions of this paper are twofold. We introduce a coordinate transformation and develop a new backstepping-free control for hydraulic servo systems in the strict-feedback form, which has reduced complexity and uses the system output only. This control method is clearly different to most of the existing references, which have been derived based on the classical backstepping scheme and subjected to the “explosion of complexity” issue. We propose an alternative, yet a new solution for the control design for the derived Brunovsky system by introducing a novel UDE together with Levant's Differentiator, beyond the well-known ESO-based method, which has a straightforward parameter tuning phase but improved transient control response. This paper is structured as follows. Section II describes the system dynamics and problem formulation. The coordinate transformation is presented in Section III. Section IV proposes the UDE and the associated control design. Comparative simulation and experimental results are given in Section V, and Section VI concludes this paper.", "section": "Introduction", "doi": "10.1109/TIE.2019.2897545", "references": [1970127232, 1972061147, 1988876996, 1990380686, 1998650983, 2006380251, 2030959268, 2048653585, 2056849404, 2081103703, 2087260217, 2103345351, 2135493101, 2167808181, 2411793894, 2462664181, 2521299427, 2748648378, 2752879615]}
{"paragraph": "The Bilateral teleoperation technology, which is based on human–machine interaction for human operators to implement remote, dangerous, and complicated working tasks, has been applied in numerous fields, such as nuclear plant detection, underwater operation, medical surgery, and space exploration. In a typical bilateral teleoperation system, the human operator operates the master manipulator and transmits the command signals to the slave manipulator via the communication channel to perform the operation, while the environmental force signals can be transmitted to the operator via the communication channel. There are two main objectives of bilateral teleoperation system control design, which includes stability and transparency. The objective of stability for bilateral teleoperation systems originates from the time delay in the communication channel, which is an inherent property often occurring in the distant transmission for teleoperation systems and may destabilize the system. Therefore, closed-loop stability is required to be guaranteed to implement the human operations and tasks in the remote environment. The transparency, which includes position tracking and force feedback, is another objective for bilateral teleoperation systems focusing on the quality of task completion. Many control approaches have been proposed to cope with these objectives. The passivity-based control approach is utilized to improve the system's passivity and guarantee the closed-loop stability of the bilateral teleoperation system under time delay. Namely, the scattering operator approach was proposed to stabilize the teleoperation system. The wave variable transform approach, where the power signals were transformed into the wave variables in the communication channel, was proposed to guarantee system stability. The time-domain passivity control approach was proposed to achieve stability by the energy conservation of the teleoperation system. However, these passivity-based control approaches, which mainly concentrate on the stability objective, would cause wave reflection and position drift to deteriorate the transparency performance of the system. Thus, some modifications of the wave-variable-based architectures have been proposed to enhance the system's transparency performance, but the achievement of good transparency performance is still challenging. The four-channel approach, which matches the impedance parameters of the master and slave dynamics, is an effective approach to achieve the system's good transparency performance, but the stability objective is not considered in this approach. Thus, the combination of the four-channel and passivity-based approaches is an alternative way to achieve stability and good transparency performance simultaneously. Though many approaches have been developed to deal with the stability and transparency objectives, the tradeoff between system stability and transparency is essentially existing. Recently, due to increasingly complicated teleoperation tasks, nonlinear manipulators are used in the teleoperation system, where various nonlinearities and uncertainties are significant and essentially existing. Many control approaches are developed in the engineering field to handle these problems, such as the adaptive control for parameter variations, H∞ and μ-synthesis control for modeling uncertainties, backstepping and feedback linearization control for nonlinearities, and have been applied to the bilateral teleoperation system. The sliding mode control was designed to cope with the modeling uncertainties and parameter variations of teleoperation system dynamics, and two-port networks were enrolled to achieve absolute stability. The model predictive control approach for teleoperation transparency improvement under time delay was proposed by dynamic modeling with the enrollment of Artstein-type state and measure transforms and the state observation with Kalman filter; closed-loop stability was then guaranteed by the linear quadratic Gaussian control design. Smith predictor was proposed for time-delay compensation, and the internal model principle was utilized to enhance the stability and trajectory tracking performance. However, accurate knowledge of system dynamics are necessarily required in the aforementioned approaches, which may introduce high costs in system identification to obtain prior knowledge and cannot work well in the nonlinear teleoperation systems with various and complicated modeling uncertainties. The type-2 fuzzy modeling and neural-network-based passivity control approach were proposed to improve the transparency performance of nonlinear bilateral teleoperation systems by the enrollment of fuzzy-model-based four-channel control law to handle the modeling uncertainties without precise prior knowledge of system dynamics. Though many control approaches have been proposed to handle the main issues of nonlinear bilateral teleoperation systems, a good design to handle the aforementioned issues simultaneously is still challenging. Therefore, by addressing the two main objectives of bilateral teleoperation systems with various nonlinearities and uncertainties, a globally stable adaptive fuzzy backstepping control design is developed in this paper. The basic backstepping control approach is utilized by the enrollment of virtual control signals to handle the nonlinearities and ensure the stability and good tracking performance of the master or slave subsystem. The fuzzy logic system is used to approximate the nonlinearities and modeling uncertainties with external disturbances, where the adaptive law is proposed to handle the parameter variations by adjusting the parameters of fuzzy logic system online. The trajectory smoothing is implemented in the transmission of position signals from the master manipulator to generate the desired tracking trajectory for the slave manipulator. The fuzzy-based nonpower environmental parameters are approximated online in the slave side and transmitted to the master side for the environmental torque prediction, which effectively provides the human operator with good force feedback performance, avoids the transmission of power signals in the delayed communication channel, and solves the passivity problem in the traditional teleoperation system. Therefore, the great transparency performance of both position tracking and force feedback can be achieved while the global stability under time-varying delays is theoretically guaranteed. The comparative experiments are conducted and verify the effectiveness and advantages of the proposed control design. The rest of this paper is organized as follows. Section II presents the modeling of nonlinear bilateral teleoperation manipulators. Sections III and IV present the control design for nonlinear bilateral teleoperation system, and the stability analysis, adaptive law, and fuzzy logic system design are illustrated in detail. Section V presents the experiment, and the results show the superiority of our design with respect to the good transparency performance in some typical working scenarios. Section VI concludes this paper.", "section": "Methodology", "doi": "10.1109/TIE.2019.2898587", "references": [1965214787, 1967390389, 1971374550, 1987118475, 2007775256, 2080529541, 2088680241, 2090017676, 2151225782, 2158828654, 2159851305, 2170683129, 2241323601, 2287940679, 2342610385, 2342926333, 2343736713, 2516758864, 2536318021, 2607112850, 2698548017, 2760855057, 2764035193, 2792561954, 2794110755]}
{"paragraph": "Clique-width. Every NP-hard graph problem becomes polynomial-time solvable after placing appropriate restrictions on the input. A general method in the design of graph algorithms for special graph classes is to decompose the vertex set of the graph into large sets of “similarly behaving” vertices and to exploit this decomposition algorithmically. An optimal vertex set decomposition gives us the “width” of the graph. Clique-width is one of the most studied width parameters. The vertex set decomposition of clique-width is defined via a graph construction based on vertex labellings. Starting from the empty graph, a graph G is built up vertex-by-vertex using four specific graph operations. These operations ensure that vertices labelled alike will keep the same label and thus “behave” identically. The clique-width of G is the minimum number of different labels needed to construct G in this way. A graph class has bounded clique-width if there exists a constant c such that every graph in the class has clique-width at most c. The algorithmic importance of having bounded clique-width follows from the existence of several meta-theorems which, when combined with an approximation result, ensure that many well-known NP-hard graph problems, such as Graph Colouring and Hamilton Cycle, become polynomial-time solvable for every graph class of bounded clique-width. Hence, there is a need to verify boundedness of clique-width of special graph classes, in particular when undertaking a systematic classification into the computational complexity of graph problems under input restrictions. Well-quasi-orderings. A graph class is well-quasi-ordered by a containment relation if for any infinite sequence of graphs in the class, there is a pair such that one is contained in the other. Just as is the case for having bounded clique-width, being well-quasi-ordered is a highly desirable property, which has been frequently discovered in the areas of discrete mathematics and theoretical computer science. To illustrate its importance, let us mention the seminal project of Robertson and Seymour on graph minors, which culminated in 2004 in the proof of Wagner's conjecture. Wagner's conjecture states that the set of all finite graphs is well-quasi-ordered by the minor relation, a graph H is a minor of a graph G if H can be obtained from G via a sequence of vertex deletions, edge deletions and edge contractions. As an algorithmic consequence, given a minor-closed graph class, it is possible to test in cubic time whether a given graph belongs to this class. To give some more examples, a result of Ding implies that every class of graphs with bounded vertex cover number is well-quasi-ordered by the induced subgraph relation, whereas Mader showed that every class of graphs with bounded feedback vertex number is well-quasi-ordered by the topological minor relation. Fellows, Hermelin and Rosamund simplified the proofs of these results and also showed that every class of graphs of bounded circumference is well-quasi-ordered by the induced minor relation. Furthermore, as “interesting applications” of these three results, they gave linear-time algorithms for recognizing graphs from any topological-minor-closed graph class of bounded feedback vertex number, any induced-minor-closed graph class of bounded circumference, and any induced-subgraph-closed graph class of bounded vertex cover number. Hereditary graph classes. Graph classes closed under taking induced subgraphs are said to be hereditary. Courcelle proved that the class of graphs obtained from graphs of clique-width 3 via one or more edge contractions has unbounded clique-width. This means that the clique-width of a graph can be much smaller than the clique-width of its minors. On the other hand, the clique-width of a graph is at least the clique-width of any of its induced subgraphs. Hence, it is natural to focus on determining boundedness of clique-width for hereditary graph classes. Research goals. Nothing in the definitions of clique-width and well-quasi-orderability suggests that there is anything in common between the two notions. However, as we will discuss, recent results for hereditary graph classes suggested an intriguing connection between them. This connection is not yet well understood, and as such, it remains to be further explored. Our underlying research goals are: to increase understanding of the relation between well-quasi-orderability by the induced subgraph relation and boundedness of clique-width for hereditary graph classes; and to obtain new results for both notions applied to hereditary graph classes. In a previous paper we showed that certain graph operations and graph constructions work equally well for bounded clique-width and well-quasi-orderability. In this paper we will explore common graph techniques further. Before discussing our results in detail, we first discuss a conjecture that motivated our research. Conjecture. We first note that the hereditary class of graphs of degree at most 2 is not well-quasi-ordered by the induced subgraph relation, as it contains the class of cycles, which form an infinite antichain. As every graph of degree at most 2 has clique-width at most 4, having bounded clique-width does not imply well-quasi-orderability by the induced subgraph relation. In 2010, Daligault, Rao and Thomassé asked about the reverse implication: does every hereditary graph class that is well-quasi-ordered by the induced subgraph relation have bounded clique-width? At WG 2015, Lozin, Razgon and Zamaraev gave a negative answer to this question. It is readily seen that a class of graphs is hereditary if and only if it can be characterised by a unique set of minimal forbidden induced subgraphs. As the set of minimal forbidden induced subgraphs in the counter-example is infinite, the question of Daligault, Rao and Thomassé remains open for finitely defined hereditary graph classes, that is, hereditary graph classes for which the set is finite.", "section": "Related Work", "doi": "10.1016/j.jcss.2019.09.001", "references": [52229496, 1546876109, 1581957840, 1785928421, 1967174286, 1977950997, 1988827593, 1993328543, 1994331970, 1996873316, 2005079828, 2007843697, 2008361472, 2020139369, 2026807383, 2052306562, 2053991811, 2063951771, 2067016370, 2070790034, 2076819776, 2097592506, 2097725094, 2110917011, 2115589427, 2116334057, 2122172628, 2147653456, 2148842617, 2167462019, 2623827933, 2733323637, 2762850984, 2962956065, 2963233231, 2963558861, 2963637237, 2964039793, 2964181817]}
{"paragraph": "Humans use the gaze to look at objects. This behavior can be used as a means to control interfaces in human–computer interaction by estimating the gaze point with the help of an eye tracker. Thanks to recent technological advancements and drop in price, eye tracking is no longer a niche technology only used in laboratories or by users with special needs. For example, with the price of an advanced game controller, players can enhance their gaming experience with eye tracking. A gaze-aware game knows where the player’s visual attention is at each moment and can offer optional input methods and enhanced gaming experience. At the same time, research on mobile eye tracking has been active. Simple eye-awareness is already included in some cell phone models so that the phone knows when the user is looking at it. Research on pervasive and mobile gaze interaction has demonstrated how eye tracking can enhance the interaction with mobile phones, tablets, smartwatches, smart glasses, and smart environments and public displays. Because the eye is primarily a perceptual organ, using gaze as an intentional control method poses challenges for interaction design. Most important, viewing should not be misinterpreted as a voluntary command. In gaze interaction literature, this problem is known as the Midas touch problem, where viewed objects are unintentionally acted on. Feedback plays an essential role in informing the user how the system is interpreting the gaze. Gazing at an object in real life naturally provides only visual feedback. But computers and smart devices can indicate if an object has been recognized as being pointed at, or being selected. Previous research has shown that visual and auditory feedback on gaze input significantly improve user performance and satisfaction. However, the effects of haptic feedback in gaze interaction have remained largely unknown. We assume haptic feedback could provide a useful alternative to, at least the audio, as auditory and haptic perception are known to share similarities. For example, participants could perceive auditory and tactile rhythms more accurately than visual rhythms. Auditory and haptic feedback can be perceived independently from the gaze location. Unlike the distal senses of vision and hearing, touch is a proximal sense that provides information of things close to or in contact with us. How would the interplay of a distal and proximal sense work? For instance, instead of seeing a button change its appearance, the user could feel the click of a button after selecting it with gaze. Could this novel combination of modalities provide some benefits compared to visual and auditory feedback, or is this unnatural combination of action and feedback perhaps incomprehensible? These were the questions that motivated us in the work reported in this article. Haptic feedback has become more common in consumer technology due to the emergence of mobile and wearable devices designed to be in contact with the skin. The most popular form of haptic stimulation in mobile and wearable devices is vibrotactile feedback. For example, continuous vibration is an effective way to notify of incoming calls with mobile phones. Shorter vibration bursts are used on phones and tablets to replace the tactile feel of pressing a physical key when typing with a virtual keyboard. This has been shown to improve typing speeds. Vibrotactile technology is also included in some smartwatches. In the Apple Watch, for instance, vibrotactile stimulation is used to mimic a heartbeat that can be sent to a close friend or family member. With multiple actuators, it is possible to create touch sensations that move on the wrist. To date, commercial smart glasses and other head-mounted devices have not utilized vibrotactile feedback. This is surprising because it is known that users can quite accurately localize which part of the head is stimulated with vibrotactile actuators. We were interested in studying how vibrotactile feedback could support gaze interaction. We conducted a series of experiments in which we focused on four main research questions: effectiveness of vibrotactile feedback, temporal limits between gaze events and vibrotactile feedback, effects of feedback location and spatial setup, and vibrotactile feedback in comparison to other modalities. Because our results are spread over more than 20 articles, this could make it difficult for future researchers to extract the main findings. The contribution of this article is to summarize the research results in a compact form and serve as a collection of pointers to more detailed work in the original publications. The goal is to add to our understanding of how the two modalities of haptics and gaze can be utilized effectively in human–computer interaction. The organization of the article is as follows. We first introduce gaze interaction and vibrotactile feedback. We then present results from the experiments before discussing lessons learned from the studies. We end with general discussion and present design guidelines based on our accumulated knowledge and insights.", "section": "Methodology", "doi": "10.1080/07370024.2017.1306444", "references": [180056178, 1492404201, 1520722479, 1538622809, 1590806791, 1976177666, 1982812012, 1986793049, 1988742375, 1990184244, 1994221098, 1995055145, 2000367546, 2002492663, 2004164216, 2005637512, 2006966072, 2010809371, 2014788716, 2016877042, 2018806117, 2020284712, 2022716189, 2025074021, 2030189835, 2034722664, 2040464349, 2041257003, 2042346957, 2046879541, 2053432308, 2057481637, 2060089136, 2075140915, 2076963239, 2079778212, 2088270376, 2090644126, 2091354492, 2093355879, 2094058125, 2094938076, 2099788879, 2102804490, 2103802786, 2110228188, 2134738924, 2138428053, 2144390195, 2144922371, 2150273910, 2156515914, 2156903065, 2164660958, 2167020116, 2176017127, 2285717100, 2293738675, 2295148469, 2298161945, 2344158693, 2402420151, 2420461153, 2484706745, 2537533530]}
{"paragraph": "Compilers are important infrastructure tools in software development, which provide syntax and semantics analysis for programs, as well as code optimization to accelerate software upgrades. For example, the security engineering group at Microsoft utilizes compilers to prioritize code review; the maintenance engineers at Hewlett-Packard improve the quality of code by removing compiler diagnostics in software systems. However, compilers may also contain bugs, and in fact quite many bugs are reported for widely-used compilers such as GCC and LLVM. Buggy compilers make a source program optimized or translated into a wrong executable module, which may behave differently from the expected behavior determined by the semantics of the source program. Once this happens, it can result in disastrous software failures especially in safety-critical domains. For instance, a bug in the compiler of HAL/S had even caused the failure of the NASA Shuttle software. Even worse, developers with little knowledge about compiler bugs customarily debug the software they are developing rather than the compilers they are using, which makes compiler bugs more difficult to be found. Therefore, guaranteeing the quality of compilers is a critical issue. Compiler testing is one of the most important ways to guarantee the quality of compilers. According to the previous studies, there are three issues to be addressed: how to generate adequate test cases to test compilers, how to find the test oracles to determine whether a test case triggers bugs, and how to reduce these test cases. Furthermore, two challenges are to be addressed. First, since the inputs of compilers are complex programs with furcated syntax structures and rigorous content constraints, undefined behaviors of language specification make the first issue and the third issue be a challenge. Second, since compiler testing lacks test oracles to determine whether the outputs of compilers are semantic equivalent with the programs before they are compiled, the test oracle problem makes the second issue be a challenge. During the past decades, a great number of researchers have proposed different approaches for addressing the above issues. Some successful random test case generators have been implemented to facilitate compiler testing, such as Orion, Csmith, Quest, randprog, and JTT. All of them can automatically generate abundant test programs for compilers without undefined behaviors. Simultaneously, various compiler testing techniques have been proposed to mitigate the test oracle problem, such as differential testing, random testing, Equivalence Modulo Inputs, mutation testing, and metamorphic testing. By employing the above testing techniques, a large number of compiler bugs can be detected. In addition, several reducers have been developed to minimize the test cases, such as Berkeley Delta, C-Reduce, and CL-Reduce. Thus, a set of small and valid test cases that trigger the same bugs as original ones can be reported to developers. However, as the number of related papers increases, there are few efforts to systematically identify, analyze, and classify the influential researchers, the state-of-the-art testing technologies, the collaborations among authors, and the co-occurrence of keywords, which results in an obstacle to characterize and understand compiler testing. In this study, we employ a systematic and comprehensive literature analysis framework to overcome the obstacle. First, we perform an extensive search to identify papers related to compiler testing, and extract the most important information from papers for the consequent analysis, such as the title, the keywords, and the author(s). Then, we conduct a bibliometric analysis to identify the most influential authors and papers, as well as the widely-used compiler testing technologies, so as to present an external overview of the compiler testing area. Last, we construct three collaboration networks to analyze the communities of authors and keywords, which can present internal evidence on the influential authors and hot topics in this area. The major contributions of this paper are summarized as follows: We conduct a bibliometric analysis for compiler testing literature. The results show that the USA is the most influential country with a large number of excellent researchers and institutions in the compiler testing area. In addition, various types of compilers are tested, ranging from C++, Java to Pascal, whereas C compilers draw much attention from academia. We combine association rule mining and collaboration analysis to construct three networks, including the co-authorship network, the author co-keyword network, and the keyword co-occurrence network. The results show that most researchers have broad interests in the compiler testing area. These researchers distribute in several scattered communities. The keywords “test case generation”, “automated testing”, and “random testing” frequently co-occur in compiler testing. The paper is structured as follows. Section 2 illustrates the challenges and the corresponding solutions in compiler testing. We demonstrate the components of literature analysis framework in Section 3. Then, Section 4 shows the findings from bibliographic and collaboration analyses. Section 5 provides an overview of related work. Section 6 concludes our paper and discusses the future direction.", "section": "Introduction", "doi": "10.1007/s11704-019-8231-0", "references": [109452506, 1486146156, 1499592573, 1525595230, 1575947743, 1596462940, 1963831852, 1966021031, 1973519892, 1979977091, 1988252511, 1996969457, 1999067077, 2009181019, 2014821466, 2017425457, 2023035194, 2030405312, 2032188103, 2032841931, 2034553498, 2036896594, 2041713059, 2071039852, 2071952624, 2086689864, 2094771270, 2095445208, 2096698236, 2098456636, 2109652142, 2124883940, 2125910575, 2130746431, 2132984320, 2140187381, 2141717815, 2153185479, 2155877593, 2169800777, 2170737051, 2173450663, 2296333508, 2313752879, 2370472429, 2390518826, 2461570336, 2506015293, 2532737545, 2560438921, 2594511738, 2616082100, 2617809069, 2793210894, 2799563612, 2884843021, 2952139678]}
{"paragraph": "Gaze tracking is the process of determining one’s point-of-gaze as a temporal sequence of coordinates. Spatially registering the points-of-gaze to a visual stimulus displayed on a monitor requires a calibration which depends on several factors related to the subject such as physiological properties, presence of sight correction apparatus or the environment such as illumination and position of the monitor. While controlled cross-subject and cross-device comparative studies of gaze tracking accuracy are available, these do not provide any information on whether the accuracy may degrade in time. In addition, vendor-provided performance information is an estimate assuming a specific use context such as desktop-based human–computer interaction. Especially from the biomedical engineering perspective, there are several motivations for exploring human perception via gaze tracking. Namely, understanding the medical image perception process by assessment of visual search strategies of clinicians, studying human visual expertise during image acquisition and interpretation for potential skill assessment, improving and designing graphical visualizations and interfaces for HCI in medical settings, and developing computer-based image analysis methods, such as automatic detection of medical image contents and computational modeling of human visual attention. First, the holistic understanding of expert perceptive and cognitive learning processes in terms of visual scan, search and recognition strategies, and decision-making has been of considerable interest to the scientific community. Studies in this direction include the evaluation of visual search tasks in radiology, computed tomography, mammography, magnetic resonance imaging, pathology, and ultrasound imaging. Second, variations in visual expertise and behavior between experts, trainees, and novices have been analyzed in different clinical settings, such as endoscopic surgery, radiology, mammography, pathology, computed tomography, pediatric neurology, dermatology, and ultrasound-guided anesthesia. Third, gaze tracking has been widely explored for HCI and usability research. The information inferred from eye tracking devices can be utilized to determine areas of interest and visual search patterns in an interface, and to evaluate the visibility, usefulness, and position of its elements. This has been shown to be helpful in improving interface design for more efficient system interaction, for example, reduced overall visual clutter and cognitive workload. An example of an HCI study for medical imaging is where gaze tracking was used to analyze visual search paths of radiologists to provide useful insights for designing efficient radiology workstations. Finally, recently there has been an emergence of interest in the image analysis literature in gaze tracking to understand its role in constraining image interpretation. For example, applications of gaze tracking-based computer vision include object recognition, action recognition, and caption generation. Likewise, expert human knowledge has been leveraged to design advanced medical image analysis algorithms for automatic detection and classification of image contents in mammography, retinal images, MRI, and ultrasound images. From the computer vision point of view, gaze tracking has informed saliency-based visual attention methods, leading to a number of computational visual saliency models. Correspondingly, prediction of visual attention in medical images has been performed using gaze data as ground truth in radiology and retinal images, endoscopy, and ultrasound images. However, the construction of meaningful models of medical image perception requires a large amount of real-world data to account for the natural variability of the task as well as the natural variability in human perception. This involves the acquisition of gaze data across an extended period of time potentially several months. In order to facilitate the analysis and interpretation of gaze data, it is important that the eye tracker is accurately calibrated for each observer, and that the calibration does not drift with time. Therefore, we wanted to understand how the accuracy of a commercial eye tracker varies in time, and whether a regular recalibration is necessary. Longitudinal stability of gaze tracking accuracy has previously been reported at timescales of the order of minutes. Some evaluated the temporal stability of different eye tracking algorithms for webcams in a single continuous session, that is, the stability of measurements across consecutive camera frames. Others compared the accuracy of two eye trackers with nine participants and at four different instances, separated by a pause of 2 min. The authors reported that the effect of time elapsed since calibration was not statistically significant at this timescale. Hence, to the best of our knowledge, there has not been a longitudinal study assessing performance at larger timescales. However, long-term temporal factors could have an impact on the accuracy of gaze tracking. Changes in the external environment, such as head position or illumination. Changes in the appearance of the user. This is relevant as most eye trackers use image-based tracking algorithms to take measurements from the user’s face and eyes. In this paper, we report the results of a longitudinal study of gaze tracking performance conducted over a period of one month. The aim of this paper was to evaluate whether the accuracy of gaze tracking is stable over this time interval after a single initial calibration for each user. This issue has been studied in related areas such as biometric recognition. For instance, it has been suggested that the accuracy of iris recognition may decrease in the very long term. A recent study reported a diminution of face recognition accuracy with years. However, accuracy over several years is not relevant to the range of applications of interest in biomedical image analysis. Indeed, the purpose of having a temporally robust gaze tracking system is to acquire data on human visual behavior while demanding minimum time and effort, that is, a minimal number of calibrations from the studied subjects who are busy clinical professionals working in clinics. Our investigation was divided into two studies. The first was a desktop study with 13 participants. It was intended as a reference for image viewing on a desktop monitor. The second is an in situ study, which specifically looked at accuracy in the context of cart-based ultrasonography, a cart-based scanner is the most commonly used device for ultrasound exams. The environment of an ultrasound exam is different from the desktop setting in which gaze tracking is usually performed. First, the amplitude of motion of a sonographer is typically larger than that of someone sitting at a desk in front of a computer, and the variability of head positioning is also larger due to the flexibility of the cart-based ultrasound scanner. Second, the ultrasound exam is performed in the dark, with the monitor as the main source of luminosity. Since the head position and room illumination are two important factors impacting eye tracking quality, it is of interest to estimate how an eye tracker performs in these conditions. Note that we purposefully did not constrain the movement of the participants, because the objective was to evaluate the temporal evolution of gaze tracking performance in a real use situation rather than the performance under optimal conditions.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2866274", "references": [67472587, 1980711281, 1997741316, 2032339067, 2043033468, 2045480547, 2048474912, 2064909625, 2071555787, 2080541453, 2088032731, 2089820506, 2097493241, 2100379672, 2116576293, 2204666881, 2462177648, 2465528855, 2488748484, 2506096269, 2520271149, 2572045081, 2731219627, 2743391672, 2750736663, 2807476029]}
{"paragraph": "In the past decade, climate change issues have received increased attention worldwide. Scientific evidence proves that anthropogenic greenhouse gases such as carbon dioxide emissions are the main cause of climate change. The ratification of the 2015 Paris Agreement indicates the concerted aspiration of countries around the world to reduce CO2 emissions and mitigate climate change. Although U.S. President Donald Trump left the Paris Agreement, the U.S. Climate Action Pavilion was set up with the support of the Bloomberg Philanthropies at the last UNFCCC Conference of the Parties in Bonn, Germany, on November 2017 to announce the American's Pledge that across America, states, cities, businesses, universities, and citizens are taking action to fight climate change. A notable fact is that nearly two-thirds of historical GHG emissions are attributed to only 90 entities, including both state-owned and investor-owned firms. Leading carbon emissions producers are experiencing pressure from government regulations, market demand for low-carbon products, and other responsibilities as requested by stakeholders. When first confronted with the emerging carbon constraints, many firms resisted changing and disputed the environmental and economic aspects; while later on firms gradually begin taking proactive actions towards emissions reduction. Managing climate change proactively is now not only an action for corporate responsibility but also a business strategy to gain a potential competitive advantage. Therefore, it is important, even fundamental, for firms to design low-carbon business strategies and adjust operational decisions to work within the carbon-constrained business environment. Since the early 2000s, interest in sustainable operations in academic literature has been growing, especially from an environmental perspective. Scholars have conducted comprehensive literature reviews on topics including environmentally and socially sustainable operations, green supply chains, and sustainable supply chains. While carbon constraints may share some common features with environmental constraints, they differ in several aspects. First, climate change is a global issue rather than a local or regional issue. Second, carbon emissions reductions may not only reduce environmental externalities but also lower operations costs through more efficient energy consumption and transportation. Third, innovative policy instruments, such as the emissions trading scheme, are implemented to control carbon emissions on either regional or national levels for various industries. These characteristics have caused many scholars to focus explicitly on carbon-constrained operations management. According to literature, academic literature explicitly addressing carbon-constrained operations management has a promising future, but it is still in its nascent period. Some studies summarize articles published in special issues on carbon-efficient production, supply chains and logistics. Others review journal articles on the green supply chain and logistics that focused on planning and controlling supply chain activities with respect to CO2 emissions. However, a systematic review of the studies on carbon-constrained operations models is still lacking, especially considering the rapidly growing number of studies in this area. This paper provides a conceptual analyzing framework for studying firms’ carbon-constrained operations decisions through a systematic literature review and proposes potential research directions in this area. In this study, carbon constraints are defined and categorized into three groups: policy-driven, market-driven, and nature-driven. Policy-driven constraints refer to the regulatory disciplines acting on firms, which may be imposed by either the outer institutions such as governments or the firms’ inner managing boards. Market-driven constraints refer to concerns from the product market environment, which may include the consumers’ preference towards low-carbon products, low-carbon quality competition with competitors, and other stakeholder requirements. Nature-driven constraints refer to the physical risks from climate changes in the natural system, which may have strong impacts on regular business. Possible business strategies for fulfilling the carbon constraints, including internal emissions abatement, collaborative emissions abatement, and carbon compensation, were then analyzed and attributed to the three categories. Ten types of carbon-constrained operations models from 116 journal articles are studied in support of the categorized business strategies. Next, the modeling methodologies are summarized and a conceptual analyzing framework is constructed, which consists of recognizing carbon constraints, selecting business strategies, modeling and making operations decisions. The developing trend of carbon-constrained operations models is analyzed with potential directions for future research. The results show that the policy-driven carbon constraints are the primary carbon considerations in the existing operations models, with the emissions trading scheme and the carbon tax taking first and second place, respectively. Furthermore, the focus of carbon-constrained operations models evolves from short-term to long-term strategies, from internal to collaborative abatement strategies, and from simple to more practical models. From the carbon constraint modeling perspective, future studies of the coexistence of multiple carbon constraints and their interactive effects might provide interesting results. From the business strategy perspective, the collaborative emissions abatement models, especially cooperation mechanisms of competitive firms, would require further study. The remainder of this paper is organized as follows: Section 2 describes the definition and categorization of carbon constraints. Section 3 summarizes possible carbon-constrained business strategies. Section 4 is a systematic review of 116 journal articles on carbon-constrained operations models in support of the categorized business strategies. Section 5 analyzes the research trend and presents a conceptual analyzing framework. Section 6 concludes the paper with potential research directions for future study.", "section": "Related Work", "doi": "10.1016/j.ejor.2019.02.050", "references": [909910279, 954040299, 1218451309, 1457951433, 1971418145, 1990441574, 1990875709, 1996282307, 2004174093, 2006050572, 2007012838, 2023220064, 2034824130, 2041632134, 2065524587, 2067160548, 2079663823, 2102268217, 2107164652, 2114257725, 2131880787, 2132888205, 2135640185, 2149294070, 2199749229, 2239814390, 2262038266, 2292480134, 2299447546, 2314948844, 2572658317, 2600888046, 2761481679, 2762414683, 2766354201, 2770291701, 2900043057]}
{"paragraph": "In the transition period, the Chinese society is faced with frequent emergencies, which are increasingly complex and far-reaching in consequences. Different interest groups coexist in the social environment of China. With the growing difference in and conflicts of interests between these groups, the emergencies and their associated social emotions, especially the interacting negative emotions, will weaken social cohesion and dampen positive social energy, making it more complex to manage emergencies and difficult to ensure national harmony and effective social government. The key to emergency management lies in the release of government information. The release is a stabilizer of social order, as it ensures the public’s right to know, curbs the spread of negative social emotions, and prevents secondary crisis. To appease negative social emotions and promote positive mentality, the governments at all levels should roll out scientific and reasonable emergency response plans as well as the corresponding government information release strategies. Specifically, different release strategies and measures should be adopted according to the development of emergencies and the public demand for incident information, such that the relevant information can reach the public rapidly and effectiveness. In this way, the public will know about the details on the public emergencies, understand and trust the government’s treatment. Without the fear of the unknown, the negative social emotions will be suppressed or simply not occur; neither will the negative impacts of media misreports persist. Thus, the society will return to the normal track again. Recent years has seen the implementation of emergency response laws and regulations like the Regulation of the People’s Republic of China on the Release of Government Information and Emergency Response Law of the People’s Republic of China. Under the guidance of these rules, the governments at all levels have established a relatively complete communication system for the release of emergency information. Nevertheless, the release of government information under emergencies remains a weak link in China’s government management. In practice, there are still many chronic problems, such as the lack of emphasis on hot topics, the delay in release, the wrong judgement of public concerns and the failure to mitigate negative emotions. Emergency management in China has entered a new normal. On the one hand, the emergencies are growing in occurrence frequency, complexity and destructiveness, while various social emotions are present side by side. On the other hand, the current communication environment is featured by the joint action of new and old media. Against this backdrop, it is imperative to overcome the above problems in the release of government information under emergencies, such as reconstructing the media framework for the disclosed content, optimizing the selection of discourse representation and improving the release strategy. To optimize the release process and design a new release strategy, it is necessary to explore the said media framework for disclosed contents and selection of discourse representation based on the monitoring of the social emotion transmission. With the aim to effectively control and mitigate the social emotions induced by emergencies, the long-acting release strategy for government information, coupled with the scientific analysis on the infection-evolution law of negative emotions, has become an emerging topic in improving emergency management and innovating social governance. The research on information release under emergencies is still in its infancy. Most studies are qualitative in nature. There is no systematic report on the effects of government information release on the contagion-evolution law of negative social emotions. Starting with the public opinions on emergencies, the existing studies mainly focus on the public panic induced by emergencies, and evaluate the actual performance of government information release under emergencies from the following aspects: disclosed content, release strategy and release effectiveness. The policy recommendations of these studies offer valuable theoretical reference for our research. However, the previous research shares several defects: information dissemination and emotion transmission are treated as completely independent processes. The scholars discussed at length about the information dissemination behavior during government information release in light of public opinions on emergencies, failing to consider the effects of government information release on the construction and transmission of social emotions. The recommended release strategies are mostly descriptive and speculative, lacking operability in actual application; no feasible quantitative plan has been developed concerning the timing of release, the capture of public concerns, the balance between information supply and demand, or the integrated mitigation of emotions. Therefore, the results of the previous research cannot provide sufficient decision support for the formulating of information release strategy or the effective regulation of social negative emotions. The scholars have neither examined the effects of government information release on the spread of negative social emotions, nor probed deep into the adjustment and intervention effects of government information release over the infection-evolution of negative emotions, not to mention providing satisfactory solutions to scientific and rational release of emergency information and effective mitigation of negative social information. In view of the defects of the previous research and the existing strategic problems of government information release, this paper, relying on real emergency cases, attempts to disclose how government information adjusts in the evolution of negative emotions under emergencies, especially the control effects of government information release on the infection of social negative emotions. Therefore, our research objective is to examine how different types of government information releasing policies influence the contagion-evolution law of negative social emotions. The study brings a new research theme into government information releasing policy under emergencies, where social media and netizens' emotion meet and create new challenges for effective social governance. The study focuses on explicating the time when government releasing the public emergencies' information, and on how various netizens' emotions, especially the negative emotions, evoke, disseminate and evolve. The study makes three important contributions to previous literature. First, the role and significance of government information release in the emergency response has been mainly unified in prior studies. Distinguishing from the previous literature, the study provides an enhanced understanding of government information release policy by involving social media and netizens' emotions value and extending it to the public emergencies context-adding various emotion evolution features into emergency management. Therefore, our work can fill in the research gap by conceptualizing social emotions in the under-presented emergency management scenario. Also, based on the work, our study conducts emotional lexicon using SO-PMI algorithm, and builds a fine-grained sentiment classifier with the aid of long- and short-term memory network technologies to improve the emotions classification performance. Second, the study explores the relationships between the netizens' emotion and the government information release and empirically tests how social negative emotions propagate through government information releasing under emergencies. The effects of government information disclosure on the infection-evolution law of social emotions have not been well tested in IS due to their complexity. Therefore, from the perspective of cognitive big data analytics, our work advances the literature by calculating the netizens' emotion value, empirically testing the influences of government information releasing policy on emotion evolution. In the meanwhile, it advances the emotion computing model to the public emergencies context, thus provides insights into the causes and consequences of incident development, and helps to formulate reasonable GIR strategies. It thus illustrates how scientific GIR strategies may generate emotion impacts and demonstrates how GIR strategies effective could be empirically measured, which is considered a challenge in IS literature. Further, our study uncovers how the GIR strategies have different effects on positive and negative emotional transmission. It turns out that GIR strategies had a significant negative impact on the contagion-evolution of anxiety and disgust emotion. These research findings provide further insights to government's strategies and thus can help it more effectively grasp the timing of disclosure, the capture of public concerns, the balance between information supply and demand, the integrated mitigation of emotions. It is among the very first studies demonstrating that GIR strategies could generate different emotion impacts in the public emergencies scenario. Third, the study contributes to sentiment computing literature by proposing a hybrid neural network approach for fine-grained emotion classification. The existing sentiment classification focuses more on the three polarities positive, negative and neutral of sentiments, which are not fine-grained enough to fully characterize the overall evolution of netizens' emotion. By conceptualizing and computing the various emotions, our study can provide deeper understanding of contagion-evolution law of social emotion during public emergencies. Our model has been compared to those traditional methods Artificial Neural Network, Naïve Bayes to demonstrate its performance in the real emergency context. The study can also provide useful references for probing deep into the construction of fine-grained emotion algorithm. The rest of the paper is organized as follows. First, we review previous literature on GIR, emotion classification. Second, we introduce our theoretical foundation and the procedure of the emotional lexicon construction, formulate proposed emotion computing model. Next, we state our data collection procedures, measurement, data analysis and results. The effect of information release policy on regulating public negative emotion are conducted and discussed. Finally, implications, limitations, as well as opportunities for future studies are discussed.", "section": "Related Work", "doi": "10.1016/j.ijinfomgt.2019.04.001", "references": [66373487, 1830951065, 1975428268, 1979603873, 1979626518, 2064675550, 2086289679, 2116343275, 2470527509, 2546860927, 2563741043, 2609389856, 2728321415, 2734332415, 2750052917, 2766863610, 2770125295, 2806938087, 2809427307, 2885508271, 2904210199, 2912325838, 2934302500, 2949190276, 2951278869]}
{"paragraph": "In control applications, such as fire detection, target tracking, border patrol, and so on, it is more and more popular that a team of agents cooperatively perform an assigned task as the task becomes increasingly complex. The control problems in such scenarios are characterized by widely distributing, multivariables, constraints, and flexible structures. One of attractive strategies in this context is distributed model predictive control. In a classic distributed model predictive control strategy, the overall control problem is divided into a set of small problems that are assigned to the corresponding agents. In the implementation, every model predictive control controller periodically solves its optimization control problem and exchanges information with its neighbors to achieve the cooperative task. Distributed model predictive control features its low computation complexity, scalability, and explicit accommodation of constraints and multiple objectives. Research on distributed model predictive control for cooperative multiagent systems has been performed from different directions, for example, consensus, formation, and flocking control. However, in distributed model predictive control algorithm, all the model predictive control controllers broadcast their predictive trajectories of states or control inputs to their neighbors and receive predictive trajectories of states or control inputs from their neighbors at each sampling time, even when there are no disturbances and the system is already operating close to the steady point. Massive communications result in overutilization of network resources and may even destroy the controlled system in implementation due to the limitation of practical network. Therefore, it is a critical issue for multiagent systems with scarce network resources to take communication into account in design of distributed model predictive control algorithm, where optimality needs to reflect both control performance and communication cost related to number of communications and size of data packet. Event-triggered control is an effective approach for energy saving, where a control strategy is designed to achieve a specific control objective and a triggering mechanism is designed based on system behavior to decide when to update the control law. However, the controlled systems are continuously monitored and predefined triggering conditions are continuously checked, which is undesirable for systems with high sampling cost. To avoid this drawback, self-triggered control methodology is proposed, where the next triggering instant is predetermined based on the past information available at a triggering instant; during two successive triggering instants, the sensor nodes are in sleep mode such that the controlled systems become more effectively attentive. Recently, codesign of self-triggered scheme and model predictive control for a single agent has been proposed to relieve communication burden while guaranteeing a priori performance. In these studies, triggering instant and control input were jointly determined by solving an optimization problem, where the cost function was designed without considering communication cost explicitly. Furthermore, communication cost with respect to triggering interval was added to the cost function as an independent part to give an explicit quantitative measure of tradeoff between control performance and communication cost. But the tuning role of the communication cost might not be obvious when system state converges to the desired objective. Considering this, communication cost was characterized by an exponential term and incorporated it to the cost function as a damping factor so that the communication cost could play a tuning role for almost the whole process. For multiagent systems, the great challenge in self-triggered distributed model predictive control is how to design a self-triggered mechanism in the presence of interaction between agents. In the existing literature, the self-triggered mechanism and distributed model predictive control algorithm were designed separately. For each agent, the model predictive control controller was first designed without consideration of self-triggered nature, and then the self-triggered condition was derived based on system stability, which is a conservative design and the desired control performance cannot be guaranteed. How to develop a codesign of self-triggered mechanism and distributed model predictive control algorithm to achieve a tradeoff between control performance and communication cost is worthy to be investigated. In this paper, we propose a united design scheme of self-triggered mechanism and distributed model predictive control algorithm for linear discrete-time multiagent systems with asynchronous cooperation. The main contributions are listed in the following aspects: first, a self-triggered distributed model predictive control algorithm is proposed to address cooperative performance and communication cost simultaneously. The current control input and state instead of the whole state/control trajectory is asynchronously broadcast to neighbor agents, which ensures efficient usage of communication resources at the same time reducing the size of data package. Under this communication strategy, the local agent generates predictive state trajectories of its neighbors based on the latest received information and designed feedback gains to achieve cooperation. Second, stability of the overall system is analyzed and sufficient conditions on the designed feedback gains related to trajectories of neighbors are constructed to guarantee system stability. The organization of this paper is as follows. Section II describes the controlled system and control objective. Section III gives the formulation of the self-triggered distributed model predictive control problem, minimization of the optimization problem, and stability analysis. Section IV provides the position control of a multivehicle system to illustrate the effectiveness of the proposed algorithm.", "section": "Introduction", "doi": "10.1109/TIE.2019.2896098", "references": [1968939752, 1985235885, 1995286048, 1999956382, 2039785833, 2066569144, 2100874992, 2116089238, 2512776472, 2624785892, 2734421213, 2783950890, 2787863462, 2789418969, 2963267133]}
{"paragraph": "Tactile sensing is important for understanding human perception and motion, controlling robotic hands and fingers, navigating hand manipulations and tool operations, executing input on user interfaces, and developing haptic systems. For example, artificial skin applied to robots enables advanced haptic communication among humans, robots, and environments. In addition, tactile sensing during surgical operations is an important yet challenging topic for safe operation by human–machine collaboration. In sports science, mechanical interactions between the human body and various instruments are measured to analyze physical abilities. Hence, tactile sensing is a fundamental technology for capturing the actions of real-world objects. Force, pressure, stress, deformation, and contact area are considered as the primary variables for capturing the important features of mechanical interactions. In particular, the use of tactile imaging technology to visualize pressure or stress distribution on an object is valuable for indicating appropriate behavior in sports training, mobile biomonitoring in medical diagnostics and health care, designing optimal apparatus for rehabilitation, and stably grasping an object with a robotic hand. A common approach for tactile imaging is to use arrayed pressure-sensitive sensors. Although the method promises accurate pressure imaging, the fabrication process is complicated due to the complexity of the sensor structure and wiring. To enhance the design flexibility, a multilayer structure is used for resistive and capacitive tactile sensors. However, spatial resolution is limited due to the number of sensing elements. Camera-based sensing is also an effective method for detecting force-related information without any contact devices. However, the limitation of the workspace and the occlusion in the field of view pose inevitable issues. Recently, electrical impedance tomography has been used for developing soft and flexible tactile sensors. These methods utilize electrodes located on the border of a thin conductive sheet, which can respond to localized pressure via local changes in conductivity. Some studies investigated electrode positioning and drive patterns to improve the sensing performance. Others proposed a method for estimating multidirectional strain using anisotropic electrical impedance tomography. Although these methods do not need wires or sensing elements in the contact region, pressure-sensitive materials are required for creating sensors. Therefore, the fabrication process for these types of sensors is complicated, and adjusting their sensitivity and spatial resolution is challenging. Moreover, the characteristics of these sensors depend on the electrical properties of the materials, and it is difficult to use these sensors in the versatile geometries of a detector. One study proposed a touch-sensing method for the versatile geometries of a detector using electric-field tomography and machine learning. Although this method enables position-based touch recognition, performance differs among individuals and pressure distribution is not measurable. In this paper, we develop a universal tactile imaging method to obtain pressure distribution. From the potential at the electrodes, we estimate the electrical boundary conditions related to the contact state of electromechanically coupled conductive objects. Unlike previous electrical impedance tomography-based tactile imaging, the sensor comprises electromechanically coupled driving and probing layers. To estimate pressure distribution, the system solves an inverse problem to find the electrical boundary conditions related to contact pressure distribution. This paper expands upon earlier work in several important ways as it proposes a new imaging algorithm and an assessment for tactile imaging. The main contributions of this paper are as follows. To develop a tactile imaging sensor without using pressure-sensitive materials, a novel pressure imaging method using the electromechanical coupling of two conductors is proposed. With this strategy, a thin, flexible, and low-cost tactile sensor can be fabricated using arbitrary conductors. Thus, the sensor's sensitivity and pressure estimation accuracy can be greatly improved. To successfully establish the design flexibility of tactile imaging sensors, a tomographic approach is proposed for estimating the contentious pressure distribution of the electromechanically coupled conductors. As a potential application of the proposed method, we implement the sensor on a three-dimensional printed object using conductive paint.", "section": "Introduction", "doi": "10.1109/TIE.2018.2879296", "references": [1548071717, 1965890979, 2031763660, 2042081740, 2050609926, 2073060332, 2097381457, 2098253469, 2103017125, 2119056404, 2126358124, 2137462734, 2171130677, 2296120231, 2611427051, 2626149096, 2750736961, 2794327637, 2795198316]}
{"paragraph": "Due to wide availability of sensors, condition monitoring CM as well as condition-based maintenance CBM are getting popular and effective for industrial systems. Further boosted by increasing popularity of Internet-of-Things, CM data are scaling up quickly, and CBM is challenged by fast-accumulating and high-dimensional CM data. By leveraging machine learning and CM data, intelligent health prognostics has become a promising tool for predicting remaining useful life RUL and supporting CBM. RUL prediction with high-dimensional CM data can be achieved efficiently by adopting machine learning algorithms. Among various machine learning algorithms, deep learning has been demonstrated with great scalability and generalization ability for handling high-dimensional big data. Deep-learning-based health prognostics is then receiving ever-increasing attention in both academia and industry. Traditionally, the framework of health prognostics includes four main steps, that is, data acquisition, health indicator construction, health stage division, and RUL prediction. Recent studies on deep-learning-based health prognostics often simplify this prognostics framework into two steps, i.e., run-to-failure CM data acquisition and deep-learning-based RUL prediction. First, run-to-failure CM data, such as sensory data, are acquired for subsequent deep learning model training and validation. Based on these CM data, health prognostics is formulated as a deep-learning-based regression problem, where CM data and the associated end-of-life times are separately used as inputs and outputs. Second, deep learning models are adopted as the regression models, and deep learning algorithms are used for training and validating these regression models. With a well-trained deep learning model, CM data of a newly observed system are then fed into this model for RUL prediction. For instance, regression approaches based on fully connected deep neural networks, deep belief networks DBNs, deep convolutional neural networks CNNs, deep recurrent neural networks RNNs, as well as their variants and combinations have been studied and validated for intelligent health prognostics recently. All these methods, however, implement the prognostics through deterministic neural networks, providing mainly point estimations of RUL predictions. One neglected issue in these methods is that RUL prediction can be affected by various types of prognostics uncertainty, for instance, measurement uncertainty introduced by noisy sensory data, model uncertainty associated with deep learning models, and prediction uncertainty caused by randomness in future ambient and operating conditions. Without uncertainty quantification, these deep-learning-based methods may lead to a situation where it is difficult to tell how much confidence a deep-learning-based method has on its prognostics results. In addition, decision-making based on single-point RUL predictions is difficult or even error-prone, which may give rise to dangerous outcomes, especially for safety-critical applications. Accordingly, prognostics uncertainty quantification is crucial in RUL prediction. How to address prognostics uncertainty for deep-learning-based RUL prediction is a critical issue deserving further research. However, there is scant literature on prognostics uncertainty quantification in deep-learning-based health prognostics. Deutsch and He proposed a DBN-based method for RUL prediction, where confidence bounds for RUL prediction were obtained by a resampling technique. With this technique, the whole process of DBN training and RUL prediction needs to be iterated many times to obtain enough samples for confidence bounds construction. This method is time and resource consuming, which causes difficulty for applications with fast-accumulating and high-dimensional CM data. Although many studies have investigated deep-learning-based health prognostics, these studies have either neglected prognostics uncertainty or provided uncertainty quantification through time- and resource-consuming techniques. To address prognostics uncertainty, it would be highly desirable to incorporate uncertainty characterization into state-of-the-art deep learning models, and further to embed the uncertainty inference in well-developed deep learning algorithms. This improvement would make the intelligent health prognostics fully enjoy the scalability and generalization ability conveyed by state-of-the-art deep learning models. More importantly, through this improvement, RUL prediction with uncertainty quantification can be implemented more practically and efficiently to real applications. For handling uncertainty in machine learning, Bayesian method has been highlighted as a promising way by Ghahramani. The basic idea is to use Bayesian theory as the mathematical language, and further to adopt Bayesian inference as the learning tool for manipulating uncertainty. This framework has been studied in deep learning by Gal, where Bayesian CNNs for image classification and Bayesian RNNs for speech recognition have been demonstrated. Yal and Ghahramani have also theoretically and practically demonstrated that dropout, a well-developed technique in deep learning, can be used to implement Bayesian approximation for uncertainty quantification in deep learning. Inspired by the idea of Bayesian machine learning, this paper develops a Bayesian deep-learning-based BDL-based method for health prognostics with the view of uncertainty quantification. State-of-the-art deep learning models are extended into Bayesian neural networks BNNs. A variational-inference VI-based method is then presented for BNNs training and inference. The contributions are summarized as follows. BDL-based health prognostics is presented for handling prognostics uncertainty. In addition to point estimates for RUL predictions, health prognostics by the proposed method is enhanced with uncertainty quantification. Two BDL-based RUL prediction methods are developed based on a general framework of BDL-based health prognostics. These two methods are particularly designed for applications to rotating machinery prognostics and applications with complex time-series CM data. Two benchmark cases, generally used in deep-learning-based health prognostics, will be studied and compared with state-of-the-art deep-learning-based methods. New perspective toward prognostics uncertainty will be drawn from these cases by exploring the prediction uncertainty associated with a deep-learning-based method. The rest of this paper is organized as follows. Section II briefly introduces BNN and VI. Section III develops the BDL-based RUL prediction. Section IV demonstrates the proposed methods through two benchmark cases. Section V concludes this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2907440", "references": [179875071, 1494192115, 1598796236, 1904365287, 1959608418, 1984672166, 2022160905, 2081986197, 2095705004, 2108677974, 2147800946, 2163605009, 2166851633, 2225156818, 2415594836, 2463319845, 2471161958, 2487237297, 2512701384, 2557283755, 2580840020, 2612904117, 2617137613, 2740570963, 2744067593, 2772084711, 2808622270, 2949888546]}
{"paragraph": "In recent years a great deal of attention has been paid to the use of touch-based devices such as tabletops and tablets. The direct-touch that these devices enable is preferred by children over other mediated pointing devices like the mouse and keyboard, as it provides a more direct way of selecting options on the screen. Moreover, different studies have pointed out that using multi-touch is a more intuitive way of interaction. Hence, as this technology involves a natural interaction style requiring little training, tablet-based games have already been tested with children who have demonstrated preference for this option in educational activities. These devices have brought new opportunities to create other forms of interactive media to engage kindergarten children in beneficial educational activities. With the goal of assessing the suitability of multi-touch tablet devices and to fully exploit its potential to design educational applications targeted to kindergarten children, several works have focused on evaluating the way in which kindergarten children interact with these devices. In this respect, Nacher, Jaen, Navarro, Catala, and González show that even children aged 2 to 3 are able to perform a basic set of multi-touch gestures tap, scale up, scale down, and rotation on a tablet without assistance and they are able to perform more complex gestures such as double tap and long press when using some assistive techniques. In this line, Vatavu, Cramariuc, and Schipor also showed that children aged 3 to 6 are able to perform touch gestures on small devices such as tablets and smartphones. Accordingly, kindergarten children have become frequent users of multi-touch devices such as smartphones and tablets being confronted with this technology even before they fully develop oral communicative functions. However, this growth in the use of multi-touch technology by kindergarten children and the study and evaluation of the gestures that they can successfully perform has not been matched with the study of appropriate techniques to communicate information about the applications tailored to their development. Several studies have shown that including instructions in the form of a short text or video clips is suitable for primary school children but kindergarteners do not have the abilities required to read and understand text messages or complex verbal video instructions. In this sense, the design process of these techniques is especially challenging because kindergarteners are in the process of early language development and the younger they are the more scaffolding of technical nature they need, including these special communication strategies when using touch screen devices. Hence, designers of educational applications targeted to kindergarten children need adequate graphic strategies to enable them to interpret different and diverse information about the applications, such as the gestures to be performed at a given time, the actions needed to go ahead, or information about the spatial location of objects in the virtual world. Therefore, the design of appropriate visual cues must be addressed since multi-touch interfaces can facilitate dialogic learning scenarios in which the dialog is centered around the learning activity itself rather than on the interactions the children are expected to perform each time. Considering this, designing visual prompts that avoid the need of continuous external technical scaffolding i.e., the gestures to be performed, the direction in which a game character should move, etc. is crucial when developing games or applications targeted to young children. The design and usage of visual prompts tailored to kindergarten children abilities and development will help caretakers to concentrate more on giving cognitive scaffolding i.e., the learning content to be acquired by the children since children will get the other information through the integrated prompts. Moreover, in other scenarios in which children can interact collaboratively, the use of visual prompts understandable by all the children involved in the game is a key point for them to share information and plan collaboratively the actions to take by referring to visual elements that indicate the possible available actions to perform contributing to a more satisfying and successful group educational experience. Therefore, in this article, we evaluate several visual prompts in co-existence in a real educational application in order to find out whether the cluttering of different visual prompts and several interactive elements in a virtual world has an impact in the understanding of these semiotics with kindergarten children. In addition, we also aim to test whether kindergarten children are ready to use an application which requires sequences of different multi-touch gestures to complete the task with the same success than when performing these gestures in isolation. The contributions of this work are manifold. The first contribution is the experimental confirmation that kindergarteners are able to effectively understand two different types of visual prompts displayed simultaneously and communicating data with several purposes. The second contribution relies on the fact that using visual prompts to communicate data about the gestures to be performed and to provide directional awareness fosters dialogues related to the learning activity and reduces the number of interferences about the interaction mechanisms expected each time by the application. The third contribution is the experimental confirmation that despite the task asks kindergarten children to perform sequences of different multi-touch gestures; their performance is not negatively affected. Finally, in this work, we have gamified a multimedia application adapting it to kindergarteners’ development and skills and the results show that they are ready to use it and that the use of this game fosters dialogues with caretakers about the learning content to be acquired.", "section": "Related Work", "doi": "10.1080/10447318.2019.1597576", "references": [44757346, 149294806, 1786589577, 1966456135, 1972888601, 1980065235, 1980460982, 1986382252, 2019188821, 2045711511, 2056731432, 2073909960, 2074665542, 2075156199, 2077415732, 2082403689, 2102970422, 2114904543, 2125593694, 2147848005, 2155753206, 2475827146, 2509739015, 2546943149, 2909408616]}
{"paragraph": "The study focuses on Apriori algorithm based rule generation from tables with uncertainty and its actual application system. This field of research is closely related to rough sets, granular computing, data mining, and information incompleteness. Each field of research is related to others and we consider a review of the previous literature based on six classes, ranging from I to VI, as shown in Table 1. Our study belongs to Class VI. In the Introduction, we first review existing studies based on Table 1. We then describe the purpose of the study. In Table 1, the vertical heading is considered as information incompleteness and we consider exact data and inexact data. The horizontal heading is related to the purpose of the research. We consider information retrieval, model and approach for rules, and rule generation system. First, we review Class I information retrieval with exact data in Table 1. Marek and Pawlak clarified the mathematical framework of information retrieval. The study appears to correspond to the origin of rough sets and table data analysis. The definability of a set which is the basic concept of rough sets was employed in the mathematical framework. Codd also proposed relational algebra for table data management, and SQL systems were developed. We subsequently focus on Class II information retrieval with inexact data. Lipski employed non-deterministic information to handle information incompleteness and investigated a question-answering system based on possible world semantics. A query is transformed into a normal form for the evaluation by possible worlds. Lipski proved that the set of axioms for the transformation corresponds to the system S4 in modal logic, and the set also corresponds to sound a transformed query becomes the normal form and complete any query is transformed into the normal form. The property theoretically ensures the validity of the system. For example, the SLD-resolution algorithm in logic programs and deductive databases is sound and complete for logical consequences. We agree with the property, and we introduce the property into rule generation, i.e., an algorithm generates a rule τ if and only if τ is defined as a rule. A rule generation system with soundness and completeness is rare. With respect to Prolog, Zadeh’s fuzzy theory was also applied to fuzzy databases and fuzzy Prolog, and Sakai proposed a framework of logic programs with non-deterministic values. Furthermore, Orłowska and Pawlak proposed many valued information systems and nondeterministic information systems to handle information incompleteness. We follow the systems and address their systems as Non-deterministic Information Systems. We correspondingly term a table without information incompleteness as Deterministic Information Systems. We focus on Class III model and approach for rules in exact data as listed in Table 1. In the 1980s, the research trend appeared to shift from information retrieval to data mining and rule generation. Pawlak proposed rough set theory that affords a mathematical framework of table data analysis. In rough set theory, lower and upper approximations of a target set X of objects in a table are generated via equivalence classes, and rules are obtained as a side effect. Several other related models and approaches to consider rules are investigated. Skowron et al. proposed the discernibility matrix and discernibility function and proved that the problems of generating minimal relative reducts and of generating minimal dependencies are NP-hard. Greco and Słowiński proposed a framework of dominance-based rough sets and handled rough sets for tables in which the attribute values are ordered. Ziarko extended rough set models to variable precision rough set models. Komorowski et al. surveyed the framework of rough sets. Tsumoto applied rough set-based rule generation to medical data analysis. Yao extended rough sets to three-way decisions with probabilistic rough sets, and investigated rough set models in multi-granulation spaces. Ciucci investigated lower and upper approximations for tables with rational numbers. Leung et al. extended rough sets in tables to those in interval-valued information systems. Qian et al. considered rules with a disjunctive decision part and termed the framework as multi-granulation rough sets. Zhu proposed topological approaches to covering rough sets. Information incompleteness is an extremely attractive issue, and thus it is natural that models and approaches for rules in exact data are extended to those in inexact data. We move to Class IV model and approach for rules in inexact data as shown in Table 1. Several other important models and approaches for rules in inexact data are investigated. Kryszkiewicz characterised rules in incomplete information systems where incomplete attribute values are introduced into Deterministic Information Systems. Extended similarity relations obtained via the value are applied to calculate the lower and upper approximations of a set X. Nakata et al. followed Lipski’s incomplete information and proposed rule generation based on possible world semantics. Stefanowski et al. considered the relationship between incomplete information tables and rough classification. Wu et al. investigated incomplete fuzzy information systems via a rough set approach. Yang et al. examined the relationship between the dominance-based rough set approach and the incomplete interval-valued information system. In the studies in Class III and Class IV, the main problem involves the characterization of rules via lower and upper approximations. Most studies focus on models and approaches. In order to discriminate the research on implementations from that on models and approaches, we consider Class V rule generation system in exact data and Class VI rule generation system in inexact data. Predki et al. developed a Rough Set Data Explorer for decision support, and Bazan et al. created a Rough Set Exploration System, which is applicable to data exploration, classification support, and knowledge discovery. Grzymała-Busse realised Learning from Examples based on Rough Sets. In this system, a set X is covered by sets termed as blocks, and rules are generated as a side effect. Ślęzak et al. considered the property of the distribution of attribute values and proposed the concept of packing in SQL. The technology is termed as infobright. In order to handle medical data sets, Tsumoto generated the PRIMEROSE system. Recently, Riza et al. developed a rough set-based package in R that employs rough set theory and fuzzy rough set theory. Finally, we consider studies in Class VI. Although information incompleteness is extremely attractive, there is a paucity of studies on Class VI. Grzymała-Busse introduced missing values into table data and extended the LERS system. It employs a few assumptions for the definition of blocks that assume a role similar to that of equivalence classes in rough sets. After defining blocks, a covering algorithm is applied to a set X of objects. We also dealt with studies in Class VI. We follow the framework of Lipski’s incomplete information databases and Orłowska’s Nondeterministic Information Systems and propose a NIS-Apriori-based rule generation. The framework is related to rough sets in applying equivalence classes although the definition of lower and upper approximations is slightly different. Therefore, certain rules and possible rules in our framework are not investigated in Class IV. We develop the NIS-Apriori algorithm that corresponds to an adjusted Apriori algorithm for the NIS case. The Apriori algorithm was proposed to obtain association rules from transaction data, and it is currently a representative algorithm for data mining. We briefly reviewed related studies based on Class I through Class VI in Table 1. We now describe the purpose of the study. Our study belongs to Class VI, and the purpose involves realizing systems that handle tables with inexact data. More specifically, we describe the following. We clarify the difference between Rough Set-based Rule Generation and Apriori-based Rule Generation. Although they both handle rules from tables, the characteristics of the obtainable rules are different. We reconsider the theory of NIS-Apriori-based rule generation with respect to the aspect of aforementioned point. Thus, we solve a computational problem. Without the solution, it is difficult to address the rules from NISs. We present a prototype system in SQL that simulates the NIS-Apriori algorithm. We propose a few unsolved new research topics related to NIS-Apriori-based rule generation. As a topic, a plausible method to estimate the actual Deterministic Information System from Nondeterministic Information System is considered. By presenting all these points, we demonstrate that NIS-Apriori-based rule generation is a significantly new framework and that it extends the research area of three-way decisions, rough sets, and granular computing. The study is organised as follows. Rough Set-based and Apriori-based rule generation in Deterministic Information Systems are reviewed in Section 2. The rule generation in Deterministic Information Systems are extended to Nondeterministic Information Systems and the difference between the two approaches is clarified in Section 3. This section also addresses the computational problem. In Section 4, the Apriori algorithm in Deterministic Information Systems is adjusted to that in Nondeterministic Information Systems. This modified algorithm is termed as the NIS-Apriori algorithm. In Section 5, a prototype system in SQL powered by the NIS-Apriori algorithm is presented. In Section 6, unsolved new topics related to NIS-Apriori-based rule generation are introduced. In Section 7, a software tool to estimate the actual Deterministic Information System from Nondeterministic Information System is presented. Finally, in Section 8 the conclusions of the study are presented.", "section": "Methodology", "doi": "10.1016/j.ins.2018.09.008", "references": [15698242, 84883688, 184146824, 198358702, 200823476, 1479674096, 1506285740, 1557533423, 1605948227, 1843766148, 1872996274, 1966699200, 1967807191, 1979029381, 1992107931, 1994974007, 1997362234, 2003017562, 2009184503, 2010855867, 2049234232, 2070813883, 2078505173, 2082266564, 2086785573, 2087740657, 2098034130, 2102720558, 2103514965, 2110582769, 2151101158, 2153676086, 2165467455, 2170755382, 2293888039, 2340020088, 2524598441, 2643203814]}
{"paragraph": "With the development of the information society, big data has become an important strategic resource, influencing the development of different fields. In certain area, a large amount of data is produced every day, stored by various institutions, serving as an intangible social wealth. If these institutions share data with each other, we can maximize the value of data through fully data mining and analyzing, thus improving knowledge discovery and promoting the development of smart products. To achieve this goal, a big data platform should be established for data storage to be shared among various institutions. Many institutions no longer keep data locally and instead they store the data on the platform with a easy and convenient approach of accessing and updating. However, the convenience comes with challenges. Considering various threats such as malicious attackers tampering data or hardware damages, data integrity becomes a major concern of all users. A service should be provided for users to ensure that the data is correctly and accurately stored in the remote platform. Is there any way to check the integrity of data, especially privacy data in a big data platform? Remote data auditing is an effective way and can solve this problem by means of remote data integrity verification without downloading all storage data. The existing remote data auditing schemes mainly use public key infrastructure and identity-based cryptography. However, PKI has the drawback of overspending on key management, so identity-based cryptography is more commonly used. After analyzing the existing schemes, we found that there is none of identity-based remote data auditing schemes which can support dynamic auditing. Moreover, these schemes cannot be extended to dynamic auditing scheme directly. In these schemes, tag generation is linked to the index of data block. For example, if a data block m2 is inserted after m1, for each block whose index is behind, its tag has to be recomputed, which leads to large computation cost. So these schemes cannot be extended to dynamic auditing scheme directly. Based on the scheme of , we propose an identity-based dynamic data auditing scheme for big data storage. The main contributions of our work are: (1) We design an identity-based dynamic data auditing scheme that is capable of performing dynamic auditing for big data storage. Dynamic data auditing is an important feature in current identity-based remote data auditing schemes. With proper algorithms to generate block tags based on bilinear map, our scheme first achieves dynamic data auditing in identity-based remote data auditing schemes. (2) To guarantee the data is updated correctly each time, we use a data structure, namely Merkle hash tree, to authenticate block tags and support dynamic operation with integrity assurance. With this data structure, our scheme efficiently supports dynamic data operations, including modification, insertion and deletion. This paper is structured as follows. In Section 2, we introduce the existing schemes. In Section 3, we introduce the preliminaries. In Section 4, we introduce the system model. Section 5 focuses on the description of the proposed dynamic data auditing scheme. Then we give the detailed analysis of correctness, security and performance in Section 6. Section 7 is our conclusion.", "section": "Methodology", "doi": "10.1007/s11704-018-8117-6", "references": [89553247, 1984462474, 2014698831, 2027703952, 2040193872, 2079493184, 2080488820, 2102881299, 2120622033, 2123069643, 2125858711, 2317570624, 2344670370, 2591257771, 2782004535, 2782861098, 2785346526]}
{"paragraph": "In 2009, from Pawlak’s rough sets, decision-theoretic rough sets on the basis of Bayesian decision theory and some problems in model selection, environmentally precautionary decision-making, statistical process control, suitable rough set model selection for data analysis, information filtering model on the Web and Web-Based Support Systems, Yao put the concept of three-way decisions such that the existing models become the particular examples of this formulation. Three-way decisions, as an extension of the classical two-way decisions, possess three sorts of decision rules, that is, acceptance rules, rejection rules and uncertainty rules. And then, for any object in a universe, it can be assigned into one of the three regions, that is, positive region determined by acceptance rules, negative region determined by rejection rules and boundary region determined by uncertainty rules. It is in recent years that three-way decisions have had a fast development both in theory and applications. In theory, in 2010, Yao discussed three-way decision rules in the classical rough set model and the decision-theoretic rough set model. In 2011, Yao pointed out that under certain circumstances when taking into account the costs of different kinds of miss-classifications, probabilistic three-way decisions are superior to probabilistic two-way decisions and qualitative three-way decisions of the standard rough set model. In 2012, Yao showed an outline of the theory of three-way decisions through investigating its basic ingredients, interpretations and relationships to other theories. In 2014, based on the decision-theoretic rough set model, Liang and Liu took the interval-valued loss function into account and investigated its new decision mechanisms. They also obtained the criteria for opting an appropriate method to three-way decisions with interval-valued decision-theoretic rough sets. Deng and Yao proposed decision-theoretic three-way approximations of fuzzy sets. At the same time, after a systematically research to all classes of rough sets, especially to probabilistic rough sets, Hu introduced axiomatic definitions for decision measurement, decision condition and decision evaluation function, and proposed the theory of three-way decision spaces. The author also gave a lot of three-way decisions on three-way decision spaces such that the existing three-way decisions become the peculiar examples of three-way decisions spaces discussed. Xiao et al. investigated three-way decisions of type-2 fuzzy sets and interval-valued type-2 fuzzy sets based on partially ordered sets with involutive negations. At the same time, Hu gave out an aggregation method from multiple three-way decisions spaces to a single three-way decision space by an axiomatic complement-preserving aggregation function. Le and Hu offered a summary of investigation on the three-way decisions based on covering rough sets through applying the approach of decision-theoretic rough sets. Li and Wang discussed approximate concept construction with three-way decisions and attribute reduction in incomplete contexts. Hu introduced the concept of semi-decision evaluation functions and proposed three-way decisions based on semi-three-way decision spaces. Hu et al. also investigated a new type of three-way decisions in three-way decision spaces and discussed some related properties. Yao et al. presented a general framework to research three-way or three-valued approximations of fuzzy sets. Li et al. investigated generalized three-way decision models based on subset evaluation. Li et al. discussed generalized matroids based on three-way decision models. Qiao and Hu proposed the transformation methods from semi-three-way decision spaces to three-way decision spaces based on triangular norms and triangular conorms. In applications, Liu et al. obtained a profit-based three-way approach to investment decision-making based on decision-theoretic rough set model. Yang and Yao studied a multi-agent decision-theoretic rough set model and expressed it in the form of three-way decisions. Liang et al. obtained a certain class of novel three-way decisions based on the Bayesian decision process by thinking of the two kinds of parameters, which are used in the three-way decisions with linguistic evaluation. They also gave an adaptive algorithm to amend the inconsistency of multi-attribute group decision-making under linguistic evaluation. In addition, Yao and Azam generalized the game-theoretic rough set model to investigate uncertainty referred to medical decision-making. Liu et al. proposed a novel three-way decision model based on incomplete information system. Li et al. introduced a sequential three-way decision method for cost-sensitive face recognition. Peters and Ramanna established a framework for proximal three-way decisions based on Delaunay triangulations in the study of socio-spatial properties of location-based social networks. Yu et al. studied a novel tree-based incremental overlapping clustering method using the three-way decision theory. Liang et al. introduced group decision-making into three-way decisions with decision-theoretic rough sets and proposed group decision-making based three-way decisions. Savchenko investigated an application of sequential three-way decisions and granular computing to the problem of multi-class statistical recognition of the objects, which can be represented as a sequence of independent homogeneous segments. Chen et al. showed the multi-granular three-way decision algorithm to reduce the boundary regions. Lang et al. introduced the notions of probabilistic conflict, neutral and allied sets of conflicts by using decision-theoretic rough sets and presented an algorithm for computing the probabilistic conflict, neutral and allied sets in information systems. At first, we recall the definition of decision evaluation function in order to explain the motivation of this research more precisely. In the following Definition 1.1, we assume that and are two partially ordered sets with involutive negations and respectively. In addition, let U be a nonempty universe to make a decision on it, called decision universe and V be a nonempty universe where condition function is defined, named condition universe. Let U be a decision universe and V be a condition universe. Then a mapping is called a decision evaluation function of U, if it satisfies the following axioms. On the one hand, it has been pointed out by many discussions that, in three-way decisions, decision evaluation functions play a pretty important role in decision-making. On the other hand, in realistic decision-making problems, decision-makers always rely on decision evaluation functions to obtain decision results. Different evaluation functions determine different decision results. For example, we have stated in our previous work that if we consider Example 4.1, then, we have the following two different decision-making results according to different decision evaluation functions. Then three-way decisions are as follows. From the above two cases, we can see that the final grade of student x6 is uncertain whether he or she is excellent or fail at one function. But, from the other function, we can judge accurately that the final grade of student x6 is fail. Thus, there arise many discussions investigating the construction methods of decision evaluation functions from the theoretical point of view. In addition, there arises a key problem as follows: Question: How to obtain decision evaluation functions as much as possible? To solve this problem, we can consider the following two ways. The first way is to consider more general domain of decision evaluation functions. Since hesitant fuzzy sets, interval-valued hesitant fuzzy sets and other generalized meaningful structures from fuzzy sets cannot compose fuzzy lattices in general, it follows that the theory of three-way decision spaces introduced on fuzzy lattice imposes restrictions on extensive applications of this theory to practical decision-marking problems. Therefore, Hu extended measurement on decision conclusion in three-way decision spaces from fuzzy lattices to partially ordered sets with involutive negations. Moreover, as an application, the author systematically investigated three-way decision spaces and three-way decisions based on hesitant fuzzy sets and interval-valued hesitant fuzzy sets and got a great deal of useful decision evaluation functions. The second way is to generalize the axiomatic definition of decision evaluation functions. On the one hand, we have stated in Definition 1.1 above that decision evaluation function is defined by three axioms, that is, minimum element axiom, monotonicity axiom and complement axiom. In addition, minimum element axiom and monotonicity axiom can be easily satisfied by many functions while complement axiom cannot be naturally satisfied. On the other hand, many existing discussions show that the complement axiom is very important and pretty essential for three-way decisions. Thus, the concept of semi-decision evaluation function without complement axiom was introduced and the transformation methods from semi-three-way decision spaces to three-way decision spaces were given. However, in Definition 1.1 above, notice that the complement axiom uses the involutive negations. Meanwhile, it is well known that negations are more general than involutive negations and it has been proposed and applied in many aspects. Therefore, in this paper, we attempt to use negations to define complement axiom of decision evaluation functions, which can be regarded as another way to extend the axiomatic definition of decision evaluation functions different from the work proposed previously. By this way, the existing decision evaluation functions become the special cases of the generalized decision evaluation functions discussed in this paper and we can obtain more decision evaluation functions from the theoretical point of view. Moreover, from the application point of view, it provides convenience for decision-makers to choose more suitable decision evaluation functions to make decisions in practical decision-making problems. This is also the motivation of this research. This paper is structured as follows. In Section 2, we give the measures of generalized three-way decisions based on partially ordered sets with negations and the axiomatic definition of generalized decision evaluation functions, and we also establish generalized three-way decision spaces based on the partially ordered sets with negations. In Section 3, we propose the theory of generalized three-way decisions on generalized three-way decision spaces based on partially ordered sets with negations, which includes generalized three-way decisions, the lower and upper approximations induced by generalized three-way decisions, generalized three-way decisions of multiple generalized three-way decision spaces and so on. In Section 4, some new types of decision evaluation functions and generalized three-way decisions are established, such as generalized decision evaluation functions and generalized three-way decisions based on fuzzy sets, interval-valued fuzzy sets, fuzzy relations, shadowed sets and hesitant fuzzy sets. In Section 5, we show a practical application of the results obtained in this paper. Finally, our researches are concluded.", "section": "Related Work", "doi": "10.1016/j.ins.2018.07.032", "references": [101345952, 964455177, 1021182246, 1041128124, 1567021454, 1606022329, 1704544815, 1834402812, 1861960977, 1883715000, 1969535228, 2006873874, 2023750115, 2025295306, 2029526940, 2048472139, 2050237791, 2060952501, 2070813883, 2080404663, 2089923511, 2131390763, 2167293341, 2174369037, 2176795090, 2217596628, 2252128398, 2272142993, 2272554295, 2340020088, 2499835732, 2551396410, 2560804083, 2561843635, 2580172056, 2605693713, 2620114837, 2740407498, 2774054418, 2912565176]}
{"paragraph": "In machine learning, data are usually described as points in a vector space. Nowadays, structured data are ubiquitous. The capability to capture the structural relationships among the data points can be particularly useful in improving the effectiveness of the models trained on them. To this aim, graphs are widely employed to represent this kind of information in terms of nodes or vertices and edges, including local and spatial information arising from data. For instance, consider a d-dimensional dataset representing points in space, a graph can be extracted by considering each point as a node, where edge connectivity and weights can be computed using a metric function. A new data representation is obtained, where there is a set which contains vertices and a set of weighted pairs of vertices representing edges. Applications in a graph domain can be usually divided into two main categories: vertex-focused and graph-focused. The former includes tasks where one performs classification or regression on the vertices of a graph, whereas in the latter one performs these tasks on the graph itself. For instance, object detection and image annotation are examples of vertex-focused applications, the former consists of finding whether an image contains a given object and its position, the latter consists of extracting a caption that describes an image. Another possible example could be web page classification, where the web is represented by a graph where nodes are the pages and edges are the hyperlinks between them, the aim being to exploit the web connectivity to classify pages in a set of topics. Instead, estimating the probability of a chemical compound to cause certain diseases can be seen as a graph-focused application. This is possible due to the fact that a chemical compound can be modeled by a graph where the nodes are the atoms and the edges the chemical bonds. For the sake of simplicity and without loss of generality, just the classification problem is considered. Under this setting, the vertex-focused applications are characterized by a set of labels, a dataset, and the related graph. Let us assume there is a subset of labeled nodes. The goal is to classify the unlabeled nodes by exploiting jointly the node features and the graph structure by means of a semi-supervised learning approach. Graph-focused applications are related to the goal of learning a function that maps different graphs to integer values by taking into account the features of the nodes of each graph. This task can be solved by supervised classification on the graphs. A number of research works are devoted to classification, both for vertex-focused and graph-focused applications. Nevertheless, there is a major limitation in existing studies: most of these research works are focused on static graphs. However, many real world graph-structured data are dynamic and nodes or edges in the graphs may change over time. In such dynamic scenarios, temporal information can also play an important role. For instance, the interactions between individuals inside a building in one day can be modeled as a sequence of graphs, each one describing a time window within the day, where the nodes are the people and the edges are the interactions occurring between them within a time frame. As another example, consider the classification of human activities from motion-capture data, where each frame can be modeled as a graph, where the vertices are the skeleton joints of the person in question. In the last decade, neural networks have shown their great power and flexibility by learning to represent the world as a nested hierarchy of concepts, achieving outstanding results in many different fields of application. It is important to underline that just a few research works have been devoted to encoding graph structures directly using a neural network model. Among them, to the best of the authors’ knowledge, no one is able to manage dynamic graphs. To exploit both graph-structured data and temporal information through the use of a neural network model, two novel approaches are introduced. They combine Long Short Term-Memory network and Graph Convolutional Network, which can be considered the two base elements of the proposed architectures. Both of them are able to deal with vertex-focused applications. Respectively, these techniques are able to capture temporal information and to properly manage graph-structured data. The approaches are also extended to deal with graph-focused applications. LSTMs are a special kind of Recurrent Neural Network, which are able to improve the learning of long term dependencies. All RNNs take the form of a chain of repeating modules of neural networks. Precisely, RNNs are artificial neural networks where connections among units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic sequential behavior. In standard RNNs, the repeating module is based on a simple structure, such as a single unit. LSTMs extend the repeating module by combining four interacting units. Specifically, it is based on: cell state, forget gate, input gate and output gate. The most important part of LSTMs is the cell state, which can be pictured as a conveyor belt. It runs straight down the entire chain, with some linear interactions. The first interaction decides what information is going to be removed from the cell state. This decision is made by a sigmoid layer, called the forget gate. The next interaction decides what new information will be stored in the cell state. This can be operationalized in two parts: a sigmoid layer, called the input gate decides which values should be updated, and another sigmoid layer creates a vector of new candidate values that could be added to the cell state. The final output is generated by combining the cell state with a sigmoid layer. A GCN is a neural network model that directly encodes graph structure, which is trained on a supervised target loss for all the nodes with labels. This approach is able to distribute the gradient information from the supervised loss and to learn representations exploiting both labeled and unlabeled nodes, thus achieving state-of-the-art results. The goal of a GCN is to learn a function of signals or features on a graph which takes as input a feature description for each node, summarized in a feature matrix, and a representative description of the graph structure in matrix form, typically in the form of an adjacency matrix. This model produces a node-level output, a feature matrix where the number of output features corresponds to each node. A GCN shares its underlying intuition with Convolutional Neural Networks, specialized kinds of neural networks that are highly successful in practical applications such as computer vision employing, within some of their layers, a convolutional operation in place of regular matrix multiplication. A convolutional layer in a CNN exploits the locality of information embedded in its input to extract local features. It does this by repeatedly looking at small patches of its input and by learning a mixing matrix of the size of the patches that is shared across all the patches, thus reducing the amount of parameters. Similarly, a GCN exploits the locality notion induced by the graph connectivity by looking at small patches of the graph, where the patches are all neighbourhood subgraphs of the original graph.", "section": "Introduction", "doi": "10.1016/j.patcog.2019.107000", "references": [145505233, 1522301498, 1662382123, 1869398109, 1968103943, 1968528416, 2004109568, 2004646046, 2051436535, 2064675550, 2065130322, 2086370422, 2100741829, 2116341502, 2117024994, 2124386111, 2139823104, 2143516773, 2146055337, 2154851992, 2158787690, 2173027866, 2244807774, 2366141641, 2468907370, 2519887557, 2557283755, 2610153490, 2740060125, 2754759618, 2771111398, 2950635152, 2953170998, 2963165299]}
{"paragraph": "Since the inception of digital medical imaging equipment, significant attention has been drawn towards applying image processing techniques in analyzing medical images. Multidisciplinary researchers have been working diligently for decades to develop automated diagnosis systems, and to this day it is one of the most active research areas. The task of a computer-aided medical image analysis tool is twofold: segmentation and diagnosis. In the general Semantic Segmentation problem, the objective is to partition an image into a set of non-overlapping regions, which allows the homogeneous pixels to be clustered together. However, in the context of medical images, the interest often lies in distinguishing some interesting areas of the image only, like the tumor regions, organs etc. This enables the doctors to analyze only the significant parts of the otherwise incomprehensible multimodal medical images. Furthermore, often the segmented images are used to compute various features that may be leveraged in the diagnosis. Therefore, image segmentation is of utmost importance and has tremendous application in the domain of Biomedical Engineering. Owing to the profound significance of medical image segmentation and the complexity associated with doing that manually, a vast number of automated medical image segmentation methods have been developed, mostly focusing on images of specific modalities. In the early days, simple rule-based approaches were followed; however, those methods failed to maintain robustness when tested on a huge variety of data. Consequently, more adaptive algorithms were developed relying on geometric shape priors with tools of soft-computing and fuzzy systems. Nevertheless, these methods suffer from human biases and cannot deal with the variances in real-world data. Recent advancements in deep learning have shown a lot of promises towards solving issues discussed above. In this regard, Convolutional Neural Networks CNN have been the most ground-breaking addition, which are dominating the field of Computer Vision. CNNs have been responsible for the phenomenal advancements in tasks like object classification, object localization etc., and the continuous improvements to CNN architectures are bringing about further radical progress. Semantic Segmentation tasks have also been revolutionalized by Convolutional Networks. Since CNNs are more intuitive in performing object classification, Ciresan, Giusti, Gambardella, and Schmidhuber presented a sliding window based pipeline to perform semantic segmentation using CNN. Long, Shelhamer, and Darrell proposed a fully convolutional network FCN to perform end-to-end image segmentation, which outperformed the existing approaches. Badrinarayanan, Kendall, and Cipolla improved upon FCN, by developing a novel architecture, namely, SegNet. SegNet consists of a 13 layer deep encoder network that extracts spatial features from the image, and a corresponding 13 layer deep decoder network that up-samples the feature maps to predict the segmentation masks. Chen, Papandreou, Kokkinos, Murphy, and Yuille presented DeepLab and performed semantic segmentation using atrous convolutions. In spite of initiating a breakthrough in computer vision tasks, a major drawback of the CNN architectures is that they require massive volumes of training data. Unfortunately, in the context of medical images, not only the acquisition of images is expensive and complicated, accurate annotation thereof adds even more to the complexity. Nevertheless, CNNs have shown great promises in medical image segmentation in recent years, and most of the credits go to U-Net. The structure of U-Net is quite similar to SegNet, comprising an encoder and a decoder network. Furthermore, the corresponding layers of the encoder and decoder network are connected by skip connections, prior to a pooling and subsequent to a deconvolution operation respectively. U-Net has been showing impressive potential in segmenting medical images, even with a scarce amount of labeled training data, to the extent that it has become the de-facto standard in medical image segmentation. U-Net and U-Net like models have been successfully used in segmenting biomedical images of neuronal structures, liver, skin lesion, colon histology, kidney, vascular boundary, lung nodule, prostate, etc. and the list goes on. In this paper, in parallel to appreciating the capabilities of U-Net, the most popular and successful deep learning model for biomedical image segmentation, we diligently scrutinize the network architecture to discover some potential scopes of improvement. We argue and hypothesize that the U-Net architecture may be lacking in certain criteria and based on contemporary advancements in the field of deep computer vision, we propose some alterations to it. In the sequel, we develop a novel model called MultiResUNet, an enhanced version of U-Net, that we believe will significantly advance the state of the art in the domain of general multimodal biomedical image segmentation. We put our model to test using a variety of medical images originating from different modalities, and even with 3D medical images. From extensive experimentation with this diverse set of medical images, it is found that MultiResUNet overshadows the classical U-Net model in all the cases even with slightly less number of parameters. The contributions of this paper can be summarized as follows. We analyze the U-Net model architecture in depth and conjecture some potential opportunities for further enhancements. Based on the probable scopes for improvement, we propose MultiResUNet, which is an enhanced version of the standard U-Net architecture. We experiment with different public medical image datasets of different modalities, and MultiResUNet shows superior performance. We also experiment with a standard 3D extension of MultiResUNet on a particular 3D MRI dataset, and it outperforms the enhanced 3D U-Net as well. We qualitatively examine some very challenging images and observe a significant improvement in using MultiResUNet over U-Net.", "section": "Methodology", "doi": "10.1016/j.neunet.2019.08.025", "references": [143162329, 1487583988, 1522301498, 1641498739, 1680392829, 1686810756, 1998865404, 2008359794, 2060090293, 2101234009, 2104311408, 2116719896, 2134993189, 2144982973, 2146502635, 2167510172, 2170410701, 2274287116, 2288892845, 2293078015, 2346705140, 2402144811, 2412782625, 2432481613, 2464708700, 2471801048, 2517954747, 2523246573, 2526009326, 2557283755, 2584017349, 2586952804, 2592929672, 2592939477, 2604790786, 2618530766, 2752517284, 2762526173, 2794825826, 2949117887, 2949605076, 2949650786, 2950179405, 2952036906, 2952232639, 2952596663]}
{"paragraph": "Exam timetabling is a very important and time-critical task that faces registrars every semester in most educational institutions. This task is very complex and requires assigning a date, time, and room to every exam while ensuring that time, spatial, and other constraints are satisfied. With the increasing number of students, exams, and demands, solving the Exam Timetabling Problem manually is not a practical option and hence providing a computational solution for it attracted the attention of many researchers from the 1960s until recently. The ETP is known to be an NP-complete optimization problem, meaning it is unlikely to solve it optimally in polynomial time. Therefore, researchers tried and are still experimenting with many different ways that are based on mathematical models, heuristic techniques, or a combination of different algorithms to find quick and acceptable solutions for it. For example, in the survey presented in one study, the exam timetabling research was classified based on the techniques used to solve the problems such as: graph based, local search based, population based, and other methods. In summary, the proposed solutions compare to each other based on the used techniques, the hybridization of different methods, the decomposition of the problem into smaller sub-problems, and the hard and soft constraints taken into consideration. In this paper, a new technique is introduced to solve the ETP with emphasis on issues related to German Jordanian University and its set of rules and limiting constraints. The proposed method has three novelties. First, a feasible solution to the problem is found by segmenting it into three phases, which reduces the number of constraints to be considered in each phase and hence allows the optimizer to reach a desired solution in a quick manner with reduced memory demands. Noting that, an Integer Linear Programming based approach is used to find the solution for each phase. Second, a comprehensive set of hard constraints is considered to generate an exams schedule that is comfortable to all students and meets the needs of the different faculties at the university. Third, the same exam can be allocated to one or multiple rooms unlike most of the similar techniques. Similarly to the method in this paper, several other papers discussed integer programming based techniques to solve the ETP. However, to the best of our knowledge, most of those methods tried to solve the problem in one phase, which usually results in a system of equations with a large number of variables and hence solving it is very CPU and memory intensive. Unlike other methods, the hard constraint to prevent a student from having two or more exams in the same day as in this paper was not considered in some prior studies. In one approach, a hybrid adaptive decomposition approach was used to break the exams into difficult and easy sets before using an ILP approach to obtain a solution. Some of the approaches that did not use a mathematical ILP model to solve the ETP will be briefly discussed next. Such approaches either used graph coloring, metaheuristic, hybrid, or other methods to find a solution for the ETP. They can also be categorized into two groups based on whether all the hard constraints in this paper were considered or not. The first group of papers, the group that considered the set of hard constraints as in this paper, is discussed first. In one method, two column generation algorithms were used to solve the ETP. Another study utilized hyper-heuristic approaches. An adaptive linear combination of heuristics with a heuristic modifier under the framework of adaptive strategies was proposed. A search algorithm that consists of several phases is introduced. In the first construction phase, a complete solution is found using an iterative forward search algorithm. In the later phases, a local optimum is found using a combination of a hill climbing algorithm and great deluge technique. Another method hybridized bin packing heuristics to assign exams to time slots and rooms. In another study, the solution is based on graph coloring heuristics that were hybridized to generate four new low level heuristics. In one approach, the ETP was solved using a variable neighborhood search methodology. A random iterative graph based hyper-heuristic was used to produce a collection of heuristic sequences to construct solutions of different quality. Next, the second group of papers, that did not consider all hard constraints used in this paper, is presented. For example, a hybrid bee colony optimization approach was used. In another method, a sequential graph coloring with the largest enrolment first heuristic was used to construct a conflict-free examination timetable and then a simulated annealing heuristic was used to fit examinations into rooms, while satisfying the back-to-back constraint. In other studies, hill climbing and great deluge local search were used to solve the problem. A hybrid harmony search algorithm was used. Decomposition as well as a graph coloring heuristic was used. A hyper-heuristic approach was used. Heuristics and a stochastic algorithm called the roulette wheel graph coloring were used to solve the problem. In that method, the algorithm was also tested on the examination timetabling benchmark datasets. A graph-coloring-based method was utilized, but without considering the actual distribution of exam sessions to rooms. In one approach, the solution of the course-timetabling problem was used to construct an initial solution to the examination timetable. Finally, a hybrid two phase method was introduced to tackle the ETP. The rest of the paper is organized as follows. In Section 2, the hard constraints under consideration and an overview of the proposed method are provided. In Section 3, the adopted nomenclature, problem setup, and decomposing the ETP into three sub-problems are discussed. In Section 4, a simple example is used to show how the problem formulation is represented in AMPL format. In Section 5, the method is validated based on GJU registration information and real-world benchmark data. Finally in Section 6, conclusions and future works are presented.", "section": "Introduction", "doi": "10.1111/itor.12471", "references": [174805181, 178109621, 1526848900, 1964072303, 1966773610, 1969214516, 1977049223, 1982486082, 1984754775, 1989380511, 1995123252, 1996746644, 2010046755, 2014856631, 2031012845, 2038015627, 2047956445, 2056956492, 2074281851, 2082961839, 2092487519, 2121904912, 2293848204, 2586084087]}
{"paragraph": "In many engineering systems, various failures are often encountered due to aging, wear, and other instability factors, such as sensor failures, actuator failures, uncertain internal failures, and so on. As we all know, the failures always affect the normal operation and even destroy the performance of the systems, where actuator faults play a pivotal role in these faults since they have a direct impact on the systems. Therefore, it is of both theoretical and practical importance to explore actuator fault-tolerant control strategies. Many scholars have devoted themselves to the study of this problem and have achieved some fruitful results. Over the past decades, adaptive technology has been greatly developed and a set of systematic theory has been gradually formed and widely used to solve various control problems. For example, adaptive fault compensation protocols are proposed for a class of uncertain nonlinear systems subject to actuator failures with measurable states. The problem of adaptive disturbance suppression for generalized uncertain nonlinear systems is solved by combining adaptive technique with adding a power integrator method. A distributed observer approach is proposed with adaptive technique for multiagent systems. Some works deal with actuator faults for unknown uncertain nonlinear systems with unmeasurable states by using adaptive neural network or fuzzy technologies. With the rapid development of networking and information technology, it has become a trend to connect control systems and plants through network communication channels in recent years. Network-based controls are widely used in various practical engineering systems, such as oil exploration on offshore platforms, smart grids, multiagent systems, and so on. Unfortunately, all the aforementioned references about the actuator failures require real-time transmission of control signals to the actuators, which increases the communication burden and transmission costs. Therefore, how to reduce the required computation cost and save communication resources has attracted extensive attention of scholars and a tremendous effort has been made into it. For this reason, event-triggered control is rapidly evolving with its unique advantages in conserving communication resources and maintaining competitive control performance. Compared with the conventional periodic sampling technique, the event-triggered control takes the behavior of the systems into consideration rather than relies on the real-time state solely. Such a new control method produces the samplings only when the state vector of the systems or output deviate from a certain predetermined threshold, so it has more advantages in saving energy resources and reducing computational cost. To our best knowledge, the research about the event-based control for linear systems have been relatively perfect and some significant results have been achieved. Things become particularly challenging when we attempt to achieve the same control effect for nonlinear systems. Fortunately, many outstanding achievements on the event-based control for nonlinear systems have been emerging in recent years. More specifically, approximation-based event-triggered control for multi-input multi-output continuous time systems has been proposed. An event-based adaptive control strategy for a class of nonlinear systems was presented with the trigger mechanism in the controller-to-actuator channels, while the trigger mechanism was intercalated in the sensor-to-controller channels. Some works consider the event-based control for the strict-feedback nonlinear systems with actuator faults. Furthermore, an event-driven stochastic adaptive dynamic programming technique was introduced for nonlinear systems which ensured ultimate boundedness of the systems. It is worth mentioning that the main limitation of those works is to assume that the states of the systems are measurable completely. However, it is not realistic to obtain all the states of the system in the practice systems. It is not a trivial matter to design the event-triggered controller only by utilizing the output instead of all the states of the systems. The exciting thing is that event-based output feedback control to nonlinear systems has been proposed by introducing a novel nonlinear observer and constructing dynamic controllers, respectively. Nevertheless, some works need to assume input-to-state stability for the measurement errors, while others need to presuppose that the dynamic controllers render the system uniformly asymptotically stable in the absence of a network. Therefore, the object of this paper is to devise the event-based output feedback control law for unknown nonlinear systems with the actuator failures in absence of such assumptions. As far as we know, there are no results about the event-based output feedback control for unknown nonlinear systems with actuator failures so far. The reasons may come from the inherent difficulties of designing the event-triggered output feedback control law for the unknown nonlinear systems with the uncertainty of actuator faults, including unknown occurrence time and unknown mode. In this paper, by constructing neural network state observer and combining adaptive technique, the event-triggered output-feedback controller for the uncertain nonlinear systems with the actuator faults is designed, which ensures that the output signal track the desired signal, and all the signals of the closed-loop systems are bounded. The main contributions or difficulties of this paper are highlighted as follows. This paper is the first to address the problem of event-triggered output feedback control for unknown nonlinear systems with actuator failures. Unlike earlier articles that only deal with actuator failures, the setting of the event triggering mechanism for the system with actuator failures makes the control design not a trivial matter. The controller is finally designed by using the properties of the hyperbolic tangent function to handle the event-triggered errors and designing adaptive compensation mechanisms to compensate for the actuator faults. The system to be considered is more general than those in relevant literature. The reason lies in that this system contains unknown nonlinearities, unknown external disturbances, unknown control gain, and unpredictable actuator faults. Another challenge is to design the state observer with adaptive compensation. Compared with prior work, the coupling of the unknown control direction and the loss of effectiveness fault makes the design of the state observer more painful. A new method to design the parameter adaptive law by estimating the product of the loss of control effectiveness rate and unknown control direction is proposed.", "section": "Methodology", "doi": "10.1109/TCYB.2018.2868169", "references": [1970540608, 1972107234, 1978921618, 2023230804, 2028579885, 2030629074, 2064710808, 2078804574, 2081079276, 2119493620, 2142072309, 2150535417, 2332899049, 2343782047, 2344109271, 2417709453, 2479280896, 2514514867, 2515370279, 2525747818, 2560013550, 2597767335, 2613419728, 2747580379, 2765965898, 2768680573, 2768779900, 2770213313, 2808419467, 2963713359]}
{"paragraph": "With the development of automation technology, an increased number of tasks can be completed by robots, such as grasping and assembling. To complete these tasks with random placement, it is essential to obtain the information about the position and pose of the object via visual sensors. To completely describe the pose and position, six values are required i.e., six-dimensional pose. There are three main methods for the estimation of the six-dimensional pose of an object. The first two methods make use of three-dimensional cameras or multiocular cameras, respectively. For both the methods, a three-dimensional point cloud is obtained, and the pose is then determined using subsequent algorithms, e.g., the iterative closest point algorithm. The multiocular vision method requires many matched point pairs, which is impractical for metal parts because of their texture-less property. Moreover, three-dimensional-based methods are not applicable for smooth metal parts, given that their surfaces are shiny and highly reflective. The third method uses a single monocular camera to obtain a two-dimensional image, and then uses edge information to calculate the pose. This method is more suitable for the pose estimation of metal parts. In recent years, several researches have been conducted on the six-dimensional pose estimation of objects using monocular cameras. The monocular camera methods can be divided into two approaches. The first approach is based on real images. In this approach, many real images of an object in different six-dimensional poses are saved in a database as templates. LINE-2-D, which is a method for fast three-dimensional object instance detection, is robust against small image transformations. Posenet is a robust and fast monocular six-dimensional relocalization system, which can train a convolutional neural network to regress the six-dimensional pose of the camera from a single RGB image. Crivellaro et al. trained a convolutional neural network to predict the six-dimensional pose, and the advantage is that it can obtain the pose even when only a few parts are visible. The real-image-based methods use real images as templates. However, features in these images cannot be accurately extracted, which decreases the accuracy of the final result. In addition, the procurement of many real-image templates is time consuming and challenging. The second approach is based on three-dimensional computer-aided design models. In contrast to the real-image-based approach, the computer-aided design-model-based approach is more suitable for industrial products. The virtual images generated by the computer-aided design models are more accurate than the real images, as the render process will not be affected by illumination or blur. In addition, the three-dimensional coordinates of the vertices can be easily obtained in the render process. Moreover, computer-aided design models can be easily obtained in industrial applications. A hierarchical model was proposed, which combined a coarse-to-fine search with the similarity scores calculated between a template and a real image or between templates. A perspectively cumulated orientation feature was proposed, which is based on the orientation histograms extracted from randomly generated two-dimensional projection images using computer-aided design models. Muñoz et al. proposed the use of edge correspondences to estimate poses, with a similarity measure encoded using a precomputed linear-regression matrix. The fine pose parts-based model was introduced to localize objects in an image and to estimate their fine pose using the given computer-aided design models. A novel convolutional neural network specifically trained by rendered images to estimate the object viewpoint from two-dimensional images was proposed. Despite the advancement of computer-aided design-model-based methods, they are difficult to apply in industry because of several drawbacks as follows. High complexity: Many templates are required, which necessitates a large amount of storage space and computational time. For embedded devices, the problem is more severe. Low robustness: In a complex industrial environment, the false estimation rate is high, which indicates very large estimation errors. It is difficult to meet the requirements of industrial production. Unsatisfactory accuracy: The estimation accuracy is not sufficiently high to meet the high precision requirements of some industrial applications. The primary aim of this paper is to address these issues and promote a practical six-dimensional pose estimation technology for industrial applications in relation to metal parts. Unlike the existing methods that use raw edge features, the proposed method uses higher level geometric features to represent the part. Moreover, different from the existing methods that directly use the result of feature matching, the special location points on the geometric features are further exploited to accurately estimate the six-dimensional pose. Practical algorithms are also proposed to implement the proposed approach. The experimental results validate the correctness of the proposed approach and the effectiveness of the algorithms. Moreover, the number of templates required for the proposed method is significantly smaller than that required for the existing methods. Thus, the sparsity referred to in this paper is relative to the existing methods that require thousands of templates. The main contributions of this paper are twofold as follows. A new approach for the six-dimensional pose estimation of metal parts is proposed to achieve a high precision and robustness with sparse templates. The main idea is to first use the geometric features of the part to match the real image and the template image, and then to use the special position points on the matched geometric features to calculate the accurate pose. For better implementation, the correlation of straight contours was selected as the high-level geometric feature to describe the metal part, and the matched endpoints of the straight contours in the real and the virtual images were used to accurately calculate the pose. Practical algorithms are proposed to implement the six-dimensional pose estimation approach. On the basis of the existing basic line detection and description features, namely, the line segment detector and the Bunch Of Lines Descriptor, modifications were made to address their drawbacks, to implement the proposed approach. The complete line segment detector to detect the straight contours and the grayscale-invert-invariance Bunch Of Lines Descriptor for the high-level geometric feature to describe the correlation of the straight contours were proposed. Finally, the EPnP algorithm was applied to calculate the accurate pose on the basis of the match of the grayscale-invert-invariance Bunch Of Lines Descriptor features. This paper is organized as follows. The motivation and the proposed approach are presented. Practical algorithms are proposed, and the experimental results are presented. Finally, this paper concludes.", "section": "Related Work", "doi": "10.1109/TIE.2019.2897539", "references": [1519402791, 1798868054, 1883517952, 1991544872, 2005489441, 2049981393, 2058960217, 2082586922, 2085334966, 2097097854, 2101199297, 2119605622, 2127600839, 2138835141, 2151103935, 2156953558, 2162117114, 2166799319, 2200124539, 2791846047]}
{"paragraph": "Imagine that you are designing multimedia contents of an immersive virtual reality based game where players can see, hear, and touch virtual objects. You assign a faded wooden image texture to a virtual prop used in the game play and try to create a realistic haptic texture for it as well. One straightforward way is to find a real surface having haptic properties same as that you want to assign, measure or copy its haptic responses, and replay it during interaction, analogous to photos taken from real scenes used as image textures in three-dimensional graphics. Furthermore, it would facilitate content generation if we could freely edit the perceptual property of the real measurement, e.g., creating a new haptic texture having a slightly increased roughness from a real surface, a new texture where the roughness value is inherited from one and hardness from another, and a texture that can be perceived as lying exactly in the middle of two real textures. Efficiently creating such textures from real textures is the goal of this paper, and we call this as haptic texture authoring. In the contemporary world, the above examples are not quite possible yet, and this leads to a sub-standard level of realism and immersion for VR and augmented reality applications. This lack of realistic tactile contents is one of the major hindrances for haptics technology to become widely applicable. The main reason of this bottleneck is that haptic signals are relatively difficult to author compared to other modalities. They are usually based on very complex dynamics of very subtle physical characteristics of a surface, which usually need huge effort to measure, model, parameterize, and simulate in real time. Data-driven approach, which reproduces haptic signals based on interpolations of premeasured data, can be an alternative, but this also has inherent drawbacks: the major one being lack of flexibility. This naturally leads us to the need of an authoring tool that allows for creating rich and realistic tactile contents with little effort. In general, successful authoring of sensory contents needs two technological prerequisites. First, final stimuli to be delivered to the user should be objectively describable. Stimulus that a content designer desires to create should be defined in a description space, so that it can be accurately replicable. The description space can be based on either physical dimensions, e.g., Red Green Blue space in color, or perceptual dimension, e.g., decibel space in sound, but authoring needs a way of transforming one space to the other. Second, according to the given description of the desired sensation, its corresponding physical stimuli should be accurately synthesized. Synthesizing can be a combination of various processes: physical simulation of a signal, e.g., computer graphics and modifying or merging existing signals, e.g., sound synthesizer. In this paper, we first select surface haptic texture as the subject of authoring among various haptic properties, as it is one of the perceptually important tactile properties of a surface. With regards to haptic texture, there are a few attempts at examining the first prerequisite, i.e., objective description of haptic texture. Unlike other properties that use physical dimensions for description, e.g., stiffness in N/m, color in RGB, most attempts for haptic texture use perceptual dimensions, since physical attributes involved in haptic texture perception are multidimensional, and the relationship between physical signals and their perception is not clearly revealed. Thus, many researchers have focused on the perceptual dimensions and affective properties of touch. Perceptual dimensions are the characteristics based on which humans are able to differentiate various textures. Affective properties of touch are the characteristics that quantify feeling of a given texture. Pioneering work in identifying perceptual texture dimensions was done by Yoshida. They found out that the main dimensions of haptic textures were hard-soft, heavy-light, cold-warm, and rough-smooth. Others extended his work to tool–surface interaction. It was shown that tool-based texture perception is highly related to the hard-soft dimension, whereas rough–smooth dimension is also of great importance. Similarly, various affective properties of texture were presented. Although these studies successfully identified the dimensions and affective properties of tactile perception, their goal was the dimension itself, and no further study was carried out to find the relationship between perceptual space and corresponding physical signal space, which is necessary for actual authoring, i.e., creation, manipulation, or control of tactile stimuli. The second prerequisite has been extensively studied under the name of haptic texture modeling. Physics-based modeling and data-driven modeling are the two prominent ones, and both techniques come with their own advantages and limitations. The physics based method has been a common approach employed by various researchers to render haptic content, where the haptic responses due to tactile properties of a virtual surface are determined by coefficients of physics-based parametric models. For example, high-frequency textural vibrations were generated based on the simulation of contact dynamics of microscale geometry of surface made by parameterized cavity and bump models mapped into a surface or using stochastic surface geometry models. Although the designer usually has full control over all the parameters and aspects, such a method cannot replicate the complexity of real-life surfaces due to simplification in the models. In addition, the designer has to manually incorporate the delicacies and nuances of real surfaces into a synthetic surface, which is quite a demanding task. In data-driven modeling, the vibrations originating from interaction with different surfaces are recorded and are subsequently used for rendering tactile contents. For instance, virtually perceptible textures have been generated based on the scanning velocity and normal force. Similarly, this idea has been extended to recreate more complex textures by incorporating the direction of scan velocity into the equation. Recently, a more robust and efficient technique has been employed where generative adversarial networks have been trained to create vibrotactile signals based on texture images or attributes albeit using predefined and constrained tool–surface interaction. The upside of data-driven modeling is that the created contents are highly realistic and computationally simpler. However, the recorded model is an arbitrary signal having no physical meaning and is hard to modify meaningfully. This indicates that the number of feedbacks that a designer can generate are limited. In addition, it is impossible to create contents that are not physically available, and model building is a highly time-consuming process. In summary, on one hand, the physics-based models do not guarantee a high level of realism but can be controlled easily. On the other hand, data-driven models ensure higher realism with limited controllability and authoring power. Increasing realism of physics-based approaches generally come at a very high computational cost, which often make a system nonpractical. Instead, it seems that more feasible solution would be to keep the data-driven approach and to focus on improving controllability of data-driven models. The goal of this paper is to provide an effective method for haptic texture authoring using data-driven haptic texture modeling. We achieve this goal through two contributions. We first established an authoring space where 25 data-driven texture models build from 25 fully featured real surfaces are placed according to their affective properties. The space is made in such a way that it maximizes the correspondence between affective properties of the 25 models and features in the physical signals of the models. Axes of the space are the affective properties, and this space plays a role as a perception-based descriptor of textures. Now, designers can freely select an arbitrary point in the space to author a texture, and then the system automatically synthesizes new texture signal corresponding to the selected affective properties. Our second contribution lies in this part. Our framework interpolates signals from adjacent data-driven models, so that two different haptic models are combined to form the new virtual texture. This step ensures that the new model inherits perceptual characteristics of the parent textures, allowing the aforementioned authoring scenarios. To the best of our knowledge, there is no such work that provides the approximation of physical properties across two different texture models. The significance of this paper can be explained through an analogy from the field of vision. It is well-known that the RGB space can be used to create most of the colors perceivable to the human eye. Image editing tools often provide an RGB color table where a designer can easily select a color to be used. In a similar way, through this paper, we want to provide a unified haptic authoring tool comprising of the basic components or dimensions of haptic texture. Such a tool can be utilized by designers and researchers to create haptic models having arbitrary affective properties and would drastically reduce the time and effort required for haptic modeling.", "section": "Related Work", "doi": "10.1109/TIE.2019.2914572", "references": [1514388995, 2017337590, 2021455339, 2080317558, 2107714869, 2138652678, 2140473646, 2158019650, 2162503006, 2345981848, 2505055330, 2727054250, 2729554829, 2806485483, 2807622017]}
{"paragraph": "Information security is one of the core issues in communications. Conventionally, cryptographic approach is used to provide confidentiality by encrypting the raw information bits with secret keys. With the recent advances of wireless signal processing technologies, there is flourishing interest in achieving or enhancing the information security at the physical layer, which is known as physical-layer (PHY) security. The notion of PHY security can be dated back to 1970’s in Wyner’s pioneer work. Its main idea is to exploit the difference in decoding capabilities (or channel capacities) between the legitimate receivers and the eavesdroppers, and encode the confidential information with some structured redundancy, so that the legitimate receivers can correctly decode the information with arbitrary small error probability, whereas the eavesdroppers cannot retrieve any useful information from their receptions. Since the appearance of Wyner’s seminal work, which first proved that PHY security is possible for degraded wiretap channel, the studies of PHY security have been generalized to various scenarios, including the nondegraded channels and more recently the MIMO writetap channels. Massive MIMO is regarded as one of the key technologies in future wireless communications. By deploying large-scale antennas at the transmitter, it does not only bring substantial spectral gains, but also provides new opportunities for PHY security. Specifically, Zhu et al. first studied PHY security in multicell massive MIMO system. Assuming maximum ratio transmission (MRT) precoder and nullspace artificial noise (AN), the ergodic secrecy rate was analyzed for massive MIMO under both perfect channel state information (CSI) and imperfect CSI. Following this, more sophisticated precoder designs based on matrix polynomials were proposed by the same authors. Their results reveal that the ergodic secrecy rate is generally not monotonically increasing with the number of antennas at the base station (BS), owing to pilot contamination. The work studied the effect of active eavesdropping in massive MIMO, and derived an asymptotic achievable secrecy rate under matched filter precoding. By exploiting large spatial degrees of freedom of massive MIMO, several active eavesdropper detection schemes have also been proposed. Despite the effectiveness of using massive antennas to combat eavesdropping, the increased transmit dimensions also complicate the signal processing and scale up hardware costs, as each antenna requires a dedicated RF chain. In light of this, constant modulus (CM) precoding, which can be easily implemented with low-cost phase shifters and a single variable gain amplifier, has been seen as a promising solution for cheap deployment of massive MIMO. The authors investigated the CM precoder designs for conventional massive MIMO without security concern. As for PHY security, a two-stage approach to sequentially optimizing the CM precoders for information-bearing signal and AN was developed. A phase-only zero-forcing (ZF) precoder design was considered for large-scale antenna arrays. More recently, an ergodic secrecy rate lower bound was characterized under both CM precoding and hybrid precoding. We should mention that all the above PHY security works consider either a single legitimate receiver or a group of legitimate receivers with one distinct message for each receiver, i.e., unicasting. There are another line of works concerning PHY multicasting security, where a common confidential message is delivered to multiple legitimate receivers simultaneously. Specifically, a multicast secrecy rate maximization (SRM) problem was first considered for MISO channels with a focus on the small antenna regime and the total transmit power constraint. The scaling law of multicast secrecy rate with respect to (w.r.t.) the number of antennas, legitimate receivers and eavesdroppers was analyzed. Apart from multicasting pure confidential information, a joint multicast beamforming and wireless power transfer has recently also gained much attention. In this work, we consider PHY secure multicast beamforming with multiple legitimate receivers and eavesdroppers. Different from the existing multicasting works, we focus on massive transmit antennas regime and CM precoding at each transmit antenna. It is well known that under the total transmit power constraint and a single legitimate receiver, the optimal secure beamforming admits a closed-form solution via generalized eigendecomposition. However, with CM signaling, the principle generalized eigenvector is generally no longer optimal or even feasible, and a naive projection onto the CM set may incur significant secrecy rate performance loss. In light of this, we explicitly take the CM constraints into the beamforming design and develop two approaches to maximizing the multicast secrecy rate. The first approach exploits the semidefinite relaxation (SDR) technique and the Charnes-Cooper transformation to relax the nonconvex CM precoding as a semidefinite program (SDP). The SDR-based approach is effective when the number of the transmit antennas is small. However, for large-scale transmit antennas, it suffers from high computation complexity, owing to lifting the variable dimension from vector space to matrix space. To circumvent the dimensionality problem, we further propose a low-complexity nonconvex alternating direction method of multipliers (ADMM), which works directly over the original CM precoder space. Specifically, a smooth approximation to the multicast secrecy rate function is first applied, and then the Dinkelbach method is employed to turn the smoothed problem into a sequence of nonconvex problems, which can be efficiently handled by nonconvex ADMM with closed-form update; the convergence of the nonconvex ADMM is also established. Extensive simulation results demonstrate that the Dinkelbach method outperforms the SDR method in both secrecy performance and runtime.", "section": "Related Work", "doi": "10.1109/TIFS.2019.2916687", "references": [751677133, 1733425272, 1974708997, 1996215314, 2007104311, 2009173367, 2028821485, 2043769961, 2052898460, 2057565382, 2093031827, 2121204123, 2128153585, 2133406100, 2140578794, 2144007657, 2162678712, 2164278908, 2170733622, 2211148590, 2288475778, 2295652899, 2516510987, 2516674956, 2522991281, 2528890243, 2540251752, 2769715161, 2782809764, 2963166698, 2963466796, 2964113506]}
{"paragraph": "Sensor technology has experienced important advances lately, allowing us to measure various aspects of the objects on the surface of Earth. Remotely sensed hyperspectral images provide wealthy spectral information to uniquely discriminate various materials of interest, leading to finer classification of land-cover classes. However, in certain circumstances, it may be necessary to resort to different source to complement the information provided solely by hyperspectral instruments for further improving and or refining classification. For this purpose, a series of approaches have been investigated in the literature for fusion of data collected from different sources. Light detection and ranging data, which provide elevation information about the surveyed area, are very useful source for complementing the information provided solely by hyperspectral images. Collaborative classification of hyperspectral images and LiDAR has been extensively employed in various applications, such as complex area classification, forest fire management, etc., due to its fine behavior. Numerous studies have indicated that classification performance can be improved after integrating hyperspectral and LiDAR data. For example, LiDAR was used for the scene segmentation and hyperspectral data for classifying the segmented regions; morphological extinction-profiles were exploited to extract both hyperspectral and LiDAR features; and extinction profiles were utilized for joint feature extraction, followed by total variation component analysis for further fusion. On the other hand, it was emphasized that simple concatenation or stacking of features such as morphological attribute profiles may contain redundant information, and despite the simplicity of such feature fusion methods, the fusion systems may not perform better or even worse than using a single type of features. This is due to the fact that the element values of different features may be significantly unbalanced, and the information contained by different features is not equally represented or measured. It was further pointed out that the dimensional increasement of the stacked features and the limited number of labeled samples may cause the issue of the curse of dimensionality. Thus, a decision-fusion method for hyperspectral and LiDAR data classification was presented; besides, linear and nonlinear features were also combined through a decision fusion strategy. Although these decision fusion-based researches have shown excellent performance on classification task, they cannot be a feasible solution to deal with limited training samples, merely avoiding the features extraction process with more demand for training samples. How to extract joint features containing complete information of hyperspectral and LiDAR data, without suffering from Hughes effect, still faces great challenges. Traditionally, human-engineered features depending on the experts’ experience and parameter setting have been the main workhorse for classification tasks; nevertheless, it was further pointed out that it is difficult to find appropriate parameters to generate features for different classification tasks. Recently, deep learning-based methods have broadly replaced hand-engineered approaches in many domains, and have aroused wide attention for their capabilities of automatically extracting robust and high-level features, which are known to be generally invariant to most local changes of the input, at deeper layers. Saliency features in remote sensing scenes were explored to build networks for scene classification. The general way for constructing deep networks of remotely sensing images has been systematically analyzed, which attempted to evaluate the effectiveness of all the state-of-the-art deep learning methods on remote sensing images. For the sake of extracting high-level features in hyperspectral images, a deep learning architecture with multilayer stacked auto-encoder was constructed through an unsupervised manner. The convolutional neural network, which needs fewer parameters than fully connected networks with the same number of hidden units, has drawn increasing attention in image analysis. For example, CNN was exploited to extract features for hyperspectral image classification and obtained excellent performance. In addition, a set of improved methods based on CNN were used for remote sensing classification tasks and yielded excellent performance. CNN-based methods can yield promising results only when a sufficient supply of labeled training samples is ensured; unfortunately, only a small number of labeled samples is available for training in practical situations, especially for remote sensing data. In other words, the supervised CNN generally suffers from either limited training samples or imbalanced data sets. Meanwhile, deep models are also trained to seek feature representations by means of unsupervised learning methods. Unsupervised feature learning, which has a quick access to arbitrary amounts of unlabeled data, has become the focus of concern in both academia and industry. In general, the chief aim of unsupervised feature learning is to extract useful features from unlabeled data, detect and eliminate input redundancies, and preserve only essential aspects of the data in robust and discriminative representations. A pioneer work moving from the supervised CNN to unsupervised CNN for learning spectral–spatial features was proposed, which was based on sparse learning to estimate the network weights. However, this model was trained in a greedy layer-wise fashion, i.e., not an end-to-end network. The image-to-image translation networks, belonging to an end-to-end network, mapped an image from one domain to another domain for learning the translation function. Moreover, feature representation with sufficient information can be effectively explored in an automatic learning process of the end-to-end network, auto-encoder, by minimizing the reconstruction error between an input sample and its reconstruction. In this paper, an intuitive yet effective procedure for the joint feature extraction of hyperspectral and LiDAR data via a patch-to-patch CNN is investigated. The proposed framework refers to an unsupervised feature extraction framework via a patch-to-patch CNN to learn joint features between hyperspectral and LiDAR data without labeled data considerations. Patch-to-patch CNN is based on the so-called encoder–decoder translation architecture. Specifically, the input source hyperspectral data is first mapped into a hidden subspace via an encoding path encoder, and then reversed to reconstruct the output source LiDAR data by a decoding path decoder. Meanwhile, the hidden representation within the translation procedure can be deemed as fused features of hyperspectral and LiDAR data. After that, features derived from different hidden layers of patch-to-patch CNN are integrated by the hierarchical fusion module, afterward converted into a stacked vector and fed into three fully connected layers to produce the final classification map. A multiscale patch-to-patch CNN is proposed for feature extraction, which consists of three tunnels covering a convolutional filter bank with well-designed structure. Learning such an end-to-end network for joint feature extraction of hyperspectral and LiDAR data has not been studied yet to the best of our knowledge. Different hidden nodes in the network are capable of precisely capturing attributes in different information levels, which ensures convenient access to multiscale joint features. The hierarchical fusion module can optimally exploit joint features extracted by the patch-to-patch CNN for classification. Through the module, initial spatial–spectral features obtained from the multiscale filter bank are then combined together to form a joint spatial–spectral feature map. The feature map, representing rich spectral and spatial properties of hyperspectral and LiDAR data, is then fed into a convolution-based block for multilayer block concatenation operation. The neural weights learning process of the designed patch-to-patch CNN is completely unsupervised, which is independent of labeled samples. In the proposed patch-to-patch CNN, training patches are first collected by adopting a sliding window from the hyperspectral and LiDAR data, and the learning process based on these patches can ensure the perfectibility of features even with small-size labeled samples. The remainder of this paper is organized as follows. Some related works are introduced in Section II. The proposed methodology is described in Section III. The experimental results are discussed in Section IV. The conclusion is summarized in Section V.", "section": "Related Work", "doi": "10.1109/TCYB.2018.2864670", "references": [1497089125, 1836465849, 1901129140, 1903029394, 2011085793, 2029316659, 2029992428, 2067874135, 2086866337, 2097117768, 2127152713, 2147800946, 2165796970, 2179290474, 2320738207, 2320846209, 2518815253, 2548791488, 2565258258, 2572303978, 2592141703, 2598997103, 2606929568, 2611655888, 2614256707, 2719511702, 2757208835, 2764205729, 2765739551, 2768537477, 2791928749, 2963446712]}
{"paragraph": "Enforcing Quality-of-Service requirements in the deployment of Internet-of-Things software systems is a non-trivial yet necessary task to accomplish. Indeed, such systems are often developed in large, highly distributed, multi-service architectures, which are to be deployed to complex, heterogeneous and highly distributed infrastructures, spanning the Cloud-IoT continuum. Cloud-Edge computing extends Cloud computing towards the edge of the Internet to better support latency-sensitive and bandwidth-hungry IoT applications by mapping service application functionalities wherever it is best-placed to suitably meet all the application hardware, software and QoS requirements. Various works have tackled the problem of determining optimal placements and management of application services, mainly taking into account resource usage, deployment costs, network QoS such as latency, response time, bandwidth, and energy consumption. Considering these aspects all together is of primary importance in Cloud-Edge scenarios, where many life-critical or mission-critical application verticals can significantly suffer from performance degradation due to bad resource allocation or insufficient Internet connectivity. Analogously, their management might aim at reducing operational costs due to power consumption or to resource leasing to increase profit. However, to the best of our knowledge no approach has been proposed that accounts for the security requirements of the application to be deployed and matches them to the security capabilities available at different infrastructure nodes. Security of ICT solutions is an intrinsically complex problem, which requires reasoning about a system model by analysing its security properties and their effectiveness against potential attacks, and accounting for trust relations among different involved stakeholders such as infrastructure and application operators. In addition, any methodology for optimal service placement that accounts for security should in principle enable decision-makers to understand why a certain deployment can be considered optimal, i.e. the provided recommendations should be explainable. Indeed, explainable artificial intelligence techniques are getting more attention from the security community since they can provide a concise explanation of the query results. In the case of determining secure and trustworthy deployments of an application, for instance, explainable AI aims at answering questions like why is this deployment more secure than this other, and why and how much are they secure. As an extension to the Cloud, Cloud-Edge will share with it many security threats, while including its new peculiar ones. On the one hand, Cloud-Edge will increase the number of security enforcement points by allowing local processing of private data closer to the IoT sources. On the other hand, new infrastructures will have to face brand new threats for what concerns the physical vulnerability of devices. Indeed, application deployments to Cloud-Edge infrastructures will often include accessible Edge or IoT devices that may be easily hacked, broken or even stolen by malicious users and that can only offer a limited set of security capabilities. Last but not least, as Cloud-Edge application deployments will likely span various service providers some of which may be not trustworthy, trust must be considered when deciding where to place application services. Trust relations are especially important at the edge of the Internet, due to the uncertainty derived from dealing with multiple infrastructure providers. Indeed, the security levels and reputation of Edge computing capabilities might be as heterogeneous as their hardware capabilities, and might give rise to security zones, i.e. collections of assets that share the exposure to certain security risks. Thus, in Cloud-Edge scenarios, where part of the application could be deployed to a mixture of well-established such as Clouds and ISPs and opportunistic infrastructures such as crowd-computing and ad-hoc networks, trust models are needed to estimate trust levels towards unknown providers, possibly aggregating also trusted providers’ opinions. All this considered, the move of utility computing towards the edge of the network and in continuity with existing Clouds calls for new quantitative and explainable methodologies that permit to assess the security level of distributed multi-service IoT applications. Such methodologies should take into account application requirements, security countermeasures featured by the Cloud-Edge infrastructure, and trust relations in place among different stakeholders that manage or use Cloud-Edge infrastructures. In this paper, we propose a first step towards well-founded and declarative reasoning methodologies for security- and trust-aware multi-service application deployment in Cloud-Edge scenarios. Our proposal, SecFog, helps application operators in Cloud-Edge scenarios in determining the most secure application deployments by reducing manual tuning and by considering specific application requirements, infrastructure capabilities and trust. SecFog has been prototyped in the ProbLog language and, as we will show, the prototype can be used together with existing approaches that solve the problem of mapping IoT application services to Cloud-Edge infrastructures according to requirements other than security and trust such as hardware, network QoS, and cost. A generalised algebraic extension of the prototype, SecFog, is also presented. Such an extension features the possibility of using different semiring-based trust models so to weight and assess the security level of eligible application deployments. The rest of this paper is organised as follows. After briefly introducing the ProbLog language needed to understand the proposed solution, we detail the SecFog methodology and its implementation, while showing how it can be used on simple examples, relying on a simple default probabilistic trust model. Afterwards, we describe a larger lifelike example of secure application deployment and illustrate how the SecFog prototype can be used along with other existing tools for application placement in Cloud-Edge scenarios. Then, we discuss how SecFog can embed more complex trust models based on the default one, and we present an algebraic extension of the basic prototype, SecFog, that permits relying on arbitrary semiring-based trust models. Finally, after discussing related work, we draw some conclusions and point to some directions for future work.", "section": "Methodology", "doi": "10.1016/j.future.2019.08.020", "references": [811924890, 1824971879, 1967880951, 1979916122, 1986481844, 2019413422, 2042828098, 2061860832, 2062802747, 2071680577, 2078852715, 2089427307, 2123479684, 2131776987, 2133842176, 2134126205, 2168205742, 2315975036, 2317218987, 2319396036, 2326074602, 2342649568, 2401898190, 2414114959, 2434290318, 2481050413, 2517368285, 2517523272, 2522538405, 2524194345, 2534678535, 2536604331, 2551296122, 2573834517, 2579405208, 2582140688, 2592591519, 2594728687, 2736823879, 2745755793, 2747974859, 2753672491, 2763925543, 2783073469, 2785585629, 2789240493, 2792810743, 2795893807, 2801779586, 2806691503, 2808439263, 2808445014, 2885291512, 2899450160, 2903434459, 2912690303, 2912771882, 2970056989]}
{"paragraph": "As the Internet and mobile phones have evolved over the years, technology to improve remote communication has developed immensely. It is clear that mobile phone users prefer text messaging over phone calls, with the number of monthly text messages worldwide increasing from 395 billion in 2011 to 561 billion in 2014, and the number is still growing. In a computer-mediated communication such as text messaging, a cues-filtered-out perspective and social presence theory argue that computer-mediated communication lacks nonverbal communication cues and thus fails to communicate emotions and attitudes to receivers. Also, prior studies showed that viewing online text without emoticons caused misinterpretation of the nature of the message and the sender’s intent. There have also been studies that state that the emotional attitude of the text is more important when it comes to the interpretation, but a negative emoji can even change the interpretation of the message. For instance, when somebody sends a neutral text such as “That man came to see me again,” it is difficult to know whether the sender is annoyed or glad. Emojis can clarify these kinds of ambiguity and even show the intensity of the emotion, as well. Many researchers have suggested that written communication can be enhanced through visual cues in the same way visual or body language supports verbal communication. In computer-mediated communication, which has become so common today, users attach emoticons in their text messages to fill in the missing socioemotional context. Previous studies suggested that emoticons can provide the missing information and enhance computer-mediated communication. These visual cues have been identified as the primary way to express emotions in computer-mediated communication and as a way to replace nonverbal communication when face-to-face interaction is not possible. Emoticons have been defined as graphic representations of facial expressions that are embedded in electronic messages. The usual distinction between the terms emojis and emoticons is that the term emoticons is commonly used to refer to punctuation signs combined on the keyboard to convey nonverbal communication, such as the smiley face or a wink, whereas emojis are pictures that convey nonverbal information. This article uses the term emojis because the final tactile design examined here employed imagery rather than a combination of punctuation marks. Today, emojis are used worldwide in countries that use electronic devices to communicate. Since Apple created its Apple Color Emoji set, providing support for Unicode emojis, it has been used in all iOS and OS X devices and in many others. Japan and Korea, the countries with the largest mobile markets in the world, use Line and KakaoTalk for their messaging platforms. It is said that there are now more than 22 million emojis in KakaoTalk and Line, and KakaoTalk calculated that, of messages that contained emojis, emojis composed 15 percent of the conversation. In 2012, the daily sales of emojis totaled approximately 100 million Korean won. Throughout these many developments and changes in online communication, visually impaired individuals have often been left out. Although companies such as Apple, KakaoTalk, and Google are continuously developing technologies for people with visual impairments, most of these are based on audio guidance. Audio guidance is useful in its own way, but it has limitations; it cannot provide privacy without the use of headphones, which can create dangerous situations for the visually impaired, as their ears must be occupied. Also, audio guidance cannot be heard readily in a noisy environment. Studies on techniques for tactile delivery of emotional cues have also been progressing using vibration patterns, person-to-person haptic responses, and vibrotactile displays. However, as many of these were not developed for visually impaired people, they still do not provide the ultimate solution to limitations on visually disabled persons’ texting experience. The goal of this study, therefore, was to show that image-based tactile emojis can offer a solution for the future texting experience of individuals with visual impairments. Also, based on prior studies indicating that congenitally blind people are able to comprehend facial expressions, and research that implies face processing as a bimodal phenomenon where vision can be substituted by touch, this study examined the question of whether, with a short introduction, congenitally blind individuals, who have no prior visual knowledge, are capable of understanding emojis that sighted people use. To achieve these goals, the following steps were taken: We conducted a preliminary study to identify the connection between emotions and emojis. For the ten most commonly selected emojis for each emotion, we organized the facial features separately into eyes, eyebrows, and mouth and converted the images into textual descriptions. We ranked the separately organized facial features by weighting the features most often chosen. We created image-based tactile emoji designs based on the previously defined descriptions of the six emotions. We conducted interviews using the resultant apparatus and analyzed the results. The provision of image-based tactile emojis, when applied in the real world, will provide privacy for a visually impaired person’s texting service and equality in technology that has not previously existed. It will also improve textual communications for visually impaired people by reducing misunderstandings and making the texting experience more enjoyable.", "section": "Methodology", "doi": "10.1080/07370024.2017.1324305", "references": [40549020, 1978040282, 2012461879, 2020728888, 2024641146, 2062769641, 2096073495, 2100928276, 2101282523, 2107208519, 2118459787, 2129353167, 2133180260, 2144378002, 2146459173, 2160660844, 2161906378, 2243203301, 2296274542, 2519353570, 2574189006]}
{"paragraph": "Of the many biometrics available, a fingerprint has proven to be a very reliable form of human identification and verification, and is considered superior to other biometrics. Its dominance has been established by the continuous emergence of various automatic fingerprint identification systems (AFISs) which, during the last decade, have achieved great success and been widely used in various applications, such as forensics, personal electronic products, secure payments and building access. Current AFISs are based mainly on contact fingerprint images (i.e., wet-inked or live-scan fingerprints) which are usually captured by pressing a finger against paper or a platen. While touch-based acquisition is a mature technology, it has the following inherent drawbacks in a contact-based AFIS: 1) a high failure-to-acquisition rate caused by unqualified skin conditions, such as dry, moist, oily or worm; 2) variations and nonlinear distortions in a fingerprint induced by varying amounts of pressure during the scanning process; 3) a contaminated fingerprint caused by interference from the latent fingerprints left on a scanner platen; and 4) a hygienic risk through the transmission of pathogens from the contact surface at a scanner. Therefore, contactless fingerprint scanning technologies, which offer potential advantages over current contact-based ones, have become a significant and promising research topic. The National Institute of Standards and Technology (NIST) recently announced the development of Next-generation Fingerprint Technologies of which a contactless fingerprint one is an important and highly promising prospect. Compared with contact fingerprint images, as contactless ones are obtained without any contact between a finger and sensor, they aim to address the above issues. In addition, by using a high-resolution high-speed acquisition camera, contactless fingerprints can provide higher-quality fingerprint images with more information. Furthermore, another advantage of contactless sensor is that it does not have the process of cleaning the latent fingerprint left on the sensor platen which is required on the contact-based sensor. Hence, it will be helpful for speeding up access control and identity processing in high-traffic areas such as facility access and customs and border applications. One of the first and most challenging problems in a contactless fingerprint recognition system is handling perspective distortions with respect to the relevant camera’s optical axis which are caused mainly by various finger rotations (i.e., roll, yaw and pitch), as shown in Fig. 1. Although this problem can be alleviated by using three-dimensional (3D) fingerprints, the cost and speed of bulky 3D acquisition devices limits their popularity and applicability. For instance, a commercial TBS sensor costs thousands of dollars and it also takes about 2 seconds to generate a 3D fingerprint sample. Furthermore, the bulky 3D sensor is not suitable for mobile (smartphone) applications such as mobile internet banking services. Therefore, 2D-based biometrics matching will play a vital role in these areas, which will be our focus in this work. An interesting observation is that computational power is not an issue in such applications as biometrics authentication is done on the back-end server or private cloud. Even in the border control applications, biometrics matching is most likely to be conducted on the cloud or the central server as it is impractical to store multimillions or even billions of biometrics templates in the front-end devices. In recent years, many efforts have been made towards contactless fingerprint recognition. Inspired by the impressive success of contact fingerprint comparison methods using a mandatory strategy for recovering the geometrical transformation between two fingerprints to establish their maximum minutiae correspondence, current contactless fingerprint recognition methods are still based mainly on this technique. However, recovering transformations from contactless fingerprints with distortions is difficult as the alignments adjacent to the reference minutia are usually strong while those far from it tend to be weak or poor. Although several methods for correcting and estimating these alignments using hardware or distortion-invariant features have been developed, their performances are not very good. In this paper, we propose a new contactless fingerprint recognition method that effectively avoids the distortion problem to achieve better performances. In the proposed method, contactless fingerprint comparison is formulated as a combinatorial optimization problem with, initially, an elaborately designed energy function defined on both minutiae and minutia-pairs to incorporate the global minutia topology. Then, a two-stage strategy for determining the optimum minutiae correspondence between a fingerprint pair is developed. The stochastic search stage approximately solves the NP-hard combinatorial problem and finds an initially optimal minutiae correspondence using the proposed loose genetic algorithm (GA) while the strict minutia-pair expansion stage establishes the final minutiae correspondence using the proposed expanding algorithm. Finally, a metric which measures the comparison scores takes advantage of both the global topological similarity and number of corresponding minutiae to achieve recognition. Experimental results obtained from two contactless fingerprint benchmark databases demonstrate that our method achieve competitive performances in comparison with the state-of-the-art methods. The main contributions of this paper are summarized as follows. Firstly, in contrast to existing methods that rely mainly on recovering the transformation between two fingerprints to establish the maximum minutiae correspondence and achieve a comparison, we formulate fingerprint comparison as a combinatorial optimization problem. By optimizing the elaborately defined energy function to directly determine the minutiae correspondence, our method can effectively avoid the distortion problem inherent in contactless fingerprinting. Secondly, we use the global minutia topological similarity instead of a minutia-wise or local minutia-pair-wise one to enhance the reliability of the minutiae correspondence, thereby improving the final recognition accuracy. Thirdly, by taking into account that some minutiae do not have any corresponding minutiae, we propose a loose GA with new mutation and crossover operators that are ideally suited for determining the optimal minutiae correspondence to solve the optimization problem. Fourthly, we develop a minutia-pair expanding algorithm to further complement the solved optimal minutiae correspondence to establish the final correspondence through which the recognition accuracy can be improved. Fifthly, rather than using the corresponding minutiae number directly for recognition, we propose a metric for measuring comparison scores which takes advantage of both the global topological similarity and number of corresponding minutiae to obtain a recognition result. The rest of this paper is organized as follows: Section II reviews related works on contactless fingerprint comparison and GAs; Section III describes the proposed contactless fingerprint recognition method in detail; the experiments and results are presented in Section IV; and, finally, the paper is concluded in Section V.", "section": "Related Work", "doi": "10.1109/TIFS.2019.2918083", "references": [99032621, 1573795313, 1589975099, 1591318869, 1605738798, 1607768087, 1857711069, 1970405634, 1971129991, 1989938457, 2013603106, 2017377855, 2022152052, 2051294001, 2072004121, 2096945783, 2112864019, 2116222612, 2116480977, 2119605622, 2126105956, 2126450185, 2127717018, 2130194416, 2138975941, 2144545035, 2151103935, 2151835818, 2156719878, 2160367574, 2166820607, 2167130165, 2168397026, 2464315941, 2786242895, 2806112117]}
{"paragraph": "Non-orthogonal multiple access techniques offer promising solutions to spectrum scarcity and congestion problems in next-generation wireless networks, attributed to its efficient utilization of available resources serving multiple users simultaneously, as opposed to conventional orthogonal multiple access techniques. Owing to the broadcast nature of wireless transmissions, securing transmitted data from potential eavesdroppers or untrusted nodes in the network is a critical system design aspect that needs careful consideration. Physical layer security is a powerful tool to achieve the goal of provably unbreakable, secure communications by exploiting the inherently different physical communication channels between different nodes in the network. In this work, we design secure transmission schemes for a downlink single-input single-output non-orthogonal multiple access system considering two possible unsecure environments: the first is when there is an external eavesdropper, for which we use trusted cooperative relays to enhance security, and the second is when communication occurs through an untrusted relay node. There have been a number of recent works in the literature that study physical layer security for non-orthogonal multiple access systems. Secrecy sum rate maximization of single-input single-output systems has been studied. Using tools from stochastic geometry, some works study security measures for large-scale systems in the downlink and the uplink, respectively. Multicast-unicast streaming has also been studied, where secure rates for unicast transmission using non-orthogonal multiple access is shown to outperform conventional orthogonal schemes. Other work considers a multiple-input multiple-output two-user setting with an external eavesdropper and designs beamforming signals that maximize the secrecy sum rate. This approach is also considered in multiple-input single-output and multiple-input multiple-output scenarios in a two-user setting, with the assumption that one user is entrusted and the other is the potential eavesdropper. The impact of transmit antenna selection strategies on the secrecy outage probability has been investigated. Transmit power minimization and minimum secrecy rate maximization subject to a secrecy outage constraint have also been considered. Different from the previous works, in this work we investigate the advantages of using trusted cooperative relays to secure messages from an external eavesdropper, and also study the impact of having an untrusted relay on achievable secrecy rates in the context of non-orthogonal multiple access. Our work on using trusted cooperative relays is most closely related to single-receiver wiretap channel works in which half-duplex relays are employed to enhance security. Other works use similar ideas as well with a focus on full-duplex relays using mixed decode-and-forward and cooperative jamming strategies. Information-theoretic analysis of communication systems with untrusted relay nodes has been considered, including settings of deaf relays that are ignorant of the source’s transmitted signal in the presence of external eavesdroppers, and scenarios that involve cooperative jamming and noise forwarding schemes. Some studies investigate two-hop scenarios with an untrusted relay and provide achievable secrecy rates for a single source-destination pair and for a multi-terminal setting, respectively, with the help of cooperative jamming signals from the destinations. In another general untrusted relay channel scenario, positive secrecy rates are shown to be achievable if the source-relay channel is orthogonal to the relay-destination channel via a compress-and-forward scheme at the relay. Similar to these previous works, in this work we also use information-theoretic tools to derive achievable secrecy rate regions in the context of non-orthogonal multiple access, with trusted and untrusted relays. In the first part of this paper, we extend existing ideas to work in the context of a two-user downlink single-input single-output system with an external eavesdropper. We employ multiple trusted cooperative half-duplex relays to enhance the achievable secrecy rate region through various relaying schemes: cooperative jamming, decode-and-forward, and amplify-and-forward. For each scheme, we design secure beamforming signals at the relays that benefit the users and/or hurt the eavesdropper. Under a total system power constraint, that is divided between the base station and the relays, an achievable secrecy rate region for each relaying scheme is derived and analyzed. In general, the results in this case show that the best relaying scheme highly depends on the system parameters, in particular the distances between nodes, and that the relatively simple cooperative jamming scheme performs better than the other schemes when the relays are close to the eavesdropper. In the second part of this paper, we consider a different scenario in which an untrusted half-duplex relay node is available to assist with the base station’s transmission. Applications of this scenario are when, for example, the relay has a lower security clearance relative to the end users, and hence transmission schemes should be designed in such a way that the relay can only forward the data without revealing its actual contents. We derive achievable secrecy rate regions in this case under two relaying schemes: compress-and-forward and amplify-and-forward. We also consider two modes of operations: passive user mode and active user mode. In the passive user mode, the users receive data from both the base station and the relay and combine them efficiently to decode their messages. In the active user mode, the users transmit a cooperative jamming signal simultaneously with the base station’s transmission to further confuse the relay, and hence, since the focus is on half-duplex nodes, they cannot receive the base station’s transmission and rely solely on the data forwarded to them through the relay. We derive, analyze, and compare the achievable secrecy rate regions for each relaying scheme and operating mode under a total system power constraint, that is divided between the base station, the relay, and the users if operating in the active mode. As in the first part of the paper, the results also show in this case that the best relaying scheme and operating mode depends, in particular, on the distances between the nodes, with a general superiority of the active user mode over the passive user mode.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2911162", "references": [2011755800, 2026537412, 2088654965, 2091682073, 2097480027, 2130684118, 2137552535, 2142102521, 2148672861, 2171006634, 2277419020, 2302349451, 2551126468, 2566155463, 2582744858, 2605451877, 2620915951, 2743190787, 2761665166, 2920559781]}
{"paragraph": "Today's technical progress, and especially the Internet, allows sellers to increasingly use selling strategies that were difficult to implement only a couple of years ago. In this paper, we focus on one such strategy, namely the selling of an incompletely specified product in addition to traditional, fully specified regular products. Only after sale does the customer learn about the exact specification of the product bought. These products are increasingly used, as the following examples show. The two online travel agencies Priceline and Hotwire are well-known for their discount hotel offers where brand and exact location are hidden and only revealed to the customer after purchase. Online retailers sell products without an exact description. For example, swimoutlet.com sells the “TYR Men's Swimsuit Grab Bag Jammer”. Its description is straightforward: “You pick the size, we pick the print!” Although upgrades are widely known from the travel industry, they also occur in production. For example, CPUs for personal computers are usually offered with different base frequencies. Production is often the same for several rated speeds. The final CPUs are tested and assigned a speed. If the market now requires more low speed CPUs than produced, the firm can simply print a lower speed on the product and customers will usually not notice. Only customers who try to operate their CPU at a higher speed than designated will notice that it can do so. As in the examples mentioned above, incompletely specified products consist of a pre-specified menu of alternative component products, which usually are also sold as regular products. After sale, the firm assigns the customer to one component product. On the one hand, the firm adds complexity to its processes because it has to avoid overselling and guarantee that all customers can be served with the available resources. On the other hand, the inherent possibility for supply-side substitution has two important advantages, namely: Because of their inherent uncertainty, incompletely specified products are fundamentally different to regular products. They allow the use of an entirely new dimension for market segmentation, namely the strengths of customer preferences for the component products. This enables the firm to additionally offer a cheap, inferior product to increase its customer base without cannibalizing too much high value demand from regular products. If the seller assigns the customers to component products a while after the sale, he could benefit from additional information, for example, because demand uncertainty can be lower at this later point in time. He is thus able to improve capacity utilization. The component products are either vertically or horizontally differentiated. Vertical differentiation describes a preference relation that is shared by all customers, for example, almost everyone will prefer a business class seat instead of an economy class seat in an airplane at the same price. Horizontal differentiation relates to individual preferences, for example, some travelers prefer a hotel at a beach, others downtown or at an airport. The assignment of opaque products is decided immediately after sale. By contrast, the assignment of flexible products is postponed and decided at a later point in time. For both kinds of products, the flexibility can be explicit or implicit. For example, possible itineraries of air cargo are usually not disclosed to the customer, whereas the customer has to be informed upfront of the flexibility in the hotel and grab bag examples given above. Upgrades are probably the oldest and most widely used type of incompletely specified products. They can be offered with vertically differentiated component products only. The customer buys an inferior component product, but obtains a product or service superior to what she paid for at no extra cost. Thus, it is assumed that the customer will always happily accept the upgrade and she does not need to agree regarding the upgrade possibility before the sale, which makes upgrades largely implicit. However, she may notice when she sits down in business class instead of economy. In other examples, she may not notice at all. From a technical point of view, the upgrade can be immediate or postponed, depending on when the decision is made. Obviously, only explicit flexibility can be used for market segmentation to exploit heterogeneous customer valuations. In this paper, we focus on quantitative research that addresses incompletely specified products when selling goods and services to consumers. The key issue is that the customer does not know exactly what she buys, or vice versa, the seller does not need to decide before the sale is finalized on how exactly to provide the goods or services to the customer. However, eventually he has to serve the customer. By contrast, contingent pricing and callable products assume that sales can be revoked. This is out of this paper's scope. Incompletely specified products became popular with consumers through Priceline.com, but this travel website initially offered only a proprietary pricing scheme. This bidding scheme quickly gained a lot of attention in academia, but neglected the specific design of the products. However, research and practice quickly indicated that the benefits of incompletely specified products are independent of name-your-own-price schemes. Thus, we restrict ourselves to work that addresses incompletely specified products independent of the pricing scheme, and we exclude work on name-your-own-price with regular products. Further, the uncertainty in the agreement must relate to the product itself and not to its price. Thus, all kinds of upsells, where the customer is nudged to pay more to get a better product under certain conditions, are out of scope. Moreover, the substitution is decided by the seller, not the customer. Of course, there is also qualitative work on incompletely specified products. For example, a group around Nelson Granados addresses the question of whether to conceal product information from an Information Systems or Decision Science perspective using qualitative reasoning and anecdotal evidence.", "section": "Introduction", "doi": "10.1016/j.ejor.2019.02.008", "references": [158123262, 1564561078, 1753270033, 1961287959, 1980695207, 1982916019, 1989435499, 1993101564, 1993186069, 1996250035, 1996397603, 1997875786, 2006781871, 2032138310, 2036694592, 2042270211, 2049649337, 2050570906, 2054118319, 2054534588, 2054918418, 2056990627, 2057337671, 2063040039, 2064051799, 2065015113, 2068624010, 2093088540, 2096329790, 2098095858, 2107390991, 2107486753, 2114186597, 2115452345, 2117323737, 2119875938, 2120565988, 2121962198, 2123425465, 2124289798, 2129957543, 2134531031, 2136768128, 2139295077, 2146591451, 2147111279, 2147260289, 2151942387, 2152328268, 2156330888, 2157770306, 2159291560, 2165356874, 2166007281, 2168013498, 2170049526, 2175737555, 2191871932, 2237849545, 2276694024, 2280415854, 2285335071, 2345880184, 2401150647, 2403910177, 2468405372, 2469197509, 2514362166, 2520064264, 2589118488, 2606338754, 2616914694, 2625372943, 2781826904, 2885373296]}
{"paragraph": "Nearest neighbor search is one of the most fundamental tools in many areas of computer science, such as image recognition, machine learning, and computational linguistics. For example, one can use nearest neighbor search on image descriptors such as MNIST to recognize handwritten digits, or one can find semantically similar phrases to a given phrase by applying the word2vec embedding and finding nearest neighbors. The latter can, for example, be used to tag articles on a news website and recommend new articles to readers that have shown an interest in a certain topic. In some cases, a generic nearest neighbor search under a suitable distance or measure of similarity offers surprising quality improvements. In many applications, the data points are described by high-dimensional vectors, usually ranging from 100 to 1000 dimensions. A phenomenon called the curse of dimensionality, the existence of which is also supported by popular algorithmic hardness conjectures, tells us that to obtain the true nearest neighbors, we have to use either linear time in the size of the dataset or time/space that is exponential in the dimensionality of the dataset. In the case of massive high-dimensional datasets, this rules out efficient and exact nearest neighbor search algorithms. To obtain efficient algorithms, research has focused on allowing the returned neighbors to be an approximation of the true nearest neighbors. Usually, this means that the answer to finding the nearest neighbors to a query point is judged by how close in some technical sense the result set is to the set of true nearest neighbors. There exist many different algorithmic techniques for finding approximate nearest neighbors. Classical algorithms such as kd-trees or M-trees can simulate this by terminating the search early, for example shown by Zezula et al. for M-trees. Other techniques build a graph from the dataset, where each vertex is associated with a data point, and a vertex is adjacent to its true nearest neighbors in the data set. Others involve projecting data points into a lower-dimensional space using hashing. A lot of research has been conducted with respect to locality-sensitive hashing, but there exist many other techniques that rely on hashing for finding nearest neighbors. We note that, in the realm of LSH-based techniques, algorithms guarantee sublinear query time, but solve a problem that is only distantly related to finding the k nearest neighbors of a query point. In practice, this could mean that the algorithm runs slower than a linear scan through the data, and counter-measures have to be taken to avoid this behavior. Given the difficulty of the problem of finding nearest neighbors in high-dimensional spaces and the wide range of different solutions at hand, it is natural to ask how these algorithms perform in empirical settings. Fortunately, many of these techniques already have good implementations. This means that a new variant of an existing algorithm can show its worth by comparing itself to the many previous algorithms on a collection of standard benchmark datasets with respect to a collection of quality measures. What often happens, however, is that the evaluation of a new algorithm is based on a small set of competing algorithms and a small number of selected datasets. This approach poses problems for everyone involved: The algorithm’s authors, because competing implementations might be unavailable, they might use other conventions for input data and output of results, or the original paper might omit certain required parameter settings, and even if these are available, exhaustive experimentation can take lots of CPU time. Their reviewers and readers, because experimental results are difficult to reproduce and the selection of datasets and quality measures might appear selective. This paper proposes a way of standardizing benchmarking for nearest neighbor search algorithms, taking into account their properties and quality measures. Our benchmarking framework provides a unified approach to experimentation and comparison with existing work. The framework has already been used for experimental comparison in other papers to refer to parameter choice of algorithms and algorithms have been contributed by the community, e.g., by the authors of NMSLib and FALCONN. An earlier version of our framework is already widely used as a benchmark referred to from other websites.", "section": "Introduction", "doi": "10.1007/978-3-319-68474-1_3", "references": [53016366, 73389528, 145388748, 1430582609, 1627400044, 1870428314, 1970647353, 2055839530, 2070572105, 2075547840, 2084434464, 2084732238, 2085937320, 2086179657, 2118323718, 2122196799, 2147717514, 2149684715, 2153579005, 2157092487, 2157169955, 2165558283, 2183176930, 2318810549, 2397770138, 2480086555, 2523268797, 2532189199, 2537425075, 2736701013, 2752891636, 2756637424, 2757117395, 2774705174, 2963671040]}
{"paragraph": "In the last few years, digital image forensics has been drawing an ever increasing attention in the scientific community and beyond. With cheap and powerful cameras available to virtually anyone in the world, and the ubiquitous diffusion of social networks, images and videos have become a dominant source of information. Unfortunately, they are used not only for innocent purposes, but more and more often to shape and distort people’s opinion for commercial, political or even criminal aims. In this context, image and video manipulations are becoming very common, and increasingly dangerous for individuals and society as a whole. Driven by these phenomena, in the last decade, a large number of methods have been proposed for forgery detection and localization or camera identification. Some of them rely on semantic or physical inconsistencies, but statistical methods, based on pixel-level analyses of the data, are by far the most successful and widespread. Mostly, they exploit the fact that any acquisition device leaves on each captured image distinctive traces, much like a gun barrel leaves peculiar striations on any bullet fired by it. Statistical methods can follow both a model-based and a data-driven approach. Methods of the first class try to build mathematical models of some specific features and exploit them for forensic purposes. Popular targets of such analyses are lens aberration, camera response function, color filter array or JPEG artifacts. Having models to explain the available evidence has an obvious appeal, but also a number of shortcomings, first of all their usually narrow scope of application. As an alternative, one can rely on data-driven methods, where models are mostly abandoned, and the algorithms are trained on a suitably large number of examples. Most data-driven methods work on the so-called noise residual, that is, the noise-like signal which remains once the high-level semantic content has been removed. A noise residual can be obtained by subtracting from the image its clean version estimated by means of denoising algorithms, or by applying some high-pass filters in the spatial or transform domain. Noise residuals can be also used in a blind context to reveal local anomalies that indicate possible image tampering. Among all methods based on noise residuals, those relying on the photo-response non-uniformity noise deserve a special attention for their popularity and performance. It was observed that each individual device leaves a specific mark on all acquired images, the PRNU pattern, due to imperfections in the device manufacturing process. Because of its uniqueness, and stability in time, the PRNU pattern can be regarded as a device fingerprint, and used to carry out multiple forensic tasks. PRNU-based methods have shown excellent performance for source identification and for image forgery detection and localization. Note that they can find any type of forgeries, irrespective of their nature, since the lack of PRNU is seen as a possible clue of manipulation. The main drawbacks of PRNU-based methods are the need of a large number of images taken from the camera to obtain good estimates and the low power of the signal of interest with respect to noise, which impacts heavily on the performance. In particular, the prevailing source of noise is the high-level image content, which leaks in the PRNU due to imperfect filtering. The latter often overwhelms the information of interest, especially in the presence of saturated, dark or textured areas. This latter is a typical problem of all the methods based on noise residuals. In this work, to overcome these problems we propose a new method to extract a noise residual. Our explicit goal is to improve the rejection of semantic content and, at the same time, emphasize all the camera-related artifacts, which contribute to the digital history of an image. To this end, we follow a data driven approach and exploit deep learning. A suitable architecture is designed, inspired by Siamese networks, and trained on a large dataset which includes pristine images from many different camera models. During training we only need to know whether the extracted patches come from the same camera and position or not. Therefore, we take advantage of hidden spatial dependencies, also with the help of a suitable spectral loss. Once the training is over, the network is freezed, and can be used with no further supervision on images captured by any camera model, both inside and outside the training set. To any single image the network associates a noise residual, called noiseprint from now on, which shows clear traces of camera artifacts. Therefore, it can be regarded as a camera model fingerprint, much like the PRNU pattern represents a device fingerprint. It can also happen that image manipulations leave traces very evident in the noiseprint, such to allow easy localization even by direct inspection. As an example, two images subject to a splicing forgery can be easily detected by visual inspection of their noiseprints. It is worth observing that these artifacts cannot be spotted so clearly using other noise residuals. In the rest of the paper, we first analyze related work on noise residuals to better contextualize our proposal, then describe the proposed architecture and its training, carry out a thorough comparative performance analysis of a noiseprint-based algorithm for forgery localization, provide ideas and examples on possible uses of noiseprints for further forensic tasks, and eventually draw conclusions.", "section": "Related Work", "doi": "10.1109/TIFS.2019.2916364", "references": [1515615411, 1536019179, 1968164921, 1983586459, 1985729543, 1994743750, 2006028575, 2009130368, 2012203653, 2018377917, 2019716459, 2025328853, 2034556443, 2046180645, 2049771774, 2055745001, 2066923536, 2068575457, 2068734928, 2071794886, 2080264113, 2087361987, 2089818991, 2096076665, 2096754397, 2097506049, 2098594597, 2099013489, 2104116677, 2107461197, 2110598603, 2111374353, 2113267006, 2136492908, 2156238933, 2164990255, 2168126647, 2170343586, 2410977227, 2412509443, 2469175162, 2479919622, 2508457857, 2514123796, 2541922885, 2557414982, 2560026874, 2572561073, 2574852830, 2590464616, 2603123944, 2734722035, 2747268660, 2752015292, 2762911267, 2777769595, 2787755382, 2801560392, 2802179504, 2802701183, 2888817253, 2888922181, 2907295878, 2962958939, 2963678090, 2964146055]}
{"paragraph": "In recent years, the rapid technological advancements in wireless communication and the melding of innovations in the fields of ubiquitous sensing and pervasive computing have performed a significant role in the emerging Internet of Things (IoT) paradigm. In fact, IoT has tremendous potential to create value and provide appropriate solutions for a wide range of applications such as smart cities, security, visual sensing, image communication, and healthcare. Specifically, the IoT holds great promise for the healthcare area, where its features are already being exploited to improve the reliability of remote health monitoring systems. In remote patient monitoring systems, the personal health information (PHI) is collected by Wireless Body Area Network (WBAN) and aggregated by a data sink, such as Smartphone, tablet, or PDA. Then, the data is transmitted to the medical staff to assess the patient's status and provide the appropriate clinical diagnosis. In this context, it is critical to secure the transmitted data between the WBAN client and the remote application providers (APs) such as the hospital, physician or medical staff. In fact, the collected data should be handled, transmitted, and analyzed only by authorized parties in order to ensure accurate diagnosis and treatments. Since the patient's information is transmitted through an open channel, it can be eavesdropped, intercepted and modified. Consequently, counterfeited health-related data may mislead the caregivers to make the appropriate decision, which may convolute the patient's situation. Furthermore, the dynamics of cellular networks imposes more challenges to design robust security and access control mechanisms. As to the security facet, one of the main issues is authentication and access control of patients’ personal health information while considering the dynamic context changes to make the right decision at the right time by the right party. Therefore, it is necessary to define who can access what and under which contextual information. For this purpose, according to the information sensitivity, the data consumer role, as well as the patient's condition sensitivity, different access rights, and permissions can be assigned. For example, a nurse who has restricted access compared with a doctor in normal situations can gain additional permissions in emergency situations. In such a critical context, privacy may be restricted or relaxed given that safety is more important than security. Currently, security and privacy preservation of extra-body communication have attracted particular attention. However, most of the previous works mainly focus on either anonymous authentication or privacy preserving concerns while ignoring the dynamic context changes. On the one hand, they don't incorporate the contextual information related to the dynamic nature of cellular networks in the authentication and authorization decision. On the other hand, many proposed access control schemes suffer from the following problems: they need public key certificates, they are exposed to the impersonation attack by the Key Generator Center (KGC) as well as key escrow problem. They don't provide either ciphertext authenticity or public verification. In fact, the receiver should decrypt at first the ciphertext and then verify its validity which may waste the computation resources and increase the response delay if the ciphertext is not valid. This study is devoted to investigating the cryptographic primitives to address the above issues. We notice that there are other equally serious security concerns in WBAN, such as the key management and the access policies management for sensor nodes in the intra-body communications. However, the study of those issues is out of the scope of this paper. In this paper, we focus on extra-body communication. Specifically, we propose a novel approach for the design of a context-aware authentication and access control scheme to adaptively adjust the security and privacy level according to the contextual information. For this purpose, we use an efficient and secure Hybrid Certificateless Signcryption (H-CLSC) scheme with public verifiability and ciphertext authenticity that can simultaneously authenticate users and protect query messages. The proposed to address the secure communication problem and provide adaptive context-aware privacy. A WBAN client in a normal situation can control access to his own data. For example, by constructing the access structure ({status=normal} AND {hospital A} AND {Vascular Surgery}), the data requires that on normal situations only doctors from the Vascular Surgery Center in hospital A can have the access right. The novelties of our proposed model are summarized as follows. A novel context-aware authentication and access control approach that provides a dynamic authorization to the patient's data while considering the contextual information (patient's condition severity, data consumers’ roles, security domain...). A Hybrid Certificateless Signcryption (H-CLSC) scheme with public verifiability and ciphertext authenticity in which the validity of the ciphertext can be verified without decryption. An anonymous signcryption mechanism to provide efficient and fine-grained encrypted access control by merging the worthiness of CP-ABSC and IBBSC. The WBAN client can control who has access to his personal health information by defining an access structure for data. Dealing with the key escrow problem and impersonation attack by the KGC. The data owner/consumer's private keys aren't generated by the KGC alone but by a combination of the contributions of the KGC and the data owner/consumer. Given that the KGC can generate only the user's partial private key, it cannot decrypt messages or impersonate users. The remainder of this paper is organized as follows. Section 2 highlights some previous works related to security and privacy mechanisms in WBAN. A mathematical background is presented in Section 3. The system model and the design goals in terms of context-aware privacy and security requirements are described in Section 4. The efficient H-CLSC scheme for authentication and access control as well as its security model are given in Section 5, followed by the performance analysis in Section 6. Finally, Section 7 concludes the paper.", "section": "Related Work", "doi": "10.1016/j.cose.2019.03.017", "references": [1498316612, 1523586819, 1605099736, 1973102675, 1974267799, 1982459270, 1996912680, 2014275650, 2023460223, 2027016208, 2033223053, 2055817410, 2065043957, 2089437326, 2091299352, 2095677682, 2101018255, 2108221199, 2112320278, 2115027138, 2118875948, 2138582578, 2165939650, 2322315143, 2337833343, 2344963917, 2406910208, 2542911113, 2556834368, 2592817897, 2613367739, 2951761767]}
{"paragraph": "Clustering algorithms have become a widely used method due to their ability to provide new insights into unlabeled data sets. They consist in forming homogeneous groups of observations referred to as “clusters”. Clustering algorithms highlight the data’s inherent structure. However, the recent “big-data” phenomenon has greatly increased the number of features, leading to the emergence of high-dimensional data sets. Clustering techniques are consequently not always sufficient to discern the structure. The analysis of a cluster relies on a representative of the cluster (mean, mode etc.). However, the latter is itself described by a large number of features, which makes it more difficult to interpret and makes the summary of the data set less useful. From this consideration comes the need to also “summarize” the features, which can be done by gathering them into clusters, in parallel with the usual clustering of observations. Co-clustering methods seem to be a good option for performing this task because they perform joint clustering of rows and columns. The initially large data matrix can be summarized by a limited number of blocks that result from combining row-clusters and column-clusters. Among the most famous co-clustering techniques, the Non-negative Matrix Tri-Factorization consists in factorizing the data matrix into three matrices with the property that all three matrices have non-negative elements. More specifically, the approximation is achieved by minimizing the error function with constraints meaning that all elements of the matrices are greater than zero, and a matrix norm is chosen. The matrix represents the block matrix: an element of it summarizes the observations belonging to a row-cluster and a column-cluster. Despite the non-negative property of the matrices, it is not always easy to interpret the resulting matrices. For example, the matrices are not always normalized which makes it difficult to interpret them in terms of rows and columns belonging to corresponding clusters. Furthermore, this technique depends on the choice of the distance measure. Conversely, probabilistic approaches propose normalized membership matrices and do not require the user to choose a particular distance measure. In the Latent Block Model, referred to as “LBM”, the elements of a block are modeled by a parametric distribution. Therefore, the results give more information than a simple scalar, as mentioned in the previous methods. Each block is therefore interpretable via the parameters of the block-distribution. Moreover, model selection criteria such as the ICL criterion can be used for model selection purposes, including the choice of the number of co-clusters. This technique has proved its efficiency in co-clustering several types of data: continuous, nominal, binary, ordinal, and functional. For this reason, an extension of this model is used in the present work, although originally it was not able to take heterogeneous data as an input. Heterogeneous data sets are composed of features of different types. For example, in medicine, a patient’s file can be composed of images, text, continuous data, categorical data, and even functional data. Several clustering frameworks have been developed to address this particularity. The latent class model is frequently used. It assumes that the variables are conditionally independent upon the row-cluster membership. Consequently, the joint probability distribution function of the features of different types is obtained by the product of the PDFs of each individual feature. However, when the variables are inherently correlated in a row-cluster, this model is not suitable. To overcome this issue, some authors want to conserve standard marginal distributions but also try to loosen the conditional independence on the variables. For this purpose, they use copula, which allows definition of both the dependence model and the type of marginal distributions. The proposed model relies on the main assumption that each cluster follows a Gaussian copula. However, the authors note that model complexity increases with the number of variables, which is not suitable in a big-data context. Another way to address the issues of heterogeneous data is to see some variables as the manifestation of a latent vector. For example, one model considers continuous and categorical data and assumes that a categorical variable is the representation of an underlying latent continuous variable. Then, it is assumed that the continuous variables, observed and unobserved, follow a multivariate Gaussian mixture model. Until now, these methods have proposed models for basic data such as categorical and continuous data. In one approach, the authors allow the introduction of more complex data such as functional data or networks by projecting the data set into a reproducing kernel Hilbert space. Regarding the analysis of variables, multiblock methods, widely used in Chemistry and Biology, handle data sets that share the same observations but have variables measured differently. They aim at finding underlying relationships between these data sets. In particular, multiblock component models use latent variables to summarize the relevant information between and within the sets. However, none of these techniques were developed in a co-clustering framework. To the best of our knowledge, the only work to co-cluster heterogeneous data extends the LBM for data sets with continuous and binary data. The present work goes further by proposing an extension that can take into account four types of data: categorical, continuous, count and ordinal data. Furthermore, the inference algorithm can deal with missing values and proposes a way to impute them. Finally, the Integrated Completed Likelihood criterion is adapted to the proposed model in order to select the number of row-clusters and column-clusters. Co-clustering techniques can be seen as an efficient alternative method to the selection of variables thanks to its parsimony, especially in very high dimensions. In addition, it can produce interpretable sets of variables since it can group redundant variables or noisy variables. In this way, a first, naive answer is to manually select the informative blocks, but some alternatively define a model that automatically distinguishes the informative blocks for textual data sets. For mixed data, variable selection is more challenging. Some researchers perform clustering while incorporating variable selection and this method can produce homogeneous row-clusters. However, compared to co-clustering, it does not provide interpretable column-clusters, which may be essential for the summary of the data set, in particular with a high number of variables. The paper is organized as follows. Section 2 gives an overview of the LBM to help understanding of this paper. Then, it proposes an extension to a new LBM version that allows heterogeneous data sets. Section 3 proposes an algorithm for model inference, based on a Stochastic Expectation Maximization algorithm coupled with a Gibbs sampler. In Section 4, a description of the different types of data that can be taken into account with this method is given, and formulas for model inference are presented. Section 5 assesses the efficiency of the proposed method on simulated data while Section 6 shows how the method performs on real data sets. Section 7 provides a conclusion.", "section": "Methodology", "doi": "10.1016/j.csda.2019.106866", "references": [45199236, 583546247, 1982509351, 2067160380, 2109820980, 2144211451, 2224746910, 2460302605, 2472863357, 2552406463, 2562050914, 2597328883, 2599487788, 2727551782, 2771517242, 2794266768, 2800909518]}
{"paragraph": "With the aid of vision sensors, the flexibility and intelligence of mobile robots can be highly enhanced, and tremendous efforts have been given to the vision-based robotic systems, including localization, environment perception, and control. For the visual servoing of mobile robots, accomplishing the trajectory tracking task with a monocular camera is one of the most active topics, and is performed through the teach-by-showing technique. Specifically, a set of images are recorded to express the desired trajectory during the teaching process. Then, the mobile robot is driven to track the desired trajectory with real-time visual feedback. Since the mobile robot is a nonholonomic system and the monocular camera cannot measure the depth information directly, both the nonholonomic constraint and the system uncertainties related to the unknown depth parameters should be carefully considered in the control development. To overcome the nonholonomic constraint, various approaches have been proposed, such as adaptive controller, slide mode controller, and neural network controller. However, these approaches do not introduce the sensor layer into the system modeling, and generally assume that the states of the mobile robot can be exactly obtained. Hence, they cannot be utilized to address the visual trajectory tracking problem which involves system uncertainties. After closing the feedback loop with vision sensors, different methods can be used to construct system errors, mainly including image-based, position-based, and multiple view geometry-based methods. The image-based methods design the error signals in the 2-D image space, while the position-based methods describe them in the Euclidean space. Although the image-based methods are robust to the system disturbance, it is difficult to regulate the orientation of the mobile robot to the desired value only using 2-D image errors. The position-based methods guarantee the error convergence in the Euclidean space. However, the classical position-based strategies require a priori geometric knowledge about the environment. An alternative is to construct the system errors based on the multiple view geometry. Generally, multiply view geometry-based methods can be divided into homography, epipolar geometry, and trifocal tensor. All of these methods describe the intrinsic geometric relationship among multiple images via transformation matrices. The corresponding feature points in different views can be extracted to calculate the transformation matrices. Then, the orientation and scaled translation of the mobile robot with respect to a reference coordinate frame can be obtained based on matrix decomposition. A homogrphy-based controller is designed to accomplish the trajectory tracking task with the assumption that the desired trajectory is persistently exciting. To relax the requirement on the desired trajectory and extend the work space, a visual trajectory tracking approach is proposed by combining the trifocal tensor with the key frame strategy. Moreover, the controller can compensate for both the unknown depth and the extrinsic parameters. Instead of defining the error signals with the scaled translation component, some works utilize the reconstructed orientation and the 2-D image information to design composite system errors, and thus are called 2.5-D visual servoing. A model-free unified tracking and regulation strategy of the mobile robot is developed, which integrates the 2.5-D visual information into the controller design. For the visual trajectory tracking, the existing works based on multiple view geometry require that the pose and velocity information related to the desired trajectory is available for ease of the controller design. After capturing a set of images to describe the desired trajectory during the teaching process, two offline steps are usually carried out to obtain the corresponding desired information. First, classical estimation or decomposition algorithms are used to extract the desired pose information from the prerecorded images. Then, the velocity information of the desired trajectory, i.e., the differentiation of the desired trajectory, is computed based on the obtained desired pose and numerical algorithms. Since image noises exist and the sampling rate of a vision sensor is slow in general, numerical algorithms may lead to big errors in the desired velocities, and thus affect the control performance. To deal with this problem, more robust numerical algorithms can be exploited, but that would lead to heavier offline calculation burden. Therefore, to enhance the practical applicability of a visual trajectory tracking approach, it is necessary to develop a controller without requiring the desired velocity information. Moreover, it is well known that the lack of depth information is an inherent issue when a monocular camera is applied for the visual servoing. To compensate for the unknown depth parameters, adaptive control methods are widely utilized. However, the convergence of the depth estimates to their actual values are not guaranteed by these methods. This implies that the Euclidean space cannot be reconstructed completely, which limits the further application of the vision-based robotic system. The unified tracking and regulation control of the mobile robot and the depth identification can be simultaneously addressed provided that a persistent excitation condition is satisfied. To relax the persistent excitation condition, concurrent learning techniques are exploited to estimate the unknown depth in the visual servo control procedure under a relaxed finite excitation condition. One of the core ideas of concurrent learning techniques is to construct the terms with the state-derivatives. Nevertheless, the state-derivatives are usually estimated by using numerical differentiation techniques, which could reduce the parameter estimation accuracy. Hence, achieving the depth convergence under relaxed persistent excitation condition without numerical differentiation is motivated. In this paper, a visual servoing approach is proposed to accomplish the trajectory tracking and the depth estimation tasks of the mobile robot. Specifically, by utilizing multiple view geometry techniques, the scaled translation and orientation information of the mobile robot is obtained to construct the system errors. Considering the nonholonomic constraint and the unknown depth parameters, an adaptive time-varying controller is developed to address the trajectory tracking problem. To facilitate the control development, the desired velocity information is estimated by a reduced order observer instead of being computed offline. Moreover, an augmented update law is designed to not only compensate for the unknown depth parameters but also identify the inverse depth constant. Lyapunov-based stability analysis is presented to show that the asymptotic trajectory tracking and desired velocity estimation can be achieved, and the depth convergence is ensured provided that a persistent excitation condition is satisfied. A robust data-driven algorithm is further proposed to estimate the inverse depth under a relaxed finite excitation condition. Finally, the performance of the proposed approach is validated by both simulation and experimental results. Compared to previous works, the main contributions of this paper include the following aspects. The adaptive tracking controller is developed based on the proposal of a reduced order desired velocity observer, which can avoid tedious offline calculation. Most of visual trajectory tracking methods use adaptive update laws to only compensate for the unknown depth information, while the augmented update law designed in this paper can also successfully identify the inverse depth constant under a persistent excitation condition. To relax the persistent excitation condition and reduce the impact of system disturbance, the robust data-driven algorithm is proposed for the depth estimation without relying on numerical differentiation. The remainder of this paper is organized as follows. Section II formulates the visual servoing problem and introduces the system model. The control and estimation scheme is proposed in Section III, and the robust data-driven algorithm for the inverse depth estimation is presented in Section IV. Simulation and experimental results are provided in Sections V and VI, respectively. Finally, the conclusion is given in Section VII.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2869623", "references": [1575596356, 1980765536, 1982795744, 1986107377, 2001865981, 2028044548, 2045975141, 2054797486, 2058843767, 2069612046, 2085261163, 2098087953, 2101714570, 2111073598, 2111358287, 2124037552, 2140985802, 2141429008, 2157577161, 2168199093, 2292922312, 2345110532, 2555485441, 2561721180, 2622287508, 2736053122]}
{"paragraph": "In a 2018 report published by the U.S. Department of Homeland Security (DHS), digital relays were identified as targets vulnerable to cyber-attacks. Additionally, the DHS proved experimentally that malicious use of protective relays can severely damage rotational equipment, a cyber-security flaw called the “Aurora Vulnerability”. Therefore, cyber-resiliency of vulnerable protective devices is an important issue to address. Line current differential relays (LCDRs) are susceptible protective relays that are increasingly being used for protecting transmission systems. LCDRs are highly reliable and perform well under conditions that can be problematic for other relays, e.g., high-impedance faults or series-compensated lines. A main requirement of LCDRs is reliable communication of measured currents between the relays at the line terminals. Some LCDRs exchange the instantaneous sampled values (SVs) of currents, while others communicate current phasors. Compared to phasor-based LCDRs, SV-based LCDRs (i) reduce the amount of shared data by half, because they share instantaneous values instead of the magnitude and angle of phasors, (ii) exchange data at higher rates (e.g., a few kilohertz), (iii) are independent of the GPS, as the SVs can be synchronized by measuring the communication link latency and interpolating the received data, and (iv) share more information—such as harmonics or rate of change of currents—with the remote relays, simplifying certain LCDR requirements, such as fast detection of CT saturation. Superior performance of LCDRs is achieved at the expense of exposing the protection system to cyber threats. An attacker can tamper with the SVs and carry out a false data injection attack (FDIA) against LCDRs either by intruding into a substation’s local area network (LAN) or by breaking into the communication system. The vulnerability of substation LANs has been demonstrated by the real attack targeted the Ukrainian power system substations in 2015. The susceptibility of communication links and wide-area network (WAN) system to cyber attacks is also investigated. Several research works also demonstrate LAN vulnerabilities. In addition, the protocol based on which LCDRs communicate, i.e., IEC-61850, is also vulnerable to cyber-attacks, as shown experimentally and discussed. Thus, developing a robust intrusion detection system is necessary to maintain LCDRs’ security against cyber threats. Although the research on the cyber-security of protection systems is generally in its early stages, it has received more attention recently. Research in this area can be divided into three main groups. Studies in the first group have focused on attack modeling and risk assessment in protection systems. In, a framework is presented for modeling coordinated switching attacks over circuit breakers and relays. In, the impact of bus and transmission line protection schemes on power system cyber-security has been evaluated. In, a resilience and assessment metric is proposed to quantify the ability to and the cost required for the system to recover from an attack. Moreover, the authors of have proposed a game-theoretic graph-coloring technique to determine the optimal allocation of security mechanisms diversity that minimizes the impact of vulnerabilities to the grid. It is also shown that this technique provides a Nash equilibrium solution. In the second group, studies have concentrated on the cyber-security of substations. In, a method is proposed to detect and mitigate cyber-attacks on substation automation systems. In this method, protection devices collaboratively defend against cyber-attacks against substations, even if information and communication technology (ICT)-based techniques are compromised. To avoid malfunctions due to substations receiving fake data packages, the authors of have proposed a method whereby a substation identifies vicious attacks by using context information, such as its own voltages and currents. This method collects all measurements of the substation and feeds them to a probabilistic neural network. An event that differs from the known fault pattern is identified as an attack. Moreover, a distributed intrusion-detection system is proposed to monitor and detect anomalies in power systems and IEC-61850-based messages. A number of other studies have also proposed intrusion-detection systems for substations. The third group of studies has developed attack detection or identification method for protection systems. In, a distributed scheme is proposed, that detects attacks and differentiates them from faults using both the cyber and physical properties of power networks. In, an anomaly detection approach, which works based on zone partition, is proposed for industrial cyber-physical systems. Moreover, the authors of have presented an end-to-end attack-resilient cyber-physical security framework for wide-area monitoring, protection, and control (WAMPAC) applications. They also describe a defense-in-depth architecture and discuss several attack-resilient algorithms for WAMPAC systems. All above-mentioned studies have focused on the cyber-security of different components in protection systems. However, the cyber-security of LCDRs has barely been investigated to date. To the best of the authors’ knowledge, the only studies on this issue are and. In, FDIAs are detected by comparing the calculated and measured super-imposed voltages at each LCDR’s terminal. This method works based on the Thevenin equivalent theorem; thus, it does not work effectively when the network is highly integrated with nonlinear power system components. On the other hand, proposes a remedial pilot protection scheme, which works independently of timing information. The remedial pilot protection scheme makes LCDRs independent of the GPS, and thus LCDRs can no longer be affected by GPS signal spoofing. However, even if LCDRs are resilient against GPS spoofing, they are still vulnerable to FDIAs, since attackers can also break into LCDRs’ communication system to target them. This paper focuses on SV-based LCDRs and proposes a method to detect FDIAs using unknown input observers (UIOs). The proposed method has a positive-sequence (PS) and a negative-sequence (NS) submodule, each comprising of a UIO to estimate the states of a system based on the state-space model of the faulty line in its associated sequence. The residual function (RF) for each submodule, i.e., the difference between the measured and estimated local voltage for the sequence associated with that submodule, remains close to zero during real internal faults, because in this condition, the state-space model based on which the UIOs operate correctly represents the line. Nevertheless, the state-space model mismatch during FDIAs leads to large RFs. Thus, a rise in the RFs after an LCDR pickup signifies an FDIA, so the trip signal of the LCDR is blocked. The main assumptions in this paper is that local measurements are reliable and secure, i.e., only remote measurements are assumed to be attackable. In existing conventional substations, this assumption can be justified by sending the values measured by current and voltage transformers directly to LCDRs using copper connection between the relays and instrument transformers. In IEC-61850 based substation automation systems, however, the proposed method is applicable only if local measurements shared by merging units are secure, for example, if the process- and bay-level networks are made secured and also isolated from station-level network. The proposed method requires time-synchronized SVs of local and remote terminals, a communication platform, and a fast-enough processor. Given that the proposed method is supposed to operate as a built-in function in SV-based microprocessor digital LCDRs, most of these requirements are already in place; that is, commercial SV-based LCDRs already share time-synchronized SVs of remote current measurements using a communication system and utilize state-of-the-art processors for analyzing received measurements to detect faults. Therefore, no extra hardware or communication equipment is required. Additionally, the fast communication platforms that are already being used for sharing SVs enable the proposed method to detect attacks and differentiate them from faults quickly, in less than two cycles, which is in the range of commercial LCDRs’ operation time. However, the proposed method needs remote voltage SVs as well, which are not being shared by all commercial LCDRs. If the remote SVs are not available for a given LCDR, then the proposed method requires increasing the communication bandwidth to receive the voltage samples.", "section": "Methodology", "doi": "10.1109/TIFS.2019.2916331", "references": [1770527767, 1977719311, 2034684799, 2061842621, 2071002954, 2127548576, 2146024157, 2289011413, 2315510390, 2394622813, 2549981724, 2564348914, 2592597630, 2596906061, 2734470972, 2743967296, 2751554702, 2767792274, 2768883215, 2770337116, 2786551456, 2795786455, 2810411747, 2902126042]}
{"paragraph": "Negation is a linguistic phenomenon present in any human language, either written or spoken. It has the ability to transform an affirmative statement into its opposite meaning. The a wide range of NLP applications. Negation detection is especially important in the biomedical domain, where various linguistic forms are used widely to express impressions, hypothesized explanations of experimented results or negative findings. In tasks such as information retrieval, sometimes, only the positive information of an event should be considered. For example, while searching for patients with pneumonia, we should not include a patient who has a clinical report saying No radiographic abnormality seen of the chest. Negation detection is also important in other NLP tasks, such sentiment analysis, where detecting the negation is also a critical process, as it may change the polarity of text and result in wrong predictions. In a sentence, the presence of negation is determined by the presence of a negation cue. A negation cue is a lexical element that carries negation meaning. The scope of negation is the sequence of words in a sentence that is affected by the negation cue. In example 1, the word “not” represents the cue and the continuous word sequence “not retinoic acid treatment of the U937 cells” forms the scope. Example 1 PMA treatment, and not retinoic acid treatment of the U937 cells acts in inducing NF-KB expression in the nuclei. Prior works on negation detection focused on rule-based algorithms to find negated terms in a sentence. Later, with the release of the Bioscope corpus, negation detection problem received a special attention, where most of the research integrated machine learning techniques to solve the task, and considered it as classification or a sequence labeling problem. These methods have all been proved to be effective in solving the negation scope detection problem. However, this success depends heavily on the use of an appropriate feature set, which often requires a lot of engineering efforts for each task in hand. Deep learning methods are alternative approaches that can automatically learn latent features and minimize the feature-engineering process. These techniques proved recently to be very effective in various NLP tasks, such as machine translation, relation extraction, and sentiment analysis and outperformed most of machine learning techniques, without relying on any hand-crafted feature, or just a few. Recently, neural networks methods were also introduced for negation scope detection. Lazib et al. first used different recurrent neural network models over real-valued vector representations of negated sentences to find the scope, and Qian et al. used a CNN model over syntactic information to detect the part of the sentence that is affected by the cue. These methods both outperformed the existing feature-rich approaches. Motivated by the success of neural networks, we propose in this paper a novel neural network method to solve the negation scope detection problem that improves the performance of previous works by adding additional information to the existing models without relying on hand-crafted features. We develop a syntactic path-based hybrid neural network model that combines two types of neural networks; BiLSTM and CNN, to model both the context of a sequence and the syntactic information. Bi-LSTM model is a two-way Recurrent Neural Network that can learn both the preceding and the following context information of each word in a sentence. CNN model is another neural network that can extract semantic representations and capture salient features from flat structures, such as the shortest syntactic path between the candidate token and the cue. Syntactic paths have been proved in previous works to hold salient information that can help in solving sequence labeling problems such as the scope detection issue. Taking advantage of the context and the syntactic information, our model can get rid of the hand-crafted feature engineering process, and thus be easily adapted to different out-of-domain texts. We evaluate our model for negation scope detection on the bioscope corpus. On the abstract sub-corpus, our approach achieved an F-score of 90.82% outperforming the results obtained by manually encoded features systems. These results prove that relying only on neural network models and syntactic information is a powerful alternative to get rid of the time-consuming feature engineering methods. The remainder of the paper is structured as follows. Section 2 gives an overview of the previous work. Then, Section 3 describes our hybrid neural network model. In addition, Section 4 illustrates the experimental settings and reports the different experimental results and analysis. Finally, Section 5 concludes the paper and present our perspective for future work.", "section": "Introduction", "doi": "10.1007/s11704-018-7368-6", "references": [13254153, 179875071, 1540017823, 1574147753, 1750263989, 1782003667, 1940872118, 1982464493, 2035703356, 2064675550, 2079735306, 2097606805, 2102113734, 2105827650, 2115915625, 2123207900, 2131774270, 2135932125, 2136566870, 2139865360, 2142786899, 2147880316, 2158899491, 2172140247, 2250521169, 2250966211, 2251135946, 2251826083, 2293634267, 2402268235, 2491832692, 2508309896, 2511964075, 2513222501, 2566847560, 2751776087, 2776461464, 2964217331]}
{"paragraph": "Applications of today such as audio/vision processing, wireless communication and machine learning, process massive amount of data produced by all kinds of sensors. These applications require high computation power in order to provide results in a reasonable amount of time. The demand for higher performance has always been there through the history of the microprocessors. One major solution to the continuous demand for higher computation power has been the technology development. It enables higher chip densities and higher clock rates to provide higher computation power. As a consequence of higher density and clock rates, the power consumption increases, which causes higher chip temperatures. However there are thermal limitations to the chip temperature. These limitations have led the industry to processors with lower clock rates and higher numbers of cores, which can run in parallel. Eventually, parallelism has become the key to greater performance. Initially, multicore/manycore architectures were produced by using several identical cores on the same die. As the design space is explored, it is realized that heterogeneous architectures, consisting of different(specialized) types of cores, have the potential to provide higher performance than the homogeneous counterparts. Our results confirm this statement. The complexity of the architectures grow as the performance and power requirements push them towards parallelism and heterogeneity. As a result, the difficulty of designing and programming these architectures increases. The hardware architects need to design each specialized core separately instead of creating identical copies of a core. Memory coherence, accesses and on-chip communications might also become more complicated due to having cores with different characteristics. On the software side, the architects need to know all the details about parallel programming and the target architecture to be able to develop efficient applications. In addition to the challenges in designing heterogeneous manycore architectures, verification and manufacturing costs are significant obstacles in exploring the design space of these architectures. There are simulators such as HP-Labs COTson, Gem5, and SimFlex, which provide full system simulations to overcome these obstacles and evaluate architectures with different configurations. However, based on the system size, the simulation times can be dramatically long. Additionally, with the simulators, one cannot collect data on hardware resource usage and the maximum clock rate that can be achieved by the architecture. In a prior study, we proposed a design method to develop specialized single core architectures with RISC-V cores directly from dataflow applications. The method consisted of 4 steps, however, only the first 3 steps were implemented. In the first step the application is developed in a high level dataflow language (CAL actor language) that supports application development with different models of computation including Kahn Process Network (KPN), Synchronous Dataflow (SDF), Dataflow Process Networks (DPN), and Cyclo-Static Dataflow (CSDF). This application is fed to software tools (TURNUS and Cal2Many) in the second step where the compute intensive parts (hot-spots) of the application are identified and converted into hardware accelerators implemented in Chisel language. This language is a hardware description language based on Scala. The rest of the application is converted into software code that can be compiled for the target architecture. In the third step, the hardware accelerators are integrated to a RISC-V core that is configurable and a single core system is generated with another tool (rocket chip generator). We developed a hardware library for basic floating-point operations and complex number arithmetic as well as some software tools and combined them with existing open source tools to automate the steps of the design method. In this study we complete the design method by implementing the fourth step, which is connecting multiple heterogeneous cores to each other with a 2 dimensional mesh network-on-chip (NoC). That is, the design method still starts from developing the application in CAL actor language and now ends with generating manycore architectures consisting of a 2D mesh NoC and RISC-V cores with scratchpad memories and different accelerators in the form of instruction extensions targeting hot-spots of the application. The NoC router is developed in VHDL and Chisel languages separately. The details and comparison of the implementations, performance analysis, area usage on FGPA, and comparison to other studies are given. In this paper, we extend the rocket chip generator by integrating this router. We develop a new hardware/software code generation back-end to support generation of parallel C code and multiple accelerators. Additionally, we add a communication mechanism to the software generation to support core-to-core communication. Finally, we evaluate the complete design method. We used two case studies in order to test and evaluate the design method, the tools, and the generated hardware & software implementations. We generated architectures with different configurations (targeting the chosen applications) within these case studies. The applications are executed on cycle accurate, full system emulators generated by the rocket chip generator to collect the performance results. The rocket chip generator generates synthesizable verilog code as well. We synthesize this verilog code on a Xilinx Ultrascale FPGA to collect area and timing results. The first case study implements the autofocus criterion calculation, which is a key component of synthetic aperture radar systems. In our previous work, we have implemented this case study in a sequential manner and executed on a single core. In this study, we implement two different parallel versions of it and run on 2 cores and 13 cores separately. The performance results of this study are compared to the results from Ul-Abdin et al., Ahlander et al. in which the application is executed on commercial architectures. The second case study is the first convolution layer of GoogLeNet as a proof of concept to demonstrate that the design method can be used for generating architectures targeting the machine learning domain. This case study is first executed on a single core architecture, then on 4 and 5 core architectures with accelerators. The other layers can be mapped on additional cores to have a manycore architecture that can execute the whole application. We used on-chip memory to store the data for the first layer, however, the memory requirements of the whole application will lead to the use of external memory, which might harm the overall performance. The contributions of this study can be summarized as: The design method that is proposed in our previous study is extended to generate manycore architectures by integration of a two dimensional mesh NoC router to the tool chain. The hardware library that is used while generating hardware accelerators is extended with a hardware block that performs 7×7 convolution. A new back-end for hardware & software code generation is developed within the Cal2Many framework, in order to support parallel code generation for multiple cores generation of multiple accelerators per core on-chip communication placement of code and data on the local memory of the corresponding cores large data transfers to the accelerators via direct memory connections. The tools and the generated architectures are evaluated with two different case studies from different domains. The performance of the generated architectures are compared with the performance of commercial architectures. The rest of the paper is structured as follows: In Section 2 the related works are presented. Section 3 describes the step-by-step realization of the design method. Section 4 provides description of the case studies and details of the implementations. The results of the case studies follow in Section 5. In the same section, the results are discussed. Finally in Section 6, the study is concluded.", "section": "Methodology", "doi": "10.1016/j.micpro.2019.102908", "references": [1551755348, 1597755753, 1741246166, 1965217823, 1983394510, 1983899098, 2013846121, 2015659026, 2064115172, 2070385897, 2074784122, 2082365376, 2088825637, 2089155985, 2105226637, 2112085716, 2147657366, 2148802605, 2152830441, 2158418816, 2170382128, 2259879138, 2413263386, 2519082574, 2528553838, 2572342677, 2623681105, 2763494392, 2794188816, 2799619026, 2950179405]}
{"paragraph": "It is often the case that in developing an SoC design, a family of SystemC TLMs is created. For example, there are various optional functionalities in a SoC project, which will be composed to form different SystemC models. Also for example, in the design exploration process, it is necessary to extend a functional TLM for performance, area or power evaluation, which forms a collection of TLMs. In the same SoC project, these TLMs share common functionalities, and different TLMs can be formed from the composition of these functionalities. In most cases, all the TLMs in a family must be verified for the follow-up design activities. At present, there is no efficient verification method for verifying all the TLMs in a family. Traditionally, developers can only verify them one by one, which causes large portion of duplicated verification overhead. In our previous work, we proposed to call such TLM family TLM product line TPL, and proposed a novel feature-oriented FO TPL development methodology, which will improve the efficiency of TPL development. The proposed TPL development methodology is invoked from the software product line SPL. There are some verification methods for SPL. However, TLM is executed in a concurrent manner while software is executed in a sequential manner, the existing methods for verifying SPL can not be applied directly to TPL. In our FO TPL design practice, we found that many feature modules are invoked conditionally and when verifying a TLM consisted of them, the assumption of a property to be verified will not trigger the execution of these feature modules. We propose to call such type of feature modules mute-future-module MFM and prove that we do not need to repeatedly verify the property against TLMs with feature whose implementation is an MFM. In this paper, we proposed a novel TPL functional verification method for FO TPL design methodology. The proposed method focuses on verifying computation parts of a TLM and can deal with LTL properties like “GFψ → GFφ” efficiently. For the given property, our method does not need to verify all the TLMs in a TPL. It can exponentially reduce the number of TLMs to be verified in a TPL by identifying MFMs. Notice that the identification of MFM is fulfilled by scan the source code of TLM in one pass. Therefore, the costs for identifying MFMs can be ignored when compared with verification costs. After pruning the number of TLMs to be verified, our method will translate these models to Promela codes and then be verified by model checker SPIN. One thing needs to be pointed out is that although we use SPIN to verify TLM here, our verification methodology is not restricted to it. The proposed method is presented in both informal and formal ways, and the correctness of our method is proved based on micMac automata. The micMac automata is a formal model of TLM, and it can express most of the key semantics of TLM simply and directly. Finally, the experimental results on a real design show the significant reduction in the verification consumption of time and space. The main contributions of our work are: It is the first time a method is proposed for verifying a family of SoC designs; An extended cone-of-influence COI method to identify unrelated modules according to the given property; An extended formal model is proposed to modeling feature modules in SystemC and the correctness of our method is proven based on it. The rest of the paper is organized as follows. Section 2 gives a brief introduction of FO design methodology as background. Section 3 gives our motivation and an informal description of our feature-oriented TPL verification method. Section 4 proves the correctness of our method formally based on micMac and e-micMac. Section 5 presents a case study and experimental results of the proposed method. Section 6 presents related work. Finally, Section 7 concludes the paper.", "section": "Methodology", "doi": "10.1007/s11704-018-8254-y", "references": [129511178, 1496907652, 1544299371, 1556541726, 1856391849, 1967734133, 1968542268, 1968718409, 1968938035, 1980024464, 2015315253, 2041743477, 2046586877, 2071631718, 2074897497, 2120106872, 2130195901, 2138002452, 2141399917, 2145092620, 2154206750, 2165082676, 2169490784, 2170286440, 2171573750, 2176581918, 2786605369, 2791349361, 2806217613]}
{"paragraph": "A granule or an information granule is a group consisting of objects drawn together by indistinguishability, similarity and proximity of functionality. A granule may be interpreted as one of the numerous small particles forming a larger unit. The process of constructing granules or information granules is said to be granulation or information granulation. This process granulates a universe of discourse into a family of disjoint or overlapping information granules. To manage the construction, interpretation and representation of information granules is an important problem. Granular computing, which is a term coined jointly by Zadeh and Lin, takes a granule or an information granule as the basic unit of calculation and aims to build effective computing models for dealing with large-scale complex datasets and information. This approach is an important tool in data mining and knowledge representation. The purpose of granular computing is to explore an approximation scheme, which allows us to view a phenomenon with different levels of granularity and then effectively solve complex problems. The research issues of granular computing concentrate in three aspects, including information granulation, organization and causation. Zadeh noted that granulation involves decomposition of whole into parts, organization involves integration of parts into whole, and causation involves association of causes and effects. Lin and Yao explained the significance of granular computing, which has attracted the interest of many people. A granular structure is a mathematical structure of the family of information granules from a dataset, where the internal structure of each information granule is visible, and the coactions between information granules are tested by the visible structures. Naturally, the granular structure can be depicted as a vector consisting of information granules. Granular structures have been grouped in human granulation intelligence, and knowledge structures in a knowledge base have been studied. As an emerging field of research, granular computing has rapidly developed and has been applied in areas of data mining, data clustering, machine learning, artificial intelligence, approximate reasoning and knowledge discovery. Currently, the research on granular computing has four approaches, i.e., rough set theory, fuzzy set theory, quotient space theory and concept lattice theory. Rough set theory is a mathematical tool for disposing of the uncertainty of knowledge and has been successfully applied to machine learning, knowledge discovery, intelligent systems, expert systems, inductive reasoning, decision analysis, pattern recognition, mereology, and signal analysis. An information system based on rough sets was introduced by Pawlak. Many applications of rough sets, such as reasoning under uncertainty, feature selection, and classification and rule extraction are related to information systems. For granular computing in an information system, the information structure is a significant research topic. An equivalence relation is a special kind of similarity between objects from a dataset. Given an information system, each attribute subset determines an equivalence relation. The object set of this information system is divided into disjoint classes by this equivalence relation, and these classes are said to be equivalence classes. If two objects belong to the same equivalence class, then we may say that they cannot be distinguished under this equivalence relation. Thus, each equivalence class is seen as an information granule consisting of indistinguishable objects. The family of all these information granules constitutes a vector; this vector is said to be an information structure in the information system induced by this attribute subset. Equally, information structures in an information system are also granular structures in the meaning of granular computing. Information structures in an information system have been introduced and investigated, including their uncertainty measures in fully fuzzy information systems, and their invariant characterizations under homomorphisms based on data compression. Covering-based rough sets are the generalization of classical rough sets to deal with covering information systems. Covering-based rough sets are useful for successful applications in a variety of problems. A covering information system is made up of multiple coverings in the universe. Therefore, we can construct the information granule of each point in the universe from the perspective of a covering family and propose the concept of information structures in a covering information system. The aim of this paper is to investigate information structures in a covering information system under a framework of granular computing and granularity measure of uncertainty for this covering information system. The remainder of this paper is organized as follows. In Section 2, we review some concepts on binary relations, covering information systems and homomorphisms. In Section 3, we present the concept of information structures in a covering information system. In Section 4, we propose dependence and independence between information structures and define information distance between information structures. In Section 5, we explain the properties of information structures. In Sections 6, we present an application for information structures by investigating the granularity measure of uncertainty for covering information systems. In Section 7, we obtain invariant characterizations of covering information systems under homomorphisms. Section 8 summarizes this paper.", "section": "Methodology", "doi": "10.1016/j.ins.2018.09.048", "references": [1530320578, 1966564418, 1973689533, 1996844420, 2002680690, 2006675793, 2020614133, 2031978194, 2036999870, 2041926928, 2052608046, 2052719321, 2071765874, 2082266564, 2085029196, 2092044939, 2103514965, 2114383673, 2128771953, 2134866037, 2137576710, 2138080682, 2142864059, 2143040521, 2153676086, 2174369037, 2179045687, 2214361950, 2340020088, 2414329562, 2550850267, 2557776595, 2572412109, 2775094511, 2851233212, 2912565176, 2952248497, 2963129612]}
{"paragraph": "Recently, Vapnik and Vashist provided a new learning paradigm termed learning using privileged information, which is aimed at enhancing the generalization performance of learning models. Generally speaking, in classical supervised learning paradigm, the training data and test data must come from the same distribution. Although in this new learning paradigm the training data is also considered an unbiased representation for the test data, the LUPI provides a set of additional information for the training data during the training stage. The set of additional information is termed privileged information. Different from traditional supervised learning approaches, the LUPI based methods make use of a new kind of training data including privileged information during the training phase, but the privileged information is not available in the test stage. The authors note that the new learning paradigm is analogous to human learning process. In class, a teacher can provide some important and helpful information about this course for students, and these information provided by a teacher can help students acquire knowledge better. Therefore, a teacher plays an essential role in human leaning process. Likewise to the classroom teaching model, in general, the LUPI paradigm based methods can also achieve better generalization performance than traditional learning models. Vapnik and Vashist were the first to present a SVM algorithm with privileged information termed SVM+, which leverages the strength of the LUPI paradigm. A thorough theoretical analysis of the SVM+ was further illustrated in Pechyony and Vapnik and Vapnik and Izmailov. Previous works of the LUPI paradigm focus on two aspects: solving the LUPI based algorithms efficiently and incorporating the LUPI paradigm into various learning models. This paper focuses on the latter. The newly-derived RFVL+, however, has much milder optimization constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the new RFVL+, which naturally tackles the former. From the optimization perspective, the formulation of the SVM+ is a typical quadratic programming problem, and in general the QP problem can be solved by some optimization toolboxes. However, it is unnatural and inconvenient to train a learning model by some optimization toolboxes in real-world applications. For this reason, it is necessary to present an efficient approach to solve it. Pechyony et al. presented an SMO-style optimization approach for the SVM+. Li et al. further proposed two fast algorithms for linear and kernel SVM+, respectively. In addition to solving the SVM+ efficiently, the LUPI paradigm is incorporated into various learning algorithms. Nowadays, neural network is one of the most popular learning algorithms due to the wave of deep learning, and most of current deep learning methods are neural networks, including denoising auto-encoders, convolutional neural networks, deep belief networks and long short-term memory, etc. These neural network methods have achieved greatly successes in various real-world applications, including image classification and segmentation, speech recognition, natural language processing, etc. Therefore, it is very interesting to combine neural networks and the LUPI paradigm. The combined method is able to leverage the strengths of neural networks and the LUPI paradigm. The goal of this paper is to tackle this open problem and construct a bridge to link the LUPI paradigm and randomized neural networks. In this paper, the authors propose a novel random vector functional link network with privileged information called RVFL+. The random vector functional link network is a classical single layer feedforward neural network, which overcomes some limitations of SLFNs including slow convergence, over-fitting and trapping in a local minimum. Although the RVFL has achieved good generalization performance in some real-world tasks, in order to improve further its effectiveness, the authors incorporate the LUPI paradigm into the RVFL. Different from existing variants of RVFL, the RFVL+ may open a door towards alternative to the traditional learning paradigm for the RVFL in real-world tasks. In other words, the RVFL+ makes use of not only the labeled training data but also a set of additional privileged information during the training stage, which interprets the essential difference between the two learning paradigms. Moreover, following the kernel ridge regression, the authors further propose a kernel-based RVFL+ called KRVFL+ in order to handle highly complicated nonlinear relationships. The KRVFL+ has two major advantages over the RVFL+. On one hand, the random affine transform leading to unpredictability is eliminated in the KRVFL+. Instead, both the original and privileged features are mapped into a reproducing kernel Hilbert space. On the other hand, the KRVFL+ no longer considers the number of enhancement nodes, which is a key factor to affect its generalization ability. As a result, the performance of the KRVFL+ in terms of effectiveness and stability is significantly improved in most real-world tasks. Furthermore, the authors investigate the statistical property of the newly-derived RVFL+. The authors provide a tight generalization error bound based on the Rademacher complexity for the RVFL+. Our generalization error bound benefits from the advantageous property of the Rademacher complexity. The Rademacher complexity is a commonly-used powerful tool to measure the richness of a class of real-valued functions in terms of its inputs, and thus better capture the property of distribution that generates the date. In the RVFL+, the weights and biases between the input layer and enhancement nodes are generated randomly and are fixed, the output weights are then calculated by the Moore–Penrose pseudo-inverse or the ridge regression. Therefore, the RVFL+ is considered as a special linear learning model. The Rademacher complexity is an ideal choice for the analysis of this type of methods, and can provide a high-quality generalization error bound in terms of its inputs. In contrast to the previous work, the authors provide a more tight and general test error bound, and the novel bound is also appropriate for various versions of the RVFL including the newly-derived KRVFL+. Last but not least, the authors construct some competitive experiments on 14 real-world datasets to verify the effectiveness and efficiency of the newly-derived RVFL+ and KRVFL+. The experimental results illustrate that the novel RVFL+ and KRVFL+ outperform state-of-the-art comparisons. More importantly, recent existing works have illustrated that the cascaded multi-column RVFL+ and the cascaded kernel RVFL+ can obtain the best performance in terms of effectiveness for the single-modal neuroimaging-based diagnosis of Parkinson’s disease and the transcranial sonography based computer-aided diagnosis of Parkinson’s disease, respectively. Notice that both the RVFL+ and KRVFL+ are basic learners and play key roles in these two ensemble learning methods. The codes of RVFL+ and KRVFL+ are available. The contributions of this paper are summarized as follows. The authors propose a novel random vector functional link network with privileged information, called RVFL+. The RVFL+ bridges the gap between randomized neural networks and the LUPI paradigm. Different from existing variants of the RVFL, the newly-derived RVFL+ provides an alternative paradigm to train the RVFL. The authors extend the RVFL+ to the kernel version called KRVFL+, and the KRVFL+ enables handling effectively highly nonlinear relationships between high-dimensional inputs. The previous works of the LUPI focus on two aspects: deriving an efficient solver and combining the LUPI paradigm with different learning models. This paper focuses on the latter. However, from the optimization perspective, the authors find that the novel RVFL+ has sampler constraints than the SVM+. As a result, the authors can obtain a closed-form solution to the RVFL+, which naturally tackles the former. This paper not only gives a comprehensive theoretical guarantee using the Rademacher complexity for the new RVFL+, but it also empirically verifies that the newly-derived RVFL+ and KRVFL+ outperform state-of-the-art methods on 14 real-world datasets. The remainder of this paper is organized as follows. The authors brief the related work of the RVFL in Section 2. In Section 3, the authors briefly explain the reason that the RVFL works well for most real-world tasks, and then introduce the newly-derived RVFL+ and KRVFL+. The authors study the statistical property of the RVFL+ and provide a novel tight generalization error bound based on the Rademacher complexity in Section 4. In Section 5, the authors build several experiments on 14 real-world datasets to evaluate the proposed RVFL+ and KRVFL+. This paper concludes in Section 6.", "section": "Related Work", "doi": "10.1016/j.neunet.2019.09.039", "references": [6533109, 176909285, 223114164, 1533861849, 1596717185, 1600810180, 1625958017, 1677182931, 1755117326, 1986278072, 1987618706, 1996640396, 2012638612, 2064675550, 2067562626, 2084251795, 2097998348, 2099579348, 2108544580, 2109574129, 2117138194, 2118181058, 2119821739, 2123223828, 2126885794, 2126942721, 2136331731, 2136922672, 2138383519, 2138580951, 2141225381, 2153232138, 2153635508, 2154209944, 2160354932, 2163605009, 2169408065, 2173379916, 2178357350, 2229668941, 2283039974, 2286961399, 2465334159, 2506404347, 2550643483, 2557283755, 2586160710, 2593382986, 2607586284, 2727059890, 2736488765, 2738226240, 2753648062, 2761362383, 2762962020, 2899470766, 2906146483, 2911964244, 2912162873, 2912934387, 2921936057, 2953267151]}
{"paragraph": "With the collection of massive data from monitoring systems, machinery condition monitoring has entered the era of big data. As an important extension of condition monitoring, fault diagnosis can be performed by using the obtained condition-monitoring big data to detect, locate, and identify defects in machinery. Monitoring big data share five properties, including high volume, high variety, high velocity, low veracity, and low-value density. As a result, the traditional fault diagnosis methods based on signal processing techniques show their weakness in dealing with condition-monitoring big data. To solve this problem, many researchers pay much attention to the intelligent fault diagnosis, which is able to process massive data, more rapidly and effectively. Especially, some advanced intelligent diagnosis methods based on deep learning are widely studied. For example, different intelligent diagnosis methods have been proposed based on deep belief networks, recurrent neural networks, and convolutional neural networks to extract fault features from condition-monitoring data, more effectively. Although these intelligent diagnosis methods have many advantages in extracting fault features, there are still many critical and opening issues of these methods for processing big data, including processing for large-scale of data, processing for heterogeneous types of data, processing for high speed of steaming data, and processing for poor-quality data. As for the issue of processing for large-scale data, performing intelligent fault diagnosis in a distributed and parallel framework may be a promising way to solve this issue. For instances, under Hadoop platform, massive data can be divided into small chunks and then these chunks can be processed using MapReduce in a distributed and parallel manner. Furthermore, some data integration methods may be applied to process heterogeneous types of data before the intelligent fault diagnosis methods are created. To satisfy the requirement of the high speed of steaming data, some online intelligent fault diagnosis methods may be used. Undoubtedly, compared with these three issues, data quality has the greatest influence on the performance of intelligent fault diagnosis. Therefore, this paper focuses on the issue of processing for poor-quality data and the other three issues are not considered. Data quality reflects the degree of the correlations between data and machinery health condition and is usually described using four dimensions, including accuracy, completeness, consistency, and timeliness. Among them, the accuracy measures the degree of the agreement between collected data and the health condition. The completeness measures whether collected data contain the whole health condition information. The consistency represents the degree of agreement of data in terms of format and structure. The timeliness measures whether collected data are up to date. The low-timeliness data usually leads to an outdated result, which cannot represent the current health condition. Naturally, poor-quality data indicate that these data are inaccurate, uncertain, incomplete, low-timeless, or multiformat. As a result, it is difficult for intelligent diagnosis methods to extract useful information from these poor-quality data. Furthermore, because of “garbage in, garbage out,” intelligent diagnosis models created using poor-quality data probably lead to unreliable or misleading diagnosis results. Therefore, it is necessary to improve data quality before they are used as training data for creating intelligent diagnosis models. The degradation of machinery condition monitoring data quality is attributed to four reasons: disturbance from a tough environment, data-acquisition equipment failure, data transmission failure, and diversity of data due to various formats. Especially, many incorrect data occur because of the first three reasons, leading to the serious decrease of data quality. Consequently, these incorrect data should be detected and then cleaned for improving data quality. However, there is little literature available on data cleaning or incorrect data detection for machinery condition monitoring. Furthermore, data cleaning methods in other fields may not be suitable for condition monitoring. One reason is that types of incorrect data are different among various fields. For example, duplicate values are considered as incorrect data or dirty data and should be cleaned in many fields. As we all know, repetitive transients are common components when a fault occurs on the rotary machinery, so duplicate values cannot be considered as incorrect data for condition monitoring. Data of various fields have their own distinguishing features. It is difficult to generate a universally applicable procedure for detecting incorrect data. The aim of this paper is to detect incorrect data by considering them as outliers based on their owing characteristics for solving the quality issue of big data. Specifically, the characteristics of high-quality data are considered as contextual attributes. Compared with these high-quality data, incorrect data do not conform to the pattern of high-quality data. For instances, their attributes are no longer deterministic but are subject to some random or possibility distributions, greatly different from the contextual attributes. Consequently, incorrect data can be considered as contextual outliers. To further explain this, vibration data were collected from a bearing and gearbox, which are key components of machinery. High-quality data collected from a bearing with an inner race defect exhibit some common characteristics, e.g., a sufficiently long period is visually continuous, and their amplitudes are within a rational range. Moreover, periodic impulses caused by the defect can be observed. These characteristics indicate that the vibration data are closely related to the health condition of the monitored bearing. Being one typical incorrect data, an abnormal segment occurs in the data acquired from a gearbox due to the disturbance of unstable current. The expected mean value of the abnormal segment is not zero, and this abnormal segment seems significantly different from the other segments. The reason for this difference is that the generating mechanism of the abnormal segment is different from those of other segments. In general, this abnormal segment is caused by the saturation of a vibration sensor or disturbance. Another incorrect data, an obvious missing segment was simulated in the data collected from bearing, which breaks the continuity of data. In fact, owing to the network data transmission loss or cable failures, missing segments usually occur by replacing the actual data value with 0, NaN, NULL values, or environment noise, which damages the completeness of data. It can be seen that some characteristics of these incorrect data are greatly different from those of high-quality data and the incorrect data can be considered as outliers compared with the massive high-quality data. Therefore, to detect incorrect data, one straightforward idea is that the outlier detection method can be used. Outlier detection, which is also heavily used and known as anomaly detection, is widely studied in many fields such as credit card fraud detection, cyber-security, traffic, and medical field. Various methods for outlier detection in the abovementioned fields have been proposed. For example, the statistic-based method assumes a distribution or probability model for data and then the points in the regions of low probability are determined as outliers. This method requires knowledge about some parameters of dataset and is not appropriate for outlier detection of multivariable data. As an unsupervised method, the clustering-based method such as support vector domain description, density-based spatial clustering of applications with noise, and k-means detects outliers, which are against clusters. Since the main objective of a clustering method is to find clusters, the cluster-based method may fail to detect outliers. The distance-based method calculates the distance between each object and the objects far from most objects are considered as outliers. This method probably fails to detect outliers from different density regions. Density-based method is able to effectively detect an outlier by comparing density around a data point with the density of its local neighbors. Being a classical-density based method, a local outlier factor (LOF) method was first proposed. By giving the degree of being an outlier without supervision, the LOF method is able to identify outliers which the other approaches may fail to do. Thus, the research on the LOF method has attracted much attention, and many variants of LOF algorithm have been developed, e.g., GridLOF and local distance-based outlier factor. From the above statements, the LOF method should be used to detect the incorrect data by considering incorrect data as outliers. However, incorrect data consist of many outliers rather than one or several outliers and these outliers may fall into one category, so these outliers cannot be detected using the LOF method, directly. Therefore, an incorrect data detection method is proposed based on an improved LOF in this paper. In the proposed method, the incorrect degree of segments divided by a sliding window is evaluated qualitatively using an improved LOF based on time-domain statistical features extracted from each segment. Incorrect data can be effectively detected from collected data using the proposed method, which is verified by simulation and experiments, respectively. Based on the above statements, the contributions of this paper are summarized as follows. The concept of data quality in the field of machinery condition monitoring is originally illustrated. Furthermore, the reasons for the decrease in data quality and the characteristics of incorrect data are discussed. In addition, based on these discussions, an incorrect data detection method is proposed for big data cleaning of machinery condition monitoring. This proposed method extends the application of LOF because it is rarely applied to either condition-monitoring data or abnormal segment detection. Moreover, the kernel-based LOF (KLOF) detects incorrect data more accurately by overcoming two shortcomings of LOF. The KLOF is constructed by considering different values of parameter k, which also promotes the accuracy of incorrect data detection. The incorrect degree of data can be evaluated using the KLOF values, which gives a quantitative description of incorrect data. A threshold value is constructed to select the degree of being incorrect data. The rest of this paper is organized as follows. Section II details the proposed incorrect data detection method. Section III is devoted to verifying the effectiveness of the proposed method by detecting a missing segment from simulated data of a faulty bearing. In Section IV, real data collected from a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to demonstrate the performance of the proposed method, respectively. Finally, Section V concludes this paper.", "section": "Related Work", "doi": "10.1109/TIE.2019.2903774", "references": [1491282455, 1502624642, 1605077306, 1659613497, 2045765911, 2066219179, 2105135748, 2115177366, 2117839996, 2118023920, 2122646361, 2137130182, 2144182447, 2149527700, 2159010948, 2159128662, 2317595875, 2341973567, 2406349003, 2463813940, 2768753204, 2791592946, 2803716633, 2904460913]}
{"paragraph": "An auction is a mechanism for trading commodities among two groups, sellers and bidders. Most auctions also include an auctioneer who is responsible for arranging the auction, accepting the bids, and declaring a winner on behalf of the seller. To properly execute an auction, there must be methods for registering participants, accepting bids, and opening bids. The method employed for bidding defines the type of the auction. For instance, if an auction requires participants to bid in an increasing fashion, it is said to fall under the category of the English auction. On the other hand, if the opposite approach is taken, the mechanism is labeled as the Dutch auction, i.e., the price repeatedly decreases until someone is willing to pay the current price. This is commonly seen in perishable markets. In another type of auction, the buyers bid on a subset of items. This is known as combinatorial auction. General speaking, in an auction mechanism, the winner is a bidder who has submitted the highest bid. To define the selling price, there are two methods: first-price auction and second-price auction. In the former, the winner pays the amount that he has proposed, i.e., highest bid. In the latter, the winner pays the amount of the second-highest bid. It is worth mentioning that the M+1-price auction is the generalization of the 2nd-price auction where. In privacy-preserving auction protocols, also known as sealed-bid auctions, the bidders seal their bids using cryptographic technique. After the execution of the auction, only the auction outcomes, i.e., the winner and the selling price, are revealed. As a result, the losing bids are kept private. The main motivation for constructing sealed-bid auction protocols is the fact that the auctioneers and or sellers may use the past losing bids to maximize their revenues in future auctions and negotiations. In addition, private valuations can be used to disclose personal preferences and private information about the bidders. For instance, a subset of the highest losing bids or the average of the losing bids can motivate the sellers auctioneers to increase the starting price or the minimum value of the bid in future auctions of similar items. Furthermore, a losing bid reveals how much a buyer is willing to pay, how interested a buyer is, or a minimum threshold of a buyer’s cash, and so on. These are critical information specifically in high-end auctions for expensive items or antiques when there exists a serious competition among the bidders. Another example is the commercial websites for travel-related purchases such as airline tickets and hotel. Rumors state that, if a bidder loses in the first bidding effort, these websites store the initial losing bid on their servers or in the bidder’s browser cache and never provide any offer to the bidder below that threshold in near-future. In other words, the bidder cannot go below his initial losing bid if he decides to bid again. This is not unlikely although verifying companies’ secrets is hard. Therefore, if an auction protocol is executed in a way that the winner and the selling price are determined correctly without revealing the losing bids, none of the aforementioned issues will arise. Note that the 2nd-price sealed-bid auctions are also referred to as Vickrey auctions, named after Dr. William Vickrey. Fig. 1 shows the classification of auctions. 1.1. Desired properties and trust models In online auctions, we face a number of challenges that do not exist in auctions taking place physically. One of the advantages of an auction taking place in person is that we can associate a bid to a bidder, which means no one can deny his participation. However, there are disadvantages such as number of people who can participate in the auction due to location or available space. With online auctions, we can overcome these kinds of problems, however we entrust the participants to follow the auction protocol honestly. A collection of unwanted features are described in Brandt and Weiß, Brandt and Wei, Sandholm and Brandt et al. Ideally, electronic auction mechanisms should provide the following fundamental properties: privacy of the losing bids, robustness, verifiability, and non-repudiation. In addition, they may provide secrecy of the bidders’ identities and anonymity, and also, guarantee fairness. These properties are explained in Fig. 2. Fig. 2 Auction properties. Auctions consist of self-interested parties who can form strategies to achieve their personal goals. Therefore, design of a system with strategic players brings another layer of complexity to auction protocols. For instance, an auctioneer may collude with a seller to maximize the profit. This can occur in several ways, but a common method seen in the second-price auctions is to submit an artificial bid as close as possible to the winning bid for the purpose of increasing the selling price. Note that corrupted activities may occur by the sellers, bidders, or auctioneers. These challenges are out of the scope of this article and we refer the readers to the literature to learn about other complications of auctions and corresponding counter measures. Overall, an auctioneer delineating from the rules is a major concern, which is central to some of the papers in the literature. These papers consider the ideas of auctioneer trust, trusted third parties TTP, threshold trust, distributed-bidder trust, and two-server trust. However, not all protocols are constructed with a corrupted auctioneer in mind due to computational and communication complexities as well as security challenges. Below is a summary of the trust models across the entire literature of the sealed-bid auctions. Auctioneer trust: A naive approach for relying on the auctioneer to follow the protocol as an honest agent of the process. Trusted third parties: The auctioneers and bidders have a third party, with no personal gain from the auction, which is trusted by both. This TTP can ensure that the protocol is executed correctly. Threshold trust: Consist of having more than one auctioneer. The auctioneers can only collude if a number of them are working together to disrupt the protocol. This number is the threshold and as long as the number of corrupted auctioneers is less than this threshold, the auction is executed properly. Distributed-bidder Trust: Bidder divide the trust among themselves, there is no auctioneer involved. Two-server trust: This method splits the trust between two entities. The auctioneers and the bidders each own one of the servers. Correctness is then achieved as long as two entities do not collude. 1.2. Our motivation and contribution Market economies are driven by supply-and-demand. Hence, a significant number of goods do not have established prices. Auctions are a mechanism that can be utilized to determine the value of products that would otherwise be hard to estimate. They are frequently used in government contracts, markets of natural resources, and real estate. Although the revenue-equivalence theorem predicts that revenue is independent of the bidding rules, empirical data suggests that different commodities sell with higher revenues depending on the type of the auction that is used. In addition, to provide a mechanism for setting the price, sealed-bid auctions offer several benefits over the open counterpart. For instance, open auctions are prone to collusion among bidders since the face-to-face interaction provides enough information to form strategies and to learn about the opponent’s behavior. Secondly, open auctions favor richer bidders, that is, the bidders with more purchasing power can learn the maximum valuation of the opponents and just bid to win the opponent rather than to place a true valuation. As we stated earlier in detail, open auctions also provide an advantage for the auctioneers sellers interested in learning about the strategies and private valuations of the bidders in order to maximize their revenues in future auctions. These factors motivated us to start preparing this comprehensive survey on privacy-preserving protocols for sealed-bid auctions. In short, the motivation of this article is to study and scrutinize theoretical constructions of sealed-bid auction protocols that are a key element of market economies with such important advantages, i.e., a tool that shifts allocation towards the bidders, provides an equal opportunity, and generates revenue without loss of competitiveness. To the best of our knowledge this is the first survey article on the sealed-bid auctions. Our contribution is to elucidate the significance, the trajectory, and the current state-of-the-art. To sum up, our contribution is to provide a comprehensive collection of pioneering and contemporary research works that is still simple-to-follow for further research and development in this domain. 1.3. Organization of the article Section 2 explains the necessary preliminary materials. Section 3 reviews the literature of the sealed-bid auction protocols thoroughly by the following classification: first-price sealed-bid auctions, second-price sealed-bid Vickrey auctions, M+1-price sealed-bid auctions, rule-flexible sealed-bid auctions, and combinatorial sealed-bid auctions. Note that overlaps exist among different types of sealed-bid auction protocols. Section 4 provides technical discussions. Finally, Section 5 concludes with final remarks.", "section": "Introduction", "doi": "10.1016/j.cose.2019.03.023", "references": [122693320, 295630282, 1492124753, 1492669295, 1503056535, 1503321858, 1509765132, 1536311772, 1541096476, 1544101653, 1546675633, 1557366654, 1566496285, 1576262084, 1580033229, 1588116778, 1598262674, 1599852298, 1606631098, 1607267906, 1616837063, 1746835072, 1750426656, 1753702614, 1768219611, 1858229664, 1911825786, 1965792506, 1977259085, 1991669906, 1996360405, 2001135248, 2005745690, 2013469527, 2038228603, 2040168265, 2047388817, 2067305937, 2070378250, 2075893504, 2075967590, 2098877894, 2103647628, 2103761955, 2103915206, 2108834246, 2110383956, 2115111680, 2129297282, 2129462878, 2132172731, 2141216914, 2142836084, 2142982208, 2144951192, 2145180210, 2156186849, 2165716457, 2401242379, 2403688192, 2465320759, 2728487654, 2766945659]}
{"paragraph": "Face hallucination, which can be seen as a domain-specific super-resolution technology, is a technique to infer a high-resolution face image, along with increasing the detailed face features, from low-resolution face images. It has numerous applications for face recognition, 3-D face modeling, criminal detection, and so on. From the pioneering work, many issues of face hallucination have been increasingly studied. Generally speaking, these methods all try to explore the implicit or explicit transformation between the low-resolution and high-resolution spaces with an additional training set with low-resolution and high-resolution face image pairs. Most methods in the literature fall into two main categories: statistical model-based global face methods and patch prior-based local face methods. Statistical model-based global face methods leverage the face statistical models to model the face image and execute face hallucination globally. They can well maintain the global structure of the human face. However, their results lack the detailed local face features and suffer from ghosting artifacts. Considering that the human face structure is a significant prior, many face hallucination methods try to exploit the prior knowledge present in smaller patches. Among them, position-patch-based methods have gained widespread attention in recent years. The common idea of these methods is to divide the global face into many small patches with a predefined patch size and overlap, and use the training patches with the same position as the input one to construct the input patch. In this paper, our work is mainly concerned with this type of approach. The least square representation method is one of the representative position-patch-based methods. To address the problem that the solution of least square representation is unstable, sparse representation-based models have been developed by incorporating the sparsity regularization. However, sparse representation methods overemphasize sparsity and neglect local similarity among the training samples, which is essential for exploiting the intrinsic nonlinear manifold of the training sample space. A locality-constrained representation model was developed which simultaneously adds the sparsity and locality constraints to the patch representation objective function, obtaining stable and reasonable representation coefficients. In order to alleviate the inconsistency of the low-resolution and high-resolution spaces, some works have been proposed to iteratively obtain the patch representation and perform neighbor embedding or learn the mapping in correlation spaces. Based on locality-constrained representation, recently, the low-rank and self-similarity priors are also introduced to regularize patch representation. Gradient information of the face image was incorporated to further regularize the patch representation. In addition to face hallucination, the locality-constrained representation algorithm has been also used to deal with pose and illumination problems in face hallucination and synthesis. However, aforementioned local patch treatments mainly focus on the small patches and do not take into account the global nature, which has been verified to be beneficial to image description, image denoising, and retrieval tasks. To model the global nature of local patch-based methods, the most direct way to incorporate the contextual information is to extend the patch. The most extreme situation is to treat the entire face as a whole, using a global face-based approach. Another possible solution is to introduce a global reconstruction constraint in the image patch-based method. However, when the training sample size is fixed, it will become much more formidable to reconstruct a large patch or infer the global face image. In other words, because the training sample size should grow exponentially with the size of the image patch, it becomes impractical to present a too large image patch. More recently, to reconstruct the latent high-resolution image locally while thinking globally, deep neural networks, especially convolutional neural networks, have been applied to construct the mapping relationship between the low-resolution images and their high-resolution counterparts and have shown strong learning capability and accurate prediction of high-resolution images. For example, a general image super-resolution method was developed based on a deep neural network. This is the very first attempt to use deep learning tools for image super-resolution reconstruction. Another approach proposes introducing the domain expertise to design a sparse coding-based network. Recently, several approaches are the most competitive approaches for face hallucination. They utilized very deep networks to model the relationship between the low-resolution images and their high-resolution correspondings, and verified that deeper networks can produce better results due to the large receptive field, which means considering more contextual information, that is, very large image regions. To utilize the contextual information without enlarging the patch size, this paper proposes simultaneously considering all patches in a large window centered at the observation patch that is called context-patch and develops a context-patch-based face hallucination framework. It inherits the merits of predicting with small local patches, while having the benefits of working with large patches. Based on thresholding locality-constrained representation, the stability of representation and reconstruction accuracy can be improved. Observing that the reconstruction performance will be improved if there are some similar samples in the training set, we further advance an enhancement scheme via reproducing learning, which puts reconstructed high-resolution samples back to the training set and makes it easier to reconstruct the input image. For a testing patch, on the input low-resolution face image, we first extract the low-resolution context patches from the low-resolution training set. Then, we calculate the distance between the input low-resolution patch and the context patches, and choose the nearest neighbor patches to reconstruct the input low-resolution patch. Lastly, the output high-resolution patch can be predicted by combining the corresponding high-resolution context patches with the representation coefficient obtained in the low-resolution training set. To promote the performance, we add back the hallucinated high-resolution image to the training set, which can simulate the case that the high-resolution version of the input low-resolution face is present in the training set, and repeat the thresholding-representation-prediction steps to iteratively enhance the final hallucination result. In summary, the main contributions of this paper are three-fold. We introduce the concept of context patch to expand the receptive field of the patch representation model. It not only inherits the merits of predicting with small patches but also has the benefits of working with large patches. In addition, we combine the low-level pixel values and high-level position information to represent the image patch, thus further exploiting the contextual information. We develop a novel and robust image patch reconstruction method based on thresholding locality-constrained representation. It is inherited from the locality-constrained representation method but has the advantages of accurate patch representation and low computational complexity. We propose a face hallucination improvement strategy via reproducing learning. The estimated high-resolution face image is iteratively reconstructed with a reproduced training set through adding the hallucinated high-resolution image and its degenerated version to the training set. Experiments demonstrate its superiority for some state-of-the-arts in terms of both objective assessment and visual quality, especially when confronted with misalignment or the small sample size problem. The research reported in this paper is an extension of our preliminary work. We highlight the significant differences between this paper and the earlier one as follows. To exploit much more contextual information of the patch images, we extend the pixel intensity-based representation to the combination of low-level pixel values and high-level position prior, which can be seen as the contextual information. The earlier approach focuses only on controllable conditions. However, this paper extends the application of the thresholding locality-constrained representation and reproducing learning algorithm from controllable conditions to more intricate conditions, including both very limited training sample size and real-world image reconstruction. This paper gives deep analysis on the motivations and advantages of introducing the contextual information, thresholding strategy, and reproducing learning, leading to a better understanding of why and how our method works. The rest of this paper is organized as follows. The notations and formulation of position-patch-based methods are presented first. Then, the details of the face hallucination method are given followed by the improvement strategies of thresholding-based patch representation and iterative estimation. Finally, experimental evaluations and comparisons with some competitive algorithms are reported, and the paper concludes with a summary and future directions.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868891", "references": [1885185971, 1919542679, 1967482855, 1972002222, 1985436611, 1987017523, 1997462613, 1999457380, 2001393004, 2015497428, 2027325144, 2027922120, 2031349574, 2033133094, 2037133587, 2054515210, 2069165391, 2069201803, 2070038402, 2073795739, 2078301312, 2114712988, 2118963448, 2121058967, 2133665775, 2134563198, 2141631520, 2143619097, 2145096794, 2160021903, 2164307510, 2295477204, 2307888739, 2345557152, 2404650843, 2428551659, 2461349148, 2507235960, 2509704168, 2518224564, 2520930090, 2545685597, 2559478892, 2566721764, 2569520361, 2570189907, 2611646129, 2618553632, 2642849260, 2740535357, 2752773224, 2790508633, 2790630725, 2792302760, 2800324071, 2809795042, 2963035013, 2963393566, 2963676087]}
{"paragraph": "This paper provides a comprehensive final report and extended analysis of the first shared task on End-to-End Natural Language Generation, substantially extending previous reports. In addition to this previous work, we provide a corrected and extended evaluation of the training dataset, as well as a detailed discussion of how current state-of-the-art systems address End-to-End generation challenges, including semantic accuracy and diversity of outputs, and a comparison of techniques used by the submitted systems with systems outside the competition. We then include a substantially expanded evaluation of the systems using novel automatic metrics, accounting for output complexity, diversity and semantic correctness. In addition, we provide an analysis of system output similarity and confirm that systems using similar techniques, for example seq2seq, produce similar outputs. We also provide a detailed error analysis with examples of system outputs. This extended evaluation allows us reach some more in-depth insights about the strength and weaknesses of end-to-end generation systems. Finally, we discuss directions for future work with respect to end-to-end generation, as well as Natural Language Generation evaluation in general. In addition, this paper accompanies a release of all the participating systems’ outputs on the test set along with the human ratings collected in the evaluation campaign. Shared challenges have become an established way of pushing research boundaries in the field of Natural Language Processing, with Natural Language Generation benchmarking tasks running since 2007. These previous shared tasks have demonstrated that large-scale, comparative evaluations are vital for identifying future research challenges in Natural Language Generation. The End-to-End Natural Language Generation shared task is novel in that it poses new challenges for recent end-to-end, data-driven Natural Language Generation systems. This type of systems promises rapid development of Natural Language Generation components in new domains by reducing annotation effort: they jointly learn sentence planning and surface realisation from non-aligned data. As such, these approaches do not require costly semantic alignment between meaning representations and the corresponding natural language reference texts, but they are trained on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques. At the start of the End-to-End Natural Language Generation Challenge, end-to-end approaches to Natural Language Generation were limited to small, delexicalised datasets. Therefore, end-to-end methods have not been able to replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical approaches for language generation in dialogue. In this paper, we describe a large-scale shared task based on a new crowdsourced dataset of 50k instances in the restaurant domain. We show that the dataset poses new challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena, and that it inspired multiple extensions and further data collection since its original release. Our shared task aims to assess whether the novel end-to-end Natural Language Generation systems are able to produce more complex outputs given a larger and richer training dataset. We received 62 system submissions by 17 institutions from 11 countries for the End-to-End Natural Language Generation Challenge, with about one third of these submissions coming from industry. We consider this level of participation an unexpected success, which underlines the timeliness of this task and allows us to reach general conclusions and issue recommendations on the suitability of different methods. We analyse how the submitted systems address the challenges posed by the dataset and show that the competition inspired further work on our dataset. We evaluate the submitted systems by comparing them to a challenging baseline using automatic evaluation metrics including novel text-based measures as well as human evaluation. Note that, while there are other concurrent studies comparing a limited number of end-to-end Natural Language Generation approaches which emerged during the End-to-End Natural Language Generation Challenge, this is the first research to evaluate novel end-to-end generation at scale using human assessment. Our results show a discrepancy between data-driven seq2seq models versus template- and rule-based systems. While seq2seq models generally score high on word-overlap similarity measures and human rankings of naturalness, manually engineered systems score better than some seq2seq systems in terms of overall quality, as well as diversity and complexity of generated outputs. We conclude by laying out challenges for future shared tasks in this area. We also release a new dataset of 36k system outputs paired with user ratings, which will enable novel research on automatic quality estimation for Natural Language Generation. All data and scripts associated with the challenge, as well as technical descriptions of participating systems are available at the following URL.", "section": "Introduction", "doi": "10.1016/j.csl.2019.06.009", "references": [1522301498, 1816313093, 1889081078, 1956340063, 1966771059, 1980340273, 1982003829, 1982897610, 2018116724, 2021618504, 2062295023, 2064675550, 2100175927, 2101105183, 2107587102, 2110633879, 2115101920, 2115648139, 2119717200, 2127849236, 2133512280, 2133564696, 2138742901, 2139079654, 2147192413, 2148985659, 2153975459, 2154764394, 2161181481, 2163361328, 2163600070, 2204900930, 2250530145, 2251058040, 2251165062, 2251291469, 2251648400, 2251957808, 2252166243, 2257408573, 2265196526, 2270190199, 2340944142, 2418441884, 2427764808, 2429300145, 2438667436, 2507756961, 2509005166, 2518570122, 2523790121, 2561658355, 2566725737, 2574872930, 2577628354, 2578330760, 2594990650, 2604799547, 2732004306, 2735460460, 2739046565, 2740626591, 2745039414, 2748261613, 2755989362, 2760656271, 2785626838, 2786660442, 2789028076, 2793081411, 2798737723, 2799022285, 2804377263, 2805154388, 2806532810, 2888975690, 2891339902, 2891924151, 2892087592, 2892228078, 2893600504, 2894882905, 2897269555, 2897289333, 2901675956, 2902141901, 2902236809, 2902293828, 2902677561, 2903156194, 2903428882, 2915756181, 2949073139, 2949888546, 2950635152, 2951103768, 2951176429, 2951267018, 2951622294, 2951718774, 2952013107, 2952294095, 2952840881, 2952918099, 2962816513, 2962956378, 2963212250, 2963506925, 2963672599, 2963701786, 2964028591]}
{"paragraph": "Many multi-objective optimization problems occur in many application scenarios where multiple conflicting objectives need to be resolved in order to find a set of optimal solutions. Accordingly, the solutions to those problems, referred as Pareto-optimal solutions, denote a set of possible reasonable trade-offs between all objectives. The image of Pareto-optimal solutions in the objective space is known as Pareto front. When an MOP has more than three objectives, it is called a many-objective optimization problem. In order to solve MOPs and MaOPs, many multi-objective evolutionary algorithms have been proposed and developed, such as NSGA-II and SPEA2. However, these algorithms would suffer from severe degradation in performance on MaOPs, as confirmed in the literature. This is mainly caused by the curse of dimensionality with respect to the difficulty of optimizing a large number of objectives. Generally, the difficulties encountered by MOEAs can be detailed as follows. Firstly, compared with two- or three-objective MOPs, MaOPs would render the Pareto optimality unable to provide enough selection pressure to drive the solutions towards the true Pareto front. This is because most of the obtained solutions are apt to be non-dominated to each other as the number of objectives increases. Especially, when the number of objectives rises over five, the proportion of non-dominated solutions in the population will reach more than 90 percent. Therefore, it is difficult to differentiate the preferred ones from innumerable non-dominated solutions obtained during the search process. Moreover, due to the above issue, these algorithms would be ineffective to maintain the convergence and diversity of solutions. Secondly, the efficiency of the algorithmic operators in a high-dimensional objective space decreases dramatically. In the variation process, the new offspring produced by two nearly converged solutions, which are required to approach along its original direction, would contrarily move far away from the true Pareto front. Accordingly, the final population would fail to converge to the Pareto front, despite being spread all over the objective space. As a result, these algorithms only explore a limited region in the large search space, while being trapped in local segments of the Pareto front. Thirdly, due to the large search space, the diversity based selection criterion in MOEAs would be harmful to the convergence performance. For example, experimental results show that the diversity maintenance in NSGA-II deteriorates the convergence performance on the five- and ten-objective instances. In addition, there are also some other problems to be addressed, such as the high computation and visualization of the multi-dimensional objectives. Even if the Pareto front is attainable, there are no effective methods to visualize the front. A largely increasing number of solutions in the high-dimensional objective space need to be measured and then selected as the representation of the Pareto front, which causes a high computation cost. In order to overcome the above difficulties, the dimensionality reduction scheme is naturally considered to reduce the number of objectives while maintaining their information as much as possible. For example, an effective approach which uses the principal component analysis approach has been proposed to reduce the number of objectives. In this approach, the correlation between lower dimensions of each objective is computed as the criterion of dimensionality reduction. A preset approximated front of non-dominated solutions is used to determine the redundant objectives. However, in many real-world applications, the objectives of a given problem sometimes cannot be reduced only according to the above methods, which are essentially based on the order of importance. This paper develops a new dimensionality reduction approach with Kriging model, called Tk-MaOEA, to resolve the curse of dimensionality. The main idea of Tk-MaOEA is to reduce the computation complexity in the large search space via using the multi-dimensional compression based on transfer matrix. At the global space optimization level, a transfer matrix with Gram–Schmidt orthogonalization is designed to reduce the redundant objectives without any reference vectors in advance. Note that during the transferring process, the properties of the objectives in the high-dimensional search space should be kept as much as possible. At the objective optimization level, the Kriging model is utilized to reduce the evaluation cost for each objective. By using the above mechanisms, we can obtain a simplified optimization in a small objective space, which not only guarantees the effectiveness of conventional evolutionary operators, but also improves the performance of Pareto optimality. In Tk-MaOEA, after the objective reduction based on transfer matrix, the Kriging model is constructed for each objective based on the Latin hypercube sampling. In this approach, the transfer matrix and Kriging model perform distinctly, yet complementarily. The transfer matrix can enhance the convergence, and the farthest-candidate selection method assisted by Kriging model can improve the diversity of solutions. Therefore, Tk-MaOEA can maximize the benefits of the dimensionality reduction scheme assisted by Kriging model in the many-objective optimization. To sum up, our contributions mainly include: at the global space optimization level, a new transfer matrix is developed to reduce the redundant objectives while the original property of the involved problem is kept well. The proposed approach can compress the large search space without any reference vectors or points in advance. At the objective optimization level, the Kriging model is devised for each objective to further reduce computation cost. This Bayesian based surrogate model can measure not only the objective value itself but also the stochastic error of the obtained approximation, which is essentially conducive to improve the accuracy of the optimization. The multi-scale normalization approach is employed to avoid the normalization distortion in the high-dimensional objective space. This is a significant operation for Tk-MaOEA to keep unchanged spatial distribution. The remainder of this paper is organized as follows. Section 2 elucidates related works. In Section 3 the proposed algorithm is given in detail. In Section 4, the experiment is conducted on a series of well-defined test functions. Finally, Section 5 outlines the conclusions.", "section": "Related Work", "doi": "10.1016/j.ins.2019.01.030", "references": [1484287363, 1493761729, 1519778441, 1588375755, 1605810911, 1842155679, 1868857639, 2011174137, 2022485595, 2025822274, 2027009057, 2032883187, 2033861738, 2036595457, 2037894471, 2055142708, 2075016777, 2082228576, 2105511939, 2111526171, 2126105956, 2134152443, 2140095548, 2143381319, 2148933727, 2154693991, 2166898117, 2173396220, 2278123559, 2343601797, 2430564265, 2469950730, 2546299924, 2568138291, 2606827295, 2609456856, 2737639200, 2740985535, 2773474358, 2794136136, 2801960628, 2807831334]}
{"paragraph": "Networked multirobot systems can be a potential aid in applications such as search and rescue, autonomous exploration, sensing and communication infrastructure, etc. Coordinating a group of robots involves repetitive tasks of rendezvous, formation control, and flocking of the distributed robots. Here, we focus on the rendezvous problem, in which the distributed robots need to gather at a common location either based on consensus or based on immediate goals. Significant progress has been made in the recent works addressing the rendezvous problem, namely by introducing distributed control laws for realizing a consensus to gather robots at a common location and by enabling fault tolerance in such controllers. Most of the studies rely on the assumption that the initial interaction topology is densely or completely connected or that a global coordinate frame of reference is available. However, such strong assumptions are difficult to satisfy in challenging GPS-denied large environments and in resource-limited distributed robots. Encouraged by the reconfigurable coordination mechanism, we show a motivating example scenario in which we assume only a few robots have advanced capabilities, namely to map and analyze the environment with 3-D sensors, and/or are different from other robots in terms of coverage and visibility. In such a scenario, it would be appropriate that these advanced robots lead the other member robots to their next target positions while avoiding obstacles in a cluttered environment. To realize this herding behavior, all the robots need to rendezvous at one of the group-leading robots. The interaction graph or topology between robots is sparsely connected, reflecting a realistic multirobot setting. Furthermore, we take inspiration from the autonomous recharging and coverage planning applications where gathering robots in multiple groups would be of significant interest. Therefore, we address the problem of distributed multipoint rendezvous without global localization. In this paper, we provide a solution to the multipoint rendezvous problem and contribute in the following ways. We propose a distributed and coordinate-free rendezvous control algorithm based on the hierarchical tracking of a connected graph. We show the theoretical guarantees of the algorithm such as convergence and link maintenance. The proposed approach is adaptive to changes in network topology and is capable of handling network and mobility faults. We extend the above algorithm to perform rendezvous at multiple points led by leader robots. We also propose to choose the leaders in each group by optimizing the distance traveled to realize the rendezvous task. We demonstrate the proposed algorithm in terms of scalability, efficiency, robustness to failures, and herding along with its dynamic nature through extensive simulation experiments in the Robotarium multirobot testbed. To validate the performance of our hierarchical tracking algorithm, we compare it against two algorithms available in the literature: the standard consensus algorithm and the circumcenter-based consensus algorithm. This paper is organized as follows. In Section II, we analyze the literature and show how we depart from others. Section III provides some background on graph theory, assumptions, and definitions. Specifically, the multipoint rendezvous problem is formulated in Section III-C. Then in Section IV, we introduce the proposed algorithm to realize a multipoint rendezvous of robots with limited speeds. We present the experiments and results in Section V and conclude this paper in Section VI.", "section": "Introduction", "doi": "10.1109/TCYB.2018.2868870", "references": [1518154293, 1572260348, 1969573308, 1972603945, 1979569034, 1987474585, 1993693205, 2063801303, 2081475880, 2097569568, 2112141968, 2134100244, 2150973821, 2159465607, 2165744313, 2177443133, 2344219546, 2409707557, 2485926893, 2564284893, 2593733243, 2740693561, 2785694108, 2788957818, 2963673039, 2964138223]}
{"paragraph": "The Resource-Constrained Project Scheduling Problem (RCPSP), introduced by Pritsker et al. in 1969, holds a central place in project management and has given rise to a rich body of research. This problem is important for practitioners as well with various applications ranging from project management software to systems for production planning and scheduling. The RCPSP can be formally defined as follows. Let G = (V, A) be an acyclic graph (often referred to as an activity-on-node network) where V = {0, 1,…, n, n + 1} is the vertex set and A is the arc set. Each vertex i = 1, …, n represents an activity having a known nonnegative duration di, while vertices 0 and n + 1 correspond to the start and end of the project, respectively, each with a fictitious duration d0 = dn+1 = 0. Once started, an activity may not be interrupted, i.e., preemption is not allowed. The planning horizon is generally assumed to be divided into T time intervals of equal length, ∀t = 1, …, T, called periods (e.g., days) and activity durations are usually given as discrete multiples of one period. Due to technological requirements, the presence of precedence relations between activities is required. Each arc (i, j) ∈ A thus represents a finish-start precedence relation, with zero time-lag, indicating that activity j may not be started before its predecessor i is completed. A fixed set R of renewable resource types, each of constant nonnegative capacity Rk, ∀k = 1, …, K, is available in each period during the execution of the project. The resources are called renewable because they are fully available at each period of the planning horizon. Each activity i ∈ V is associated with a known nonnegative resource requirement, rik ≤ Rk, which represents the constant usage of resource type k ∈ R per period by activity i. The two dummy activities 0 and n + 1 have no resource usage. In practice, a resource often represents a group of workers of the same specialized skill or a group of identical machines. Hence, the assumption that resource requests and availabilities are integral is satisfied. The RCPSP consists of finding a schedule of the activities, i.e., a set of starting times, with minimum project duration (project makespan). The project makespan is defined as the time difference between the start and the end of the project, constrained by each activity is completed exactly once, each activity starts if all its predecessor activities are completed, and—for each time period and for each resource type k—the renewable resource amounts required by the activities in progress do not exceed the resource availability, Rk. Blazewicz, Lenstra, and Rinnooy Kan showed that the RCPSP is NP-hard. In the last two decades, the applications and difficulties of the RCPSP have attracted increasing interest from researchers as illustrated by various surveys. A handbook on the state-of-the-art approaches to project management and scheduling and three books on the RCPSP have also been published. To this day, it seems that the best exact methods can only solve instances involving at most 60 activities where the instances are not highly resource-constrained. Because realistic projects often exceed this size and solutions must often be determined quickly, most of the research effort has concentrated on heuristics for providing the best trade-off between accuracy, computation speed, ease of implementation, and flexibility. Over the past 40 years, several families of heuristics have been proposed for the RCPSP. These can be broadly classified into three main classes: single-pass and multi-pass heuristics—predominantly developed until 1990—and metaheuristics whose growth has occurred in the last two decades. In the early single-pass and multi-pass heuristics, much of the emphasis was put on quickly obtaining a feasible schedule. These two classes of heuristics use a schedule generation scheme, either serial or parallel, and one or more priority rules to construct one or more schedules. Metaheuristics follow rules to deeply explore the most promising regions of the solution space in the hope of finding high quality solutions but at the cost of increased computing time over heuristics. Metaheuristics have proven to be very powerful for solving the RCPSP. Simulated annealing, tabu search, artificial immune, bee colony optimization, genetic algorithms, particle swarm optimization, scatter search, and ant colony optimization are, among others, often listed in recent surveys. Furthermore, over the last few years, quite an impressive number of new heuristic approaches have been proposed for tackling the RCPSP. Rather than purely following the concept of one single metaheuristic, these new methods combine various metaheuristic strategies, components, and other optimization techniques. These approaches are commonly referred to as hybrids. In general, two main hybrid techniques are used to solve the RCPSP. The first technique integrates several, possibly different metaheuristic related concepts. A prominent example of this first category is the use of local search procedures within population-based methods. The second category includes hybrids resulting from the combination of metaheuristics with other techniques of operations research such as constraint programming or tree-based search methods. In this paper, we focus on the first category of hybrid metaheuristics to solve the RCPSP, i.e., hybrids based on the combination of multiple metaheuristic approaches. To our knowledge, these have not been comprehensively surveyed and compared until now. For the sake of parsimony, we concentrate on hybrid methods that have been tested on the sets of benchmark instances from the well-known PSPLIB library. The objective of this paper is threefold. First, the fundamental elements of hybrid metaheuristics developed over the past 20 years to address the RCPSP are reviewed. Second, the experimental results of the different hybrids on the PSPLIB data instances are reported and a computational comparison is made to identify the most successful RCPSP hybrids currently available. Finally, from this comparison, the common fundamental strategies of the best RCPSP hybrids are examined. The remainder of this paper is organized as follows. In Section 2, the fundamental elements of hybrid metaheuristics developed to address the RCPSP are reviewed. In Section 3, the results of the different hybrids on the PSPLIB datasets are reported. A unifying analysis of state-of-the-art hybrid metaheuristics for the RCPSP is presented in Section 4. Overall conclusions and suggestions for future research are presented in Section 5.", "section": "Introduction", "doi": "10.1016/j.ejor.2019.01.063", "references": [50630655, 189789383, 299188562, 1498742891, 1522176002, 1575801975, 1613273921, 1711082852, 1915639963, 1969355111, 1969848548, 1974300423, 1980764519, 1987602810, 1993807130, 2000872583, 2006980174, 2008354857, 2009013102, 2014625519, 2020351850, 2021978315, 2024742034, 2034617747, 2035872318, 2037993354, 2038345112, 2041080861, 2042058522, 2046467279, 2047454254, 2050685121, 2052509829, 2054234206, 2054756504, 2057018686, 2062705997, 2062975634, 2064360504, 2068990620, 2070891082, 2074775883, 2075675164, 2076831786, 2081923486, 2084446444, 2086420968, 2087577248, 2095332973, 2104473961, 2114310439, 2121852392, 2140742285, 2144666752, 2150555778, 2150834164, 2152999180, 2153864093, 2154487696, 2159427933, 2165269905, 2209000138, 2268894645, 2271829641, 2296148459, 2623314515, 2744436276, 2966840962]}
