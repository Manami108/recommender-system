{"paragraph": "This paper presents our work to enhance a state-of-the-art level generator (Sketch-to-Level Generator) that generates levels for an Angry-Birds-like game from drawn sketches. To achieve this task, Cycle-Consistent Adversarial Networks (CycleGAN) are used. CycleGAN is trained using two datasets: sketch drawings and typical level-structures. The former are taken from Google’s Quick, Draw! datasets, and the latter from the winning level generator at the 2017 and 2018 AIBIRDS level generation competitions. The output of the trained CycleGAN is used as the input of Sketch-to-Level Generator. Our results show that the proposed preprocessing technique using CycleGAN allows Sketch-to-Level Generator to more successfully generate levels from arbitrary sketch drawings. Sketch-based applications have been widely used in various research themes. Sketch-based has also moved to research with game themes to make games more interesting. Here, we focus on a state-of-the-art example of such techniques that to generate a level from a drawn sketch with the aim of promoting players’ creativity due to drawing their own levels. In particular, their generator, henceforth called Sketch-to-Level Generator, transforms a human drawing to a level in a game called Science Birds, a clone version of Angry Birds widely employed for academic research. Although Sketch-to-Level Generator can in most cases generate levels similar from input drawing that contain rectangle-shape objects, it struggles to generate a stable level when an input sketch contains objects with curve lines. To solve the aforementioned issue in Sketch-to-Level Generator, this study proposes to first preprocess a given sketch using Cycle-Consistent Adversarial Networks (CycleGAN), developed by Zhu et al.. CycleGAN is a powerful technique that learns a transformation between two image distributions. In particular, we expect CycleGAN to transfer a block-shape style, usually seen in Angry Birds levels, to the content of a given sketch, from which Sketch-to-Level Generator can readily generate a level. Quick, Draw! is an online drawing game by Google. This game uses neural networks to learn the player’s doodling. Currently, Google provides datasets of this game, containing 345 categories of drawings and more than 10.000 images per category. Our study used parts of the Quick, Draw! datasets for training CycleGAN, in particular, those selected by extracting the first ten images in each category. IratusAves is a level generator for Science Birds developed by Stephenson and Renz. It was the winning entry at the 2017 and 2018 AIBIRDS level generation competitions. This generator is capable of generating stable and solvable Science Birds levels with a high variety. In our study, to prepare a dataset containing typical Science Birds structures, IratusAves was used to generate 3500 levels that contain no pig and TNT (explosive) objects, subject to a constraint that only rectangular block types are used. This constraint is due to the specification of blocks used by Sketch-to-Level Generator. These levels will be used as the second dataset in the training of CycleGAN. CycleGAN is an artificial neural network that can perform image-to-image translation without pairing information, facilitates the preparation of training data. Its developers publicly provide CycleGAN on both Pytorch and Torch implementations. Due to not using paired information during the training of CycleGAN, the training datasets do not need to contain pairs of a sketch and a level similar to it. As a result, our study was able to prepare training data from Quick, Draw!’s sketch images and IratusAves’ block-structure images separately. During the training, given an image from the former dataset, the generator in CycleGAN is aimed at generating an image similar to a Science-Birds-like structure, and the discriminator attempts to gain the ability to detect that it is a faked one and label any of those in the latter dataset a real one. Sketch-to-Level Generator provides rectangular and non-rectangular modes for a given image input to generate a stable level. Their algorithm detects corners of the input sketch and identifies horizontal and vertical edges between detected corners. The identified edges will be reformed to a rectangle. Our study used the generator with the rectangular mode to validate results when CycleGAN is used and the non-rectangular mode otherwise.", "name": "fajar", "number": "1"}
{"paragraph": "Generative Adversarial Nets (GANs) have made a dramatic leap in modeling high dimensional distributions of visual data. In particular, unconditional GANs have shown remarkable success in generating realistic, high quality samples when trained on class specific datasets (e.g., faces, bedrooms). However, capturing the distribution of highly diverse datasets with multiple object classes (e.g. ImageNet) is still considered a major challenge and often requires conditioning the generation on another input signal or training the model for a specific task (e.g. super-resolution, inpainting, retargeting). Here, we take the use of GANs into a new realm – unconditional generation learned from a single natural image. Specifically, we show that the internal statistics of patches within a single natural image typically carry enough information for learning a powerful generative model. SinGAN, our new single image generative model, allows us to deal with general natural images that contain complex structures and textures, without the need to rely on the existence of a database of images from the same class. This is achieved by a pyramid of fully convolutional light-weight GANs, each is responsible for capturing the distribution of patches at a different scale. Once trained, SinGAN can produce diverse high quality image samples (of arbitrary dimensions), which semantically resemble the training image, yet contain new object configurations and structures. Modeling the internal distribution of patches within a single natural image has been long recognized as a powerful prior in many computer vision tasks. Classical examples include denoising, deblurring, super resolution, dehazing, and image editing. The most closely related work in this context is where a bidirectional patch similarity measure is defined and optimized to guarantee that the patches of an image after manipulation are the same as the original ones. Motivated by these works, here we show how SinGAN can be used within a simple unified learning framework to solve a variety of image manipulation tasks, including paint-to-image, editing, harmonization, super-resolution, and animation from a single image. In all these cases, our model produces high quality results that preserve the internal patch statistics of the training image. All tasks are achieved with the same generative network, without any additional information or further training beyond the original training image.", "name": "fajar", "number": "2"}     