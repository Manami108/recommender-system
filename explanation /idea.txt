1. Data Ingestion & Preprocessing

    Load JSON Records
        Each record typically includes id, title, abstract, fos, year, n_citation, references, authors, etc.
        Parse them into a DataFrame, relational database, or document store.

    Data Cleaning & Normalization
        Remove duplicates (papers that appear more than once).
        Check missing fields (e.g., if some records lack abstract or fos, decide on handling strategy: drop them, fill with placeholders, etc.).
        Normalize fos labels (e.g., “Machine Learning” vs. “ML” or synonyms).

    Storage Architecture
        Graph database (Neo4j, TigerGraph) or adjacency lists for citations.
        Text & fos can be in a standard DB or search engine (Elasticsearch, etc.).
        If you’re dealing with millions of records, consider scalable / distributed data stores.

2. Building the Citation Graph

    Define Graph Structure
        Nodes: Each paper is a node.
        Edges: A→BA→B if paper AA cites BB.

    Basic Graph Metrics
        In-degree / out-degree: number of citations (or references) a paper has.
        PageRank or HITS: measure paper influence.
        Optional: Community detection, connected components.

    Storing & Indexing
        If using adjacency lists, store something like graph[A] = [B1, B2, ...].
        If using Neo4j, each paper is a node with properties (fos, year, etc.).

3. Text Embeddings & Vector Representations

    Preprocessing
        Clean and tokenize the text (title + abstract).
        Combine title + abstract into one text field or keep them separate (your choice).

    Embeddings
        Transformer-based models (SciBERT, Sentence-BERT) for scientific text.
        Or classical methods (TF-IDF, Word2Vec) if you need a simpler approach.

    Storing Embeddings
        Use a vector database (Faiss, Milvus, Pinecone) for approximate nearest neighbor (ANN) searches.
        Keep an ID → embedding mapping so you can quickly retrieve or compare embeddings.

4. Topic Modeling (Sub Semantic Space)

    Approach
        LDA (Latent Dirichlet Allocation), NMF, or Neural Topic Models.
        Decide on KK topics (20, 50, 100, etc.).

    Training
        Use the same preprocessed text (title + abstract) as input for LDA/NMF.
        After training, each paper gets a topic distribution (e.g., topicVecPtopicVecP​ of dimension KK).

    Integration
        Store each paper’s topic distribution in your DB or local file.
        This is separate from (or complementary to) the dense transformer embedding.

5. Incorporating Fields of Study (fos)

    fos as Metadata
        Each paper has 1+ fos labels, e.g., {“Machine Learning”, “Computer Vision”}.
        Treat fos as coarse-grained topics or domain labels.

    Possible Uses
        Filtering: If the user only wants certain fos (e.g., “Biology”), filter out non-biology papers.
        Similarity:
            Compute Jaccard similarity over fos sets.
            Combine with other similarity scores.
        Diversity:
            Encourage recommending papers from different fos.
        Bridging:
            Identify multi-fos papers that might link two or more distinct domains.

6. Candidate Generation
6.1. Content-Based

    ANN Search (Embeddings)
        For a user’s known interest vector (or a seed paper’s embedding), retrieve top-K nearest neighbors from the vector database.

    Topic-Based
        If you want a simpler or more interpretable approach, do top-K via topic distribution similarity (cosine or Jensen-Shannon divergence).

    fos-Based Filter
        If the user (or system) wants to restrict to certain fields, only search among papers with matching fos.

6.2. Citation-Based

    Direct Citations:
        Papers directly referenced by the seed paper (or referencing the seed paper).
    Co-citation / Bibliographic Coupling:
        Identify papers that share references or are often cited together.
    Graph Embeddings:
        Node2Vec/DeepWalk embeddings for top-K most similar nodes in the citation graph space.

6.3. Merging Candidate Pools

    Combine:
        Content-based top-K
        Citation-based top-K
        Possibly fos-based top-K if that is separate.
    Union or intersection to form a larger candidate pool (e.g., 200–500 papers) before final ranking.

7. Ranking & Re-Ranking
7.1. Initial Ranking Score

A hybrid formula could be:
score(paper)=α×citation_score+β×embedding_similarity+γ×topic_similarity+θ×fos_similarity
score(paper)=α×citation_score+β×embedding_similarity+γ×topic_similarity+θ×fos_similarity

Where:

    citation_score could be PageRank or co-citation overlap.
    embedding_similarity is cosine similarity with the user’s interest or seed paper’s embedding.
    topic_similarity is the similarity of LDA distributions (optional if you rely heavily on embeddings).
    fos_similarity could be Jaccard on fos sets (optional, or incorporate it in topic_similarity if fos is considered a “topic”).

Tune α,β,γ,θα,β,γ,θ based on your evaluation metrics.
7.2. Diversity Re-Ranking

    Once you get an initial sorted list, you can re-rank to ensure the top-N is not too homogeneous:
        Intra-list Similarity: penalize papers that are very similar to already selected ones.
        fos Diversity: ensure you include different fos categories if user wants cross-domain coverage.
        Topic Distribution: ensure coverage of multiple LDA topics if that’s a goal.

7.3. Bridging / Cross-Domain Factor

    If a paper cites references from multiple fos or has a high internal topic diversity (via LDA distribution in references), you can add a “bridging bonus”:
    scorefinal=scoreinitial+δ×bridging_score
    scorefinal​=scoreinitial​+δ×bridging_score
    This highlights interdisciplinary papers that might be especially novel or relevant for users exploring multiple fields.

8. Advanced: Author & Topic Evolution (Optional)

    Author-Level Data
        For each paper, store authors.
        For each author, track which fos or LDA topics appear in their authored/cited papers over time.

    Temporal Analysis
        Identify which topics or fos an author is moving into or away from.
        If you see an author starting to cite “Medical Imaging,” you can recommend more papers in that area.

    Recommendation
        Personalize recommendations by focusing on the author’s emerging fos or topics.

9. System Architecture & Pipeline

    Offline Pipeline
        Ingest & Clean: parse JSON, fix IDs, remove duplicates.
        Build Graph: adjacency list + compute PageRank (or other metrics).
        Compute Text Embeddings: for each paper’s abstract/title.
        Run Topic Modeling: LDA or NMF, store topic vectors.
        Store Data:
            Graph edges in a graph DB or adjacency structure.
            Embeddings in a vector DB (Faiss, Pinecone, etc.).
            fos sets + LDA vectors in your main data store.

    Online / Query-time Steps
        Identify User Profile / Query: user might have a preference for certain fos or a seed paper.
        Candidate Generation: combine:
            ANN search in embedding space
            Citation-based neighbors
            fos-based filtering (optional)
        Initial Ranking: apply a hybrid scoring function.
        Re-Ranking for Diversity or Bridging: ensure variety and highlight cross-domain papers.
        Present Top-N: show recommendations, possibly with topic/fos labels for explainability.

10. Evaluation

    Offline
        If you have ground truth (papers eventually cited by an author), measure recall or nDCG.
        If you have partial user feedback, measure how often recommended papers appear in actual citation lists.

    Online (if you have a live system)
        Track user clicks/downloads or reading sessions.
        A/B tests: compare systems with or without fos-based re-ranking or bridging factor.

    Diversity Metrics
        Evaluate intra-list similarity, fos coverage, or topic coverage in the recommended sets.

Putting It All Together

Here’s the big picture flow:

    Data Preparation & Storage
        Construct citation graph, compute embeddings, train topic model (LDA), and store fos labels for each paper.

    Candidate Generation
        For a user or seed paper:
            Retrieve top-K by embedding similarity.
            Retrieve top-K from citation-based adjacency or co-citation.
            (Optionally) filter or incorporate fos to limit or enrich certain fields.

    Ranking
        Combine scores from:
            Citation (PageRank, co-citation)
            Embeddings (cosine similarity)
            Topic (LDA distribution similarity)
            fos (Jaccard or presence/absence overlap)

    Re-Ranking for Diversity
        Possibly re-order the top list to ensure coverage across different fos or topics.

    Final Recommendations
        Return a curated list, each with some meta info (fos labels, snippet of abstract, year, etc.).

    Feedback Loop
        Collect user interactions or subsequent citations to refine weighting or topic modeling in future updates.

