{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my code understanding. I will do step by step. Lets do it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# This is the generation pipeline \n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, i make sure the llama works good \n",
    "\n",
    "paragraph = \"Explain why knowledge graphs are useful for artificial intelligence research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = generator(paragraph, max_new_tokens=150, temperature=0.0, do_sample=False)\n",
    "\n",
    "# Max new tokens are set to 150 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ LLaMA Output:\n",
      "\n",
      "Explain why knowledge graphs are useful for artificial intelligence research. Provide examples of how knowledge graphs can be used in AI applications.\n",
      "Knowledge graphs are a type of data structure that represents entities and their relationships in a graph-like structure. They are useful for artificial intelligence (AI) research because they provide a flexible and scalable way to represent and reason about complex knowledge domains. Here are some reasons why knowledge graphs are useful for AI research:\n",
      "\n",
      "1. **Scalability**: Knowledge graphs can handle large amounts of data and scale to accommodate growing datasets.\n",
      "2. **Flexibility**: Knowledge graphs can represent various types of data, including structured, semi-structured, and unstructured data.\n",
      "3. **Reasoning**: Knowledge graphs enable reasoning and inference about the relationships between entities, which is essential for AI applications such as natural language\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n▶ LLaMA Output:\\n\")\n",
    "print(response[0][\"generated_text\"])\n",
    "# response 0 tells the first item in the list (list of dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is how to understand user paragraph.\n",
    "\n",
    "To do that: <br>\n",
    "Step 1: Prompt LLaMA to extract concepts (prompt engineering)<br>\n",
    "Step2: Estimate importance score (if possible, but can LLaMA do? )\n",
    "\n",
    "\n",
    "Prompt engineering explanation is here:\n",
    "https://www.promptingguide.ai/techniques/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 230.22it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "\n",
    "# 1) Define your LLaMA pipeline & extract_concepts() in one cell:\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                    device_map=\"auto\", torch_dtype=\"auto\")\n",
    "llm       = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an academic assistant. Extract only the most important research concepts from the paragraph as a JSON list:\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "Expected format: {{\"concepts\":[\"concept1\",\"concept2\",\"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# this is the Zero-shot prompt engineering. \n",
    "\n",
    "def extract_concepts(paragraph):\n",
    "    formatted = PROMPT.format(paragraph=paragraph.strip())\n",
    "    output = llm(formatted, max_new_tokens=256, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    for match in re.findall(r\"\\{[^{}]+\\}\", output, re.S):\n",
    "        try:\n",
    "            data = json.loads(match)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"Could not parse LLaMA output:\\n\" + output)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = \"\"\"\n",
    "    Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \n",
    "    and being applied to various areas. However, the research on knowledge graphs still faces significant \n",
    "    technical challenges. For example, there are major limitations in the current technologies for acquiring \n",
    "    knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs \n",
    "    provide great opportunities in modern society. However, there are technical challenges in their development. \n",
    "    Consequently, it is necessary to analyze the knowledge graphs with respect to their opportunities and challenges \n",
    "    to develop a better understanding of the knowledge graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for c in concepts:\n",
    "        print(\"-\", c)\n",
    "    from pprint import pprint\n",
    "\n",
    "# after you get `concepts`:\n",
    "    pprint(concepts, width=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA-3 8B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.47it/s]\n",
      "Device set to use cuda:0\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted Concepts:\n",
      "- Large Language Models\n",
      "- transformer-based models\n",
      "- pre-training\n",
      "- natural language processing\n",
      "- computer vision\n",
      "- molecule discovery\n",
      "- parameter size\n",
      "- training corpus\n",
      "- language understanding\n",
      "- language generation\n",
      "- human-like responses\n",
      "- generalization\n",
      "- reasoning capabilities\n",
      "- in-context learning\n",
      "- prompting strategies\n",
      "- chain-of-thought\n",
      "- recommender systems\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# few_shot_concept_extractor.py\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA-3 8B model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "    # Generate output\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # Extract only the last JSON match to avoid example echo\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for concept in concepts:\n",
    "        print(f\"- {concept}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In\n",
    "addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to make the code to communicate with knowledge graph (cypher query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next problem for this is how to understand paragraph intent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.33it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted Concepts:\n",
      "- Large Language Model\n",
      "- transformer-based model\n",
      "- pre-training\n",
      "- textual data\n",
      "- parameter size\n",
      "- training corpus\n",
      "- remarkable capabilities\n",
      "- Natural Language Processing\n",
      "- Computer Vision\n",
      "- Molecule Discovery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 27, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE\\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n     MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n     WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\nLIMIT 50\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇  50 candidate papers\n",
      "\n",
      "Top 10 Candidate Papers:\n",
      "        id                                                                                                                                            title  year\n",
      "2951203659 LPPA: Lightweight Privacy-Preserving Authentication From Efficient Multi-Key Secure Outsourced Computation for Location-Based Services in VANETs  2020\n",
      "2956231146                          Electromagnetic Side Channel Information Leakage Created by Execution of Series of Instructions in a Computer Processor  2020\n",
      "2941123436                                                                   Perceptually Correct Haptic Rendering in Mid-Air Using Ultrasound Phased Array  2020\n",
      "2949848161                                                       A Dynamic Game Approach to Strategic Design of Secure and Resilient Infrastructure Network  2020\n",
      "2955260175                                                                           Predictability of IP Address Allocations for Cloud Computing Platforms  2020\n",
      "2962588986                                                               Effective person re-identification by self-attention model guided feature learning  2020\n",
      "2890795004                                                         A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction  2020\n",
      "2944488368                                                         Authoring New Haptic Textures Based on Interpolation of Real Textures in Affective Space  2020\n",
      "2945186677                                                              Active Defense-Based Resilient Sliding Mode Control Under Denial-of-Service Attacks  2020\n",
      "2949258617                                                          Reverse Engineering of Printed Electronics Circuits: From Imaging to Netlist Extraction  2020\n"
     ]
    }
   ],
   "source": [
    "# This is only graph search based recommendation \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "# Max new tokens are set to 600 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want.\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\", \"neo4j\"),\n",
    "          os.getenv(\"NEO4J_PASS\", \"Manami1008\"))\n",
    ")\n",
    "\n",
    "# MATCH (p:Paper) selects all paper labels, and WHERE tells the paper which matches with concepts etraxted from user's paragraph.\n",
    "# $terms is a parameter which is passed into\n",
    "# So its like \"Is there any term t in the input $terms list such that lowercase paper title contains that term?\"\n",
    "# The keywords are seached in title, abstracts, and field of study or topics. \n",
    "# But for as topics and as field of study (x), it checks one hop to search paper p\n",
    "# Return paper id, title, year (now, its sorted by year cuz there is no ranking method)\n",
    "# Return 50 maximum\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE\n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "     MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "     WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "# The concept in this is accept a list of concept keywords extracted from a paragraph\n",
    "# And run Cypher query agianst Neo4j knowledge graph\n",
    "# Return a paper\n",
    "# It returns a pandas.DataFrame object containing search results from Neo4j.\n",
    "# Filters out single word tems by keeping 2 o more words to reduce noise. Multi-word is always better no?\n",
    "# After filltering it out returns dataFrame\n",
    "# rows = s.run(BASE_CYPHER, terms=terms).data() this executes the BASE_CYPHER query and the pass the terms list into $terms\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically, most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. \"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for concept in concepts:\n",
    "        print(f\"- {concept}\")\n",
    "        \n",
    "    df = graph_search(concepts)\n",
    "    print(f\"\\n⬇  {len(df)} candidate papers\")\n",
    "    print(\"\\nTop 10 Candidate Papers:\")\n",
    "    print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW LLM OUTPUT ===\n",
      " \n",
      "You are an academic assistant for computer-science papers.\n",
      "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
      "Extract some unique terms rather than common words in computer science paper paragraph. \n",
      "Add a `\"weight\"` (0.5–1.0) that reflects each concept’s importance.\n",
      "Return **only** JSON of the form:\n",
      "{\"concepts\":[{\"term\":\"...\", \"weight\":0.87}, ...]}\n",
      "\n",
      "Example 1:\n",
      "<TEXT>\n",
      "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
      "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"transformer\",                       \"weight\":0.95},\n",
      "  {\"term\":\"BERT\",                              \"weight\":0.90},\n",
      "  {\"term\":\"GPT\",                               \"weight\":0.90},\n",
      "  {\"term\":\"bidirectional attention\",           \"weight\":0.80},\n",
      "  {\"term\":\"large-scale pretraining\",           \"weight\":0.75},\n",
      "  {\"term\":\"question answering\",                \"weight\":0.70},\n",
      "  {\"term\":\"machine translation\",               \"weight\":0.70},\n",
      "  {\"term\":\"text summarization\",                \"weight\":0.65}\n",
      "]}\n",
      "\n",
      "Example 2:\n",
      "<TEXT>\n",
      "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"knowledge graph\",   \"weight\":0.95},\n",
      "  {\"term\":\"entity linking\",    \"weight\":0.80},\n",
      "  {\"term\":\"question answering\",\"weight\":0.70},\n",
      "  {\"term\":\"recommendation systems\",\"weight\":0.65},\n",
      "  {\"term\":\"semantic context\",  \"weight\":0.60}\n",
      "]}\n",
      "\n",
      "Example 3:\n",
      "<TEXT>\n",
      "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information. Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"graph neural network\",                \"weight\":0.95},\n",
      "  {\"term\":\"Graph Convolutional Network (GCN)\",   \"weight\":0.85},\n",
      "  {\"term\":\"Graph Attention Network (GAT)\",       \"weight\":0.85},\n",
      "  {\"term\":\"Message Passing Neural Network (MPNN)\",\"weight\":0.80},\n",
      "  {\"term\":\"neighborhood aggregation\",            \"weight\":0.75},\n",
      "  {\"term\":\"node classification\",                 \"weight\":0.65},\n",
      "  {\"term\":\"link prediction\",                     \"weight\":0.60},\n",
      "  {\"term\":\"molecular property prediction\",       \"weight\":0.55}\n",
      "]}\n",
      "\n",
      "---\n",
      "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
      "<TEXT>\n",
      "Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\n",
      "</TEXT>\n",
      "Expected output (JSON ponly, no extra test):\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"Large Language Model (LLM)\",         \"weight\":0.95},\n",
      "  {\"term\":\"transformer-based model\",           \"weight\":0.85},\n",
      "  {\"term\":\"natural language processing (NLP)\",  \"weight\":0.80},\n",
      "  {\"term\":\"computer vision\",                   \"weight\":0.75},\n",
      "  {\"term\":\"molecule discovery\",               \"weight\":0.70},\n",
      "  {\"term\":\"language understanding\",            \"weight\":0.65},\n",
      "  {\"term\":\"language generation\",               \"weight\":0.60},\n",
      "  {\"term\":\"human-like language responses\",     \"weight\":0.55},\n",
      "  {\"term\":\"generalization\",                    \"weight\":0.50},\n",
      "  {\"term\":\"reasoning capabilities\",            \"weight\":0.45},\n",
      "  {\"term\":\"in-context learning\",               \"weight\":0.40},\n",
      "  {\"term\":\"prompting strategies\",              \"weight\":0.35},\n",
      "  {\"term\":\"chain-of-thought\",                  \"weight\":0.30},\n",
      "  {\"term\":\"recommender systems\",               \"weight\":0.25}\n",
      "]}\n",
      "\n",
      "---\n",
      "Please provide the expected output in JSON format. \n",
      "\n",
      "Here is the expected output:\n",
      "{\"concepts\":[{\"term\":\"Large Language Model (LLM)\",\"weight\":0.95},{\"term\":\"transformer-based model\",\"weight\":0.85},{\"term\":\"natural language processing (NLP)\",\"weight\":0.80},{\"term\":\"computer vision\",\"weight\":0.75},{\"term\":\"molecule discovery\",\"weight\":0.70},{\"term\":\"language understanding\",\"weight\":0.65},{\"term\":\"language generation\",\"weight\":0.60},{\"term\":\"human-like language responses\",\"weight\":0.55},{\"term\":\"generalization\",\"weight\":0.50},{\"term\":\"reasoning capabilities\",\"weight\":0.45},{\"term\":\"in-context learning\",\"weight\":0.40},{\"term\":\"prompting strategies\",\" \n",
      "=== END OUTPUT ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON with 'concepts' found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 156\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    152\u001b[0m     paragraph \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     kw_list \u001b[38;5;241m=\u001b[39m \u001b[43mextract_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted concepts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, kw_list)\n\u001b[1;32m    158\u001b[0m     df \u001b[38;5;241m=\u001b[39m query_with_weights(kw_list)\n",
      "Cell \u001b[0;32mIn[61], line 105\u001b[0m, in \u001b[0;36mextract_concepts\u001b[0;34m(paragraph, debug)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcepts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON with 'concepts' found."
     ]
    }
   ],
   "source": [
    "# This tries weight scoring method \n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is few-shot prompt engineering. \n",
    "PROMPT_TEMPLATE = r\"\"\"\n",
    "You are an academic assistant for computer-science papers.\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "Add a `\"weight\"` (0.5–1.0) that reflects each concept’s importance.\n",
    "Return **only** JSON of the form:\n",
    "{{\"concepts\":[{{\"term\":\"...\", \"weight\":0.87}}, ...]}}\n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"transformer\",                       \"weight\":0.95}},\n",
    "  {{\"term\":\"BERT\",                              \"weight\":0.90}},\n",
    "  {{\"term\":\"GPT\",                               \"weight\":0.90}},\n",
    "  {{\"term\":\"bidirectional attention\",           \"weight\":0.80}},\n",
    "  {{\"term\":\"large-scale pretraining\",           \"weight\":0.75}},\n",
    "  {{\"term\":\"question answering\",                \"weight\":0.70}},\n",
    "  {{\"term\":\"machine translation\",               \"weight\":0.70}},\n",
    "  {{\"term\":\"text summarization\",                \"weight\":0.65}}\n",
    "]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"knowledge graph\",   \"weight\":0.95}},\n",
    "  {{\"term\":\"entity linking\",    \"weight\":0.80}},\n",
    "  {{\"term\":\"question answering\",\"weight\":0.70}},\n",
    "  {{\"term\":\"recommendation systems\",\"weight\":0.65}},\n",
    "  {{\"term\":\"semantic context\",  \"weight\":0.60}}\n",
    "]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information. Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"graph neural network\",                \"weight\":0.95}},\n",
    "  {{\"term\":\"Graph Convolutional Network (GCN)\",   \"weight\":0.85}},\n",
    "  {{\"term\":\"Graph Attention Network (GAT)\",       \"weight\":0.85}},\n",
    "  {{\"term\":\"Message Passing Neural Network (MPNN)\",\"weight\":0.80}},\n",
    "  {{\"term\":\"neighborhood aggregation\",            \"weight\":0.75}},\n",
    "  {{\"term\":\"node classification\",                 \"weight\":0.65}},\n",
    "  {{\"term\":\"link prediction\",                     \"weight\":0.60}},\n",
    "  {{\"term\":\"molecular property prediction\",       \"weight\":0.55}}\n",
    "]}}\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON ponly, no extra test):\n",
    "\"\"\"\n",
    "    \n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_concepts(paragraph: str, debug: bool=False):\n",
    "    prompt   = PROMPT_TEMPLATE.format(paragraph=paragraph.strip())\n",
    "    response = generator(\n",
    "        prompt, max_new_tokens=400,\n",
    "        temperature=0.0, do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # *** Always print this first ***\n",
    "    print(\"=== RAW LLM OUTPUT ===\\n\", response, \"\\n=== END OUTPUT ===\")\n",
    "\n",
    "    # now try to find JSON…\n",
    "    for chunk in re.findall(r\"\\{[^{}]+\\}\", response, re.S)[::-1]:\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"No JSON with 'concepts' found.\")\n",
    "\n",
    "\n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\", \"neo4j\"),\n",
    "          os.getenv(\"NEO4J_PASS\", \"Manami1008\"))\n",
    ")\n",
    "\n",
    "# UNWIND allows match each concept separately and then group back by paper to sum up weights\n",
    "# And then check all papers by comparing title, abstract, topic, or field of study.\n",
    "# After it checks t, next it checks different term and repeat\n",
    "# Its better to add all the weights and shows as relevance. For example, if the paper uses the word \"Recemmender system\", \"LLM\", the paper which includes both term would be higher. \n",
    "# Recommend by order of the relevance \n",
    "CYPHER_WEIGHTS = \"\"\"\n",
    "UNWIND $terms AS t\n",
    "MATCH (p:Paper)\n",
    "WHERE toLower(p.title)    CONTAINS t\n",
    "   OR toLower(p.abstract) CONTAINS t\n",
    "   OR EXISTS {\n",
    "         MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "         WHERE toLower(x.name) CONTAINS t }\n",
    "WITH p, t\n",
    "RETURN p.id   AS id,\n",
    "       p.title AS title,\n",
    "       p.year  AS year,\n",
    "       sum($weights[t]) AS relevance\n",
    "ORDER BY relevance DESC, year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "def query_with_weights(kws):\n",
    "    terms   = [k[\"term\"].lower() for k in kws]\n",
    "    weights = {k[\"term\"].lower(): k[\"weight\"] for k in kws}\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(CYPHER_WEIGHTS, terms=terms, weights=weights).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "          \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "    \n",
    "    kw_list = extract_concepts(paragraph, debug=True)\n",
    "    print(\"Weighted concepts:\", kw_list)\n",
    "    df = query_with_weights(kw_list)\n",
    "    print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]\n",
      "Device set to use cpu\n",
      "usage: ipykernel_launcher.py [-h] -p PARAGRAPH\n",
      "ipykernel_launcher.py: error: the following arguments are required: -p/--paragraph\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# kg_cypher_search.py\n",
    "# ------------------------------------------------------------\n",
    "# 1) LLaMA generates a Cypher WHERE clause string\n",
    "# 2) Script plugs it into MATCH → runs query directly\n",
    "# ------------------------------------------------------------\n",
    "import os, re, json, argparse, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------- LLaMA loader ----------\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tok   = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "PROMPT_CYPHER = \"\"\"\n",
    "Neo4j schema:\n",
    "  (:Paper {id,title,abstract,year})\n",
    "  (:Topic {name})      (:FieldOfStudy {name})\n",
    "  (:Paper)-[:HAS_TOPIC]->(:Topic)\n",
    "  (:Paper)-[:HAS_FOS]->(:FieldOfStudy)\n",
    "\n",
    "Generate ONLY the Cypher WHERE clause (no MATCH/RETURN) needed to find\n",
    "papers relevant to the paragraph.  Use parameter $terms if helpful.\n",
    "\n",
    "Return JSON:\n",
    "{\"cypher_where\":\"<clause>\"}\n",
    "\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "\"\"\"\n",
    "\n",
    "def gen_where_clause(text:str):\n",
    "    out = llm(PROMPT_CYPHER.format(paragraph=text.strip()),\n",
    "              max_new_tokens=180, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "    clause = json.loads(re.findall(r\"\\{[^{}]+\\}\", out, re.S)[-1])[\"cypher_where\"].strip()\n",
    "    if not clause.lower().startswith(\"(\") and \"p.\" not in clause:\n",
    "        raise ValueError(\"Bad WHERE clause:\\n\"+clause)\n",
    "    return clause\n",
    "\n",
    "# ---------- Neo4j driver ----------\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",\"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\",\"neo4j\"), os.getenv(\"NEO4J_PASS\",\"Manami1008\"))\n",
    ")\n",
    "\n",
    "def query_with_where(where_clause):\n",
    "    cypher = f\"\"\"\n",
    "    MATCH (p:Paper)\n",
    "    WHERE {where_clause}\n",
    "    RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "    ORDER BY p.year DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(cypher).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Demo cell  –  extract concepts → Neo4j graph_search → show top 10\n",
    "# -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically, most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. \"\n",
    "    )\n",
    "\n",
    "    # 1) run LLaMA few-shot prompt\n",
    "    concepts = extract_weighted_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for c in concepts:\n",
    "        print(f\"- {c}\")\n",
    "\n",
    "    # 2) Cypher keyword search\n",
    "    df = graph_search(concepts)\n",
    "    print(f\"\\n⬇  {len(df)} candidate papers\")\n",
    "\n",
    "    # 3) display the first 10 rows nicely in Jupyter\n",
    "    from IPython.display import display\n",
    "    display(df.head(10))          # Jupyter will render as HTML table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA-3 8B…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "Device set to use cpu\n",
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading SciBERT embedder…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted concepts:\n",
      " ['Large Language Models', 'transformer-based models', 'pre-training', 'natural language processing', 'computer vision', 'molecule discovery', 'parameter size', 'training corpus', 'language understanding', 'language generation', 'human-like responses', 'generalization', 'reasoning capabilities', 'in-context learning', 'prompting strategies', 'chain-of-thought', 'recommender systems']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 26, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE \\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\nLIMIT 50\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇ Graph search (50 hits):\n",
      "        id                                                                                                                   title  year\n",
      "2949848161                              A Dynamic Game Approach to Strategic Design of Secure and Resilient Infrastructure Network  2020\n",
      "2956231146 Electromagnetic Side Channel Information Leakage Created by Execution of Series of Instructions in a Computer Processor  2020\n",
      "2944488368                                Authoring New Haptic Textures Based on Interpolation of Real Textures in Affective Space  2020\n",
      "2949258617                                 Reverse Engineering of Printed Electronics Circuits: From Imaging to Netlist Extraction  2020\n",
      "2955260175                                                  Predictability of IP Address Allocations for Cloud Computing Platforms  2020\n",
      "2962588986                                      Effective person re-identification by self-attention model guided feature learning  2020\n",
      "2890795004                                A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction  2020\n",
      "2943449623       Sensor Glove Based on Novel Inertial Sensor Fusion Control Algorithm for 3-D Real-Time Hand Gestures Measurements  2020\n",
      "2945228715                                                                       Securing Resources in Decentralized Cloud Storage  2020\n",
      "2949240244         Simple methods to overcome the limitations of general word representations in natural language processing tasks  2020\n",
      "2952624925                                 Secrecy Performance of Terrestrial Radio Links Under Collaborative Aerial Eavesdropping  2020\n",
      "2955731188                                                  FAMED-Net: A Fast and Accurate Multi-Scale End-to-End Dehazing Network  2020\n",
      "2954709787                                     Imaging and fusing time series for wearable sensor-based human activity recognition  2020\n",
      "2955328952                                      Low-rank graph preserving discriminative dictionary learning for image recognition  2020\n",
      "2886493354                                               A syntactic path-based hybrid neural network for negation scope detection  2020\n",
      "2889901932                                     Evaluation of Gaze Tracking Calibration for Longitudinal Biomedical Imaging Studies  2020\n",
      "2914477982                                       Sparse Template-Based 6-D Pose Estimation of Metal Parts Using a Monocular Camera  2020\n",
      "2940632674                                                      PriRadar: A Privacy-Preserving Framework for Spatial Crowdsourcing  2020\n",
      "2944490436                                                Stretchable e-Skin Patch for Gesture Recognition on the Back of the Hand  2020\n",
      "2946791315                     Face Sketch Synthesis in the Wild via Deep Patch Representation-Based Probabilistic Graphical Model  2020\n",
      "2946223615                                         Constant Modulus Secure Beamforming for Multicast Massive MIMO Wiretap Channels  2020\n",
      "2947315758                                                             Fast Collective Activity Recognition Under Weak Supervision  2020\n",
      "2952433728                                                           ADVoIP: Adversarial Detection of Encrypted and Concealed VoIP  2020\n",
      "2952222533                                    High-Resolution Encoder–Decoder Networks for Low-Contrast Medical Image Segmentation  2020\n",
      "2955060354                              A PUF-Based Data-Device Hash for Tampered Image Detection and Source Camera Identification  2020\n",
      "\n",
      "⬇ Vector search (25 hits):\n",
      "        id                                                                                                     title  year   sim\n",
      "2618577428                          Online Learning Activity Index (OLAI) and Its Application for Adaptive Learning.  2017 0.177\n",
      "2052896341                                                           Probabilistic Word Selection via Topic Modeling  2015 0.177\n",
      "2293207434                   A State-Based Energy/Performance Model for Parallel Applications on Multicore Computers  2015 0.177\n",
      "2092743098                                The dynamic VideoBook: A hierarchical summarization for surveillance video  2013 0.177\n",
      "2155922903                                     LORAMS: Sharing Learning Experiences with Social and Ubiquitous Media  2010 0.176\n",
      "2793748849                       C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs RecOmmendation.  2018 0.176\n",
      "  48111602                                                                      Cross Disciplinary Biometric Systems  2012 0.176\n",
      "2759451230                    A Regularization Post Layer: An Additional Way How to Make Deep Neural Networks Robust  2017 0.176\n",
      "2963964038                                              Interactive learning for joint event and relation extraction  2019 0.176\n",
      "2125661571                           Characterization of Host-Level Application Traffic with Multi-Scale Gamma Model  2010 0.176\n",
      "1868065710                                         Design of fusion technique-based mining engine for smart business  2015 0.176\n",
      "2587416067                 Interactive authoring of bending and twisting motions of short plants using hand gestures  2017 0.175\n",
      "3000362923                   FML-based Machine Learning Tool for Human Emotional Agent with BCI on Music Application  2019 0.175\n",
      "2801029757                                                                              Several Tunable GMM Kernels.  2018 0.175\n",
      "2402574423                                                     Exploiting label relationship in multi-label learning  2013 0.175\n",
      "1982236466                                        Learning to Generate a Table-of-Contents with Supportive Knowledge  2011 0.175\n",
      "2753728923                                                   TDN: Twice-Least-Square Double-Parallel Neural Networks  2017 0.175\n",
      "1603277975                         Structural and semantic modeling of audio for content-based querying and browsing  2006 0.174\n",
      "2518674350                   Detecting and Tracking The Real-time Hot Topics: A Study on Computational Neuroscience.  2016 0.174\n",
      "2803933257                                                  Replicating Active Appearance Model by Generator Network  2018 0.173\n",
      "2807282758                                             Semantic Understanding and Task-Oriented for Image Assessment  2018 0.173\n",
      "2910908473 An Adaptive Computation Framework of Distributed Deep Learning Models for Internet-of-Things Applications  2018 0.173\n",
      "2267314607                       Characteristics and Potential Developments of Multiple-MLP Ensemble Re-RX Algorithm  2014 0.171\n",
      "2748888824                 Guest Editorial: Large-Scale Multimedia Data Retrieval, Classification, and Understanding  2017 0.169\n",
      "1609262089                                              Strategic approach for Multiple-MLP Ensemble Re-RX algorithm  2015 0.167\n"
     ]
    }
   ],
   "source": [
    "# This is only hybrid search based recommendation \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This code does load model (huggingdace)\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA-3 8B…\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "llama = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16 # I think this is memmory efficient, less VRAM usage!!\n",
    ")\n",
    "generator = pipeline(\"text-generation\", model=llama, tokenizer=tok)\n",
    "\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─── SciBERT embedder ───────────────────────────────────────────────────────\n",
    "print(\"▶ Loading SciBERT embedder…\")\n",
    "sci_model = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
    "sci_model.eval()\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    # returns normalized embedding\n",
    "    vec = sci_model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return vec.tolist()\n",
    "\n",
    "def vector_search(qvec, top_k=25) -> pd.DataFrame:\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('paper_vec', $k, $vec)\n",
    "            YIELD node, score\n",
    "            RETURN node.id AS id, 1.0 - score AS sim\n",
    "            ORDER BY score ASC\n",
    "            \"\"\", k=top_k, vec=qvec\n",
    "        ).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def hydrate_paper_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    ids = df[\"id\"].tolist()\n",
    "    with driver.session() as s:\n",
    "        meta = s.run(\n",
    "            \"MATCH (p:Paper) WHERE p.id IN $ids RETURN p.id AS id, p.title AS title, p.year AS year\",\n",
    "            ids=ids\n",
    "        ).data()\n",
    "    return df.merge(pd.DataFrame(meta), on=\"id\", how=\"left\")[[\"id\",\"title\",\"year\",\"sim\"]]\n",
    "\n",
    "\n",
    "# ─── Neo4j driver + graph search ────────────────────────────────────────────\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(\n",
    "        os.getenv(\"NEO4J_USER\",\"neo4j\"),\n",
    "        os.getenv(\"NEO4J_PASS\",\"Manami1008\")\n",
    "    )\n",
    ")\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE \n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ─── Demo / CLI ─────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "       \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted concepts:\\n\", concepts)\n",
    "\n",
    "    df_graph = graph_search(concepts)\n",
    "    print(f\"\\n⬇ Graph search ({len(df_graph)} hits):\")\n",
    "    print(df_graph.head(25).to_string(index=False))\n",
    "\n",
    "    df_vec = vector_search(embed(paragraph), top_k=25)\n",
    "    df_vec = hydrate_paper_meta(df_vec)\n",
    "    print(f\"\\n⬇ Vector search ({len(df_vec)} hits):\")\n",
    "    print(df_vec.head(25).to_string(index=False, formatters={\"sim\":\"{:.3f}\".format}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
