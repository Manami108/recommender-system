{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my code understanding. I will do step by step. Lets do it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/e-soc-student/DISK2/GR/GR2_Recommendation/faiss_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.28it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# This is the generation pipeline \n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, i make sure the llama works good \n",
    "\n",
    "paragraph = \"Explain why knowledge graphs are useful for artificial intelligence research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = generator(paragraph, max_new_tokens=150, temperature=0.0, do_sample=False)\n",
    "\n",
    "# Max new tokens are set to 150 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ LLaMA Output:\n",
      "\n",
      "Explain why knowledge graphs are useful for artificial intelligence research. Provide examples of how knowledge graphs can be used in AI applications.\n",
      "Knowledge graphs are a type of data structure that represents entities and their relationships in a graph-like structure. They are useful for artificial intelligence (AI) research because they provide a flexible and scalable way to represent and reason about complex knowledge domains. Here are some reasons why knowledge graphs are useful for AI research:\n",
      "\n",
      "1. **Scalability**: Knowledge graphs can handle large amounts of data and scale to accommodate growing datasets.\n",
      "2. **Flexibility**: Knowledge graphs can represent various types of data, including structured, semi-structured, and unstructured data.\n",
      "3. **Reasoning**: Knowledge graphs enable reasoning and inference capabilities, allowing AI systems to draw conclusions and make predictions based on the relationships between\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n▶ LLaMA Output:\\n\")\n",
    "print(response[0][\"generated_text\"])\n",
    "# response 0 tells the first item in the list (list of dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is how to understand user paragraph.\n",
    "\n",
    "To do that: <br>\n",
    "Step 1: Prompt LLaMA to extract concepts (prompt tuning)<br>\n",
    "Step2: Estimate importance score (if possible, but can LLaMA do? )\n",
    "\n",
    "\n",
    "Prompt engineering explanation is here:\n",
    "https://www.promptingguide.ai/techniques/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (397165676.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 33\u001b[0;36m\u001b[0m\n\u001b[0;31m    continue\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "\n",
    "# 1) Define your LLaMA pipeline & extract_concepts() in one cell:\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                    device_map=\"auto\", torch_dtype=\"auto\")\n",
    "llm       = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an academic assistant. Extract only the most important research concepts from the paragraph as a JSON list:\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "Expected format: {{\"concepts\":[\"concept1\",\"concept2\",\"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# this is the Zero-shot prompt engineering. \n",
    "\n",
    "def extract_concepts(paragraph):\n",
    "    formatted = PROMPT.format(paragraph=paragraph.strip())\n",
    "    output = llm(formatted, max_new_tokens=256, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    for match in re.findall(r\"\\{[^{}]+\\}\", output, re.S):\n",
    "        try:\n",
    "            data = json.loads(match)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except Exception:s\n",
    "            continue\n",
    "    raise ValueError(\"Could not parse LLaMA output:\\n\" + output)\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     paragraph = \"\"\"\n",
    "#     Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \n",
    "#     and being applied to various areas. However, the research on knowledge graphs still faces significant \n",
    "#     technical challenges. For example, there are major limitations in the current technologies for acquiring \n",
    "#     knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs \n",
    "#     provide great opportunities in modern society. However, there are technical challenges in their development. \n",
    "#     Consequently, it is necessary to analyze the knowledge graphs with respect to their opportunities and challenges \n",
    "#     to develop a better understanding of the knowledge graphs.\n",
    "#     \"\"\"\n",
    "\n",
    "#     concepts = extract_concepts(paragraph)\n",
    "#     print(\"\\n▶ Extracted Concepts:\")\n",
    "#     for c in concepts:\n",
    "#         print(\"-\", c)\n",
    "#     from pprint import pprint\n",
    "\n",
    "# # after you get `concepts`:\n",
    "#     pprint(concepts, width=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) In a *separate* cell, call and print:\n",
    "from pprint import pprint\n",
    "\n",
    "paragraph = \"\"\"\n",
    "    Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \n",
    "    and being applied to various areas. However, the research on knowledge graphs still faces significant \n",
    "    technical challenges. For example, there are major limitations in the current technologies for acquiring \n",
    "    knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs \n",
    "    provide great opportunities in modern society. However, there are technical challenges in their development. \n",
    "    Consequently, it is necessary to analyze the knowledge graphs with respect to their opportunities and challenges \n",
    "    to develop a better understanding of the knowledge graphs.\n",
    "    \"\"\"\n",
    "concepts = extract_concepts(paragraph)\n",
    "\n",
    "print(\"▶ Extracted Concepts:\")\n",
    "for c in concepts:\n",
    "    print(\" -\", c)\n",
    "\n",
    "print(\"\\n▶ Full list via pprint:\")\n",
    "pprint(concepts, width=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
