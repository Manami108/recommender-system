{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my code understanding. I will do step by step. Lets do it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# This is the generation pipeline \n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, i make sure the llama works good \n",
    "\n",
    "paragraph = \"Explain why knowledge graphs are useful for artificial intelligence research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = generator(paragraph, max_new_tokens=150, temperature=0.0, do_sample=False)\n",
    "\n",
    "# Max new tokens are set to 150 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ LLaMA Output:\n",
      "\n",
      "Explain why knowledge graphs are useful for artificial intelligence research. Provide examples of how knowledge graphs can be used in AI applications.\n",
      "Knowledge graphs are a type of data structure that represents entities and their relationships in a graph-like structure. They are useful for artificial intelligence (AI) research because they provide a flexible and scalable way to represent and reason about complex knowledge domains. Here are some reasons why knowledge graphs are useful for AI research:\n",
      "\n",
      "1. **Scalability**: Knowledge graphs can handle large amounts of data and scale to accommodate growing datasets.\n",
      "2. **Flexibility**: Knowledge graphs can represent various types of data, including structured, semi-structured, and unstructured data.\n",
      "3. **Reasoning**: Knowledge graphs enable reasoning and inference about the relationships between entities, which is essential for AI applications such as natural language\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n▶ LLaMA Output:\\n\")\n",
    "print(response[0][\"generated_text\"])\n",
    "# response 0 tells the first item in the list (list of dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is how to understand user paragraph.\n",
    "\n",
    "To do that: <br>\n",
    "Step 1: Prompt LLaMA to extract concepts (prompt engineering)<br>\n",
    "Step2: Estimate importance score (if possible, but can LLaMA do? )\n",
    "\n",
    "\n",
    "Prompt engineering explanation is here:\n",
    "https://www.promptingguide.ai/techniques/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 230.22it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "\n",
    "# 1) Define your LLaMA pipeline & extract_concepts() in one cell:\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                    device_map=\"auto\", torch_dtype=\"auto\")\n",
    "llm       = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an academic assistant. Extract only the most important research concepts from the paragraph as a JSON list:\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "Expected format: {{\"concepts\":[\"concept1\",\"concept2\",\"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# this is the Zero-shot prompt engineering. \n",
    "\n",
    "def extract_concepts(paragraph):\n",
    "    formatted = PROMPT.format(paragraph=paragraph.strip())\n",
    "    output = llm(formatted, max_new_tokens=256, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    for match in re.findall(r\"\\{[^{}]+\\}\", output, re.S):\n",
    "        try:\n",
    "            data = json.loads(match)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"Could not parse LLaMA output:\\n\" + output)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = \"\"\"\n",
    "    Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \n",
    "    and being applied to various areas. However, the research on knowledge graphs still faces significant \n",
    "    technical challenges. For example, there are major limitations in the current technologies for acquiring \n",
    "    knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs \n",
    "    provide great opportunities in modern society. However, there are technical challenges in their development. \n",
    "    Consequently, it is necessary to analyze the knowledge graphs with respect to their opportunities and challenges \n",
    "    to develop a better understanding of the knowledge graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for c in concepts:\n",
    "        print(\"-\", c)\n",
    "    from pprint import pprint\n",
    "\n",
    "# after you get `concepts`:\n",
    "    pprint(concepts, width=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA-3 8B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# few_shot_concept_extractor.py\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA-3 8B model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}.\n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\", \"entities\", \"relations\", \"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\"\"\"\n",
    "    # Generate output\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # Extract only the last JSON match to avoid example echo\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \"\n",
    "        \"and being applied to various areas. However, the research on knowledge graphs still faces significant \"\n",
    "        \"technical challenges. For example, there are major limitations in the current technologies for acquiring \"\n",
    "        \"knowledge from multiple sources and integrating them into a typical knowledge graph.\"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for concept in concepts:\n",
    "        print(f\"- {concept}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Few-shot extraction:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- knowledge graphs\n",
      "- AI systems\n",
      "- technical challenges\n",
      "- acquiring knowledge\n",
      "- multiple sources\n",
      "- integrating knowledge graph\n",
      "\n",
      "▶ Chain-of-Thought extraction:\n",
      "- knowledge graphs\n",
      "- AI systems\n",
      "- integration\n",
      "- quality\n",
      "- technical challenges\n",
      "- acquiring knowledge\n",
      "- multiple sources\n"
     ]
    }
   ],
   "source": [
    "# Chain-of-Thought concept extraction\n",
    "def extract_concepts_cot(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant. Let's think step by step:\n",
    "1) Summarize in one sentence what the paragraph is about.\n",
    "2) List the most important research concepts mentioned in the paragraph.\n",
    "Respond *only* with a JSON object: {{\"concepts\": [\"concept1\",\"concept2\",…]}}.\n",
    "\n",
    "Paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "\"\"\"\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # Extract only the last JSON\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        try:\n",
    "            data = json.loads(matches[-1])\n",
    "            return data[\"concepts\"]\n",
    "        except:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse output:\\n{result}\")\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \"\n",
    "        \"and being applied to various areas. However, the research on knowledge graphs still faces significant \"\n",
    "        \"technical challenges. For example, there are major limitations in the current technologies for acquiring \"\n",
    "        \"knowledge from multiple sources and integrating them into a typical knowledge graph.\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n▶ Chain-of-Thought extraction:\")\n",
    "    for c in extract_concepts_cot(paragraph): print(f\"- {c}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
