{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my code understanding. I will do step by step. Lets do it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# This is the generation pipeline \n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, i make sure the llama works good \n",
    "\n",
    "paragraph = \"Explain why knowledge graphs are useful for artificial intelligence research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = generator(paragraph, max_new_tokens=150, temperature=0.0, do_sample=False)\n",
    "\n",
    "# Max new tokens are set to 150 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ LLaMA Output:\n",
      "\n",
      "Explain why knowledge graphs are useful for artificial intelligence research. Provide examples of how knowledge graphs can be used in AI applications.\n",
      "Knowledge graphs are a type of data structure that represents entities and their relationships in a graph-like structure. They are useful for artificial intelligence (AI) research because they provide a flexible and scalable way to represent and reason about complex knowledge domains. Here are some reasons why knowledge graphs are useful for AI research:\n",
      "\n",
      "1. **Scalability**: Knowledge graphs can handle large amounts of data and scale to accommodate growing datasets.\n",
      "2. **Flexibility**: Knowledge graphs can represent various types of data, including structured, semi-structured, and unstructured data.\n",
      "3. **Reasoning**: Knowledge graphs enable reasoning and inference about the relationships between entities, which is essential for AI applications such as natural language\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n▶ LLaMA Output:\\n\")\n",
    "print(response[0][\"generated_text\"])\n",
    "# response 0 tells the first item in the list (list of dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is how to understand user paragraph.\n",
    "\n",
    "To do that: <br>\n",
    "Step 1: Prompt LLaMA to extract concepts (prompt engineering)<br>\n",
    "Step2: Estimate importance score (if possible, but can LLaMA do? )\n",
    "\n",
    "\n",
    "Prompt engineering explanation is here:\n",
    "https://www.promptingguide.ai/techniques/cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 230.22it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "\n",
    "# 1) Define your LLaMA pipeline & extract_concepts() in one cell:\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                    device_map=\"auto\", torch_dtype=\"auto\")\n",
    "llm       = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\n",
    "You are an academic assistant. Extract only the most important research concepts from the paragraph as a JSON list:\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "Expected format: {{\"concepts\":[\"concept1\",\"concept2\",\"...\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# this is the Zero-shot prompt engineering. \n",
    "\n",
    "def extract_concepts(paragraph):\n",
    "    formatted = PROMPT.format(paragraph=paragraph.strip())\n",
    "    output = llm(formatted, max_new_tokens=256, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    for match in re.findall(r\"\\{[^{}]+\\}\", output, re.S):\n",
    "        try:\n",
    "            data = json.loads(match)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(\"Could not parse LLaMA output:\\n\" + output)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = \"\"\"\n",
    "    Therefore, knowledge graphs have seized great opportunities by improving the quality of AI systems \n",
    "    and being applied to various areas. However, the research on knowledge graphs still faces significant \n",
    "    technical challenges. For example, there are major limitations in the current technologies for acquiring \n",
    "    knowledge from multiple sources and integrating them into a typical knowledge graph. Thus, knowledge graphs \n",
    "    provide great opportunities in modern society. However, there are technical challenges in their development. \n",
    "    Consequently, it is necessary to analyze the knowledge graphs with respect to their opportunities and challenges \n",
    "    to develop a better understanding of the knowledge graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for c in concepts:\n",
    "        print(\"-\", c)\n",
    "    from pprint import pprint\n",
    "\n",
    "# after you get `concepts`:\n",
    "    pprint(concepts, width=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA-3 8B model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.47it/s]\n",
      "Device set to use cuda:0\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/abhi/Desktop/Manami/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted Concepts:\n",
      "- Large Language Models\n",
      "- transformer-based models\n",
      "- pre-training\n",
      "- natural language processing\n",
      "- computer vision\n",
      "- molecule discovery\n",
      "- parameter size\n",
      "- training corpus\n",
      "- language understanding\n",
      "- language generation\n",
      "- human-like responses\n",
      "- generalization\n",
      "- reasoning capabilities\n",
      "- in-context learning\n",
      "- prompting strategies\n",
      "- chain-of-thought\n",
      "- recommender systems\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# few_shot_concept_extractor.py\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA-3 8B model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "    # Generate output\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # Extract only the last JSON match to avoid example echo\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for concept in concepts:\n",
    "        print(f\"- {concept}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In\n",
    "addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to make the code to communicate with knowledge graph (cypher query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next problem for this is how to understand paragraph intent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.33it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted Concepts:\n",
      "- Large Language Model\n",
      "- transformer-based model\n",
      "- pre-training\n",
      "- textual data\n",
      "- parameter size\n",
      "- training corpus\n",
      "- remarkable capabilities\n",
      "- Natural Language Processing\n",
      "- Computer Vision\n",
      "- Molecule Discovery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 27, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE\\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n     MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n     WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\nLIMIT 50\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇  50 candidate papers\n",
      "\n",
      "Top 10 Candidate Papers:\n",
      "        id                                                                                                                                            title  year\n",
      "2951203659 LPPA: Lightweight Privacy-Preserving Authentication From Efficient Multi-Key Secure Outsourced Computation for Location-Based Services in VANETs  2020\n",
      "2956231146                          Electromagnetic Side Channel Information Leakage Created by Execution of Series of Instructions in a Computer Processor  2020\n",
      "2941123436                                                                   Perceptually Correct Haptic Rendering in Mid-Air Using Ultrasound Phased Array  2020\n",
      "2949848161                                                       A Dynamic Game Approach to Strategic Design of Secure and Resilient Infrastructure Network  2020\n",
      "2955260175                                                                           Predictability of IP Address Allocations for Cloud Computing Platforms  2020\n",
      "2962588986                                                               Effective person re-identification by self-attention model guided feature learning  2020\n",
      "2890795004                                                         A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction  2020\n",
      "2944488368                                                         Authoring New Haptic Textures Based on Interpolation of Real Textures in Affective Space  2020\n",
      "2945186677                                                              Active Defense-Based Resilient Sliding Mode Control Under Denial-of-Service Attacks  2020\n",
      "2949258617                                                          Reverse Engineering of Printed Electronics Circuits: From Imaging to Netlist Extraction  2020\n"
     ]
    }
   ],
   "source": [
    "# This is only graph search based recommendation \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "# Max new tokens are set to 600 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want.\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\", \"neo4j\"),\n",
    "          os.getenv(\"NEO4J_PASS\", \"Manami1008\"))\n",
    ")\n",
    "\n",
    "# MATCH (p:Paper) selects all paper labels, and WHERE tells the paper which matches with concepts etraxted from user's paragraph.\n",
    "# $terms is a parameter which is passed into\n",
    "# So its like \"Is there any term t in the input $terms list such that lowercase paper title contains that term?\"\n",
    "# The keywords are seached in title, abstracts, and field of study or topics. \n",
    "# But for as topics and as field of study (x), it checks one hop to search paper p\n",
    "# Return paper id, title, year (now, its sorted by year cuz there is no ranking method)\n",
    "# Return 50 maximum\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE\n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "     MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "     WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "# The concept in this is accept a list of concept keywords extracted from a paragraph\n",
    "# And run Cypher query agianst Neo4j knowledge graph\n",
    "# Return a paper\n",
    "# It returns a pandas.DataFrame object containing search results from Neo4j.\n",
    "# Filters out single word tems by keeping 2 o more words to reduce noise. Multi-word is always better no?\n",
    "# After filltering it out returns dataFrame\n",
    "# rows = s.run(BASE_CYPHER, terms=terms).data() this executes the BASE_CYPHER query and the pass the terms list into $terms\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically, most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. \"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for concept in concepts:\n",
    "        print(f\"- {concept}\")\n",
    "        \n",
    "    df = graph_search(concepts)\n",
    "    print(f\"\\n⬇  {len(df)} candidate papers\")\n",
    "    print(\"\\nTop 10 Candidate Papers:\")\n",
    "    print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAW LLM OUTPUT ===\n",
      " \n",
      "You are an academic assistant for computer-science papers.\n",
      "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
      "Extract some unique terms rather than common words in computer science paper paragraph. \n",
      "Add a `\"weight\"` (0.5–1.0) that reflects each concept’s importance.\n",
      "Return **only** JSON of the form:\n",
      "{\"concepts\":[{\"term\":\"...\", \"weight\":0.87}, ...]}\n",
      "\n",
      "Example 1:\n",
      "<TEXT>\n",
      "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
      "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"transformer\",                       \"weight\":0.95},\n",
      "  {\"term\":\"BERT\",                              \"weight\":0.90},\n",
      "  {\"term\":\"GPT\",                               \"weight\":0.90},\n",
      "  {\"term\":\"bidirectional attention\",           \"weight\":0.80},\n",
      "  {\"term\":\"large-scale pretraining\",           \"weight\":0.75},\n",
      "  {\"term\":\"question answering\",                \"weight\":0.70},\n",
      "  {\"term\":\"machine translation\",               \"weight\":0.70},\n",
      "  {\"term\":\"text summarization\",                \"weight\":0.65}\n",
      "]}\n",
      "\n",
      "Example 2:\n",
      "<TEXT>\n",
      "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"knowledge graph\",   \"weight\":0.95},\n",
      "  {\"term\":\"entity linking\",    \"weight\":0.80},\n",
      "  {\"term\":\"question answering\",\"weight\":0.70},\n",
      "  {\"term\":\"recommendation systems\",\"weight\":0.65},\n",
      "  {\"term\":\"semantic context\",  \"weight\":0.60}\n",
      "]}\n",
      "\n",
      "Example 3:\n",
      "<TEXT>\n",
      "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information. Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).\n",
      "</TEXT>\n",
      "Expected output:\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"graph neural network\",                \"weight\":0.95},\n",
      "  {\"term\":\"Graph Convolutional Network (GCN)\",   \"weight\":0.85},\n",
      "  {\"term\":\"Graph Attention Network (GAT)\",       \"weight\":0.85},\n",
      "  {\"term\":\"Message Passing Neural Network (MPNN)\",\"weight\":0.80},\n",
      "  {\"term\":\"neighborhood aggregation\",            \"weight\":0.75},\n",
      "  {\"term\":\"node classification\",                 \"weight\":0.65},\n",
      "  {\"term\":\"link prediction\",                     \"weight\":0.60},\n",
      "  {\"term\":\"molecular property prediction\",       \"weight\":0.55}\n",
      "]}\n",
      "\n",
      "---\n",
      "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
      "<TEXT>\n",
      "Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\n",
      "</TEXT>\n",
      "Expected output (JSON ponly, no extra test):\n",
      "{\"concepts\":[\n",
      "  {\"term\":\"Large Language Model (LLM)\",         \"weight\":0.95},\n",
      "  {\"term\":\"transformer-based model\",           \"weight\":0.85},\n",
      "  {\"term\":\"natural language processing (NLP)\",  \"weight\":0.80},\n",
      "  {\"term\":\"computer vision\",                   \"weight\":0.75},\n",
      "  {\"term\":\"molecule discovery\",               \"weight\":0.70},\n",
      "  {\"term\":\"language understanding\",            \"weight\":0.65},\n",
      "  {\"term\":\"language generation\",               \"weight\":0.60},\n",
      "  {\"term\":\"human-like language responses\",     \"weight\":0.55},\n",
      "  {\"term\":\"generalization\",                    \"weight\":0.50},\n",
      "  {\"term\":\"reasoning capabilities\",            \"weight\":0.45},\n",
      "  {\"term\":\"in-context learning\",               \"weight\":0.40},\n",
      "  {\"term\":\"prompting strategies\",              \"weight\":0.35},\n",
      "  {\"term\":\"chain-of-thought\",                  \"weight\":0.30},\n",
      "  {\"term\":\"recommender systems\",               \"weight\":0.25}\n",
      "]}\n",
      "\n",
      "---\n",
      "Please provide the expected output in JSON format. \n",
      "\n",
      "Here is the expected output:\n",
      "{\"concepts\":[{\"term\":\"Large Language Model (LLM)\",\"weight\":0.95},{\"term\":\"transformer-based model\",\"weight\":0.85},{\"term\":\"natural language processing (NLP)\",\"weight\":0.80},{\"term\":\"computer vision\",\"weight\":0.75},{\"term\":\"molecule discovery\",\"weight\":0.70},{\"term\":\"language understanding\",\"weight\":0.65},{\"term\":\"language generation\",\"weight\":0.60},{\"term\":\"human-like language responses\",\"weight\":0.55},{\"term\":\"generalization\",\"weight\":0.50},{\"term\":\"reasoning capabilities\",\"weight\":0.45},{\"term\":\"in-context learning\",\"weight\":0.40},{\"term\":\"prompting strategies\",\" \n",
      "=== END OUTPUT ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON with 'concepts' found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 156\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    152\u001b[0m     paragraph \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m     kw_list \u001b[38;5;241m=\u001b[39m \u001b[43mextract_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted concepts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, kw_list)\n\u001b[1;32m    158\u001b[0m     df \u001b[38;5;241m=\u001b[39m query_with_weights(kw_list)\n",
      "Cell \u001b[0;32mIn[61], line 105\u001b[0m, in \u001b[0;36mextract_concepts\u001b[0;34m(paragraph, debug)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcepts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON with 'concepts' found."
     ]
    }
   ],
   "source": [
    "# This tries weight scoring method \n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import re, json\n",
    "import torch\n",
    "# This code does load model (huggingdace)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is few-shot prompt engineering. \n",
    "PROMPT_TEMPLATE = r\"\"\"\n",
    "You are an academic assistant for computer-science papers.\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "Add a `\"weight\"` (0.5–1.0) that reflects each concept’s importance.\n",
    "Return **only** JSON of the form:\n",
    "{{\"concepts\":[{{\"term\":\"...\", \"weight\":0.87}}, ...]}}\n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"transformer\",                       \"weight\":0.95}},\n",
    "  {{\"term\":\"BERT\",                              \"weight\":0.90}},\n",
    "  {{\"term\":\"GPT\",                               \"weight\":0.90}},\n",
    "  {{\"term\":\"bidirectional attention\",           \"weight\":0.80}},\n",
    "  {{\"term\":\"large-scale pretraining\",           \"weight\":0.75}},\n",
    "  {{\"term\":\"question answering\",                \"weight\":0.70}},\n",
    "  {{\"term\":\"machine translation\",               \"weight\":0.70}},\n",
    "  {{\"term\":\"text summarization\",                \"weight\":0.65}}\n",
    "]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"knowledge graph\",   \"weight\":0.95}},\n",
    "  {{\"term\":\"entity linking\",    \"weight\":0.80}},\n",
    "  {{\"term\":\"question answering\",\"weight\":0.70}},\n",
    "  {{\"term\":\"recommendation systems\",\"weight\":0.65}},\n",
    "  {{\"term\":\"semantic context\",  \"weight\":0.60}}\n",
    "]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information. Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\":[\n",
    "  {{\"term\":\"graph neural network\",                \"weight\":0.95}},\n",
    "  {{\"term\":\"Graph Convolutional Network (GCN)\",   \"weight\":0.85}},\n",
    "  {{\"term\":\"Graph Attention Network (GAT)\",       \"weight\":0.85}},\n",
    "  {{\"term\":\"Message Passing Neural Network (MPNN)\",\"weight\":0.80}},\n",
    "  {{\"term\":\"neighborhood aggregation\",            \"weight\":0.75}},\n",
    "  {{\"term\":\"node classification\",                 \"weight\":0.65}},\n",
    "  {{\"term\":\"link prediction\",                     \"weight\":0.60}},\n",
    "  {{\"term\":\"molecular property prediction\",       \"weight\":0.55}}\n",
    "]}}\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON ponly, no extra test):\n",
    "\"\"\"\n",
    "    \n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_concepts(paragraph: str, debug: bool=False):\n",
    "    prompt   = PROMPT_TEMPLATE.format(paragraph=paragraph.strip())\n",
    "    response = generator(\n",
    "        prompt, max_new_tokens=400,\n",
    "        temperature=0.0, do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # *** Always print this first ***\n",
    "    print(\"=== RAW LLM OUTPUT ===\\n\", response, \"\\n=== END OUTPUT ===\")\n",
    "\n",
    "    # now try to find JSON…\n",
    "    for chunk in re.findall(r\"\\{[^{}]+\\}\", response, re.S)[::-1]:\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "            if \"concepts\" in data:\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"No JSON with 'concepts' found.\")\n",
    "\n",
    "\n",
    "\n",
    "import os, re, json, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\", \"neo4j\"),\n",
    "          os.getenv(\"NEO4J_PASS\", \"Manami1008\"))\n",
    ")\n",
    "\n",
    "# UNWIND allows match each concept separately and then group back by paper to sum up weights\n",
    "# And then check all papers by comparing title, abstract, topic, or field of study.\n",
    "# After it checks t, next it checks different term and repeat\n",
    "# Its better to add all the weights and shows as relevance. For example, if the paper uses the word \"Recemmender system\", \"LLM\", the paper which includes both term would be higher. \n",
    "# Recommend by order of the relevance \n",
    "CYPHER_WEIGHTS = \"\"\"\n",
    "UNWIND $terms AS t\n",
    "MATCH (p:Paper)\n",
    "WHERE toLower(p.title)    CONTAINS t\n",
    "   OR toLower(p.abstract) CONTAINS t\n",
    "   OR EXISTS {\n",
    "         MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "         WHERE toLower(x.name) CONTAINS t }\n",
    "WITH p, t\n",
    "RETURN p.id   AS id,\n",
    "       p.title AS title,\n",
    "       p.year  AS year,\n",
    "       sum($weights[t]) AS relevance\n",
    "ORDER BY relevance DESC, year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "def query_with_weights(kws):\n",
    "    terms   = [k[\"term\"].lower() for k in kws]\n",
    "    weights = {k[\"term\"].lower(): k[\"weight\"] for k in kws}\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(CYPHER_WEIGHTS, terms=terms, weights=weights).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "          \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "    \n",
    "    kw_list = extract_concepts(paragraph, debug=True)\n",
    "    print(\"Weighted concepts:\", kw_list)\n",
    "    df = query_with_weights(kw_list)\n",
    "    print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]\n",
      "Device set to use cpu\n",
      "usage: ipykernel_launcher.py [-h] -p PARAGRAPH\n",
      "ipykernel_launcher.py: error: the following arguments are required: -p/--paragraph\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# kg_cypher_search.py\n",
    "# ------------------------------------------------------------\n",
    "# 1) LLaMA generates a Cypher WHERE clause string\n",
    "# 2) Script plugs it into MATCH → runs query directly\n",
    "# ------------------------------------------------------------\n",
    "import os, re, json, argparse, warnings, pandas as pd, torch\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------- LLaMA loader ----------\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tok   = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "\n",
    "PROMPT_CYPHER = \"\"\"\n",
    "Neo4j schema:\n",
    "  (:Paper {id,title,abstract,year})\n",
    "  (:Topic {name})      (:FieldOfStudy {name})\n",
    "  (:Paper)-[:HAS_TOPIC]->(:Topic)\n",
    "  (:Paper)-[:HAS_FOS]->(:FieldOfStudy)\n",
    "\n",
    "Generate ONLY the Cypher WHERE clause (no MATCH/RETURN) needed to find\n",
    "papers relevant to the paragraph.  Use parameter $terms if helpful.\n",
    "\n",
    "Return JSON:\n",
    "{\"cypher_where\":\"<clause>\"}\n",
    "\n",
    "<TEXT>{paragraph}</TEXT>\n",
    "\"\"\"\n",
    "\n",
    "def gen_where_clause(text:str):\n",
    "    out = llm(PROMPT_CYPHER.format(paragraph=text.strip()),\n",
    "              max_new_tokens=180, temperature=0.0, do_sample=False)[0][\"generated_text\"]\n",
    "    clause = json.loads(re.findall(r\"\\{[^{}]+\\}\", out, re.S)[-1])[\"cypher_where\"].strip()\n",
    "    if not clause.lower().startswith(\"(\") and \"p.\" not in clause:\n",
    "        raise ValueError(\"Bad WHERE clause:\\n\"+clause)\n",
    "    return clause\n",
    "\n",
    "# ---------- Neo4j driver ----------\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",\"bolt://localhost:7687\"),\n",
    "    auth=(os.getenv(\"NEO4J_USER\",\"neo4j\"), os.getenv(\"NEO4J_PASS\",\"Manami1008\"))\n",
    ")\n",
    "\n",
    "def query_with_where(where_clause):\n",
    "    cypher = f\"\"\"\n",
    "    MATCH (p:Paper)\n",
    "    WHERE {where_clause}\n",
    "    RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "    ORDER BY p.year DESC\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(cypher).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Demo cell  –  extract concepts → Neo4j graph_search → show top 10\n",
    "# -----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically, most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. \"\n",
    "    )\n",
    "\n",
    "    # 1) run LLaMA few-shot prompt\n",
    "    concepts = extract_weighted_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted Concepts:\")\n",
    "    for c in concepts:\n",
    "        print(f\"- {c}\")\n",
    "\n",
    "    # 2) Cypher keyword search\n",
    "    df = graph_search(concepts)\n",
    "    print(f\"\\n⬇  {len(df)} candidate papers\")\n",
    "\n",
    "    # 3) display the first 10 rows nicely in Jupyter\n",
    "    from IPython.display import display\n",
    "    display(df.head(10))          # Jupyter will render as HTML table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA-3 8B…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.27it/s]\n",
      "Device set to use cpu\n",
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading SciBERT embedder…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Extracted concepts:\n",
      " ['Large Language Models', 'transformer-based models', 'pre-training', 'natural language processing', 'computer vision', 'molecule discovery', 'parameter size', 'training corpus', 'language understanding', 'language generation', 'human-like responses', 'generalization', 'reasoning capabilities', 'in-context learning', 'prompting strategies', 'chain-of-thought', 'recommender systems']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 26, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE \\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\nLIMIT 50\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇ Graph search (50 hits):\n",
      "        id                                                                                                                   title  year\n",
      "2949848161                              A Dynamic Game Approach to Strategic Design of Secure and Resilient Infrastructure Network  2020\n",
      "2956231146 Electromagnetic Side Channel Information Leakage Created by Execution of Series of Instructions in a Computer Processor  2020\n",
      "2944488368                                Authoring New Haptic Textures Based on Interpolation of Real Textures in Affective Space  2020\n",
      "2949258617                                 Reverse Engineering of Printed Electronics Circuits: From Imaging to Netlist Extraction  2020\n",
      "2955260175                                                  Predictability of IP Address Allocations for Cloud Computing Platforms  2020\n",
      "2962588986                                      Effective person re-identification by self-attention model guided feature learning  2020\n",
      "2890795004                                A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction  2020\n",
      "2943449623       Sensor Glove Based on Novel Inertial Sensor Fusion Control Algorithm for 3-D Real-Time Hand Gestures Measurements  2020\n",
      "2945228715                                                                       Securing Resources in Decentralized Cloud Storage  2020\n",
      "2949240244         Simple methods to overcome the limitations of general word representations in natural language processing tasks  2020\n",
      "2952624925                                 Secrecy Performance of Terrestrial Radio Links Under Collaborative Aerial Eavesdropping  2020\n",
      "2955731188                                                  FAMED-Net: A Fast and Accurate Multi-Scale End-to-End Dehazing Network  2020\n",
      "2954709787                                     Imaging and fusing time series for wearable sensor-based human activity recognition  2020\n",
      "2955328952                                      Low-rank graph preserving discriminative dictionary learning for image recognition  2020\n",
      "2886493354                                               A syntactic path-based hybrid neural network for negation scope detection  2020\n",
      "2889901932                                     Evaluation of Gaze Tracking Calibration for Longitudinal Biomedical Imaging Studies  2020\n",
      "2914477982                                       Sparse Template-Based 6-D Pose Estimation of Metal Parts Using a Monocular Camera  2020\n",
      "2940632674                                                      PriRadar: A Privacy-Preserving Framework for Spatial Crowdsourcing  2020\n",
      "2944490436                                                Stretchable e-Skin Patch for Gesture Recognition on the Back of the Hand  2020\n",
      "2946791315                     Face Sketch Synthesis in the Wild via Deep Patch Representation-Based Probabilistic Graphical Model  2020\n",
      "2946223615                                         Constant Modulus Secure Beamforming for Multicast Massive MIMO Wiretap Channels  2020\n",
      "2947315758                                                             Fast Collective Activity Recognition Under Weak Supervision  2020\n",
      "2952433728                                                           ADVoIP: Adversarial Detection of Encrypted and Concealed VoIP  2020\n",
      "2952222533                                    High-Resolution Encoder–Decoder Networks for Low-Contrast Medical Image Segmentation  2020\n",
      "2955060354                              A PUF-Based Data-Device Hash for Tampered Image Detection and Source Camera Identification  2020\n",
      "\n",
      "⬇ Vector search (25 hits):\n",
      "        id                                                                                                     title  year   sim\n",
      "2618577428                          Online Learning Activity Index (OLAI) and Its Application for Adaptive Learning.  2017 0.177\n",
      "2052896341                                                           Probabilistic Word Selection via Topic Modeling  2015 0.177\n",
      "2293207434                   A State-Based Energy/Performance Model for Parallel Applications on Multicore Computers  2015 0.177\n",
      "2092743098                                The dynamic VideoBook: A hierarchical summarization for surveillance video  2013 0.177\n",
      "2155922903                                     LORAMS: Sharing Learning Experiences with Social and Ubiquitous Media  2010 0.176\n",
      "2793748849                       C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs RecOmmendation.  2018 0.176\n",
      "  48111602                                                                      Cross Disciplinary Biometric Systems  2012 0.176\n",
      "2759451230                    A Regularization Post Layer: An Additional Way How to Make Deep Neural Networks Robust  2017 0.176\n",
      "2963964038                                              Interactive learning for joint event and relation extraction  2019 0.176\n",
      "2125661571                           Characterization of Host-Level Application Traffic with Multi-Scale Gamma Model  2010 0.176\n",
      "1868065710                                         Design of fusion technique-based mining engine for smart business  2015 0.176\n",
      "2587416067                 Interactive authoring of bending and twisting motions of short plants using hand gestures  2017 0.175\n",
      "3000362923                   FML-based Machine Learning Tool for Human Emotional Agent with BCI on Music Application  2019 0.175\n",
      "2801029757                                                                              Several Tunable GMM Kernels.  2018 0.175\n",
      "2402574423                                                     Exploiting label relationship in multi-label learning  2013 0.175\n",
      "1982236466                                        Learning to Generate a Table-of-Contents with Supportive Knowledge  2011 0.175\n",
      "2753728923                                                   TDN: Twice-Least-Square Double-Parallel Neural Networks  2017 0.175\n",
      "1603277975                         Structural and semantic modeling of audio for content-based querying and browsing  2006 0.174\n",
      "2518674350                   Detecting and Tracking The Real-time Hot Topics: A Study on Computational Neuroscience.  2016 0.174\n",
      "2803933257                                                  Replicating Active Appearance Model by Generator Network  2018 0.173\n",
      "2807282758                                             Semantic Understanding and Task-Oriented for Image Assessment  2018 0.173\n",
      "2910908473 An Adaptive Computation Framework of Distributed Deep Learning Models for Internet-of-Things Applications  2018 0.173\n",
      "2267314607                       Characteristics and Potential Developments of Multiple-MLP Ensemble Re-RX Algorithm  2014 0.171\n",
      "2748888824                 Guest Editorial: Large-Scale Multimedia Data Retrieval, Classification, and Understanding  2017 0.169\n",
      "1609262089                                              Strategic approach for Multiple-MLP Ensemble Re-RX algorithm  2015 0.167\n"
     ]
    }
   ],
   "source": [
    "# This is only hybrid search based recommendation \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# This code does load model (huggingdace)\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA-3 8B…\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "llama = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16 # I think this is memmory efficient, less VRAM usage!!\n",
    ")\n",
    "generator = pipeline(\"text-generation\", model=llama, tokenizer=tok)\n",
    "\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─── SciBERT embedder ───────────────────────────────────────────────────────\n",
    "print(\"▶ Loading SciBERT embedder…\")\n",
    "sci_model = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
    "sci_model.eval()\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    # returns normalized embedding\n",
    "    vec = sci_model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return vec.tolist()\n",
    "\n",
    "def vector_search(qvec, top_k=25) -> pd.DataFrame:\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('paper_vec', $k, $vec)\n",
    "            YIELD node, score\n",
    "            RETURN node.id AS id, 1.0 - score AS sim\n",
    "            ORDER BY score ASC\n",
    "            \"\"\", k=top_k, vec=qvec\n",
    "        ).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def hydrate_paper_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    ids = df[\"id\"].tolist()\n",
    "    with driver.session() as s:\n",
    "        meta = s.run(\n",
    "            \"MATCH (p:Paper) WHERE p.id IN $ids RETURN p.id AS id, p.title AS title, p.year AS year\",\n",
    "            ids=ids\n",
    "        ).data()\n",
    "    return df.merge(pd.DataFrame(meta), on=\"id\", how=\"left\")[[\"id\",\"title\",\"year\",\"sim\"]]\n",
    "\n",
    "\n",
    "# ─── Neo4j driver + graph search ────────────────────────────────────────────\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(\n",
    "        os.getenv(\"NEO4J_USER\",\"neo4j\"),\n",
    "        os.getenv(\"NEO4J_PASS\",\"Manami1008\")\n",
    "    )\n",
    ")\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE \n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ─── Demo / CLI ─────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "       \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "\n",
    "    concepts = extract_concepts(paragraph)\n",
    "    print(\"\\n▶ Extracted concepts:\\n\", concepts)\n",
    "\n",
    "    df_graph = graph_search(concepts)\n",
    "    print(f\"\\n⬇ Graph search ({len(df_graph)} hits):\")\n",
    "    print(df_graph.head(25).to_string(index=False))\n",
    "\n",
    "    df_vec = vector_search(embed(paragraph), top_k=25)\n",
    "    df_vec = hydrate_paper_meta(df_vec)\n",
    "    print(f\"\\n⬇ Vector search ({len(df_vec)} hits):\")\n",
    "    print(df_vec.head(25).to_string(index=False, formatters={\"sim\":\"{:.3f}\".format}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Professor told me that it does not make sense to just extract keywords from LLM, so my plan is to construct knowledge graph from paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "—— RAW LLaMA OUTPUT ——\n",
      " \n",
      "You are an expert relation extractor for computer-science research text.\n",
      "\n",
      "**Task**  \n",
      "From the text enclosed by <TEXT></TEXT>  \n",
      " • Extract **up to 20** factual triples in the exact JSON format\n",
      "\n",
      "   ```json\n",
      "   [[\"head\",\"relation\",\"tail\"], …]\n",
      "   ```\n",
      "\n",
      " • Use concise relation labels (e.g. \"uses\", \"extends\", \"improves\").  \n",
      " • Heads/tails should be noun phrases that appear verbatim in the text.  \n",
      " • Output **only** the JSON list (no commentary).\n",
      "\n",
      "––––– Examples –––––\n",
      "\n",
      "Example 1  \n",
      "<TEXT>\n",
      "Knowledge graphs represent entities and relations as structured graphs.  \n",
      "They are widely used for tasks like entity linking, question answering, and recommendation systems.\n",
      "</TEXT>\n",
      "Expected:\n",
      "[[\"Knowledge graphs\",\"represent\",\"entities and relations\"],\n",
      " [\"Knowledge graphs\",\"used_for\",\"entity linking\"],\n",
      " [\"Knowledge \n",
      "——————————————\n",
      "\n",
      "Nodes: [('Large Language Models', {'type': 'entity'}), ('Natural Language Processing', {'type': 'entity'}), ('NLP', {'type': 'entity'}), ('Computer Vision', {'type': 'entity'}), ('Molecule Discovery', {'type': 'entity'})]\n",
      "Edges: []\n"
     ]
    }
   ],
   "source": [
    "# This is graph-construction based on LLM\n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import re, json, networkx as nx, spacy\n",
    "from typing import List, Tuple\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "# English pipeline, run in CPU\n",
    "nlp = spacy.load(\"en_core_web_sm\")          # ≈ 14 MB, fast CPU runtime\n",
    "\n",
    "def extract_entities(paragraph: str) -> List[str]:\n",
    "    \"\"\"Return unique entity surface strings (order preserved).\"\"\"\n",
    "    doc   = nlp(paragraph)\n",
    "    keep  = {\"ORG\", \"PERSON\", \"GPE\", \"NORP\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"}\n",
    "    ents  = [ent.text for ent in doc.ents if ent.label_ in keep]\n",
    "    # simple dedupe while preserving order\n",
    "    seen, unique = set(), []\n",
    "    for e in ents:\n",
    "        if e not in seen:\n",
    "            seen.add(e)\n",
    "            unique.append(e)\n",
    "    return unique\n",
    "\n",
    "\n",
    "# ------ 2. relation extraction with your LLaMA generator -------\n",
    "TRIPLE_PROMPT = \"\"\"\n",
    "You are an expert relation extractor for computer-science research text.\n",
    "\n",
    "**Task**  \n",
    "From the text enclosed by <TEXT></TEXT>  \n",
    " • Extract **up to 20** factual triples in the exact JSON format\\n\n",
    "   ```json\n",
    "   [[\"head\",\"relation\",\"tail\"], …]\n",
    "   ```\\n\n",
    " • Use concise relation labels (e.g. \"uses\", \"extends\", \"improves\").  \n",
    " • Heads/tails should be noun phrases that appear verbatim in the text.  \n",
    " • Output **only** the JSON list (no commentary).\n",
    "\n",
    "––––– Examples –––––\n",
    "\n",
    "Example 1  \n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and relations as structured graphs.  \n",
    "They are widely used for tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Knowledge graphs\",\"represent\",\"entities and relations\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"entity linking\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"question answering\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"recommendation systems\"]]\n",
    "\n",
    "Example 2  \n",
    "<TEXT>\n",
    "Graph Neural Networks (GNNs) extend deep learning to non-Euclidean data by aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Graph Neural Networks\",\"extend\",\"deep learning\"],\n",
    " [\"Graph Neural Networks\",\"aggregate\",\"neighborhood information\"],\n",
    " [\"Graph Convolutional Networks\",\"variant_of\",\"Graph Neural Networks\"],\n",
    " [\"Graph Attention Networks\",\"variant_of\",\"Graph Neural Networks\"]]\n",
    "\n",
    "Example 3  \n",
    "<TEXT>\n",
    "B-tree indexes support balanced, ordered data access with logarithmic search time,  \n",
    "whereas LSM-trees buffer writes in memory and flush them to disk for high throughput.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"B-tree indexes\",\"provide\",\"balanced ordered access\"],\n",
    " [\"B-tree indexes\",\"achieve\",\"logarithmic search time\"],\n",
    " [\"LSM-trees\",\"buffer\",\"writes in memory\"],\n",
    " [\"LSM-trees\",\"flush\",\"writes to disk\"],\n",
    " [\"LSM-trees\",\"provide\",\"high write throughput\"]]\n",
    "\n",
    "Example 4  \n",
    "<TEXT>\n",
    "The Raft algorithm divides consensus into leader election, log replication, and safety,  \n",
    "while Paxos relies on proposer, acceptor, and learner roles.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Raft algorithm\",\"divides_into\",\"leader election\"],\n",
    " [\"Raft algorithm\",\"divides_into\",\"log replication\"],\n",
    " [\"Raft algorithm\",\"divides_into\",\"safety\"],\n",
    " [\"Paxos\",\"relies_on\",\"proposer role\"],\n",
    " [\"Paxos\",\"relies_on\",\"acceptor role\"],\n",
    " [\"Paxos\",\"relies_on\",\"learner role\"]]\n",
    "\n",
    "Example 5  \n",
    "<TEXT>\n",
    "Transformer-based Large Language Models (LLMs) are pre-trained on massive text corpora  \n",
    "and can solve new tasks by in-context learning without gradient updates.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Transformer-based Large Language Models\",\"pre_trained_on\",\"massive text corpora\"],\n",
    " [\"Transformer-based Large Language Models\",\"solve\",\"new tasks\"],\n",
    " [\"Transformer-based Large Language Models\",\"use\",\"in-context learning\"],\n",
    " [\"in-context learning\",\"requires\",\"no gradient updates\"]]\n",
    "\n",
    "––––– Your turn –––––\n",
    "\n",
    "Now extract triples for the following paragraph.  \n",
    "Return **only** the JSON list.\n",
    "\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "\"\"\"\n",
    "\n",
    "import ast, json, re\n",
    "\n",
    "import ast, json, re\n",
    "\n",
    "def extract_triples(paragraph: str, generator):\n",
    "    \"\"\"Run LLaMA, parse the JSON list of triples, always return List[Tuple].\"\"\"\n",
    "    prompt = TRIPLE_PROMPT.format(paragraph=paragraph)\n",
    "    out = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,      # enough room, but not too long\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # ── optional: inspect the raw model reply ──\n",
    "    print(\"\\n—— RAW LLaMA OUTPUT ——\\n\", out[:800], \"\\n——————————————\\n\")\n",
    "\n",
    "    # 1. grab everything from first '[' to last ']'\n",
    "    start = out.find(\"[\")\n",
    "    end   = out.rfind(\"]\")\n",
    "    if start == -1 or end == -1:\n",
    "        return []                       # nothing that looks like a list\n",
    "    block = out[start:end + 1]\n",
    "    block = block.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # 2. parse with JSON first, then fall back to Python-style list\n",
    "    for loader in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            triples = loader(block)\n",
    "            if isinstance(triples, list):\n",
    "                # keep only 3-item lists\n",
    "                return [tuple(t) for t in triples if isinstance(t, (list, tuple)) and len(t) == 3]\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3. if parsing fails, return empty list (not None!)\n",
    "    return []\n",
    "\n",
    "# ------ 3. build the in-memory graph ---------------------------\n",
    "def build_paragraph_kg(paragraph: str, generator) -> nx.MultiDiGraph:\n",
    "    \"\"\"Return a networkx MultiDiGraph representing the paragraph KG.\"\"\"\n",
    "    ents    = extract_entities(paragraph)\n",
    "    triples = extract_triples(paragraph, generator)\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # add entity nodes\n",
    "    for e in ents:\n",
    "        G.add_node(e, type=\"entity\")\n",
    "\n",
    "    # add triples\n",
    "    for h, r, t in triples:\n",
    "        for node in (h, t):\n",
    "            if node not in G:\n",
    "                G.add_node(node, type=\"entity\")\n",
    "        G.add_edge(h, t, label=r)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# ------ 4. optional: push to Neo4j -----------------------------\n",
    "CREATE_NODE = \"\"\"\n",
    "MERGE (c:ParaConcept {name:$name})\n",
    "RETURN id(c) AS id\n",
    "\"\"\"\n",
    "CREATE_EDGE = \"\"\"\n",
    "MATCH (h:ParaConcept {name:$h}),\n",
    "      (t:ParaConcept {name:$t})\n",
    "MERGE (h)-[:PARA_REL {type:$rel}]->(t)\n",
    "\"\"\"\n",
    "\n",
    "def push_to_neo4j(G: nx.MultiDiGraph, driver: GraphDatabase.driver):\n",
    "    with driver.session() as session:\n",
    "        # nodes\n",
    "        for n in G.nodes:\n",
    "            session.run(CREATE_NODE, name=n)\n",
    "        # edges\n",
    "        for h, t, data in G.edges(data=True):\n",
    "            session.run(CREATE_EDGE, h=h, t=t, rel=data.get(\"label\", \"\"))\n",
    "\n",
    "\n",
    "# -------------------------- demo -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from hybrid_search import generator, driver  # re-use your objects\n",
    "\n",
    "    test_para =  \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    kg = build_paragraph_kg(test_para, generator)\n",
    "\n",
    "    print(\"Nodes:\", kg.nodes(data=True))\n",
    "    print(\"Edges:\", list(kg.edges(data=True)))\n",
    "\n",
    "    # Uncomment to persist in Neo4j\n",
    "    # push_to_neo4j(kg, driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "— FINAL PROMPT SENT TO MODEL —\n",
      "\n",
      "\n",
      "You are an expert relation extractor for computer-science research text.  \n",
      "A relation triple has three parts:\n",
      "  1) The **subject**: the entity that takes or undergoes the action (a noun phrase).  \n",
      "  2) The **predicate**: a verb or verb-phrase that describes the action or relationship.  \n",
      "  3) The **object**: the entity that is the factual target of the action (a noun phrase).\n",
      "\n",
      "Extract all factual information in the text as triples of the form  \n",
      "```json\n",
      "[[\"subject\",\"predicate\",\"object\"], …]\n",
      "\n",
      "Follow these rules exactly:\n",
      "• Include up to 20 triples (if there are more facts, pick the most salient ones).\n",
      "• Subjects and objects must be noun phrases exactly as they appear in the text.\n",
      "• Predicates may be short (a single verb) or longer verb phrases—copy them verbatim.\n",
      "• Output only the JSON list—no extra words, no code fences, no commentary.\n",
      "\n",
      "––––– Examples –––––\n",
      "\n",
      "Example 1\n",
      "Text:\n",
      "Transformer-based Large Language Models (LLMs) are pre-trained on massive text corpora  \n",
      "and can solve new tasks by in-context learning without gradient updates.\n",
      "\n",
      "Triples:\n",
      "[\n",
      "  [\"Transformer-based Large Language Models\",\"are pre-trained on\",\"massive text corpora\"],\n",
      "  [\"Transformer-based Large Language Models\",\"can solve new tasks by\",\"in-context learning\"],\n",
      "  [\"in-context learning\",\"does not require\",\"gradient updates\"]\n",
      "]\n",
      "\n",
      "Example 2\n",
      "Text:\n",
      "\n",
      "Graph Neural Networks (GNNs) extend deep learning to non-Euclidean data by aggregating neighborhood information.  \n",
      "Popular variants include Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).\n",
      "\n",
      "Triples:\n",
      "\n",
      "[\n",
      "  [\"Graph Neural Networks\",\"extend deep learning to\",\"non-Euclidean data\"],\n",
      "  [\"Graph Neural Networks\",\"aggregate\",\"neighborhood information\"],\n",
      "  [\"Graph Convolutional Networks\",\"are variants of\",\"Graph Neural Networks\"],\n",
      "  [\"Graph Attention Networks\",\"are variants of\",\"Graph Neural Networks\"]\n",
      "]\n",
      "\n",
      "––––– Your turn –––––\n",
      "\n",
      "Now extract triples for the following paragraph.\n",
      "Return only the JSON array of triples.\n",
      "<TEXT> Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems. </TEXT> \n",
      "\n",
      "\n",
      "\n",
      "——————————————\n",
      "\n",
      "Nodes: [('Large Language Models', {'type': 'entity'}), ('Natural Language Processing', {'type': 'entity'}), ('NLP', {'type': 'entity'}), ('Computer Vision', {'type': 'entity'}), ('Molecule Discovery', {'type': 'entity'})]\n",
      "Edges: []\n"
     ]
    }
   ],
   "source": [
    "# This is graph-construction based on LLM\n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import re, json, networkx as nx, spacy\n",
    "from typing import List, Tuple\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "# English pipeline, run in CPU\n",
    "nlp = spacy.load(\"en_core_web_sm\")          # ≈ 14 MB, fast CPU runtime\n",
    "\n",
    "\n",
    "def extract_entities(paragraph: str) -> List[str]:\n",
    "    doc   = nlp(paragraph)\n",
    "    # ORG: organization, PERSON: person, GPE: GEo-plitical entity, NORP: Nationalities Religious or political groups\n",
    "    # PRODUCT: names of items, EVENT: named event, WORK_OF_ART: titles of artistic works\n",
    "    # Others do not keep this time \n",
    "    keep  = {\"ORG\", \"PERSON\", \"GPE\", \"NORP\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"}\n",
    "    ents  = [ent.text for ent in doc.ents if ent.label_ in keep]\n",
    "    # simple dedupe while preserving order (I dont want to get same words multiple times so it has to be unique always)\n",
    "    seen, unique = set(), []\n",
    "    for e in ents:\n",
    "        if e not in seen:\n",
    "            seen.add(e)\n",
    "            unique.append(e)\n",
    "    return unique\n",
    "\n",
    "\n",
    "# https://www.promptingguide.ai/techniques/cot\n",
    "# https://medium.com/@EleventhHourEnthusiast/zero-and-few-shots-knowledge-graph-triplet-extraction-with-large-language-models-cf571eb7fc98\n",
    "# This is a few-shot prompt engineering to extract triplet (information extraction)\n",
    "# But it does not work well because of \n",
    "TRIPLE_PROMPT = \"\"\"\n",
    "You are an expert relation extractor for computer-science research text.  \n",
    "A relation triple has three parts:\n",
    "  1) The **subject**: the entity that takes or undergoes the action (a noun phrase).  \n",
    "  2) The **predicate**: a verb or verb-phrase that describes the action or relationship.  \n",
    "  3) The **object**: the entity that is the factual target of the action (a noun phrase).\n",
    "\n",
    "Extract all factual information in the text as triples of the form  \n",
    "```json\n",
    "[[\"subject\",\"predicate\",\"object\"], …]\n",
    "\n",
    "Follow these rules exactly:\n",
    "• Include up to 20 triples (if there are more facts, pick the most salient ones).\n",
    "• Subjects and objects must be noun phrases exactly as they appear in the text.\n",
    "• Predicates may be short (a single verb) or longer verb phrases—copy them verbatim.\n",
    "• Output only the JSON list—no extra words, no code fences, no commentary.\n",
    "\n",
    "––––– Examples –––––\n",
    "\n",
    "Example 1\n",
    "Text:\n",
    "Transformer-based Large Language Models (LLMs) are pre-trained on massive text corpora  \n",
    "and can solve new tasks by in-context learning without gradient updates.\n",
    "\n",
    "Triples:\n",
    "[\n",
    "  [\"Transformer-based Large Language Models\",\"are pre-trained on\",\"massive text corpora\"],\n",
    "  [\"Transformer-based Large Language Models\",\"can solve new tasks by\",\"in-context learning\"],\n",
    "  [\"in-context learning\",\"does not require\",\"gradient updates\"]\n",
    "]\n",
    "\n",
    "Example 2\n",
    "Text:\n",
    "\n",
    "Graph Neural Networks (GNNs) extend deep learning to non-Euclidean data by aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).\n",
    "\n",
    "Triples:\n",
    "\n",
    "[\n",
    "  [\"Graph Neural Networks\",\"extend deep learning to\",\"non-Euclidean data\"],\n",
    "  [\"Graph Neural Networks\",\"aggregate\",\"neighborhood information\"],\n",
    "  [\"Graph Convolutional Networks\",\"are variants of\",\"Graph Neural Networks\"],\n",
    "  [\"Graph Attention Networks\",\"are variants of\",\"Graph Neural Networks\"]\n",
    "]\n",
    "\n",
    "––––– Your turn –––––\n",
    "\n",
    "Now extract triples for the following paragraph.\n",
    "Return only the JSON array of triples.\n",
    "<TEXT> {paragraph} </TEXT> \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re, json, ast\n",
    "\n",
    "def _first_json_list_after(text: str, anchor: str = \"</TEXT>\") -> str:\n",
    "    # Get everything after </TEXT>\n",
    "    after = text.split(anchor, 1)[-1]\n",
    "    # Find all bracketed lists\n",
    "    blocks = re.findall(r\"\\[[\\s\\S]*?\\]\", after)\n",
    "    # Return the longest one (most likely the real output)\n",
    "    return max(blocks, key=len) if blocks else None\n",
    "\n",
    "def _sanitize(block: str) -> str:\n",
    "    # Remove backtick fences and whitespace\n",
    "    blk = block.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    # Normalize quotes\n",
    "    blk = blk.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    blk = re.sub(r\"(?<!\\\\)'\", '\"', blk)\n",
    "    # Drop trailing commas before the closing bracket\n",
    "    blk = re.sub(r\",\\s*\\]\", \"]\", blk)\n",
    "    return blk\n",
    "def _balance_brackets(s: str) -> str:\n",
    "    open_count  = s.count('[')\n",
    "    close_count = s.count(']')\n",
    "    if open_count > close_count:\n",
    "        s += ']' * (open_count - close_count)\n",
    "    return s\n",
    "\n",
    "def _parse_triples(block: str):\n",
    "    blk = _sanitize(block)\n",
    "    blk = _balance_brackets(blk)    # ← auto-close any unbalanced lists\n",
    "    for loader in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            data = loader(blk)\n",
    "            return [tuple(x) for x in data if len(x)==3]\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_triples(paragraph: str, generator) -> list[tuple[str,str,str]]:\n",
    "    prompt = TRIPLE_PROMPT.format(paragraph=paragraph)\n",
    "    print(\"\\n— FINAL PROMPT SENT TO MODEL —\\n\")\n",
    "    print(prompt)\n",
    "    print(\"\\n——————————————\\n\")\n",
    "    out    = generator(prompt,\n",
    "                      max_new_tokens=1200,\n",
    "                      temperature=0.0,\n",
    "                      do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    block = _first_json_list_after(out)\n",
    "    if not block:\n",
    "        print(\"No JSON block found after </TEXT>\")\n",
    "        return []\n",
    "\n",
    "    triples = _parse_triples(block)\n",
    "    if triples is None:\n",
    "        print(\"Still could not parse block:\\n\", block[:200])\n",
    "        return []\n",
    "    return triples\n",
    "\n",
    "# ------ 3. build the in-memory graph ---------------------------\n",
    "def build_paragraph_kg(paragraph: str, generator) -> nx.MultiDiGraph:\n",
    "    \"\"\"Return a networkx MultiDiGraph representing the paragraph KG.\"\"\"\n",
    "    ents    = extract_entities(paragraph)\n",
    "    triples = extract_triples(paragraph, generator)\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # add entity nodes\n",
    "    for e in ents:\n",
    "        G.add_node(e, type=\"entity\")\n",
    "\n",
    "    # add triples\n",
    "    for h, r, t in triples:\n",
    "        for node in (h, t):\n",
    "            if node not in G:\n",
    "                G.add_node(node, type=\"entity\")\n",
    "        G.add_edge(h, t, label=r)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# ------ 4. optional: push to Neo4j -----------------------------\n",
    "CREATE_NODE = \"\"\"\n",
    "MERGE (c:ParaConcept {name:$name})\n",
    "RETURN id(c) AS id\n",
    "\"\"\"\n",
    "CREATE_EDGE = \"\"\"\n",
    "MATCH (h:ParaConcept {name:$h}),\n",
    "      (t:ParaConcept {name:$t})\n",
    "MERGE (h)-[:PARA_REL {type:$rel}]->(t)\n",
    "\"\"\"\n",
    "\n",
    "def push_to_neo4j(G: nx.MultiDiGraph, driver: GraphDatabase.driver):\n",
    "    with driver.session() as session:\n",
    "        # nodes\n",
    "        for n in G.nodes:\n",
    "            session.run(CREATE_NODE, name=n)\n",
    "        # edges\n",
    "        for h, t, data in G.edges(data=True):\n",
    "            session.run(CREATE_EDGE, h=h, t=t, rel=data.get(\"label\", \"\"))\n",
    "\n",
    "\n",
    "# -------------------------- demo -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from hybrid_search import generator, driver  # re-use your objects\n",
    "\n",
    "    test_para = \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    kg = build_paragraph_kg(test_para, generator)\n",
    "    print(\"Nodes:\", kg.nodes(data=True))\n",
    "    print(\"Edges:\", list(kg.edges(data=True)))\n",
    "\n",
    "    # Uncomment to persist in Neo4j\n",
    "    # push_to_neo4j(kg, driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is graph-construction based on LLM\n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import re, json, networkx as nx, spacy\n",
    "from typing import List, Tuple\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "# English pipeline, run in CPU\n",
    "nlp = spacy.load(\"en_core_web_sm\")          # ≈ 14 MB, fast CPU runtime\n",
    "\n",
    "\n",
    "def extract_entities(paragraph: str) -> List[str]:\n",
    "    doc   = nlp(paragraph)\n",
    "    # ORG: organization, PERSON: person, GPE: GEo-plitical entity, NORP: Nationalities Religious or political groups\n",
    "    # PRODUCT: names of items, EVENT: named event, WORK_OF_ART: titles of artistic works\n",
    "    # Others do not keep this time \n",
    "    keep  = {\"ORG\", \"PERSON\", \"GPE\", \"NORP\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"}\n",
    "    ents  = [ent.text for ent in doc.ents if ent.label_ in keep]\n",
    "    # simple dedupe while preserving order (I dont want to get same words multiple times so it has to be unique always)\n",
    "    seen, unique = set(), []\n",
    "    for e in ents:\n",
    "        if e not in seen:\n",
    "            seen.add(e)\n",
    "            unique.append(e)\n",
    "    return unique\n",
    "\n",
    "\n",
    "# https://www.promptingguide.ai/techniques/cot\n",
    "# https://medium.com/@EleventhHourEnthusiast/zero-and-few-shots-knowledge-graph-triplet-extraction-with-large-language-models-cf571eb7fc98\n",
    "# This is a few-shot prompt engineering to extract triplet (information extraction)\n",
    "# \n",
    "TRIPLE_PROMPT = \"\"\"\n",
    "You are an expert relation extractor for computer-science research text.\n",
    "\n",
    "**Task**  \n",
    "From the text enclosed by <TEXT></TEXT>  \n",
    " • Extract **up to 20** factual triples in the exact JSON format\\n\n",
    "   \n",
    "\n",
    "json\n",
    "   [[\"head\",\"relation\",\"tail\"], …]\n",
    "\n",
    "\\n\n",
    " • Use concise relation labels (e.g. \"uses\", \"extends\", \"improves\").  \n",
    " • Heads/tails should be noun phrases that appear verbatim in the text.  \n",
    " • Output **only** the JSON list (no commentary).\n",
    "\n",
    "––––– Examples –––––\n",
    "\n",
    "Example 1  \n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and relations as structured graphs.  \n",
    "They are widely used for tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Knowledge graphs\",\"represent\",\"entities and relations\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"entity linking\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"question answering\"],\n",
    " [\"Knowledge graphs\",\"used_for\",\"recommendation systems\"]]\n",
    "\n",
    "Example 2  \n",
    "<TEXT>\n",
    "Graph Neural Networks (GNNs) extend deep learning to non-Euclidean data by aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Graph Neural Networks\",\"extend\",\"deep learning\"],\n",
    " [\"Graph Neural Networks\",\"aggregate\",\"neighborhood information\"],\n",
    " [\"Graph Convolutional Networks\",\"variant_of\",\"Graph Neural Networks\"],\n",
    " [\"Graph Attention Networks\",\"variant_of\",\"Graph Neural Networks\"]]\n",
    "\n",
    "Example 3  \n",
    "<TEXT>\n",
    "B-tree indexes support balanced, ordered data access with logarithmic search time,  \n",
    "whereas LSM-trees buffer writes in memory and flush them to disk for high throughput.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"B-tree indexes\",\"provide\",\"balanced ordered access\"],\n",
    " [\"B-tree indexes\",\"achieve\",\"logarithmic search time\"],\n",
    " [\"LSM-trees\",\"buffer\",\"writes in memory\"],\n",
    " [\"LSM-trees\",\"flush\",\"writes to disk\"],\n",
    " [\"LSM-trees\",\"provide\",\"high write throughput\"]]\n",
    "\n",
    "Example 4  \n",
    "<TEXT>\n",
    "The Raft algorithm divides consensus into leader election, log replication, and safety,  \n",
    "while Paxos relies on proposer, acceptor, and learner roles.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Raft algorithm\",\"divides_into\",\"leader election\"],\n",
    " [\"Raft algorithm\",\"divides_into\",\"log replication\"],\n",
    " [\"Raft algorithm\",\"divides_into\",\"safety\"],\n",
    " [\"Paxos\",\"relies_on\",\"proposer role\"],\n",
    " [\"Paxos\",\"relies_on\",\"acceptor role\"],\n",
    " [\"Paxos\",\"relies_on\",\"learner role\"]]\n",
    "\n",
    "Example 5  \n",
    "<TEXT>\n",
    "Transformer-based Large Language Models (LLMs) are pre-trained on massive text corpora  \n",
    "and can solve new tasks by in-context learning without gradient updates.\n",
    "</TEXT>\n",
    "Expected:\n",
    "[[\"Transformer-based Large Language Models\",\"pre_trained_on\",\"massive text corpora\"],\n",
    " [\"Transformer-based Large Language Models\",\"solve\",\"new tasks\"],\n",
    " [\"Transformer-based Large Language Models\",\"use\",\"in-context learning\"],\n",
    " [\"in-context learning\",\"requires\",\"no gradient updates\"]]\n",
    "\n",
    "––––– Your turn –––––\n",
    "\n",
    "Now extract triples for the following paragraph. Do not limit to the words' examples provided. \n",
    "Think by yourself based on the logic provided. \n",
    "Especially edges, it does not have to be like example, but something verb which is wrriten or can connect noun.\n",
    "Return **only** the JSON list.\n",
    "Important: Output ONLY the JSON list. Do NOT add code fences, back-ticks, or commentary.\n",
    "\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re, json, ast\n",
    "\n",
    "def _first_json_list_after(text: str, anchor: str = \"</TEXT>\") -> str:\n",
    "    # Get everything after </TEXT>\n",
    "    after = text.split(anchor, 1)[-1]\n",
    "    # Find all bracketed lists\n",
    "    blocks = re.findall(r\"\\[[\\s\\S]*?\\]\", after)\n",
    "    # Return the longest one (most likely the real output)\n",
    "    return max(blocks, key=len) if blocks else None\n",
    "\n",
    "def _sanitize(block: str) -> str:\n",
    "    # Remove backtick fences and whitespace\n",
    "    blk = block.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    # Normalize quotes\n",
    "    blk = blk.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    blk = re.sub(r\"(?<!\\\\)'\", '\"', blk)\n",
    "    # Drop trailing commas before the closing bracket\n",
    "    blk = re.sub(r\",\\s*\\]\", \"]\", blk)\n",
    "    return blk\n",
    "def _balance_brackets(s: str) -> str:\n",
    "    open_count  = s.count('[')\n",
    "    close_count = s.count(']')\n",
    "    if open_count > close_count:\n",
    "        s += ']' * (open_count - close_count)\n",
    "    return s\n",
    "\n",
    "def _parse_triples(block: str):\n",
    "    blk = _sanitize(block)\n",
    "    blk = _balance_brackets(blk)    # ← auto-close any unbalanced lists\n",
    "    for loader in (json.loads, ast.literal_eval):\n",
    "        try:\n",
    "            data = loader(blk)\n",
    "            return [tuple(x) for x in data if len(x)==3]\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_triples(paragraph: str, generator) -> list[tuple[str,str,str]]:\n",
    "    prompt = TRIPLE_PROMPT.format(paragraph=paragraph)\n",
    "    out    = generator(prompt,\n",
    "                      max_new_tokens=1200,\n",
    "                      temperature=0.0,\n",
    "                      do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    block = _first_json_list_after(out)\n",
    "    if not block:\n",
    "        print(\"No JSON block found after </TEXT>\")\n",
    "        return []\n",
    "\n",
    "    triples = _parse_triples(block)\n",
    "    if triples is None:\n",
    "        print(\"Still could not parse block:\\n\", block[:200])\n",
    "        return []\n",
    "    return triples\n",
    "\n",
    "# ------ 3. build the in-memory graph ---------------------------\n",
    "def build_paragraph_kg(paragraph: str, generator) -> nx.MultiDiGraph:\n",
    "    \"\"\"Return a networkx MultiDiGraph representing the paragraph KG.\"\"\"\n",
    "    ents    = extract_entities(paragraph)\n",
    "    triples = extract_triples(paragraph, generator)\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # add entity nodes\n",
    "    for e in ents:\n",
    "        G.add_node(e, type=\"entity\")\n",
    "\n",
    "    # add triples\n",
    "    for h, r, t in triples:\n",
    "        for node in (h, t):\n",
    "            if node not in G:\n",
    "                G.add_node(node, type=\"entity\")\n",
    "        G.add_edge(h, t, label=r)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------ 4. optional: push to Neo4j -----------------------------\n",
    "CREATE_NODE = \"\"\"\n",
    "MERGE (c:ParaConcept {name:$name})\n",
    "RETURN id(c) AS id\n",
    "\"\"\"\n",
    "CREATE_EDGE = \"\"\"\n",
    "MATCH (h:ParaConcept {name:$h}),\n",
    "      (t:ParaConcept {name:$t})\n",
    "MERGE (h)-[:PARA_REL {type:$rel}]->(t)\n",
    "\"\"\"\n",
    "\n",
    "def push_to_neo4j(G: nx.MultiDiGraph, driver: GraphDatabase.driver):\n",
    "    with driver.session() as session:\n",
    "        # nodes\n",
    "        for n in G.nodes:\n",
    "            session.run(CREATE_NODE, name=n)\n",
    "        # edges\n",
    "        for h, t, data in G.edges(data=True):\n",
    "            session.run(CREATE_EDGE, h=h, t=t, rel=data.get(\"label\", \"\"))\n",
    "\n",
    "\n",
    "# -------------------------- demo -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from hybrid_search import generator, driver  # re-use your objects\n",
    "\n",
    "    test_para = \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    kg = build_paragraph_kg(test_para, generator)\n",
    "    print(\"Nodes:\", kg.nodes(data=True))\n",
    "    print(\"Edges:\", list(kg.edges(data=True)))\n",
    "\n",
    "    # Uncomment to persist in Neo4j\n",
    "    # push_to_neo4j(kg, driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.66it/s]\n",
      "Device set to use cpu\n",
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading SciBERT embedder…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 26, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE \\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 26, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE \\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 7, column: 26, offset: 178} for query: '\\nMATCH (p:Paper)\\nWHERE \\n  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\\n  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\\n  EXISTS {\\n    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\\n  }\\nRETURN p.id AS id, p.title AS title, p.year AS year\\nORDER BY p.year DESC\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ Top keyword hits:\n",
      "        id                                                                                                                              title  year  score  hits\n",
      "2914397182                                   Evaluating the state-of-the-art of End-to-End Natural Language Generation: The E2E NLG challenge  2020      4     2\n",
      "2599685942                                                          Gaze Interaction With Vibrotactile Feedback: Review and Design Guidelines  2020      3     2\n",
      "2805294773                                                                     A Unified Latent Variable Model for Contrastive Opinion Mining  2020      3     2\n",
      "2886493354                                                          A syntactic path-based hybrid neural network for negation scope detection  2020      3     2\n",
      "2888338418                                                                                   Noiseprint: A CNN-Based Camera Model Fingerprint  2020      3     2\n",
      "2888382142                                                 Secure Relaying in Non-Orthogonal Multiple Access: Trusted and Untrusted Scenarios  2020      3     2\n",
      "2889901932                                                Evaluation of Gaze Tracking Calibration for Longitudinal Biomedical Imaging Studies  2020      3     2\n",
      "2890795004                                           A Deep Evaluator for Image Retargeting Quality by Geometrical and Contextual Interaction  2020      3     2\n",
      "2893639283 Comments on “Revocable and Scalable Certificateless Remote Authentication Protocol With Anonymity for Wireless Body Area Networks”  2020      3     2\n",
      "2912286641           Omnidirectional Touch Probe With Adaptive Maneuvering for 3-D Object Machining and Measurement Verification Applications  2020      3     2\n",
      "2914477982                                                  Sparse Template-Based 6-D Pose Estimation of Metal Parts Using a Monocular Camera  2020      3     2\n",
      "2916106717                                        Design of a New Vision-Based Method for the Bolts Looseness Detection in Flange Connections  2020      3     2\n",
      "2940632674                                                                 PriRadar: A Privacy-Preserving Framework for Spatial Crowdsourcing  2020      3     2\n",
      "2941123436                                                     Perceptually Correct Haptic Rendering in Mid-Air Using Ultrasound Phased Array  2020      3     2\n",
      "2943449623                  Sensor Glove Based on Novel Inertial Sensor Fusion Control Algorithm for 3-D Real-Time Hand Gestures Measurements  2020      3     2\n",
      "2944488368                                           Authoring New Haptic Textures Based on Interpolation of Real Textures in Affective Space  2020      3     2\n",
      "2944490436                                                           Stretchable e-Skin Patch for Gesture Recognition on the Back of the Hand  2020      3     2\n",
      "2944930308   Smart Traffic-Aware Primary User Emulation Attack and Its Impact on Secondary User Throughput Under Rayleigh Flat Fading Channel  2020      3     2\n",
      "2946626580                                     Comments on “Unconditionally Secure, Universally Composable Privacy Preserving Linear Algebra”  2020      3     2\n",
      "2972704398                                                Speaker-Informed time-and-Content-Aware attention for spoken language understanding  2020      3     1\n",
      "\n",
      "⚡ Top vector hits:\n",
      "        id                                                                                                                              title  year   sim\n",
      "2902000181                              Multi-View and Multi-Language Description Generation for Cross-Department Medical Diagnosis Processes  2018 0.178\n",
      "1994943112                                                                    Learning ensemble classifiers via restricted Boltzmann machines  2014 0.178\n",
      "2047755165                              Learning of Fuzzy Cognitive Maps With Varying Densities Using A Multiobjective Evolutionary Algorithm  2016 0.178\n",
      "2149023499                                                                                       BioLMiner and the BioCreative II.5 challenge  2010 0.178\n",
      "2155922903                                                              LORAMS: Sharing Learning Experiences with Social and Ubiquitous Media  2010 0.178\n",
      "2026966885                                                           Object-name search by visual appearance and spatio-temporal descriptions  2009 0.177\n",
      "2941877317                                    Saliency Priority Using Bottom-up Features for Static and Dynamic Scenes Without Cognitive Bias  2019 0.177\n",
      "2137085771                                                                             Group-by and Aggregate Functions in XML Keyword Search  2014 0.177\n",
      "2750144852                                                                               Explainable Recommendation: Theory and Applications.  2017 0.177\n",
      "1572171097                                                                 Building a believable character for real-time virtual environments  2005 0.177\n",
      "2795227858 Guest Editors’ Introduction to the Special Section on Learning with Shared Information for Computer Vision and Multimedia Analysis  2018 0.177\n",
      "2052896341                                                                                    Probabilistic Word Selection via Topic Modeling  2015 0.177\n",
      "2958103579                                            Consideration of a Bayesian Hierarchical Model for Assessment and Adaptive Instructions  2019 0.177\n",
      "2759451230                                             A Regularization Post Layer: An Additional Way How to Make Deep Neural Networks Robust  2017 0.177\n",
      "2750690203                                            A hierarchical and regional deep learning architecture for image description generation  2017 0.177\n",
      "2793748849                                                C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs RecOmmendation.  2018 0.177\n",
      "2150010763                                                                         An experimental study of large-scale mobile social network  2009 0.177\n",
      "2149781590                                               NewsNetExplorer: automatic construction and exploration of news information networks  2014 0.177\n",
      "1868065710                                                                  Design of fusion technique-based mining engine for smart business  2015 0.177\n",
      "2884305338                                      Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval  2018 0.176\n"
     ]
    }
   ],
   "source": [
    "# This is hybrid search recommendation with users' historical data \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching - is solved \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np          \n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "# Max new tokens are set to 600 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want.\n",
    "\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Group concepts by priority  (overlap | paragraph-only | prev-only)\n",
    "def get_concept_sets(paragraph: str, prev_abstract: str):\n",
    "    para_kw = set(extract_concepts(paragraph))\n",
    "    prev_kw = set(extract_concepts(prev_abstract))\n",
    "\n",
    "    overlap   = sorted(para_kw & prev_kw)          # weight 3\n",
    "    para_only = sorted(para_kw - prev_kw)          # weight 2\n",
    "    prev_only = sorted(prev_kw - para_kw)          # weight 1\n",
    "    return overlap, para_only, prev_only\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# SciBERT embedding \n",
    "print(\"▶ Loading SciBERT embedder…\")\n",
    "sci_model = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
    "sci_model.eval()\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    # returns normalized embedding(Numpy array)\n",
    "    # 768 floats are returned for vector index nearest neighbor search into neo4j. \n",
    "    vec = sci_model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return vec.tolist()\n",
    "\n",
    "# Instead of searching Neo4j HNSW index by paragraph alone, we blend the paragraph embedding with the author's abstract \n",
    "# Blend paragraph and previous-abstract embeddings\n",
    "def blended_vec(paragraph: str, prev_abstract: str,\n",
    "                w_para: float = 0.7, w_prev: float = 0.3) -> list[float]:\n",
    "    v_para = np.array(embed(paragraph))\n",
    "    v_prev = np.array(embed(prev_abstract))\n",
    "    combo  = w_para * v_para + w_prev * v_prev\n",
    "    combo /= np.linalg.norm(combo)       # re-normalize\n",
    "    return combo.tolist()\n",
    "\n",
    "\n",
    "def vector_search(qvec, top_k=25) -> pd.DataFrame:\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('paper_vec', $k, $vec)\n",
    "            YIELD node, score\n",
    "            RETURN node.id AS id, 1.0 - score AS sim\n",
    "            ORDER BY score ASC\n",
    "            \"\"\", k=top_k, vec=qvec\n",
    "        ).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def hydrate_paper_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    ids = df[\"id\"].tolist()\n",
    "    with driver.session() as s:\n",
    "        meta = s.run(\n",
    "            \"MATCH (p:Paper) WHERE p.id IN $ids RETURN p.id AS id, p.title AS title, p.year AS year\",\n",
    "            ids=ids\n",
    "        ).data()\n",
    "    return df.merge(pd.DataFrame(meta), on=\"id\", how=\"left\")[[\"id\",\"title\",\"year\",\"sim\"]]\n",
    "\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(\n",
    "        os.getenv(\"NEO4J_USER\",\"neo4j\"),\n",
    "        os.getenv(\"NEO4J_PASS\",\"Manami1008\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# MATCH (p:Paper) selects all paper labels, and WHERE tells the paper which matches with concepts etraxted from user's paragraph.\n",
    "# $terms is a parameter which is passed into\n",
    "# So its like \"Is there any term t in the input $terms list such that lowercase paper title contains that term?\"\n",
    "# The keywords are seached in title, abstracts, and field of study or topics. \n",
    "# But for as topics and as field of study (x), it checks one hop to search paper p\n",
    "# Return paper id, title, year (now, its sorted by year cuz there is no ranking method)\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE \n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "\"\"\"\n",
    "\n",
    "# The concept in this is accept a list of concept keywords extracted from a paragraph\n",
    "# And run Cypher query agianst Neo4j knowledge graph\n",
    "# Return a paper\n",
    "# It returns a pandas.DataFrame object containing search results from Neo4j.\n",
    "# Filters out single word tems by keeping 2 o more words to reduce noise. Multi-word is always better no?\n",
    "# After filltering it out returns dataFrame\n",
    "# rows = s.run(BASE_CYPHER, terms=terms).data() this executes the BASE_CYPHER query and the pass the terms list into $terms\n",
    "\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Weighted merge of three priority groups\n",
    "def weighted_graph_search(overlap, para_only, prev_only,\n",
    "                           top_k_each=100, w=(3,2,1)) -> pd.DataFrame:\n",
    "\n",
    "    def _query(terms):\n",
    "        if not terms:         # empty list → empty DataFrame\n",
    "            return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "        return graph_search(terms).head(top_k_each)\n",
    "\n",
    "    df_o = _query(overlap);    df_o[\"w\"] = w[0]\n",
    "    df_p = _query(para_only);  df_p[\"w\"] = w[1]\n",
    "    df_r = _query(prev_only);  df_r[\"w\"] = w[2]\n",
    "\n",
    "    big = pd.concat([df_o, df_p, df_r], ignore_index=True)\n",
    "    if big.empty:\n",
    "        return big             # nothing matched\n",
    "\n",
    "    big[\"hit\"] = 1\n",
    "    scored = (big.groupby([\"id\",\"title\",\"year\"], as_index=False)\n",
    "                   .agg(score=(\"w\",\"sum\"), hits=(\"hit\",\"count\"))\n",
    "                   .sort_values([\"score\",\"year\"], ascending=[False,False]))\n",
    "    return scored.head(200)\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "       \"Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems.\"\n",
    "    )\n",
    "\n",
    "    prev_abstract = (\n",
    "    \"With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field. \"\n",
    "    )\n",
    "    \n",
    "    # -- extract concept groups\n",
    "    overlap, para_only, prev_only = get_concept_sets(paragraph, prev_abstract)\n",
    "    \n",
    "    # -- keyword-based graph search with weights\n",
    "    df_kw = weighted_graph_search(overlap, para_only, prev_only)\n",
    "    print(\"\\n Top keyword hits:\")\n",
    "    print(df_kw.head(20).to_string(index=False))\n",
    "    \n",
    "    # -- blended vector search\n",
    "    vcombo = blended_vec(paragraph, prev_abstract)\n",
    "    df_vec  = vector_search(vcombo, top_k=50)\n",
    "    df_vec  = hydrate_paper_meta(df_vec)\n",
    "    print(\"\\n Top vector hits:\")\n",
    "    print(df_vec.head(20).to_string(index=False, formatters={\"sim\":\"{:.3f}\".format}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading LLaMA 3-8B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s]\n",
      "Device set to use cpu\n",
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading SciBERT embedder…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "■ Concepts extracted\n",
      "P₁ (common) : ['generalization', 'language generation', 'language understanding', 'large language models', 'natural language processing', 'pre-training', 'recommender systems']\n",
      "P₂ (new)    : ['chain-of-thought', 'computer vision', 'human-like responses', 'in-context learning', 'molecule discovery', 'parameter size', 'prompting strategies', 'reasoning capabilities', 'training corpus', 'transformer-based models']\n",
      "P₃ (old)    : ['artificial intelligence', 'chatgpt', 'deep neural networks', 'feature encoder', 'fine-tuning', 'gpt4', 'item representation', 'llm-empowered recommender systems', 'prompting', 'reasoning', 'representation learning', 'textual side information', 'user representation', 'user-item interactions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The semantics of using colon in the separation of alternative relationship types will change in a future version. (Please use ':HAS_TOPIC|HAS_FOS' instead)} {position: line: 9, column: 30, offset: 215} for query: '\\nUNWIND $terms AS item\\nWITH item.term  AS term,\\n     item.w     AS w\\nMATCH (p:Paper)\\nWHERE toLower(p.title)    CONTAINS term OR\\n      toLower(p.abstract) CONTAINS term OR\\n      EXISTS {\\n        MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\\n        WHERE toLower(x.name) CONTAINS term\\n      }\\nWITH p, max(w) AS wt               // if multiple terms hit, keep the highest weight\\nRETURN p.id   AS id,\\n       p.title AS title,\\n       p.year  AS year,\\n       sum(wt) AS importance       // aggregate across different terms\\nORDER BY importance DESC, year DESC\\nLIMIT 200\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⬇ Weighted graph search (200 hits):\n",
      "        id                                                                                                                                                title  year  importance\n",
      "2998040827                                                                                                                             Voice assistance in 2019  2020         1.0\n",
      "3004133200                                           Comparing Rule-based, Feature-based and Deep Neural Methods for De-identification of Dutch Medical Records  2020         1.0\n",
      "2990024321                                                          What is the minimum training data size to reliably identify writers in medieval manuscripts  2020         1.0\n",
      "2998037504                                                                                            Entropy of Polysemantic Words for the Same Part of Speech  2020         1.0\n",
      "2999927792                                                         A Hybrid Distributed Model for Learning Representation of Short Texts with Attribute Labels.  2020         1.0\n",
      "3004373274                                        Automatic Generation of E-Learning Contents Based on Deep Learning and Natural Language Processing Techniques  2020         1.0\n",
      "2973131549                                                                  Fighting post-truth using natural language processing: A review and open challenges  2020         1.0\n",
      "2990017353                                                                                An end-to-end deep learning system for medieval writer identification  2020         1.0\n",
      "2995237919                                                                                         Sentiment analysis of tweets using refined neutrosophic sets  2020         1.0\n",
      "3000020523                                                                                    Towards the Named Entity Recognition Methods in Biomedical Field.  2020         1.0\n",
      "3002145105                                                                           A Contextual Semantic-Based Approach for Domain-Centric Lexicon Expansion.  2020         1.0\n",
      "2999912932                                                Deep Neural Learning for Automated Diagnostic Code Group Prediction Using Unstructured Nursing Notes.  2020         1.0\n",
      "3003316936                                                                                                                        Intweetive Text Summarization  2020         1.0\n",
      "3004304303                                                                 A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP  2020         1.0\n",
      "2963123788                                                                                 Linear transformations for cross-lingual semantic textual similarity  2020         1.0\n",
      "2969717756                      The effect of automated feedback on revision behavior and learning gains in formative assessment of scientific argument writing  2020         1.0\n",
      "2976395159 Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics  2020         1.0\n",
      "2989596775                                                                            Explaining Digital Humanities by Aligning Images and Textual Descriptions  2020         1.0\n",
      "2996244887                                                                 TODIM approach based on score function under hesitant 2-tuple linguistic environment  2020         1.0\n",
      "2998000750                                                                 Label Distribution Augmented Maximum Likelihood Estimation for Reading Comprehension  2020         1.0\n",
      "2997397117                                                               SCC++: Predicting the programming language of questions and snippets of Stack Overflow  2020         1.0\n",
      "3000136985                                                                                          SentEmoji: A Dataset to Generate Empathising Conversations.  2020         1.0\n",
      "3002812271                                                                   Semantic Round-Tripping in Conceptual Modelling Using Restricted Natural Language.  2020         1.0\n",
      "3002132293                                                                      R2DE: a NLP approach to estimating IRT parameters of newly generated questions.  2020         1.0\n",
      "3001059339                                                        Implementation of the String Matching Method on Anggah-Ungguhing Balinese Language Dictionary  2020         1.0\n",
      "\n",
      "⬇ Vector search (25 hits):\n",
      "        id                                                                                                     title  year   sim\n",
      "2618577428                          Online Learning Activity Index (OLAI) and Its Application for Adaptive Learning.  2017 0.177\n",
      "2052896341                                                           Probabilistic Word Selection via Topic Modeling  2015 0.177\n",
      "2293207434                   A State-Based Energy/Performance Model for Parallel Applications on Multicore Computers  2015 0.177\n",
      "2092743098                                The dynamic VideoBook: A hierarchical summarization for surveillance video  2013 0.177\n",
      "2155922903                                     LORAMS: Sharing Learning Experiences with Social and Ubiquitous Media  2010 0.176\n",
      "2793748849                       C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs RecOmmendation.  2018 0.176\n",
      "  48111602                                                                      Cross Disciplinary Biometric Systems  2012 0.176\n",
      "2759451230                    A Regularization Post Layer: An Additional Way How to Make Deep Neural Networks Robust  2017 0.176\n",
      "2963964038                                              Interactive learning for joint event and relation extraction  2019 0.176\n",
      "2125661571                           Characterization of Host-Level Application Traffic with Multi-Scale Gamma Model  2010 0.176\n",
      "1868065710                                         Design of fusion technique-based mining engine for smart business  2015 0.176\n",
      "2587416067                 Interactive authoring of bending and twisting motions of short plants using hand gestures  2017 0.175\n",
      "3000362923                   FML-based Machine Learning Tool for Human Emotional Agent with BCI on Music Application  2019 0.175\n",
      "2801029757                                                                              Several Tunable GMM Kernels.  2018 0.175\n",
      "2402574423                                                     Exploiting label relationship in multi-label learning  2013 0.175\n",
      "1982236466                                        Learning to Generate a Table-of-Contents with Supportive Knowledge  2011 0.175\n",
      "2753728923                                                   TDN: Twice-Least-Square Double-Parallel Neural Networks  2017 0.175\n",
      "1603277975                         Structural and semantic modeling of audio for content-based querying and browsing  2006 0.174\n",
      "2518674350                   Detecting and Tracking The Real-time Hot Topics: A Study on Computational Neuroscience.  2016 0.174\n",
      "2803933257                                                  Replicating Active Appearance Model by Generator Network  2018 0.173\n",
      "2807282758                                             Semantic Understanding and Task-Oriented for Image Assessment  2018 0.173\n",
      "2910908473 An Adaptive Computation Framework of Distributed Deep Learning Models for Internet-of-Things Applications  2018 0.173\n",
      "2267314607                       Characteristics and Potential Developments of Multiple-MLP Ensemble Re-RX Algorithm  2014 0.171\n",
      "2748888824                 Guest Editorial: Large-Scale Multimedia Data Retrieval, Classification, and Understanding  2017 0.169\n",
      "1609262089                                              Strategic approach for Multiple-MLP Ensemble Re-RX algorithm  2015 0.167\n"
     ]
    }
   ],
   "source": [
    "# This is hybrid search recommendation with users' historical data \n",
    "# But still, need to think about how to make it faster because now its running on cpu\n",
    "# Need to think about importance score for the keywords matching - is solved \n",
    "# Need to think about path length. hops reasoning \n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading LLaMA 3-8B...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # I think this is memmory efficient, less VRAM usage!!\n",
    "    device_map=\"auto\" #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    ")\n",
    "\n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Extract concepts with refined few-shot prompting\n",
    "def extract_concepts(paragraph: str):\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant of computer science field. Extract the most important research concepts (keywords of the from the paragraph wrapped in <TEXT> tags.\n",
    "Respond only with a JSON object of the form {{\"concepts\": [\"concept1\",\"concept2\",…]}}. Extract some unique terms rather than common words in computer science paper paragraph. \n",
    "\n",
    "Example 1:\n",
    "<TEXT>\n",
    "Transformer-based architectures, like BERT and GPT, have revolutionized NLP by enabling bidirectional attention and large-scale pretraining.\n",
    "These models achieve state-of-the-art results in tasks such as question answering, machine translation, and text summarization.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"transformer\", \"BERT\", \"GPT\", \"bidirectional attention\", \"pretraining\", \"question answering\", \"machine translation\", \"text summarization\"]}}\n",
    "\n",
    "Example 2:\n",
    "<TEXT>\n",
    "Knowledge graphs represent entities and their relations as a structured graph. They are widely used in tasks like entity linking, question answering, and recommendation systems.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"knowledge graph\",\"entity linking\", \"question answering\", \"recommendation systems\", \"semantic context\"]}}\n",
    "\n",
    "Example 3:\n",
    "<TEXT>\n",
    "Graph neural networks (GNNs) extend deep learning to non-Euclidean graph data by iteratively aggregating neighborhood information.  \n",
    "Popular variants include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and Message Passing Neural Networks (MPNNs).  \n",
    "They’ve been applied to node classification, link prediction, and molecular property prediction.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"graph neural network\", \"neighborhood aggregation\", \"Graph Convolutional Network (GCN)\", \"Graph Attention Network (GAT)\", \"Message Passing Neural Network (MPNN)\", \"node classification\", \"link prediction\", \"molecular property prediction\"]}}\n",
    "\n",
    "Example 4:\n",
    "<TEXT>\n",
    "To speed up query performance, modern database systems often employ B-tree and LSM-tree indexes.  \n",
    "B-trees support balanced, ordered data access with logarithmic search time, while Log-Structured Merge trees buffer writes in memory and batch them to disk for high write throughput.  \n",
    "Secondary indexes like inverted lists or hash indexes accelerate lookups on non-primary key columns.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"B-tree index\", \"LSM-tree index\", \"logarithmic search time\", \"write buffering\", \"batch disk writes\", \"secondary index\", \"inverted list\", \"hash index\", \"non-primary key lookup\"]}}\n",
    "\n",
    "Example 5:\n",
    "<TEXT>\n",
    "In distributed consensus, Raft and Paxos are two foundational algorithms.  \n",
    "Raft divides the problem into leader election, log replication, and safety, making it more understandable.  \n",
    "Paxos focuses on proposer, acceptor, and learner roles to reach agreement despite failures.  \n",
    "Gossip protocols and vector clock mechanisms are also widely used for state propagation and causality tracking.\n",
    "</TEXT>\n",
    "Expected output:\n",
    "{{\"concepts\": [\"distributed consensus\", \"Raft algorithm\", \"leader election\", \"log replication\", \"Paxos algorithm\", \"proposer role\", \"acceptor role\", \"learner role\", \"gossip protocol\", \"vector clock\"]}}\n",
    "\n",
    "\n",
    "---\n",
    "Now, without repeating the above examples, extract concepts for the following paragraph:\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "Expected output (JSON only, no extra text):\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=600,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "# Max new tokens are set to 600 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want.\n",
    "\n",
    "\n",
    "    # Extract valid JSON from output\n",
    "    # This line try to search all strings like this (json-looking substrings) and (re is search for patterns)\n",
    "    # The json file is made like this {\"concepts\": [\"knowledge graphs\", \"AI systems\", \"data integration\"]}\n",
    "    matches = re.findall(r\"\\{[^{}]+\\}\", result, re.S)\n",
    "    if matches:\n",
    "        last = matches[-1]\n",
    "        try:\n",
    "            data = json.loads(last)\n",
    "            if \"concepts\" in data and isinstance(data[\"concepts\"], list):\n",
    "                return data[\"concepts\"]\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    raise ValueError(f\"Could not parse model output for paragraph. Full output:\\n{result}\")\n",
    "\n",
    "# Call the extract_concepts LLaMA prompt \n",
    "def get_concepts(text: str) -> set[str]:\n",
    "    return set(map(str.lower, extract_concepts(text)))\n",
    "\n",
    "def compare_concept_sets(cur_paragraph: str, prev_abstract: str):\n",
    "    cur_set  = get_concepts(cur_paragraph)\n",
    "    prev_set = get_concepts(prev_abstract)\n",
    "\n",
    "    common      = sorted(cur_set & prev_set)          # P₁\n",
    "    new_only    = sorted(cur_set - prev_set)          # P₂\n",
    "    legacy_only = sorted(prev_set - cur_set)          # P₃\n",
    "\n",
    "    return common, new_only, legacy_only\n",
    "\n",
    "\n",
    "# SciBERT embedding \n",
    "print(\"▶ Loading SciBERT embedder…\")\n",
    "sci_model = SentenceTransformer(\"allenai/scibert_scivocab_uncased\")\n",
    "sci_model.eval()\n",
    "\n",
    "def embed(text: str) -> list[float]:\n",
    "    # returns normalized embedding\n",
    "    vec = sci_model.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return vec.tolist()\n",
    "\n",
    "def vector_search(qvec, top_k=25) -> pd.DataFrame:\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(\n",
    "            \"\"\"\n",
    "            CALL db.index.vector.queryNodes('paper_vec', $k, $vec)\n",
    "            YIELD node, score\n",
    "            RETURN node.id AS id, 1.0 - score AS sim\n",
    "            ORDER BY score ASC\n",
    "            \"\"\", k=top_k, vec=qvec\n",
    "        ).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def hydrate_paper_meta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return df\n",
    "    ids = df[\"id\"].tolist()\n",
    "    with driver.session() as s:\n",
    "        meta = s.run(\n",
    "            \"MATCH (p:Paper) WHERE p.id IN $ids RETURN p.id AS id, p.title AS title, p.year AS year\",\n",
    "            ids=ids\n",
    "        ).data()\n",
    "    return df.merge(pd.DataFrame(meta), on=\"id\", how=\"left\")[[\"id\",\"title\",\"year\",\"sim\"]]\n",
    "\n",
    "\n",
    "# This is neo4j driver (calling neo4j)\n",
    "driver = GraphDatabase.driver(\n",
    "    os.getenv(\"NEO4J_URI\",  \"bolt://localhost:7687\"),\n",
    "    auth=(\n",
    "        os.getenv(\"NEO4J_USER\",\"neo4j\"),\n",
    "        os.getenv(\"NEO4J_PASS\",\"Manami1008\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# MATCH (p:Paper) selects all paper labels, and WHERE tells the paper which matches with concepts etraxted from user's paragraph.\n",
    "# $terms is a parameter which is passed into\n",
    "# So its like \"Is there any term t in the input $terms list such that lowercase paper title contains that term?\"\n",
    "# The keywords are seached in title, abstracts, and field of study or topics. \n",
    "# But for as topics and as field of study (x), it checks one hop to search paper p\n",
    "# Return paper id, title, year (now, its sorted by year cuz there is no ranking method)\n",
    "\n",
    "BASE_CYPHER = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE \n",
    "  ANY(t IN $terms WHERE toLower(p.title)    CONTAINS t) OR\n",
    "  ANY(t IN $terms WHERE toLower(p.abstract) CONTAINS t) OR\n",
    "  EXISTS {\n",
    "    MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "    WHERE ANY(t IN $terms WHERE toLower(x.name) CONTAINS t)\n",
    "  }\n",
    "RETURN p.id AS id, p.title AS title, p.year AS year\n",
    "ORDER BY p.year DESC\n",
    "\"\"\"\n",
    "\n",
    "WEIGHTED_CYPHER = \"\"\"\n",
    "UNWIND $terms AS item\n",
    "WITH item.term  AS term,\n",
    "     item.w     AS w\n",
    "MATCH (p:Paper)\n",
    "WHERE toLower(p.title)    CONTAINS term OR\n",
    "      toLower(p.abstract) CONTAINS term OR\n",
    "      EXISTS {\n",
    "        MATCH (p)-[:HAS_TOPIC|:HAS_FOS]->(x)\n",
    "        WHERE toLower(x.name) CONTAINS term\n",
    "      }\n",
    "WITH p, max(w) AS wt               // if multiple terms hit, keep the highest weight\n",
    "RETURN p.id   AS id,\n",
    "       p.title AS title,\n",
    "       p.year  AS year,\n",
    "       sum(wt) AS importance       // aggregate across different terms\n",
    "ORDER BY importance DESC, year DESC\n",
    "LIMIT 200\n",
    "\"\"\"\n",
    "\n",
    "def weighted_graph_search(common, new_only, legacy_only, w_common=1.0, w_new=0.7, w_old=0.4):\n",
    "    # Build [{term: \"...\", w: 1.0}, …] but keep only multi-word to reduce noise\n",
    "    pack = (\n",
    "        [{\"term\": t, \"w\": w_common} for t in common     if len(t.split()) >= 2] +\n",
    "        [{\"term\": t, \"w\": w_new   } for t in new_only   if len(t.split()) >= 2] +\n",
    "        [{\"term\": t, \"w\": w_old   } for t in legacy_only if len(t.split()) >= 2]\n",
    "    )\n",
    "    if not pack:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\",\"importance\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(WEIGHTED_CYPHER, terms=pack).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# The concept in this is accept a list of concept keywords extracted from a paragraph\n",
    "# And run Cypher query agianst Neo4j knowledge graph\n",
    "# Return a paper\n",
    "# It returns a pandas.DataFrame object containing search results from Neo4j.\n",
    "# Filters out single word tems by keeping 2 o more words to reduce noise. Multi-word is always better no?\n",
    "# After filltering it out returns dataFrame\n",
    "# rows = s.run(BASE_CYPHER, terms=terms).data() this executes the BASE_CYPHER query and the pass the terms list into $terms\n",
    "\n",
    "\n",
    "def graph_search(concepts: list[str]) -> pd.DataFrame:\n",
    "    terms = [c.lower() for c in concepts if len(c.split()) >= 2]\n",
    "    if not terms:\n",
    "        return pd.DataFrame(columns=[\"id\",\"title\",\"year\"])\n",
    "    with driver.session() as s:\n",
    "        rows = s.run(BASE_CYPHER, terms=terms).data()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = \"\"\" Recently, as advanced natural language processing techniques, Large Language Models (LLMs) with billion parameters have generated large impacts on various research fields such as Natural Language Processing (NLP), Computer Vision, and Molecule Discovery. Technically most existing LLMs are transformer-based models pre-trained on a vast amount of textual data from diverse sources, such as articles, books, websites, and other publicly available written materials. As the parameter size of LLMs continues to scale up with a larger training corpus, recent studies indicated that LLMs can lead to the emergence of remarkable capabilities. More specifically, LLMs have demonstrated the unprecedentedly powerful abilities of their fundamental responsibilities in language understanding and generation. These improvements enable LLMs to better comprehend human intentions and generate language responses that are more human-like in nature. Moreover, recent studies indicated that LLMs exhibit impressive generalization and reasoning capabilities, making LLMs better generalize to a variety of unseen tasks and domains. To be specific, instead of requiring extensive fine-tuning on each specific task, LLMs can apply their learned knowledge and reasoning skills to fit new tasks simply by providing appropriate instructions or a few task demonstrations. Advanced techniques such as in-context learning can further enhance such generalization performance of LLMs without being fine-tuned on specific downstream tasks. In addition, empowered by prompting strategies such as chain-of-thought, LLMs can generate the outputs with step-by-step reasoning in complicated decision-making processes.Hence, given their powerful abilities, LLMs demonstrate great potential to revolutionize recommender systems. \"\"\"\n",
    "    prev_abs  = \"\"\" With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field. \"\"\"\n",
    "\n",
    "    # 1 Extract & compare\n",
    "    p1, p2, p3 = compare_concept_sets(paragraph, prev_abs)\n",
    "\n",
    "    print(\"\\n■ Concepts extracted\")\n",
    "    print(\"P₁ (common) :\", p1)\n",
    "    print(\"P₂ (new)    :\", p2)\n",
    "    print(\"P₃ (old)    :\", p3)\n",
    "\n",
    "    # 2 Run weighted graph search\n",
    "    df_w = weighted_graph_search(p1, p2, p3)\n",
    "    print(f\"\\n⬇ Weighted graph search ({len(df_w)} hits):\")\n",
    "    print(df_w.head(25).to_string(index=False))\n",
    "\n",
    "    # 3 Vector search on current paragraph (unchanged)\n",
    "    df_vec = vector_search(embed(paragraph), top_k=25)\n",
    "    df_vec = hydrate_paper_meta(df_vec)\n",
    "    print(f\"\\n⬇ Vector search ({len(df_vec)} hits):\")\n",
    "    print(df_vec.head(25).to_string(index=False, formatters={\"sim\":\"{:.3f}\".format}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading Llama-3 8B…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.96it/s]\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted context:\n",
      "main_topic        : ['Large Language Models']\n",
      "subtopics         : ['transformer-based models', 'pre-training', 'language understanding', 'language generation', 'generalization', 'reasoning']\n",
      "problem_statement : Existing LLMs struggle to capture human intentions and generalize to new tasks.\n",
      "technologies      : ['transformer-based models']\n",
      "research_domain   : natural language processing\n",
      "user_intent       : survey recent LLM advancements\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch, re, json\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(\"▶ Loading Llama-3 8B…\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                torch_dtype=torch.float16,  # I think this is memmory efficient, less VRAM usage!!\n",
    "                device_map=\"auto\"  #This line is to make sure model is loaded on GPU (however, in Manami's computer, it becomes CPU cuz simply, the my gpu cannot handle it. )\n",
    "            )\n",
    "\n",
    "# This is the generation pipeline \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Extract concepts with chain of thought few-shot prompting. \n",
    "# https://www.promptingguide.ai/techniques/cot\n",
    "# https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting#what-is-chain-of-thought-prompting\n",
    "def extract_context(paragraph: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "You are an academic assistant an elite writing assistant for computer-science research.\n",
    "You need to understand the context of paragraph given very well like human. \n",
    "\n",
    "\n",
    "TASK\n",
    "Step 1 – THINK\n",
    "  • Read the paragraph in <TEXT>.\n",
    "  • Deliberate step-by-step inside the tags <COT> … </COT>.\n",
    "\n",
    "Step 2 – EXTRACT six fields\n",
    "  1) main_topic        – up to 2 short keywords (list)\n",
    "  2) subtopics         – up to 5 phrases (list)\n",
    "  3) problem_statement – ONE sentence (≤ 30 tokens)\n",
    "  4) technologies      – list concrete models / algorithms / datasets\n",
    "  5) research_domain   – broad area (e.g. “machine learning”)\n",
    "  6) user_intent       – 5-15 tokens (“survey X”, “find gaps in Y”, …)\n",
    "\n",
    "Step 3 – OUTPUT\n",
    "  • Write valid compact JSON only.\n",
    "  • Keys must appear in the order shown above.\n",
    "\n",
    "RULES\n",
    "  • Read carefully with understanding of paragraph‐level cohesion and local coherence.\n",
    "  • Do NOT invent papers, references, or details not implied by the text.\n",
    "  • Outside <COT> and the final JSON, write nothing else.\n",
    "\n",
    "EXAMPLES\n",
    "\n",
    "Example 1\n",
    "<COT>\n",
    "• Identify domain terms: “BERT”, “GPT”, “bidirectional attention”.\n",
    "• Core area ≈ “Transformer-based NLP” → main_topic.\n",
    "• Sub-areas: pretraining, question answering, machine translation.\n",
    "• Problem: traditional seq-models lack deep context.\n",
    "• Tech list = {{BERT, GPT}}.\n",
    "• Domain = NLP.\n",
    "• Intent ≈ “survey modern transformer NLP work”.\n",
    "</COT>\n",
    "{{\"main_topic\":\"transformer-based NLP\",\n",
    "  \"subtopics\":[\"pretraining\",\"question answering\",\"machine translation\"],\n",
    "  \"problem_statement\":\"Sequence models before transformers struggled to capture long-range context in language tasks.\",\n",
    "  \"technologies\":[\"BERT\",\"GPT\"],\n",
    "  \"research_domain\":\"natural language processing\",\n",
    "  \"user_intent\":\"survey modern transformer literature\"}}\n",
    "\n",
    "Example 2\n",
    "<COT>\n",
    "• Key concepts: “knowledge graph”, “entity linking”, “question answering”.\n",
    "• main_topic = knowledge graphs.\n",
    "• Subtopics: entity linking, QA, recommendation.\n",
    "• Tech list none explicit → empty list.\n",
    "• Problem stmt: structuring entities for downstream tasks.\n",
    "• Domain: AI / NLP.\n",
    "• Intent: discover KG applications.\n",
    "</COT>\n",
    "{{\"main_topic\":\"knowledge graphs\",\n",
    "  \"subtopics\":[\"entity linking\",\"question answering\",\"recommendation systems\"],\n",
    "  \"problem_statement\":\"Researchers need structured representations of entities and relations to enhance downstream tasks.\",\n",
    "  \"technologies\":[],\n",
    "  \"research_domain\":\"AI / information extraction\",\n",
    "  \"user_intent\":\"discover KG application papers\"}}\n",
    "\n",
    "Example 3\n",
    "<COT>\n",
    "• Mentions: “graph neural networks”, “GCN”, “GAT”, “MPNN”.\n",
    "• main_topic = graph neural networks.\n",
    "• Subtopics: node classification, link prediction, molecular property.\n",
    "• Tech list = {{GCN, GAT, MPNN}}.\n",
    "• Problem stmt derived.\n",
    "• Domain: machine learning.\n",
    "• Intent: compare GNN variants.\n",
    "</COT>\n",
    "{{\"main_topic\":\"graph neural networks\",\n",
    "  \"subtopics\":[\"node classification\",\"link prediction\",\"molecular property prediction\"],\n",
    "  \"problem_statement\":\"Existing deep-learning methods need adaptation to non-Euclidean graph data structures.\",\n",
    "  \"technologies\":[\"Graph Convolutional Network\",\"Graph Attention Network\",\"Message Passing Neural Network\"],\n",
    "  \"research_domain\":\"machine learning\",\n",
    "  \"user_intent\":\"compare GNN variants\"}}\n",
    "\n",
    " Example 4\n",
    "<COT>\n",
    "• Read full paragraph: it describes why context‐aware recommendation is important for academic writing.\n",
    "• Sentence 1 mentions “context‐aware recommendations” → main_topic.\n",
    "• Sentence 2 contrasts “traditional systems” based on citation networks vs. a need for paragraph‐level signals → problem.\n",
    "• Sentence 3 says “we propose combining SciBERT embeddings of input text with a Neo4j knowledge graph” → technology list includes SciBERT, Neo4j.\n",
    "• Sentence 4 adds “graph neural networks” and “retrieval‐augmented reranking” → subtopics include graph neural networks, retrieval‐augmented reranking.\n",
    "• Domain is NLP / recommender systems.\n",
    "• Intent: find papers on context‐aware KG retrieval.\n",
    "</COT>\n",
    "{{\"main_topic\":[\"context-aware recommendations\"],\n",
    "  \"subtopics\":[\"graph neural networks\",\"retrieval-augmented reranking\"],\n",
    "  \"problem_statement\":\"Traditional recommendation systems based on citation networks fail to capture paragraph-level shifts during academic writing.\",\n",
    "  \"technologies\":[\"SciBERT\",\"Neo4j\"],\n",
    "  \"research_domain\":\"natural language processing / recommender systems\",\n",
    "  \"user_intent\":\"find papers on paragraph-level context-aware recommendation\"}}\n",
    "\n",
    "Example 5\n",
    "<COT>\n",
    "• Entire paragraph discusses constructing a knowledge graph from a single paragraph.\n",
    "• It says “extract entities via spaCy NER”, “resolve coreference”, “extract relational triples via LLaMA prompting” → these are technologies.\n",
    "• main_topic = paragraph knowledge-graph construction.\n",
    "• Subtopics: entity extraction, coreference resolution, triple extraction.\n",
    "• Problem: Building a mini‐KG on CPU is slow and path‐length reasoning is a challenge.\n",
    "• Domain: knowledge graphs / NLP.\n",
    "• Intent: discover KG optimization papers.\n",
    "</COT>\n",
    "{{\"main_topic\":[\"paragraph knowledge-graph construction\"],\n",
    "  \"subtopics\":[\"entity extraction\",\"coreference resolution\",\"relational triple extraction\"],\n",
    "  \"problem_statement\":\"Constructing a mini‐knowledge graph from a paragraph on CPU is slow and inefficient for multi‐hop reasoning.\",\n",
    "  \"technologies\":[\"spaCy NER\",\"neuralcoref\",\"LLaMA prompting\"],\n",
    "  \"research_domain\":\"knowledge graphs / natural language processing\",\n",
    "  \"user_intent\":\"find papers on efficient paragraph-level KG construction\"}}\n",
    "\n",
    "Example 6\n",
    "<TEXT>\n",
    "Recent advances in healthcare AI have underscored the need for privacy-preserving machine-learning pipelines that comply with strict data-sharing regulations. Federated learning lets hospitals collaboratively train diagnostic models by exchanging only gradient updates, yet vanilla schemes remain vulnerable to membership-inference attacks. To mitigate this, researchers combine differential-privacy noise and secure-aggregation protocols while exploring homomorphic encryption for gradient masking. Unfortunately, these defenses often degrade model accuracy or impose heavy computational costs, hindering deployment across resource-heterogeneous clinics. This work introduces an adaptive federated-optimization algorithm that dynamically tunes privacy budgets and compression rates according to each client’s hardware profile, maintaining GDPR-level guarantees without sacrificing performance on chest-X-ray classification. Experiments across three global healthcare networks show a 12 % accuracy gain over fixed-budget baselines while meeting privacy constraints.\n",
    "</TEXT>\n",
    "\n",
    "<COT>\n",
    "• Central theme: “privacy-preserving federated learning in healthcare” → main_topic.  \n",
    "• Subtopics: differential privacy, secure aggregation, homomorphic encryption, adaptive optimization, healthcare deployment.  \n",
    "• Problem: Existing privacy defenses hurt accuracy or add heavy compute overhead.  \n",
    "• Technologies explicitly named: federated learning, differential privacy, secure aggregation, homomorphic encryption, adaptive federated optimization.  \n",
    "• Research domain: machine learning / healthcare AI.  \n",
    "• User intent: find papers on practical, privacy-preserving FL solutions.\n",
    "</COT>\n",
    "{{\"main_topic\":[\"federated learning\"],\n",
    "  \"subtopics\":[\"differential privacy\",\"secure aggregation\",\"homomorphic encryption\",\"adaptive optimization\",\"healthcare deployment\"],\n",
    "  \"problem_statement\":\"Current privacy defenses in federated learning reduce accuracy or cause heavy computation in hospital settings.\",\n",
    "  \"technologies\":[\"federated learning\",\"differential privacy\",\"secure aggregation\",\"homomorphic encryption\",\"adaptive federated optimization\"],\n",
    "  \"research_domain\":\"machine learning / healthcare AI\",\n",
    "  \"user_intent\":\"find papers on privacy-preserving federated learning\"}}\n",
    "\n",
    "\n",
    "PARAGRAPH\n",
    "<TEXT>\n",
    "{paragraph}\n",
    "</TEXT>\n",
    "\n",
    "# After thinking in <COT>, output your JSON on a new line.\n",
    "\"\"\"\n",
    "\n",
    "# Max new tokens are set to 600 to limit the LLM's answer (but the token is in addition to the input)\n",
    "# temperature tells what kind of output. For example, 0.0 tells always same output for the same input but 1.0 has balanced randomness. so 0.0 tells conssistent and accurate answers\n",
    "# Sample also tells the randomness. so if i set sample = True, then temerature tells how much randomness i want.\n",
    "    raw = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # 2) Locate the *last* `{` and the *last* `}` in raw\n",
    "    last_open = raw.rfind(\"{\")\n",
    "    last_close = raw.rfind(\"}\")\n",
    "    if last_open == -1 or last_close == -1 or last_close < last_open:\n",
    "        raise ValueError(\"Extractor failed – could not find a well-formed JSON block.\\n\\nRaw output:\\n\" + raw)\n",
    "\n",
    "    last_json_str = raw[last_open : last_close + 1]\n",
    "\n",
    "    # 3) Parse it\n",
    "    try:\n",
    "        data = json.loads(last_json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON returned:\\n{last_json_str}\\n\\nRaw output:\\n{raw}\") from e\n",
    "\n",
    "    # 4) Sanity check required keys\n",
    "    required = {\n",
    "        \"main_topic\",\n",
    "        \"subtopics\",\n",
    "        \"problem_statement\",\n",
    "        \"technologies\",\n",
    "        \"research_domain\",\n",
    "        \"user_intent\"\n",
    "    }\n",
    "    missing = required - set(data.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing keys in returned JSON: {missing}\\n\\nReturned JSON:\\n{last_json_str}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Demo\n",
    "if __name__ == \"__main__\":\n",
    "    paragraph = (\n",
    "        \"Recently, as advanced natural language processing techniques, \"\n",
    "        \"Large Language Models (LLMs) with billion parameters have generated \"\n",
    "        \"large impacts on various research fields such as NLP, Computer Vision, \"\n",
    "        \"and Molecule Discovery. Technically most existing LLMs are transformer-\"\n",
    "        \"based models pre-trained on a vast amount of textual data from diverse sources. \"\n",
    "        \"As the parameter size of LLMs continues to scale, recent studies indicated that LLMs \"\n",
    "        \"can lead to the emergence of remarkable capabilities. More specifically, LLMs demonstrate \"\n",
    "        \"powerful abilities in language understanding and generation, enabling them to better \"\n",
    "        \"comprehend human intentions. Moreover, LLMs exhibit impressive generalization and \"\n",
    "        \"reasoning, often applying learned knowledge to new tasks with few-shot demonstrations.\"\n",
    "    )\n",
    "\n",
    "    ctx = extract_context(paragraph)\n",
    "    print(\"Extracted context:\")\n",
    "    for k, v in ctx.items():\n",
    "        print(f\"{k:18}: {v}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
